[{"id":0,"href":"/docs/%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/azure-ai%E6%9C%8D%E5%8A%A1/","title":"Azure AI服务","section":"向量数据库","content":" Azure AI服务 # 本节将指导您设置AzureVectorStore，以存储文档嵌入，并使用Azure AI搜索服务执行相似性搜索。 Azure AI Search是一个通用的云托管云信息检索系统，是微软更大的人工智能平台的一部分。除其他功能外，它允许用户使用基于向量的存储和检索来查询信息。\n前提条件 # 配置 # 启动时，AzureVectorStore可以尝试在AI Search服务实例中创建新索引，如果您已通过在构造函数中将相关的初始化模式布尔属性设置为true来选择，或者如果使用Spring Boot，则设置…​在application.properties文件中初始化schema=true。 或者，您可以手动创建索引。 要设置AzureVectorStore，您需要从上面的前提条件中检索设置以及索引名称：\nAzure AI搜索终结点 Azure AI搜索密钥 （可选）Azure OpenAI API终结点 （可选）Azure OpenAI API密钥 您可以将这些值作为操作系统环境变量提供。 export AZURE_AI_SEARCH_API_KEY=\u0026lt;My AI Search API Key\u0026gt; export AZURE_AI_SEARCH_ENDPOINT=\u0026lt;My AI Search Index\u0026gt; export OPENAI_API_KEY=\u0026lt;My Azure AI API Key\u0026gt; (Optional) 依赖关系 # 将这些依赖项添加到项目中：\n1.选择嵌入接口实现。您可以选择： # 2.Azure（AI Search）向量存储 # \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-azure-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 配置属性 # 您可以在SpringBoot配置中使用以下属性来定制Azure向量存储。\n示例代码 # 要在应用程序中配置Azure SearchIndexClient，可以使用以下代码：\n@Bean public SearchIndexClient searchIndexClient() { return new SearchIndexClientBuilder().endpoint(System.getenv(\u0026#34;AZURE_AI_SEARCH_ENDPOINT\u0026#34;)) .credential(new AzureKeyCredential(System.getenv(\u0026#34;AZURE_AI_SEARCH_API_KEY\u0026#34;))) .buildClient(); } 要创建向量存储，可以通过注入在上面的示例中创建的SearchIndexClientbean以及Spring AI库提供的EmbeddingModel来使用以下代码，该库实现所需的Embendings接口。\n@Bean public VectorStore vectorStore(SearchIndexClient searchIndexClient, EmbeddingModel embeddingModel) { return AzureVectorStore.builder(searchIndexClient, embeddingModel) .initializeSchema(true) // Define the metadata fields to be used // in the similarity search filters. .filterMetadataFields(List.of(MetadataField.text(\u0026#34;country\u0026#34;), MetadataField.int64(\u0026#34;year\u0026#34;), MetadataField.date(\u0026#34;activationDate\u0026#34;))) .defaultTopK(5) .defaultSimilarityThreshold(0.7) .indexName(\u0026#34;spring-ai-document-index\u0026#34;) .build(); } 在主代码中，创建一些文档：\nList\u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;country\u0026#34;, \u0026#34;BG\u0026#34;, \u0026#34;year\u0026#34;, 2020)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;country\u0026#34;, \u0026#34;NL\u0026#34;, \u0026#34;year\u0026#34;, 2023))); 将文档添加到向量存储：\nvectorStore.add(documents); 最后，检索类似于查询的文档：\nList\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch( SearchRequest.builder() .query(\u0026#34;Spring\u0026#34;) .topK(5).build()); 如果一切顺利，您应该检索包含文本“Spring AI rocks！！”的文档。\n元数据筛选 # 您也可以使用AzureVectorStore来利用通用的可移植元数据过滤器。 例如，可以使用文本表达式语言：\nvectorStore.similaritySearch( SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(\u0026#34;country in [\u0026#39;UK\u0026#39;, \u0026#39;NL\u0026#39;] \u0026amp;\u0026amp; year \u0026gt;= 2020\u0026#34;).build()); 或以编程方式使用表达式DSL:\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch( SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(b.and( b.in(\u0026#34;country\u0026#34;, \u0026#34;UK\u0026#34;, \u0026#34;NL\u0026#34;), b.gte(\u0026#34;year\u0026#34;, 2020)).build()).build()); 可移植的筛选器表达式会自动转换为专有的Azure Search OData筛选器。例如，以下可移植筛选器表达式：\ncountry in [\u0026#39;UK\u0026#39;, \u0026#39;NL\u0026#39;] \u0026amp;\u0026amp; year \u0026gt;= 2020 转换为以下Azure OData 筛选器表达式：\n$filter search.in(meta_country, \u0026#39;UK,NL\u0026#39;, \u0026#39;,\u0026#39;) and meta_year ge 2020 访问本机客户端 # Azure Vector Store实现通过getNativeClient（）方法提供对底层本机Azure搜索客户端（SearchClient）的访问：\nAzureVectorStore vectorStore = context.getBean(AzureVectorStore.class); Optional\u0026lt;SearchClient\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { SearchClient client = nativeClient.get(); // Use the native client for Azure Search-specific operations } 本机客户端为您提供对Azure Search特定功能和操作的访问，这些功能和操作可能不会通过VectorStore界面公开。\n"},{"id":1,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/azure-openai%E8%BD%AF%E4%BB%B6/azure-openai%E5%87%BD%E6%95%B0%E8%B0%83%E7%94%A8/","title":"Azure OpenAI函数调用（不推荐）","section":"Azure OpenAI聊天","content":" Azure OpenAI函数调用（不推荐） # 函数调用允许开发人员在其代码中创建函数的描述，然后将该描述传递给请求中的语言模型。来自模型的响应包括与描述匹配的函数的名称和用于调用它的参数。 您可以使用AzureOpenAiChatModel注册自定义Java函数，并让模型智能地选择输出包含参数的JSON对象，以调用一个或多个已注册的函数。 Azure OpenAI API不直接调用函数；相反，模型生成JSON，您可以使用它来调用代码中的函数，并将结果返回给模型以完成对话。 Spring AI提供灵活且用户友好的方法来注册和调用自定义函数。 作为开发人员，您需要实现一个函数，该函数接受从AI模型发送的函数调用参数，并将结果返回给模型。 SpringAI使得这一点就像定义一个返回java.util的@Bean定义一样简单。函数，并在调用ChatModel时提供bean名称作为选项。 在引擎盖下，Spring用适当的适配器代码包装POJO（函数），该适配器代码支持与AI模型的交互，从而避免编写冗长的样板代码。\n它是如何工作的 # 假设我们希望人工智能模型用它没有的信息来响应，例如给定位置的当前温度。 我们可以为AI模型提供关于我们自己函数的元数据，它可以在处理您的提示时使用这些元数据来检索该信息。 例如，如果在处理提示期间，AI模型确定它需要关于给定位置的温度的额外信息，则它将启动服务器端生成的请求/响应交互。人工智能模型调用客户端函数。 SpringAI大大简化了您需要编写的代码，以支持函数调用。\n快速入门 # 让我们创建一个聊天机器人，通过调用自己的函数来回答问题。 当对模型提示的响应需要回答诸如“波士顿的天气怎么样？”这样的问题时，人工智能模型将调用客户端，提供位置值作为要传递给函数的参数。这种类似RPC的数据作为JSON传递。 我们的函数可以具有一些基于SaaS的天气服务API，并将天气响应返回到模型以完成对话。在这个例子中，我们将使用一个名为MockWeatherService的简单实现，该实现对不同位置的温度进行硬编码。 以下MockWeatherService.java表示天气服务API：\npublic class MockWeatherService implements Function\u0026lt;Request, Response\u0026gt; { public enum Unit { C, F } public record Request(String location, Unit unit) {} public record Response(double temp, Unit unit) {} public Response apply(Request request) { return new Response(30.0, Unit.C); } } 将函数注册为Bean # 使用AzureOpenAiChatModelAutoConfiguration，您有多种方法可以在Spring上下文中将自定义函数注册为bean。 我们首先描述最友好的POJO选项。\n普通Java函数 # 在这种方法中，您可以像定义任何其他Spring托管对象一样，在应用程序上下文中定义@Beans。 在内部，Spring AI ChatModel将创建ToolCallback实例的实例，该实例添加了通过AI模型调用它的逻辑。\n@Configuration static class Config { @Bean @Description(\u0026#34;Get the weather in location\u0026#34;) // function description public Function\u0026lt;MockWeatherService.Request, MockWeatherService.Response\u0026gt; weatherFunction1() { return new MockWeatherService(); } ... } @Description注释是可选的，它提供了一个函数描述（2），帮助模型理解何时调用函数。它是一个重要的属性，可以帮助AI模型确定要调用的客户端函数。 提供函数描述的另一个选项是在MockWeatherService上使用@JsonClassDescription注释。请求提供功能描述：\n@Configuration static class Config { @Bean public Function\u0026lt;Request, Response\u0026gt; currentWeather3() { // (1) bean name as function name. return new MockWeatherService(); } ... } @JsonClassDescription(\u0026#34;Get the weather in location\u0026#34;) // (2) function description public record Request(String location, Unit unit) {} 最好的做法是用信息注释请求对象，以便该函数的生成JSON模式尽可能具有描述性，以帮助AI模型选择要调用的正确函数。 FunctionCallWithFunctionBeanIT.java演示了这种方法。\n工具回调包装 # 注册函数的另一种方法是创建ToolCallback实例，如下所示：\n@Configuration static class Config { @Bean public FunctionToolCallback weatherFunctionInfo() { return FunctionToolCallback.builder(\u0026#34;CurrentWeather\u0026#34;, new MockWeatherService()) // (1) function name .description(\u0026#34;Get the current weather in a given location\u0026#34;) // (2) function description .inputType(MockWeatherService.Request.class) // (3) function input type .build(); } ... } 它包装了第三方MockWeatherService函数，并将其注册为AzureAiChatModel的CurrentWeather函数，并提供了描述（2）。\n在聊天选项中指定功能 # 要让模型知道并调用CurrentWeather函数，您需要在提示请求中启用它：\nAzureOpenAiChatModel chatModel = ... UserMessage userMessage = new UserMessage(\u0026#34;What\u0026#39;s the weather like in San Francisco, Tokyo, and Paris?\u0026#34;); ChatResponse response = this.chatModel.call(new Prompt(List.of(this.userMessage), AzureOpenAiChatOptions.builder().tools(\u0026#34;CurrentWeather\u0026#34;).build())); // (1) Enable the function logger.info(\u0026#34;Response: {}\u0026#34;, response); 上述用户问题将触发3次对CurrentWeather功能的调用（每个城市一次），最终响应如下： FunctionCallWithFunctionWrapperIT.java测试演示了这种方法。\n具有提示选项的注册/调用函数 # 除了自动配置外，您还可以使用提示请求动态注册回调函数：\nAzureOpenAiChatModel chatModel = ... UserMessage userMessage = new UserMessage(\u0026#34;What\u0026#39;s the weather like in San Francisco, Tokyo, and Paris? Use Multi-turn function calling.\u0026#34;); var promptOptions = AzureOpenAiChatOptions.builder() .toolCallbacks(List.of(FunctionToolCallback.builder(\u0026#34;CurrentWeather\u0026#34;, new MockWeatherService()) // (1) function name and instance .description(\u0026#34;Get the current weather in a given location\u0026#34;) // (2) function description .inputType(MockWeatherService.Request.class) // (3) function input type .build())) .build(); ChatResponse response = this.chatModel.call(new Prompt(List.of(this.userMessage), this.promptOptions)); 这种方法允许根据用户输入动态选择要调用的不同函数。 FunctionCallWithPromptFunctionIT.java集成测试提供了一个完整的示例，说明如何使用AzureOpenAiChatModel注册函数，并在提示请求中使用它。\n"},{"id":2,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E5%9B%BE%E5%83%8F%E6%A8%A1%E5%9E%8B/azure-openai%E8%BD%AF%E4%BB%B6/","title":"Azure OpenAI映像生成","section":"映像模型API","content":" Azure OpenAI映像生成 # Spring AI支持DALL-E，这是Azure OpenAI的图像生成模型。\n前提条件 # 从 Azure门户上的Azure OpenAI服务部分获取Azure OpenAI端点和api密钥。\nexport SPRING_AI_AZURE_OPENAI_API_KEY=\u0026lt;INSERT KEY HERE\u0026gt; export SPRING_AI_AZURE_OPENAI_ENDPOINT=\u0026lt;INSERT ENDPOINT URL HERE\u0026gt; 部署名称 # 要使用运行Azure人工智能应用程序，请通过[Azure AI Portal]（oai.Azure.com/Portal）创建Azure人工智部署。 在Azure中，每个客户端都必须指定一个部署名称以连接到Azure OpenAI服务。 必须理解部署名称与您选择部署的模型不同 例如，名为“MyImgAiDeployment”的部署可以配置为使用Dalle3模型或Dalle2模型。 现在，为了简单起见，可以使用以下设置创建展开： 部署名称：MyImgAiDeployment 此Azure配置将与Spring Boot Azure AI Starter及其自动配置功能的默认配置一致。 如果使用不同的部署名称，请相应地更新配置属性：\nspring.ai.azure.openai.image.options.deployment-name=\u0026lt;my deployment name\u0026gt; Azure OpenAI和OpenAI的不同部署结构导致Azure OpenAI客户端库中名为deploymentOrModelName的属性。\n添加存储库和BOM表 # Spring AI工件发布在Maven Central和Spring Snapshot 存储库中。 为了帮助进行依赖关系管理，Spring AI提供了BOM（物料清单），以确保在整个项目中使用一致版本的Spring AI。请参阅依赖项管理部分，将Spring AI BOM添加到构建系统中。\n自动配置 # Spring AI为Azure OpenAI聊天客户端提供Spring Boot自动配置。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-azure-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-azure-openai\u0026#39; } 图像生成属性 # 前缀spring.ai.openai.image是属性前缀，允许您为openai配置ImageModel实现。\n连接属性 # 前缀spring.ai.openai用作允许连接到Azure openai的属性前缀。\n运行时选项 # OpenAiImageOptions.java提供模型配置，例如要使用的模型、质量、大小等。 启动时，可以使用AzureOpenAiImageModel（OpenAiImage Pi openAiImage API）构造函数和withDefaultOptions（OpenAiImageOptions defaultOptions）方法配置默认选项。或者，使用前面描述的spring.ai.azure.openai.image.options.*属性。 在运行时，可以通过向ImagePrompt调用添加新的特定于请求的选项来覆盖默认选项。\nImageResponse response = azureOpenaiImageModel.call( new ImagePrompt(\u0026#34;A light cream colored mini golden doodle\u0026#34;, OpenAiImageOptions.builder() .quality(\u0026#34;hd\u0026#34;) .N(4) .height(1024) .width(1024).build()) ); "},{"id":3,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E9%9F%B3%E9%A2%91%E5%9E%8B%E5%8F%B7/%E8%BD%AC%E5%BD%95api/azure-openai%E8%BD%AF%E4%BB%B6/","title":"Azure OpenAI转录","section":"转录API","content":" Azure OpenAI转录 # Spring AI支持 Azure Whisper模型。\n前提条件 # 从 Azure门户上的Azure OpenAI服务部分获取Azure OpenAI端点和api密钥。\n自动配置 # Spring AI为Azure OpenAI转录生成客户端提供Spring Boot自动配置。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-azure-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-azure-openai\u0026#39; } 转录属性 # 前缀spring.ai.openai.audio.transcription用作属性前缀，允许您配置openai映像模型的重试机制。\n运行时选项 # AzureOpenAiAudioTranscriptionOptions类提供进行转录时使用的选项。 例如：\nAzureOpenAiAudioTranscriptionOptions.TranscriptResponseFormat responseFormat = AzureOpenAiAudioTranscriptionOptions.TranscriptResponseFormat.VTT; AzureOpenAiAudioTranscriptionOptions transcriptionOptions = AzureOpenAiAudioTranscriptionOptions.builder() .language(\u0026#34;en\u0026#34;) .prompt(\u0026#34;Ask not this, but ask that\u0026#34;) .temperature(0f) .responseFormat(this.responseFormat) .build(); AudioTranscriptionPrompt transcriptionRequest = new AudioTranscriptionPrompt(audioFile, this.transcriptionOptions); AudioTranscriptionResponse response = azureOpenAiTranscriptionModel.call(this.transcriptionRequest); 手动配置 # 将spring ai openai依赖项添加到项目的Maven pom.xml文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-azure-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-azure-openai\u0026#39; } 接下来，创建AzureOpenAiAudioTranscriptionModel\nvar openAIClient = new OpenAIClientBuilder() .credential(new AzureKeyCredential(System.getenv(\u0026#34;AZURE_OPENAI_API_KEY\u0026#34;))) .endpoint(System.getenv(\u0026#34;AZURE_OPENAI_ENDPOINT\u0026#34;)) .buildClient(); var azureOpenAiAudioTranscriptionModel = new AzureOpenAiAudioTranscriptionModel(this.openAIClient, null); var transcriptionOptions = AzureOpenAiAudioTranscriptionOptions.builder() .responseFormat(TranscriptResponseFormat.TEXT) .temperature(0f) .build(); var audioFile = new FileSystemResource(\u0026#34;/path/to/your/resource/speech/jfk.flac\u0026#34;); AudioTranscriptionPrompt transcriptionRequest = new AudioTranscriptionPrompt(this.audioFile, this.transcriptionOptions); AudioTranscriptionResponse response = this.azureOpenAiAudioTranscriptionModel.call(this.transcriptionRequest); "},{"id":4,"href":"/docs/%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90rag/etl%E7%AE%A1%E9%81%93/","title":"ETL管道","section":"检索增强生成","content":" ETL管道 # 提取、转换和加载（ETL）框架在检索增强生成（RAG）用例中充当数据处理的主干。 ETL管道编排从原始数据源到结构化向量存储的流，确保数据处于人工智能模型检索的最佳格式。 RAG用例是一种文本，通过从数据体中检索相关信息来增强生成模型的能力，以提高生成输出的质量和相关性。\nAPI概述 # ETL管道创建、转换和存储文档实例。 Document类包含文本、元数据和可选的其他媒体类型，如图像、音频和视频。 ETL管道有三个主要组成部分，\n实现供应商\u0026lt;List\u0026gt;的DocumentReader 实现函数\u0026lt;List、List\u0026gt;的DocumentTransformer 实现Consumer\u0026lt;List\u0026gt;的DocumentWriter 在DocumentReader的帮助下，从PDF、文本文件和其他文档类型创建Document类内容。 要构建简单的ETL管道，可以将每种类型的实例链接在一起。 假设我们有这三种ETL类型的以下实例 PagePdfDocumentReader——DocumentReader的实现 TokenTextSplitter——DocumentTransformer的一种实现 VectorStore DocumentWriter的实现 要将数据基本加载到向量数据库中以与检索增强生成模式一起使用，请使用Java函数样式语法中的以下代码。 vectorStore.accept(tokenTextSplitter.apply(pdfReader.get())); 或者，您可以使用更自然地表达域的方法名\nvectorStore.write(tokenTextSplitter.split(pdfReader.read())); ETL接口 # ETL管道由以下接口和实现组成。\n文档阅读器 # 提供来自不同来源的文档源。\npublic interface DocumentReader extends Supplier\u0026lt;List\u0026lt;Document\u0026gt;\u0026gt; { default List\u0026lt;Document\u0026gt; read() { return get(); } } 文件变压器 # 将一批文档转换为处理工作流的一部分。\npublic interface DocumentTransformer extends Function\u0026lt;List\u0026lt;Document\u0026gt;, List\u0026lt;Document\u0026gt;\u0026gt; { default List\u0026lt;Document\u0026gt; transform(List\u0026lt;Document\u0026gt; transform) { return apply(transform); } } 文档编写器 # 管理ETL流程的最后阶段，准备存储文档。\npublic interface DocumentWriter extends Consumer\u0026lt;List\u0026lt;Document\u0026gt;\u0026gt; { default void write(List\u0026lt;Document\u0026gt; documents) { accept(documents); } } ETL类图 # 下面的类图说明了ETL接口和实现。 文档阅读器 # JSON格式 # JsonReader处理JSON文档，将它们转换为Document对象列表。\n示例 # @Component class MyJsonReader { private final Resource resource; MyJsonReader(@Value(\u0026#34;classpath:bikes.json\u0026#34;) Resource resource) { this.resource = resource; } List\u0026lt;Document\u0026gt; loadJsonAsDocuments() { JsonReader jsonReader = new JsonReader(this.resource, \u0026#34;description\u0026#34;, \u0026#34;content\u0026#34;); return jsonReader.get(); } } 构造函数选项 # JsonReader提供了几个构造函数选项：\n参数 # resource：指向JSON文件的Spring resource对象。 jsonKeysToUse：JSON中的键数组，应该用作结果Document对象中的文本内容。 jsonMetadataGenerator：一个可选的JsonMetadata发电机，用于为每个文档创建元数据。 行为 # JsonReader按如下方式处理JSON内容：\n它可以处理JSON数组和单个JSON对象。 对于每个JSON对象（在数组或单个对象中）： 它基于指定的jsonKeysToUse提取内容。 如果未指定键，则使用整个JSON对象作为内容。 它使用提供的JsonMetadataGenerator（如果未提供，则为空）生成元数据。 它使用提取的内容和元数据创建Document对象。 使用JSON指针 # JsonReader现在支持使用JSON指针检索JSON文档的特定部分。该功能允许您轻松地从复杂的JSON结构中提取嵌套数据。\nget（字符串指针）方法 # public List\u0026lt;Document\u0026gt; get(String pointer) 该方法允许您使用JSON指针来检索JSON文档的特定部分。\n参数 # 指针：JSON指针字符串（如RFC 6901中定义的），用于在JSON结构中定位所需的元素。 返回值 # 返回一个List，其中包含从指针所在的JSON元素解析的文档。 行为 # 该方法使用提供的JSON指针导航到JSON结构中的特定位置。 如果指针有效并指向现有元素： 对于JSON对象：它返回一个包含单个文档的列表。 对于JSON数组：它返回一个Documents列表，数组中的每个元素对应一个列表。 如果指针无效或指向不存在的元素，则抛出IllegalArgumentException。 示例 # JsonReader jsonReader = new JsonReader(resource, \u0026#34;description\u0026#34;); List\u0026lt;Document\u0026gt; documents = this.jsonReader.get(\u0026#34;/store/books/0\u0026#34;); JSON结构示例 # [ { \u0026#34;id\u0026#34;: 1, \u0026#34;brand\u0026#34;: \u0026#34;Trek\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;A high-performance mountain bike for trail riding.\u0026#34; }, { \u0026#34;id\u0026#34;: 2, \u0026#34;brand\u0026#34;: \u0026#34;Cannondale\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;An aerodynamic road bike for racing enthusiasts.\u0026#34; } ] 在本例中，如果将JsonReader配置为“description”作为jsonKeysToUse，则它将创建Document对象，其中内容是数组中每个自行车的“descriptions”字段的值。\n备注 # JsonReader使用Jackson进行JSON解析。 它可以通过为数组使用流来有效地处理大型JSON文件。 如果在jsonKeysToUse中指定了多个键，则内容将是这些键的值的串联。 阅读器非常灵活，可以通过定制jsonKeysToUse和JsonMetadataGenerator来适应各种JSON结构。 文本 # TextReader处理纯文本文档，将它们转换为Document对象列表。\n示例 # @Component class MyTextReader { private final Resource resource; MyTextReader(@Value(\u0026#34;classpath:text-source.txt\u0026#34;) Resource resource) { this.resource = resource; } List\u0026lt;Document\u0026gt; loadText() { TextReader textReader = new TextReader(this.resource); textReader.getCustomMetadata().put(\u0026#34;filename\u0026#34;, \u0026#34;text-source.txt\u0026#34;); return textReader.read(); } } 构造函数选项 # TextReader提供了两个构造函数选项：\n参数 # resourceUrl：表示要读取的资源的URL的字符串。 resource：指向文本文件的Spring resource对象。 配置 # setCharset（Charset Charset）：设置用于读取文本文件的字符集。默认值为UTF-8。 getCustomMetadata（）：返回一个可变映射，您可以在其中为文档添加自定义元数据。 行为 # TextReader按如下方式处理文本内容：\n它将文本文件的整个内容读入单个Document对象。 文件的内容成为文档的内容。 元数据自动添加到文档： charset：用于读取文件的字符集（默认值：“UTF-8”）。 source：源文本文件的文件名。 通过getCustomMetadata（）添加的任何自定义元数据都包含在文档中。 备注 # TextReader将整个文件内容读入内存，因此它可能不适合非常大的文件。 如果需要将文本拆分为较小的块，则可以在阅读文档后使用文本拆分器，如TokenTextSplitter： List\u0026lt;Document\u0026gt; documents = textReader.get(); List\u0026lt;Document\u0026gt; splitDocuments = new TokenTextSplitter().apply(this.documents); 阅读器使用Spring的资源抽象，允许它从各种源（类路径、文件系统、URL等）读取。 可以使用getCustomMetadata（）方法将自定义元数据添加到读取器创建的所有文档中。 HTML（JSoup） # JsoupDocumentReader处理HTML文档，使用JSoup库将它们转换为Document对象列表。\n示例 # @Component class MyHtmlReader { private final Resource resource; MyHtmlReader(@Value(\u0026#34;classpath:/my-page.html\u0026#34;) Resource resource) { this.resource = resource; } List\u0026lt;Document\u0026gt; loadHtml() { JsoupDocumentReaderConfig config = JsoupDocumentReaderConfig.builder() .selector(\u0026#34;article p\u0026#34;) // Extract paragraphs within \u0026lt;article\u0026gt; tags .charset(\u0026#34;ISO-8859-1\u0026#34;) // Use ISO-8859-1 encoding .includeLinkUrls(true) // Include link URLs in metadata .metadataTags(List.of(\u0026#34;author\u0026#34;, \u0026#34;date\u0026#34;)) // Extract author and date meta tags .additionalMetadata(\u0026#34;source\u0026#34;, \u0026#34;my-page.html\u0026#34;) // Add custom metadata .build(); JsoupDocumentReader reader = new JsoupDocumentReader(this.resource, config); return reader.get(); } } ``JsoupDocumentReaderConfig允许您自定义JsoupDocumentReader的行为：\ncharset：指定HTML文档的字符编码（默认为“UTF-8”）。 选择器：JSoup CSS选择器，用于指定要从中提取文本的元素（默认为“body”）。 分隔符：用于连接来自多个选定元素的文本的字符串（默认为“\\n”）。 allElements：如果为true，则从元素中提取所有文本，忽略选择器（默认为false）。 groupByElement：如果为true，则为选择器匹配的每个元素创建单独的文档（默认为false）。 includeLinkUrls：如果为true，则提取绝对链接URL并将其添加到元数据中（默认为false）。 Metadatags：要从中提取内容的标记名称的列表（默认为[“description”，“keywords”]）。 additionalMetadata：允许您将自定义元数据添加到所有创建的Document对象。 示例文档：my-page.html # \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;My Web Page\u0026lt;/title\u0026gt; \u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;A sample web page for Spring AI\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;keywords\u0026#34; content=\u0026#34;spring, ai, html, example\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;author\u0026#34; content=\u0026#34;John Doe\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;date\u0026#34; content=\u0026#34;2024-01-15\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;style.css\u0026#34;\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;header\u0026gt; \u0026lt;h1\u0026gt;Welcome to My Page\u0026lt;/h1\u0026gt; \u0026lt;/header\u0026gt; \u0026lt;nav\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;/\u0026#34;\u0026gt;Home\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;/about\u0026#34;\u0026gt;About\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/nav\u0026gt; \u0026lt;article\u0026gt; \u0026lt;h2\u0026gt;Main Content\u0026lt;/h2\u0026gt; \u0026lt;p\u0026gt;This is the main content of my web page.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;It contains multiple paragraphs.\u0026lt;/p\u0026gt; \u0026lt;a href=\u0026#34;https://www.example.com\u0026#34;\u0026gt;External Link\u0026lt;/a\u0026gt; \u0026lt;/article\u0026gt; \u0026lt;footer\u0026gt; \u0026lt;p\u0026gt;\u0026amp;copy; 2024 John Doe\u0026lt;/p\u0026gt; \u0026lt;/footer\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 行为： JsoupDocumentReader处理HTML内容并基于配置创建Document对象：\n选择器确定用于文本提取的元素。 如果allElements为true，则中的所有文本都将提取到单个文档中。 如果groupByElement为true，则与选择器匹配的每个元素都会创建单独的文档。 如果allElements和groupByElement都不为true，则使用分隔符连接与选择器匹配的所有元素中的文本。 文档标题、来自指定标记的内容和（可选）链接URL被添加到文档元数据中。 将从URL资源中提取用于解析相对链接的基URI。 读取器保留所选元素的文本内容，但删除其中的任何HTML标记。 降价，降价 # MarkdownDocumentReader处理Markdown文档，将它们转换为Document对象列表。\n示例 # @Component class MyMarkdownReader { private final Resource resource; MyMarkdownReader(@Value(\u0026#34;classpath:code.md\u0026#34;) Resource resource) { this.resource = resource; } List\u0026lt;Document\u0026gt; loadMarkdown() { MarkdownDocumentReaderConfig config = MarkdownDocumentReaderConfig.builder() .withHorizontalRuleCreateDocument(true) .withIncludeCodeBlock(false) .withIncludeBlockquote(false) .withAdditionalMetadata(\u0026#34;filename\u0026#34;, \u0026#34;code.md\u0026#34;) .build(); MarkdownDocumentReader reader = new MarkdownDocumentReader(this.resource, config); return reader.get(); } } MarkdownDocumentReaderConfig允许您自定义MarkdownDocumentum阅读器的行为：\nhorizontalRuleCreateDocument：当设置为true时，Markdown中的水平规则将创建新的Document对象。 includeCodeBlock：当设置为true时，代码块将与周围文本包含在同一文档中。如果为false，代码块将创建单独的Document对象。 includeBlockquote：当设置为true时，区块引号将与周围文本包含在同一文档中。如果为false，则块引号创建单独的Document对象。 additionalMetadata：允许您将自定义元数据添加到所有创建的Document对象。 示例文档：code.md # This is a Java sample application: ```java package com.example.demo; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; @SpringBootApplication public class DemoApplication { public static void main(String[] args) { SpringApplication.run(DemoApplication.class, args); } } Markdown also provides the possibility to use inline code formatting throughout the entire sentence.\nAnother possibility is to set block code without specific highlighting:\n./mvnw spring-javaformat:apply 行为：MarkdownDocumentReader处理Markdown内容并基于配置创建Document对象： - 标头成为Document对象中的元数据。 - 段落成为Document对象的内容。 - 代码块可以分为自己的Document对象，也可以包含在周围的文本中。 - 块引号可以分隔为自己的Document对象，也可以包含在周围的文本中。 - 水平规则可用于将内容拆分为单独的Document对象。 读取器在Document对象的内容中保留诸如内联代码、列表和文本样式之类的格式。 ## PDF页面 `PagePdfDocumentReader`使用Apache PdfBox库来解析PDF文档 使用Maven或Gradle将依赖项添加到项目中。 ```xml \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-pdf-document-reader\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-pdf-document-reader\u0026#39; } 示例 # @Component public class MyPagePdfDocumentReader { List\u0026lt;Document\u0026gt; getDocsFromPdf() { PagePdfDocumentReader pdfReader = new PagePdfDocumentReader(\u0026#34;classpath:/sample1.pdf\u0026#34;, PdfDocumentReaderConfig.builder() .withPageTopMargin(0) .withPageExtractedTextFormatter(ExtractedTextFormatter.builder() .withNumberOfTopTextLinesToDelete(0) .build()) .withPagesPerDocument(1) .build()); return pdfReader.read(); } } PDF段落 # ParagraphPdfDocumentReader使用PDF目录（例如TOC）信息将输入的PDF拆分为文本段落，并为每个段落输出单个文档。\n依赖关系 # 使用Maven或Gradle将依赖项添加到项目中。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-pdf-document-reader\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-pdf-document-reader\u0026#39; } 示例 # @Component public class MyPagePdfDocumentReader { List\u0026lt;Document\u0026gt; getDocsFromPdfWithCatalog() { ParagraphPdfDocumentReader pdfReader = new ParagraphPdfDocumentReader(\u0026#34;classpath:/sample1.pdf\u0026#34;, PdfDocumentReaderConfig.builder() .withPageTopMargin(0) .withPageExtractedTextFormatter(ExtractedTextFormatter.builder() .withNumberOfTopTextLinesToDelete(0) .build()) .withPagesPerDocument(1) .build()); return pdfReader.read(); } } Tika（DOCX、PPTX、HTML…​) # TikaDocumentReader使用Apache Tika从各种文档格式中提取文本，如PDF、DOC/DOCX、PPT/PPTX和HTML。有关支持格式的全面列表，请参阅 Tika文档。\n依赖关系 # \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-tika-document-reader\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-tika-document-reader\u0026#39; } 示例 # @Component class MyTikaDocumentReader { private final Resource resource; MyTikaDocumentReader(@Value(\u0026#34;classpath:/word-sample.docx\u0026#34;) Resource resource) { this.resource = resource; } List\u0026lt;Document\u0026gt; loadText() { TikaDocumentReader tikaDocumentReader = new TikaDocumentReader(this.resource); return tikaDocumentReader.read(); } } 变压器 # 文本拆分器 # TextSplitter是一个抽象基类，帮助划分文档以适应AI模型的上下文窗口。\n标记文本拆分器 # TokenTextSplitter``是TextSpliter的实现，它使用CL100K_BASE编码，根据令牌计数将文本拆分为块。\n使用 # @Component class MyTokenTextSplitter { public List\u0026lt;Document\u0026gt; splitDocuments(List\u0026lt;Document\u0026gt; documents) { TokenTextSplitter splitter = new TokenTextSplitter(); return splitter.apply(documents); } public List\u0026lt;Document\u0026gt; splitCustomized(List\u0026lt;Document\u0026gt; documents) { TokenTextSplitter splitter = new TokenTextSplitter(1000, 400, 10, 5000, true); return splitter.apply(documents); } } 构造函数选项 # TokenTextSplitter提供了两个构造函数选项：\n参数 # defaultChunkSize：标记中每个文本块的目标大小（默认值：800）。 minChunkSizeChars：每个文本块的最小大小，以字符为单位（默认值：350）。 minChunkLengthToEmbed：要包含的块的最小长度（默认值：5）。 maxNumChunks：从文本生成的最大区块数（默认值：10000）。 keepSeparator：是否在块中保留分隔符（如换行符）（默认值：true）。 行为 # TokenTextSplitter按如下方式处理文本内容：\n示例 # Document doc1 = new Document(\u0026#34;This is a long piece of text that needs to be split into smaller chunks for processing.\u0026#34;, Map.of(\u0026#34;source\u0026#34;, \u0026#34;example.txt\u0026#34;)); Document doc2 = new Document(\u0026#34;Another document with content that will be split based on token count.\u0026#34;, Map.of(\u0026#34;source\u0026#34;, \u0026#34;example2.txt\u0026#34;)); TokenTextSplitter splitter = new TokenTextSplitter(); List\u0026lt;Document\u0026gt; splitDocuments = this.splitter.apply(List.of(this.doc1, this.doc2)); for (Document doc : splitDocuments) { System.out.println(\u0026#34;Chunk: \u0026#34; + doc.getContent()); System.out.println(\u0026#34;Metadata: \u0026#34; + doc.getMetadata()); } 备注 # TokenTextSplitter使用jtokkit库中的CL100K_BASE编码，该编码与较新的OpenAI模型兼容。 拆分器试图通过尽可能打破句子边界来创建语义上有意义的块。 来自原始文档的元数据将被保留并复制到从该文档派生的所有区块。 如果copyContentFormatter设置为true（默认行为），则原始文档中的内容格式化程序（如果设置）也会复制到派生块。 此拆分器对于为具有标记限制的大型语言模型准备文本特别有用，以确保每个块都在模型的处理能力范围内。 内容格式转换器 # 确保所有文档的内容格式一致。\n关键字MetadataEnricher # KeywordMetadataEnricher是一个DocumentTransformer，它使用生成人工智能模型从文档内容中提取关键字，并将它们作为元数据添加。\n使用 # @Component class MyKeywordEnricher { private final ChatModel chatModel; MyKeywordEnricher(ChatModel chatModel) { this.chatModel = chatModel; } List\u0026lt;Document\u0026gt; enrichDocuments(List\u0026lt;Document\u0026gt; documents) { KeywordMetadataEnricher enricher = new KeywordMetadataEnricher(this.chatModel, 5); return enricher.apply(documents); } } 施工单位名称 # KeywordMetadataEnricher构造函数采用两个参数：\n行为 # KeywordMetadataEnricher按如下方式处理文档：\n自定义 # 可以通过修改类中的KEYWORDS_TEMPLATE常量来定制关键字提取提示。默认模板为：\n\\{context_str}. Give %s unique keywords for this document. Format as comma separated. Keywords: 其中，{context_str}替换为文档内容，%s替换为指定的关键字计数。\n示例 # ChatModel chatModel = // initialize your chat model KeywordMetadataEnricher enricher = new KeywordMetadataEnricher(chatModel, 5); Document doc = new Document(\u0026#34;This is a document about artificial intelligence and its applications in modern technology.\u0026#34;); List\u0026lt;Document\u0026gt; enrichedDocs = enricher.apply(List.of(this.doc)); Document enrichedDoc = this.enrichedDocs.get(0); String keywords = (String) this.enrichedDoc.getMetadata().get(\u0026#34;excerpt_keywords\u0026#34;); System.out.println(\u0026#34;Extracted keywords: \u0026#34; + keywords); 备注 # KeywordMetadataEnricher需要功能正常的ChatModel来生成关键字。 关键字计数必须大于等于1。 enricher将“excerpt_keywords”元数据字段添加到每个处理的文档中。 生成的关键字将作为逗号分隔的字符串返回。 该丰富器对于提高文档的可搜索性和为文档生成标记或类别特别有用。 汇总元数据增强器 # SummaryMetadataEnricher是一个DocumentTransformer，它使用生成AI模型为文档创建摘要，并将其作为元数据添加。它可以为当前文档以及相邻文档（上一个和下一个）生成摘要。\n使用 # @Configuration class EnricherConfig { @Bean public SummaryMetadataEnricher summaryMetadata(OpenAiChatModel aiClient) { return new SummaryMetadataEnricher(aiClient, List.of(SummaryType.PREVIOUS, SummaryType.CURRENT, SummaryType.NEXT)); } } @Component class MySummaryEnricher { private final SummaryMetadataEnricher enricher; MySummaryEnricher(SummaryMetadataEnricher enricher) { this.enricher = enricher; } List\u0026lt;Document\u0026gt; enrichDocuments(List\u0026lt;Document\u0026gt; documents) { return this.enricher.apply(documents); } } 施工单位名称 # SummaryMetadataEnricher提供了两个构造函数：\n参数 # chatModel：用于生成摘要的AI模型。 summaryTypes:SummaryType枚举值的列表，指示要生成的摘要（上一个、当前、下一个）。 summaryTemplate：用于生成摘要的自定义模板（可选）。 metadataMode：指定生成摘要时如何处理文档元数据（可选）。 行为 # SummaryMetadataEnricher按如下方式处理文档：\n自定义 # 可以通过提供自定义summaryTemplate来自定义摘要生成提示。默认模板为：\n\u0026#34;\u0026#34;\u0026#34; Here is the content of the section: {context_str} Summarize the key topics and entities of the section. Summary: \u0026#34;\u0026#34;\u0026#34; 示例 # ChatModel chatModel = // initialize your chat model SummaryMetadataEnricher enricher = new SummaryMetadataEnricher(chatModel, List.of(SummaryType.PREVIOUS, SummaryType.CURRENT, SummaryType.NEXT)); Document doc1 = new Document(\u0026#34;Content of document 1\u0026#34;); Document doc2 = new Document(\u0026#34;Content of document 2\u0026#34;); List\u0026lt;Document\u0026gt; enrichedDocs = enricher.apply(List.of(this.doc1, this.doc2)); // Check the metadata of the enriched documents for (Document doc : enrichedDocs) { System.out.println(\u0026#34;Current summary: \u0026#34; + doc.getMetadata().get(\u0026#34;section_summary\u0026#34;)); System.out.println(\u0026#34;Previous summary: \u0026#34; + doc.getMetadata().get(\u0026#34;prev_section_summary\u0026#34;)); System.out.println(\u0026#34;Next summary: \u0026#34; + doc.getMetadata().get(\u0026#34;next_section_summary\u0026#34;)); } 提供的示例演示了预期的行为：\n对于两个文档的列表，这两个文档都会收到section_summary。 第一个文档接收next_section_summary，但没有prev_section_summary。 第二个文档接收prev_section_summary，但不接收next_section_summary。 第一个文档的section_summary与第二个文档的prev_section_summary。 第一个文档的next_section_summary与第二个文档的section_sum匹配。 备注 # SummaryMetadataEnricher需要功能正常的ChatModel来生成摘要。 丰富器可以处理任何大小的文档列表，正确处理第一个和最后一个文档的边缘情况。 该丰富器对于创建上下文感知摘要特别有用，允许更好地理解序列中的文档关系。 MetadataMode参数允许控制如何将现有元数据合并到摘要生成过程中。 作家 # 文件 # File``DocumentWriter``是一个DocumentWritor实现，它将Document对象列表的内容写入文件。\n使用 # @Component class MyDocumentWriter { public void writeDocuments(List\u0026lt;Document\u0026gt; documents) { FileDocumentWriter writer = new FileDocumentWriter(\u0026#34;output.txt\u0026#34;, true, MetadataMode.ALL, false); writer.accept(documents); } } 施工人员 # FileDocumentWriter提供了三个构造函数：\n参数 # fileName：要将文档写入的文件的名称。 withDocumentMarkers：是否在输出中包含文档标记（默认值：false）。 metadataMode：指定要写入文件的文档内容（默认：metadataMode.NONE）。 append：如果为true，则数据将写入文件的末尾，而不是开头（默认值：false）。 行为 # FileDocumentWriter按以下方式处理文档：\n文档标记 # 当withDocumentMarkers设置为true时，编写器将按以下格式为每个文档包含标记：\n### Doc: [index], pages:[start_page_number,end_page_number] 元数据处理 # 编写器使用两个特定的元数据键：\npage_number：表示文档的起始页码。 end_page_number：表示文档的结束页码。 这些在写入文档标记时使用。 示例 # List\u0026lt;Document\u0026gt; documents = // initialize your documents FileDocumentWriter writer = new FileDocumentWriter(\u0026#34;output.txt\u0026#34;, true, MetadataMode.ALL, true); writer.accept(documents); 这将使用所有可用的元数据将所有文档写入“output.txt”，包括文档标记，并附加到文件（如果文件已经存在）。\n备注 # 编写器使用FileWriter，因此它使用操作系统的默认字符编码来编写文本文件。 如果在写入期间发生错误，则抛出RuntimeException，并将原始异常作为其原因。 metadataMode参数允许控制如何将现有元数据合并到写入内容中。 该编写器对于调试或创建文档集合的可读输出特别有用。 VectorStore（矢量存储） # 提供与各种向量存储的集成。\n"},{"id":5,"href":"/docs/%E6%A8%A1%E5%9E%8B%E4%B8%8A%E4%B8%8B%E6%96%87%E5%8D%8F%E8%AE%AEmcp/mcp%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%90%AF%E5%8A%A8%E5%90%AF%E5%8A%A8%E7%A8%8B%E5%BA%8F/","title":"MCP客户端引导启动程序","section":"模型上下文协议（MCP）","content":" MCP客户端引导启动程序 # Spring AI MCP（模型上下文协议）客户端引导启动程序为Spring Boot应用程序中的MCP客户端功能提供自动配置。它支持具有各种传输选项的同步和异步客户端实现。 MCP Client Boot Starter提供：\n启动器 # 标准MCP客户端 # \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-mcp-client\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 标准启动器通过STDIO（进程内）和/或SSE（远程）传输同时连接到一个或多个MCP服务器。\nWebFlux客户端 # WebFlux启动程序提供了与标准启动程序类似的功能，但使用了基于WebFlux的SSE传输实现。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-mcp-client-webflux\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 配置属性 # 公用属性 # 常见属性的前缀为spring.ai.mcp.client：\nStdio传输属性 # 标准I/O传输的属性前缀为spring.ai.mcp.client.stdio： 配置示例：\nspring: ai: mcp: client: stdio: root-change-notification: true connections: server1: command: /path/to/server args: - --port=8080 - --mode=production env: API_KEY: your-api-key DEBUG: \u0026#34;true\u0026#34; 或者，您可以使用 Claude Desktop格式的外部JSON文件配置stdio连接：\nspring: ai: mcp: client: stdio: servers-configuration: classpath:mcp-servers.json Claude Desktop格式如下所示：\n{ \u0026#34;mcpServers\u0026#34;: { \u0026#34;filesystem\u0026#34;: { \u0026#34;command\u0026#34;: \u0026#34;npx\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;-y\u0026#34;, \u0026#34;@modelcontextprotocol/server-filesystem\u0026#34;, \u0026#34;/Users/username/Desktop\u0026#34;, \u0026#34;/Users/username/Downloads\u0026#34; ] } } } 目前，Claude Desktop格式仅支持STDIO连接类型。\nSSE传输属性 # 服务器发送事件（SSE）传输的属性前缀为spring.ai.mcp.client.SSE： 配置示例：\nspring: ai: mcp: client: sse: connections: server1: url: http://localhost:8080 server2: url: http://otherserver:8081 sse-endpoint: /custom-sse 功能 # 同步/异步客户端类型 # 启动器支持两种类型的客户端：\n同步-默认客户端类型，适用于具有阻塞操作的传统请求-响应模式 异步-适用于具有非阻塞操作的反应式应用程序，使用spring.ai.mcp.client.type=ASYNC配置 客户端自定义 # 自动配置通过回调接口提供广泛的客户端规范定制功能。这些定制程序允许您配置MCP客户端行为的各个方面，从请求超时到事件处理和消息处理。\n自定义类型 # 以下自定义选项可用：\n请求配置-设置自定义请求超时 自定义采样处理程序-服务器通过客户端从LLM请求LLM采样（完成或生成）的标准化方法。该流允许客户端维护对模型访问、选择和权限的控制，同时使服务器能够利用AI功能，而不需要服务器API密钥。 文件系统（根）访问-客户端向服务器公开文件系统根的标准化方法。 事件处理程序-发生特定服务器事件时要通知的客户端处理程序： 工具更改通知-可用服务器工具列表更改时 资源更改通知-可用服务器资源列表更改时。 提示更改通知-当可用服务器列表提示更改时。 日志处理程序-服务器向客户端发送结构化日志消息的标准化方法。 根据应用程序的需要，可以为同步客户端实现McpSyncClientCustomizer，也可以为异步客户端实现McPAsyncClientCustolizer。 serverConfigurationName参数是应用自定义程序和为其创建MCP客户端的服务器配置的名称。 MCP客户端自动配置自动检测并应用在应用程序上下文中找到的任何自定义程序。 运输支持 # 自动配置支持多种传输类型：\n标准I/O（Stdio）（由spring ai starter mcp客户端激活） SSE HTTP（由spring ai starter mcp客户端激活） SSE WebFlux（由spring ai starter mcp客户端WebFlux.激活） 与Spring AI集成 # 初学者可以配置与Spring AI的工具执行框架集成的工具回调，允许将MCP工具用作AI交互的一部分。此集成是可选的，必须使用spring.ai.mcp.client.toolcallback.enabled=true属性显式启用。\n使用示例 # 将适当的启动程序依赖项添加到项目中，并在application.properties或application.yml中配置客户端：\nspring: ai: mcp: client: enabled: true name: my-mcp-client version: 1.0.0 request-timeout: 30s type: SYNC # or ASYNC for reactive applications sse: connections: server1: url: http://localhost:8080 server2: url: http://otherserver:8081 stdio: root-change-notification: false connections: server1: command: /path/to/server args: - --port=8080 - --mode=production env: API_KEY: your-api-key DEBUG: \u0026#34;true\u0026#34; MCP客户端bean将自动配置并可用于注入：\n@Autowired private List\u0026lt;McpSyncClient\u0026gt; mcpSyncClients; // For sync client // OR @Autowired private List\u0026lt;McpAsyncClient\u0026gt; mcpAsyncClients; // For async client 启用工具回调时，具有所有MCP客户端的已注册MCP工具将作为ToolCallbackProvider实例提供：\n@Autowired private SyncMcpToolCallbackProvider toolCallbackProvider; ToolCallback[] toolCallbacks = toolCallbackProvider.getToolCallbacks(); 请注意，默认情况下禁用工具回调功能，并且必须使用以下命令显式启用：\nspring: ai: mcp: client: toolcallback: enabled: true 示例应用程序 # Brave Web Search Chatbot-使用模型上下文协议与Web搜索服务器交互的聊天机器人。 Default MCP Client Starter（默认MCP客户端启动程序）-使用默认spring ai启动程序MCP客户端MCP客户端引导启动程序的简单示例。 WebFlux MCP Client Starter-使用spring ai Starter MCP客户端WebFlux-MCP客户端引导启动程序的简单示例。 其他资源 # Spring AI文档 模型上下文协议规范 弹簧防尘套自动配置 "},{"id":6,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/mistral%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/mistral%E5%87%BD%E6%95%B0%E8%B0%83%E7%94%A8%E4%B8%8D%E6%8E%A8%E8%8D%90/","title":"Mistral AI函数调用（不推荐）","section":"Mistral AI聊天","content":" Mistral AI函数调用（不推荐） # 您可以使用MistralAiChatModel注册自定义Java函数，并让Mistral AI模型智能地选择输出包含参数的JSON对象，以调用一个或多个已注册的函数。 Mistral AI API不直接调用函数；相反，模型生成JSON，您可以使用它来调用代码中的函数，并将结果返回给模型以完成对话。 Spring AI提供灵活且用户友好的方法来注册和调用自定义函数。 作为开发人员，您需要实现一个函数，该函数接受从AI模型发送的函数调用参数，并将结果返回给模型。 SpringAI使得这一点就像定义一个返回java.util的@Bean定义一样简单。函数，并在调用ChatModel时提供bean名称作为选项。 在引擎盖下，Spring用适当的适配器代码包装POJO（函数），该适配器代码支持与AI模型的交互，从而避免编写冗长的样板代码。\n它是如何工作的 # 假设我们希望人工智能模型用它没有的信息来响应，例如，给定位置的当前温度。 我们可以为AI模型提供关于我们自己函数的元数据，它可以在处理您的提示时使用这些元数据来检索该信息。 例如，如果在处理提示期间，AI模型确定它需要关于给定位置的温度的额外信息，则它将启动服务器端生成的请求/响应交互。人工智能模型调用客户端函数。 SpringAI大大简化了您需要编写的代码，以支持函数调用。\n快速入门 # 让我们创建一个聊天机器人，通过调用自己的函数来回答问题。 当模型需要回答“波士顿的天气怎么样？”这样的问题时，人工智能模型将调用客户端，提供位置值作为传递给函数的参数。这种类似RPC的数据作为JSON传递。 我们的函数调用一些基于SaaS的天气服务API，并将天气响应返回给模型以完成对话。 以下MockWeatherService.java表示天气服务API：\npublic class MockWeatherService implements Function\u0026lt;Request, Response\u0026gt; { public enum Unit { C, F } public record Request(String location, Unit unit) {} public record Response(double temp, Unit unit) {} public Response apply(Request request) { return new Response(30.0, Unit.C); } } 将函数注册为Bean # 使用 MistralAiChatModel自动配置，您可以使用多种方法在Spring上下文中将自定义函数注册为bean。 我们首先描述最友好的POJO选项。\n普通Java函数 # 在这种方法中，您可以像定义任何其他Spring托管对象一样，在应用程序上下文中定义@Bean。 在内部，Spring AI ChatModel将创建ToolCallback的实例，该实例添加了通过AI模型调用它的逻辑。\n@Configuration static class Config { @Bean @Description(\u0026#34;Get the weather in location\u0026#34;) // function description public Function\u0026lt;MockWeatherService.Request, MockWeatherService.Response\u0026gt; currentWeather() { return new MockWeatherService(); } } @Description注释是可选的，它提供了一个函数描述，帮助模型理解何时调用函数。 提供函数描述的另一个选项是在MockWeatherService上使用@JsonClassDescription注释。请求：\n@Configuration static class Config { @Bean public Function\u0026lt;Request, Response\u0026gt; currentWeather() { // bean name as function name return new MockWeatherService(); } } @JsonClassDescription(\u0026#34;Get the weather in location\u0026#34;) // // function description public record Request(String location, Unit unit) {} 最好的做法是用信息注释请求对象，以便该函数的生成JSON模式尽可能具有描述性，以帮助AI模型选择要调用的正确函数。 PaymentStatusBeanIT.java演示了这种方法。\n工具回调包装 # 注册函数的另一种方法是创建FunctionToolCallback，如下所示：\n@Configuration static class Config { @Bean public FunctionToolCallback weatherFunctionInfo() { return FunctionToolCallback.builder(\u0026#34;CurrentWeather\u0026#34;, new MockWeatherService()) // (1) function name and instance .description(\u0026#34;Get the weather in location\u0026#34;) // (2) function description .inputType(MockWeatherService.Request.class) // (3) function signature .build(); } } 它包装了第三方MockWeatherService函数，并将其注册为MistralAiChatModel的CurrentWeather函数。\n在聊天选项中指定功能 # 要让模型知道并调用CurrentWeather函数，您需要在提示请求中启用它：\nMistralAiChatModel chatModel = ... UserMessage userMessage = new UserMessage(\u0026#34;What\u0026#39;s the weather like in Paris?\u0026#34;); ChatResponse response = this.chatModel.call(new Prompt(this.userMessage, MistralAiChatOptions.builder().tools(\u0026#34;CurrentWeather\u0026#34;).build())); // Enable the function logger.info(\u0026#34;Response: {}\u0026#34;, response); 上述用户问题将触发3次对CurrentWeather功能的调用（每个城市一次），最终响应如下：\n具有提示选项的注册/调用函数 # 除了自动配置外，您还可以使用提示请求动态注册回调函数：\nMistralAiChatModel chatModel = ... UserMessage userMessage = new UserMessage(\u0026#34;What\u0026#39;s the weather like in Paris?\u0026#34;); var promptOptions = MistralAiChatOptions.builder() .toolCallbacks(List.of(FunctionToolCallback.builder(\u0026#34;CurrentWeather\u0026#34;, new MockWeatherService()) // (1) function name and instance .description(\u0026#34;Get the weather in location\u0026#34;) // (2) function description .inputType(MockWeatherService.Request.class) // (3) function signature .build())) // function code .build(); ChatResponse response = this.chatModel.call(new Prompt(this.userMessage, this.promptOptions)); 这种方法允许根据用户输入动态选择要调用的不同函数。 PaymentStatusPromptIT.java集成测试提供了一个完整的示例，说明如何使用MistralAiChatModel注册函数，并在提示请求中使用它。\n附录 # （博客）使用最新的Mistral AI API在Java和Spring AI中调用函数 # Mistral AI API函数调用流 # 下图说明了用于 函数调用的Mistral AI低级API的流程： PaymentStatusFunctionCallingIT.java提供了一个关于如何使用Mistral AI API函数调用的完整示例。\n"},{"id":7,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/%E5%A5%A5%E6%8B%89%E9%A9%ACollama/ollama%E5%87%BD%E6%95%B0%E8%B0%83%E7%94%A8%E4%B8%8D%E6%8E%A8%E8%8D%90/","title":"Ollama函数调用（不推荐）","section":"Ollama聊天","content":" Ollama函数调用（不推荐） # 您可以使用OllamaChatModel注册自定义Java函数，并让Ollama部署的模型智能地选择输出包含参数的JSON对象，以调用一个或多个已注册的函数。 Ollama API不直接调用函数；相反，模型生成JSON，您可以使用它来调用代码中的函数，并将结果返回给模型以完成对话。 作为开发人员，您需要实现一个函数，该函数接受从AI模型发送的函数调用参数，并将结果返回给模型。 SpringAI使得这一点就像定义一个返回java.util的@Bean定义一样简单。函数，并在调用ChatModel时提供bean名称作为选项。 在引擎盖下，Spring用适当的适配器代码包装POJO（函数），该适配器代码支持与AI模型的交互，从而避免编写冗长的样板代码。\n它是如何工作的 # 假设我们希望人工智能模型用它没有的信息来响应，例如，给定位置的当前温度。 我们可以为AI模型提供关于我们自己函数的元数据，它可以在处理您的提示时使用这些元数据来检索该信息。 例如，如果在处理提示期间，AI模型确定它需要关于给定位置的温度的额外信息，则它将启动服务器端生成的请求/响应交互。人工智能模型调用客户端函数。 模型客户端交互在 Spring AI函数调用流程图中进行了说明。 SpringAI大大简化了您需要编写的代码，以支持函数调用。\n快速入门 # 让我们创建一个聊天机器人，通过调用自己的函数来回答问题。 当模型需要回答“波士顿的天气怎么样？”这样的问题时，人工智能模型将调用客户端提供 我们的函数调用一些基于SaaS的天气服务API，并将天气响应返回给模型以完成对话。 以下MockWeatherService.java表示天气服务API：\npublic class MockWeatherService implements Function\u0026lt;Request, Response\u0026gt; { public enum Unit { C, F } public record Request(String location, Unit unit) {} public record Response(double temp, Unit unit) {} public Response apply(Request request) { return new Response(30.0, Unit.C); } } 将函数注册为Bean # 使用 OllamaChatModel自动配置，您可以使用多种方法在Spring上下文中将自定义函数注册为bean。 我们首先描述最友好的POJO选项。\n普通Java函数 # 在这种方法中，您可以像定义任何其他Spring托管对象一样，在应用程序上下文中定义@Bean。 在内部，Spring AI ChatModel将创建ToolCallback的实例，该实例添加了通过AI模型调用它的逻辑。\n@Configuration static class Config { @Bean @Description(\u0026#34;Get the weather in location\u0026#34;) // function description public Function\u0026lt;MockWeatherService.Request, MockWeatherService.Response\u0026gt; currentWeather() { return new MockWeatherService(); } } @Description注释是可选的，它提供了一个函数描述，帮助模型理解何时调用函数。它是一个重要的属性，可以帮助AI模型确定要调用的客户端函数。 提供函数描述的另一个选项是在MockWeatherService上使用@JsonClassDescription注释。请求：\n@Configuration static class Config { @Bean public Function\u0026lt;Request, Response\u0026gt; currentWeather() { // bean name as function name return new MockWeatherService(); } } @JsonClassDescription(\u0026#34;Get the weather in location\u0026#34;) // // function description public record Request(String location, Unit unit) {} 最好的做法是用信息注释请求对象，以便该函数的生成JSON模式尽可能具有描述性，以帮助AI模型选择要调用的正确函数。\n工具回调 # 注册函数的另一种方法是创建ToolCallback，如下所示：\n@Configuration static class Config { @Bean public FunctionToolCallback weatherFunctionInfo() { return FunctionToolCallback.builder(\u0026#34;CurrentWeather\u0026#34;, new MockWeatherService()) // (1) function name .description(\u0026#34;Get the weather in location\u0026#34;) // (2) function description .inputType(MockWeatherService.Request.class) // (3) function signature .build(); } } 它包装了第三方MockWeatherService函数，并将其注册为OllamaChatModel的CurrentWeather函数。\n在聊天选项中指定功能 # 要让模型知道并调用CurrentWeather函数，您需要在提示请求中启用它：\nOllamaChatModel chatModel = ... UserMessage userMessage = new UserMessage(\u0026#34;What\u0026#39;s the weather like in San Francisco, Tokyo, and Paris?\u0026#34;); ChatResponse response = this.chatModel.call(new Prompt(this.userMessage, OllamaOptions.builder().tools(\u0026#34;CurrentWeather\u0026#34;).build())); // Enable the function logger.info(\u0026#34;Response: {}\u0026#34;, response); 上述用户问题将触发3次对CurrentWeather功能的调用（每个城市一次），最终响应如下： OllamaFunctionCallbackIT.java测试演示了这种方法。\n具有提示选项的注册/调用函数 # 除了自动配置外，您还可以使用提示请求动态注册回调函数：\nOllamaChatModel chatModel = ... UserMessage userMessage = new UserMessage(\u0026#34;What\u0026#39;s the weather like in San Francisco, Tokyo, and Paris?\u0026#34;); var promptOptions = OllamaOptions.builder() .toolCallbacks(List.of(FunctionToolCallback.builder(\u0026#34;CurrentWeather\u0026#34;, new MockWeatherService()) // (1) function name and instance .description(\u0026#34;Get the weather in location\u0026#34;) // (2) function description .inputType(MockWeatherService.Request.class) // (3) function signature .build())) // function code .build(); ChatResponse response = this.chatModel.call(new Prompt(this.userMessage, this.promptOptions)); 这种方法允许您根据用户输入动态选择要调用的不同函数。 FunctionCallbackInPromptIT.java集成测试提供了一个完整的示例，说明如何使用OllamaChatModel注册函数，并在提示请求中使用它。\n附录： # Spring AI函数调用流 # 下图说明了OllamaChatModel函数调用的流程：\nOllamaAPI函数调用流 # 下图说明了Ollama API的流程： OllamaApiToolFunctionCallIT.java提供了一个关于如何使用Ollama API函数调用的完整示例。\n"},{"id":8,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E9%9F%B3%E9%A2%91%E5%9E%8B%E5%8F%B7/%E6%96%87%E6%9C%AC%E5%88%B0%E8%AF%AD%E9%9F%B3ttsapi/%E5%BC%80%E6%94%BE%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/","title":"OpenAI文本到语音（TTS）","section":"文本到语音（TTS）API","content":" OpenAI文本到语音（TTS） # 引言 # 音频API基于OpenAI的TTS（文本到语音）模型提供语音端点，使用户能够：\n撰写博客文章。 制作多种语言的语音。 使用流媒体提供实时音频输出。 前提条件 # 自动配置 # Spring AI为OpenAI文本到语音客户端提供Spring Boot自动配置。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件：\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-openai\u0026#39; } 语音属性 # 连接属性 # 前缀spring.ai.openai用作允许连接到openai的属性前缀。\n配置属性 # 前缀spring.ai.openai.audio.speech用作属性前缀，允许您配置openai文本到语音客户端。\n运行时选项 # OpenAiAudioSpeechOptions类提供在进行文本到语音请求时使用的选项。 例如：\nOpenAiAudioSpeechOptions speechOptions = OpenAiAudioSpeechOptions.builder() .model(\u0026#34;tts-1\u0026#34;) .voice(OpenAiAudioApi.SpeechRequest.Voice.ALLOY) .responseFormat(OpenAiAudioApi.SpeechRequest.AudioResponseFormat.MP3) .speed(1.0f) .build(); SpeechPrompt speechPrompt = new SpeechPrompt(\u0026#34;Hello, this is a text-to-speech example.\u0026#34;, speechOptions); SpeechResponse response = openAiAudioSpeechModel.call(speechPrompt); 手动配置 # 将spring ai openai依赖项添加到项目的Maven pom.xml文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件：\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-openai\u0026#39; } 接下来，创建OpenAiAudioSpeechModel：\nvar openAiAudioApi = new OpenAiAudioApi() .apiKey(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;)) .build(); var openAiAudioSpeechModel = new OpenAiAudioSpeechModel(openAiAudioApi); var speechOptions = OpenAiAudioSpeechOptions.builder() .responseFormat(OpenAiAudioApi.SpeechRequest.AudioResponseFormat.MP3) .speed(1.0f) .model(OpenAiAudioApi.TtsModel.TTS_1.value) .build(); var speechPrompt = new SpeechPrompt(\u0026#34;Hello, this is a text-to-speech example.\u0026#34;, speechOptions); SpeechResponse response = openAiAudioSpeechModel.call(speechPrompt); // Accessing metadata (rate limit info) OpenAiAudioSpeechResponseMetadata metadata = response.getMetadata(); byte[] responseAsBytes = response.getResult().getOutput(); 流式实时音频 # Speech API支持使用区块传输编码的实时音频流。这意味着在生成完整文件并使其可访问之前，可以播放音频。\nvar openAiAudioApi = new OpenAiAudioApi() .apiKey(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;)) .build(); var openAiAudioSpeechModel = new OpenAiAudioSpeechModel(openAiAudioApi); OpenAiAudioSpeechOptions speechOptions = OpenAiAudioSpeechOptions.builder() .voice(OpenAiAudioApi.SpeechRequest.Voice.ALLOY) .speed(1.0f) .responseFormat(OpenAiAudioApi.SpeechRequest.AudioResponseFormat.MP3) .model(OpenAiAudioApi.TtsModel.TTS_1.value) .build(); SpeechPrompt speechPrompt = new SpeechPrompt(\u0026#34;Today is a wonderful day to build something people love!\u0026#34;, speechOptions); Flux\u0026lt;SpeechResponse\u0026gt; responseStream = openAiAudioSpeechModel.stream(speechPrompt); 示例代码 # OpenAiSpeechModelIT.java测试提供了一些如何使用库的一般示例。 "},{"id":9,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/%E8%B0%B7%E6%AD%8Cvertexai/vertexai%E5%8F%8C%E5%AD%90%E5%BA%A7/","title":"VertexAI双子座聊天","section":"谷歌VertexAI API","content":" VertexAI双子座聊天 # Vertex AI Gemini API允许开发人员使用Gemini模型构建生成人工智能应用程序。 Gemini是一个由Google DeepMind开发的生成人工智能模型家族，专为多模态用例设计。Gemini API允许您访问Gemini 2.0 Flash和Gemini 2.0Flash Lite。 Gemini API参考\n前提条件 # 安装适用于您的操作系统的gcloud CLI。 通过运行以下命令进行身份验证。 gcloud config set project \u0026lt;PROJECT_ID\u0026gt; \u0026amp;\u0026amp; gcloud auth application-default login \u0026lt;ACCOUNT\u0026gt; 自动配置 # Spring AI为VertexAI Gemini聊天客户端提供Spring Boot自动配置。\n聊天室属性 # 前缀spring.ai.vertex.ai.gemini用作允许连接到VertexAI的特性前缀。 前缀spring.ai.vertex.ai.gemini.chat是属性前缀，允许您为VertexAI gemini聊天配置聊天模型实现。\n运行时选项 # VertexAiGeminiChatOptions.java提供模型配置，如温度、topK等。 启动时，可以使用VertexAiGeminiChatModel（api，options）构造函数或spring.ai.vertex.ai.chat.options.*属性配置默认选项。 在运行时，可以通过向Prompt调用添加新的特定于请求的选项来覆盖默认选项。\nChatResponse response = chatModel.call( new Prompt( \u0026#34;Generate the names of 5 famous pirates.\u0026#34;, VertexAiGeminiChatOptions.builder() .temperature(0.4) .build() )); 工具调用 # Vertex AI Gemini模型支持工具调用（在Google Gemini上下文中，它被称为函数调用）功能，允许模型在对话期间使用工具。\npublic class WeatherService { @Tool(description = \u0026#34;Get the weather in location\u0026#34;) public String weatherByLocation(@ToolParam(description= \u0026#34;City or state name\u0026#34;) String location) { ... } } String response = ChatClient.create(this.chatModel) .prompt(\u0026#34;What\u0026#39;s the weather like in Boston?\u0026#34;) .tools(new WeatherService()) .call() .content(); 您也可以将java.util.function bean用作工具：\n@Bean @Description(\u0026#34;Get the weather in location. Return temperature in 36°F or 36°C format.\u0026#34;) public Function\u0026lt;Request, Response\u0026gt; weatherFunction() { return new MockWeatherService(); } String response = ChatClient.create(this.chatModel) .prompt(\u0026#34;What\u0026#39;s the weather like in Boston?\u0026#34;) .tools(\u0026#34;weatherFunction\u0026#34;) .inputType(Request.class) .call() .content(); 在 工具文档中查找更多信息。\n多模态 # 多模态是指模型同时理解和处理来自各种（输入）源的信息的能力，包括文本、pdf、图像、音频和其他数据格式。\n图像、音频、视频 # 谷歌的双子座人工智能模型通过理解和集成文本、代码、音频、图像和视频来支持此功能。 Spring AI的消息接口通过引入媒体类型支持多模态AI模型。 下面是从VertexAiGeminiChatModelIT#multiModalityTest（）中提取的简单代码示例，演示了用户文本与图像的组合。\nbyte[] data = new ClassPathResource(\u0026#34;/vertex-test.png\u0026#34;).getContentAsByteArray(); var userMessage = new UserMessage(\u0026#34;Explain what do you see on this picture?\u0026#34;, List.of(new Media(MimeTypeUtils.IMAGE_PNG, this.data))); ChatResponse response = chatModel.call(new Prompt(List.of(this.userMessage))); PDF格式 # 最新的Vertex Gemini支持PDF输入类型。●●●●。\nvar pdfData = new ClassPathResource(\u0026#34;/spring-ai-reference-overview.pdf\u0026#34;); var userMessage = new UserMessage( \u0026#34;You are a very professional document summarization specialist. Please summarize the given document.\u0026#34;, List.of(new Media(new MimeType(\u0026#34;application\u0026#34;, \u0026#34;pdf\u0026#34;), pdfData))); var response = this.chatModel.call(new Prompt(List.of(userMessage))); 样本控制器 # 创建一个新的SpringBoot项目，并将Spring-ai-starter模型顶点ai-gemini添加到pom（或gradle）依赖项中。 在src/main/resources目录下添加application.properties文件，以启用和配置VertexAi聊天模型：\nspring.ai.vertex.ai.gemini.project-id=PROJECT_ID spring.ai.vertex.ai.gemini.location=LOCATION spring.ai.vertex.ai.gemini.chat.options.model=gemini-2.0-flash spring.ai.vertex.ai.gemini.chat.options.temperature=0.5 这将创建一个VertexAiGeminiChatModel实现，您可以将其注入到类中。\n@RestController public class ChatController { private final VertexAiGeminiChatModel chatModel; @Autowired public ChatController(VertexAiGeminiChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, this.chatModel.call(message)); } @GetMapping(\u0026#34;/ai/generateStream\u0026#34;) public Flux\u0026lt;ChatResponse\u0026gt; generateStream(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { Prompt prompt = new Prompt(new UserMessage(message)); return this.chatModel.stream(prompt); } } 手动配置 # VertexAiGeminiChatModel实现ChatModel.并使用VertexAI连接到Vertex AI Gemini服务。 将spring ai vertex ai gemini依赖项添加到项目的Maven pom.xml文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-vertex-ai-gemini\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-vertex-ai-gemini\u0026#39; } 接下来，创建VertexAiGeminiChatModel并将其用于文本生成：\nVertexAI vertexApi = new VertexAI(projectId, location); var chatModel = new VertexAiGeminiChatModel(this.vertexApi, VertexAiGeminiChatOptions.builder() .model(ChatModel.GEMINI_2_0_FLASH) .temperature(0.4) .build()); ChatResponse response = this.chatModel.call( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); VertexAiGeminiChatOptions提供聊天请求的配置信息。\n低级Java客户端 # 下图说明了Vertex AI Gemini本机Java API： "},{"id":10,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/%E4%BA%9A%E9%A9%AC%E9%80%8A%E5%9F%BA%E5%B2%A9/","title":"亚马逊基岩","section":"嵌入模型API","content":" 亚马逊基岩 # Amazon Bedrock是一个托管服务，提供来自各种人工智能提供商的基础模型，通过统一的API提供。 Spring AI通过实现Spring EmbeddingModel接口，支持通过Amazon Bedrock提供的嵌入AI模型。 此外，Spring AI为所有客户端提供了Spring自动配置和引导启动程序，使Bedrock模型的引导和配置变得容易。\n入门 # 有几个步骤可以开始\n将Bedrock的Spring Boot启动器添加到项目中。 获取AWS凭据：如果您尚未配置AWS帐户和AWS CLI，则本视频指南可以帮助您配置它：AWS CLI和SDK安装在4分钟内！。您应该能够获得访问密钥和安全密钥。 启用要使用的模型：转到Amazon Bedrock，从左侧的模型访问菜单中，配置对要使用的模块的访问。 项目依赖项 # 然后将Spring Boot Starter依赖项添加到项目的Maven pom.xml构建文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-bedrock\u0026lt;/artifactId\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-bedrock\u0026#39; } 连接到AWS基岩 # 使用BedrockAwsConnectionProperties配置AWS凭据和区域：\nspring.ai.bedrock.aws.region=us-east-1 spring.ai.bedrock.aws.access-key=YOUR_ACCESS_KEY spring.ai.bedrock.aws.secret-key=YOUR_SECRET_KEY spring.ai.bedrock.aws.timeout=10m 地区财产是强制性的。 AWS凭据按以下顺序解析： AWS区域按以下顺序解析： 除了标准的Spring AI Bedrock凭证和区域属性配置外，Spring AI.还支持定制的AwsCredentialsProvider和AwsRegionProvider bean。\n启用选定的基岩模型 # 以下是受支持的\u0026lt;model\u0026gt;： 例如，要启用Bedrock Cohere嵌入模型，需要设置spring.ai.kistern.Cohere.embedding.enabled=true。 接下来，您可以使用spring.ai.基岩。.embedding.*属性来配置提供的每个模型。 有关更多信息，请参阅下面每个受支持型号的文档。\nSpring AI Bedrock Cohere嵌入：Spring.AI.biters.Cohere.embedding.enabled=true 春季AI基岩泰坦嵌入：Spring.AI.kistern.Titan.embedding.enabled=true "},{"id":11,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/%E4%BA%BA%E7%B1%BB3/%E4%BA%BA%E5%B7%A5%E5%87%BD%E6%95%B0%E8%B0%83%E7%94%A8%E5%B7%B2%E5%BC%83%E7%94%A8/","title":"人工函数调用（已弃用）","section":"人类3聊天","content":" 人工函数调用（已弃用） # 您可以使用AnthropicChatModel注册自定义Java函数，并让Anthropic模型智能地选择输出包含参数的JSON对象，以调用一个或多个已注册的函数。 人工API不直接调用函数；相反，模型生成JSON，您可以使用它来调用代码中的函数，并将结果返回给模型以完成对话。 Spring AI提供灵活且用户友好的方法来注册和调用自定义函数。 作为开发人员，您需要实现一个函数，该函数接受从AI模型发送的函数调用参数，并将结果返回给模型。 SpringAI使得这一点就像定义一个返回java.util的@Bean定义一样简单。函数，并在调用ChatModel时提供bean名称作为选项。 在引擎盖下，Spring用适当的适配器代码包装POJO（函数），该适配器代码支持与AI模型的交互，从而避免编写冗长的样板代码。\n它是如何工作的 # 假设我们希望人工智能模型用它没有的信息来响应，例如给定位置的当前温度。 我们可以为AI模型提供关于我们自己函数的元数据，它可以在处理您的提示时使用这些元数据来检索该信息。 例如，如果在处理提示期间，AI模型确定它需要关于给定位置的温度的额外信息，则它将启动服务器端生成的请求/响应交互。人工智能模型调用客户端函数。 SpringAI大大简化了您需要编写的代码，以支持函数调用。\n快速入门 # 让我们创建一个聊天机器人，通过调用自己的函数来回答问题。 当对模型提示的响应需要回答诸如“波士顿的天气怎么样？”这样的问题时，人工智能模型将调用客户端，提供位置值作为要传递给函数的参数。这种类似RPC的数据作为JSON传递。 我们的函数可以使用一些基于SaaS的天气服务API，并将天气响应返回到模型以完成对话。 以下MockWeatherService.java表示天气服务API：\npublic class MockWeatherService implements Function\u0026lt;Request, Response\u0026gt; { public enum Unit { C, F } public record Request(String location, Unit unit) {} public record Response(double temp, Unit unit) {} public Response apply(Request request) { return new Response(30.0, Unit.C); } } 将函数注册为Bean # 使用AnthropicChatModel自动配置，您有多种方法可以在Spring上下文中将自定义函数注册为bean。 我们首先描述最友好的POJO选项。\n普通Java函数 # 在这种方法中，您可以像定义任何其他Spring托管对象一样，在应用程序上下文中定义@Beans。 在内部，Spring AI ChatModel将创建ToolCallback的实例，该实例添加了通过AI模型调用它的逻辑。\n@Configuration static class Config { @Bean @Description(\u0026#34;Get the weather in location\u0026#34;) // function description public Function\u0026lt;MockWeatherService.Request, MockWeatherService.Response\u0026gt; weatherFunction1() { return new MockWeatherService(); } ... } @Description注释是可选的，它提供了一个函数描述（2），帮助模型理解何时调用函数。 提供函数描述的另一个选项是在MockWeatherService上使用@JsonClassDescription注释。请求提供功能描述：\n@Configuration static class Config { @Bean public Function\u0026lt;Request, Response\u0026gt; currentWeather3() { // (1) bean name as function name. return new MockWeatherService(); } ... } @JsonClassDescription(\u0026#34;Get the weather in location\u0026#34;) // (2) function description public record Request(String location, Unit unit) {} 最好的做法是用信息注释请求对象，以便该函数的生成JSON模式尽可能具有描述性，以帮助AI模型选择要调用的正确函数。 FunctionCallWithFunctionBeanIT.java演示了这种方法。\n工具回调 # 注册函数的另一种方法是创建ToolCallback实例，如下所示：\n@Configuration static class Config { @Bean public FunctionToolCallback weatherFunctionInfo() { return FunctionToolCallback.builder(\u0026#34;CurrentWeather\u0026#34;, new MockWeatherService()) // (1) function name and instance .description(\u0026#34;Get the weather in location\u0026#34;) // (2) function description .inputType(MockWeatherService.Request.class) // (3) function signature .build(); } ... } 它包装了第三方MockWeatherService函数，并将其注册为具有AnthropicChatModel的CurrentWeather函数。\n在聊天选项中指定功能 # 要让模型知道并调用CurrentWeather函数，您需要在提示请求中启用它：\nAnthropicChatModel chatModel = ... UserMessage userMessage = new UserMessage(\u0026#34;What\u0026#39;s the weather like in Paris?\u0026#34;); ChatResponse response = this.chatModel.call(new Prompt(List.of(this.userMessage), AnthropicChatOptions.builder().toolNames(\u0026#34;CurrentWeather\u0026#34;).build())); // (1) Enable the function logger.info(\u0026#34;Response: {}\u0026#34;, response); 上述用户问题将触发3次对CurrentWeather功能的调用（每个城市一次），并产生最终响应。\n具有提示选项的注册/调用函数 # 除了自动配置外，您还可以使用提示请求动态注册回调函数：\nAnthropicChatModel chatModel = ... UserMessage userMessage = new UserMessage(\u0026#34;What\u0026#39;s the weather like in Paris?\u0026#34;); var promptOptions = AnthropicChatOptions.builder() .toolCallbacks(List.of(FunctionToolCallback.builder(\u0026#34;CurrentWeather\u0026#34;, new MockWeatherService()) // (1) function name and instance .description(\u0026#34;Get the weather in location\u0026#34;) // (2) function description .inputType(MockWeatherService.Request.class) // (3) function signature .build())) // function code .build(); ChatResponse response = this.chatModel.call(new Prompt(List.of(this.userMessage), this.promptOptions)); 这种方法允许根据用户输入动态选择要调用的不同函数。 FunctionCallWithPromptFunctionIT.java集成测试提供了一个完整的示例，说明如何使用AnthropicChatModel注册函数，并在提示请求中使用它。\n"},{"id":12,"href":"/docs/%E6%A6%82%E8%BF%B0/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A6%82%E5%BF%B5/","title":"人工智能概念","section":"引言","content":" 人工智能概念 # 本节描述Spring AI使用的核心概念。我们建议仔细阅读它，以了解Spring AI是如何实现的。\n型号 # 人工智能模型是设计用于处理和生成信息的算法，通常模仿人类的认知功能。通过从大型数据集学习模式和见解，这些模型可以做出预测、文本、图像或其他输出，增强跨行业的各种应用。 有许多不同类型的人工智能模型，每种模型都适用于特定的用例。虽然ChatGPT及其生成人工智能功能通过文本输入和输出吸引了用户，但许多模型和公司提供了不同的输入和输出。在ChatGPT之前，许多人都对文本到图像的生成模型（如Midtrivel和稳定扩散）着迷。 下表根据输入和输出类型对几个模型进行了分类： Spring AI目前支持将输入和输出处理为语言、图像和音频的模型。上一个表中的最后一行接受文本作为输入，输出数字，更常见的是嵌入文本，并表示AI模型中使用的内部数据结构。Spring AI支持嵌入，以支持更高级的用例。 将GPT等模型区分开来的是它们的预训练性质，如GPT聊天生成预训练变压器中的“P”所示。此预训练功能将AI转换为一般开发人员工具，不需要广泛的机器学习或模型训练背景。\n提示 # 提示是基于语言的输入的基础，指导人工智能模型产生特定的输出。对于熟悉ChatGPT的人来说，提示符可能看起来仅仅是输入到发送到API的对话框中的文本。然而，它包含的远不止这些。在许多AI模型中，提示的文本不仅仅是简单的字符串。 ChatGPT的API在提示中有多个文本输入，每个文本输入都被分配了一个角色。例如，有一个系统角色，它告诉模型如何行为，并设置交互的上下文。还有用户角色，通常是来自用户的输入。 制作有效的提示既是一门艺术，也是一门科学。ChatGPT是为人类对话而设计的。这与使用SQL之类的东西来“提问”有很大的不同。一个人必须与人工智能模型进行沟通，就像与另一个人交谈一样。 这种交互风格的重要性是如此之大，以至于术语“即时工程”已经成为自己的学科。有一组新兴的技术可以提高提示的有效性。在创建提示符方面投入时间可以显著提高结果输出。 共享提示已成为一种公共实践，并且正在对此主题进行积极的学术研究。作为一个例子，创建一个有效的提示可能是多么违反直觉（例如，与SQL对比），最近的一篇研究论文发现，你可以使用的最有效的提示之一是从短语“深呼吸，一步一步地工作”开始的。这应该会告诉你为什么语言如此重要。我们还没有完全理解如何最有效地使用该技术的先前迭代，如ChatGPT 3.5，更不用说正在开发的新版本了。\n提示模板 # 创建有效的提示涉及建立请求的上下文，并用特定于用户输入的值替换请求的部分。 该过程使用传统的基于文本的模板引擎来快速创建和管理。Spring AI为此使用OSS库StringTemplate。 例如，考虑简单的提示模板：\nTell me a {adjective} joke about {content}. 在SpringAI中，提示模板可以比作SpringMVC架构中的“视图”。模型对象，通常是java.util。Map，用于填充模板中的占位符。“渲染”字符串成为提供给AI模型的提示的内容。 发送到模型的提示的特定数据格式具有相当大的可变性。提示符最初是简单的字符串，现在已经发展到包括多个消息，其中每个消息中的每个字符串表示模型的不同角色。\n预埋件 # 嵌入是文本、图像或视频的数字表示，用于捕获输入之间的关系。 嵌入的工作原理是将文本、图像和视频转换为浮点数数组，称为向量。这些向量旨在捕获文本、图像和视频的含义。嵌入数组的长度称为向量的维数。 通过计算两段文本的向量表示之间的数字距离，应用程序可以确定用于生成嵌入向量的对象之间的相似性。 作为一名探索人工智能的Java开发人员，不需要理解复杂的数学理论或这些向量表示背后的特定实现。基本了解它们在人工智能系统中的作用和功能就足够了，特别是当您将人工智能功能集成到应用程序中时。 嵌入在实际应用中特别相关，如检索增强生成（RAG）模式。它们支持将数据表示为语义空间中的点，这类似于欧几里德几何的2-D空间，但具有更高的维度。这意味着，就像欧几里德几何中平面上的点如何基于它们的坐标来接近或远离一样，在语义空间中，点的接近反映了意义上的相似性。关于相似主题的句子在多维空间中的位置更近，就像图上彼此靠近的点。这种接近性有助于文本分类、语义搜索甚至产品推荐等任务，因为它允许人工智能根据相关概念在扩展的语义景观中的“位置”来识别和分组相关概念。 您可以将此语义空间视为向量。\n代币 # 代币是人工智能模型工作方式的构建块。在输入时，模型将单词转换为标记。在输出时，它们将标记转换回单词。 在英语中，一个标记大致相当于单词的75%。作为参考，莎士比亚的全部作品，总计约90万字，翻译成约120万个代币。 也许更重要的是代币=货币。在托管AI模型的上下文中，您的费用由使用的代币数量决定。输入和输出都会影响整个令牌计数。 此外，模型受到令牌限制的影响，这限制了单个API调用中处理的文本量。该阈值通常被称为“上下文窗口”。模型不处理任何超过此限制的文本。 例如，ChatGPT3具有4K代币限制，而GPT4提供不同的选项，如8K、16K和32K。人类的Claude AI模型具有100K代币限额，Meta最近的研究产生了1M代币限额模型。 要使用GPT4总结莎士比亚的作品集，您需要设计软件工程策略来分割数据，并在模型的上下文窗口限制内呈现数据。Spring AI项目可以帮助您完成这项任务。\n结构化输出 # AI模型的输出通常以java.lang.String的形式到达，即使您要求以JSON格式返回。它可能是正确的JSON，但不是JSON数据结构。它只是一个字符串。此外，在提示中询问“for JSON”不是100%准确的。 这种复杂性导致出现了一个专门的字段，涉及创建提示以产生预期的输出，然后将结果简单的字符串转换为用于应用程序集成的可用数据结构。 结构化输出转换使用精心编制的提示，通常需要与模型进行多次交互，以实现所需的格式。\n将您的数据和API引入人工智能模型 # 如何为人工智能模型配备尚未训练的信息？ 请注意，GPT 3.5/4.0数据集仅扩展到2021年9月。因此，该模型表示，它不知道在该日期之后需要知识的问题的答案。一个有趣的琐事是，该数据集约为650GB。 有三种技术可用于定制人工智能模型以合并您的数据：\n微调：这种传统的机器学习技术涉及裁剪模型并更改其内部权重。然而，对于机器学习专家来说，这是一个具有挑战性的过程，而对于GPT之类的模型，由于它们的规模，资源消耗非常大。此外，某些型号可能不提供此选项。 即时填充：一种更实用的替代方法涉及将数据嵌入到提供给模型的提示中。给定模型的令牌限制，需要技术在模型的上下文窗口中呈现相关数据。这种方法通俗地称为“填充提示符”。Spring AI库帮助您实现基于“填充提示”技术的解决方案，也称为检索增强生成（RAG）。 工具调用：该技术允许注册将大型语言模型连接到外部系统的API的工具（用户定义的服务）。SpringAI大大简化了您需要编写的代码，以支持工具调用。 检索增强生成 # 一种称为检索增强生成（RAG）的技术已经出现，以解决将相关数据合并到准确人工智能模型响应提示中的挑战。 该方法涉及批处理样式的编程模型，其中作业从文档中读取非结构化数据，对其进行转换，然后将其写入向量数据库。在高级别上，这是ETL（提取、转换和加载）管道。矢量数据库用于RAG技术的检索部分。 作为将非结构化数据加载到向量数据库的一部分，最重要的转换之一是将原始文档拆分为更小的片段。将原始文档拆分为较小的片段的过程有两个重要步骤： RAG的下一个阶段是处理用户输入。当用户的问题将由人工智能模型回答时，问题和所有“相似”的文档片段都被放在发送到人工智能模型的提示符中。这就是使用向量数据库的原因。它非常善于发现类似的内容。 ETL管道提供了有关编排从数据源提取数据并将其存储在结构化向量存储中的流的更多信息，确保数据在传递给AI模型时以最佳格式进行检索。 ChatClient-RAG解释了如何使用QuestionAnswerAdvisor在应用程序中启用RAG功能。 工具调用 # 大型语言模型（LLM）在训练后被冻结，导致知识过时，并且它们无法访问或修改外部数据。 工具调用机制解决了这些缺点。它允许您将自己的服务注册为工具，以将大型语言模型连接到外部系统的API。这些系统可以为LLM提供实时数据，并代表它们执行数据处理操作。 SpringAI大大简化了您需要编写的代码，以支持工具调用。它为您处理工具调用对话。您可以将工具作为@tool注释方法提供，并在提示选项中提供它，以使其可用于模型。此外，可以在单个提示中定义和引用多个工具。 有关如何将此功能用于不同的AI模型的更多信息，请参阅 工具调用文档。\n评估人工智能响应 # 响应用户请求有效评估人工智能系统的输出对于确保最终应用程序的准确性和有用性非常重要。有几种新兴技术能够为此目的使用预先训练的模型本身。 该评估过程涉及分析生成的响应是否与用户的意图和查询的上下文一致。相关性、一致性和事实正确性等指标用于衡量人工智能生成的响应的质量。 一种方法涉及呈现用户的请求和人工智能模型对模型的响应，查询响应是否与提供的数据一致。 此外，利用向量数据库中存储的信息作为补充数据可以加强评估过程，有助于确定响应相关性。 Spring AI项目提供了一个Evaluator API，该API目前允许访问评估模型响应的基本策略。有关更多信息，请遵循评估测试文档。\n"},{"id":13,"href":"/docs/%E5%B7%A5%E5%85%B7%E8%B0%83%E7%94%A8/%E8%BF%81%E7%A7%BB%E5%88%B0toolcallback-api/","title":"从FunctionCallback迁移到ToolCallbackAPI","section":"工具调用","content":" 从FunctionCallback迁移到ToolCallbackAPI # 本指南帮助您从弃用的FunctionCallback API迁移到Spring AI中的新ToolCallbackAPI。有关新API的更多信息，请参阅Tools Calling文档。\n变更概述 # 这些变化是Spring AI中改进和扩展工具调用功能的更广泛努力的一部分。除其他外，新API从“函数”转向“工具”术语，以更好地符合行业惯例。这涉及几个API更改，同时通过弃用的方法维护向后兼容性。\n主要变化 # 迁移示例 # 1.基本函数回调 # 之前：\nFunctionCallback.builder() .function(\u0026#34;getCurrentWeather\u0026#34;, new MockWeatherService()) .description(\u0026#34;Get the weather in location\u0026#34;) .inputType(MockWeatherService.Request.class) .build() 之后：\nFunctionToolCallback.builder(\u0026#34;getCurrentWeather\u0026#34;, new MockWeatherService()) .description(\u0026#34;Get the weather in location\u0026#34;) .inputType(MockWeatherService.Request.class) .build() 2.ChatClient使用 # 之前：\nString response = ChatClient.create(chatModel) .prompt() .user(\u0026#34;What\u0026#39;s the weather like in San Francisco?\u0026#34;) .functions(FunctionCallback.builder() .function(\u0026#34;getCurrentWeather\u0026#34;, new MockWeatherService()) .description(\u0026#34;Get the weather in location\u0026#34;) .inputType(MockWeatherService.Request.class) .build()) .call() .content(); 之后：\nString response = ChatClient.create(chatModel) .prompt() .user(\u0026#34;What\u0026#39;s the weather like in San Francisco?\u0026#34;) .tools(FunctionToolCallback.builder(\u0026#34;getCurrentWeather\u0026#34;, new MockWeatherService()) .description(\u0026#34;Get the weather in location\u0026#34;) .inputType(MockWeatherService.Request.class) .build()) .call() .content(); 3.基于方法的函数回调 # 之前：\nFunctionCallback.builder() .method(\u0026#34;getWeatherInLocation\u0026#34;, String.class, Unit.class) .description(\u0026#34;Get the weather in location\u0026#34;) .targetClass(TestFunctionClass.class) .build() 之后：\nvar toolMethod = ReflectionUtils.findMethod(TestFunctionClass.class, \u0026#34;getWeatherInLocation\u0026#34;); MethodToolCallback.builder() .toolDefinition(ToolDefinition.builder(toolMethod) .description(\u0026#34;Get the weather in location\u0026#34;) .build()) .toolMethod(toolMethod) .build() 或者使用声明式方法：\nclass WeatherTools { @Tool(description = \u0026#34;Get the weather in location\u0026#34;) public void getWeatherInLocation(String location, Unit unit) { // ... } } 并且可以使用相同的ChatClient#tools（）API来注册基于方法的工具回调：\nString response = ChatClient.create(chatModel) .prompt() .user(\u0026#34;What\u0026#39;s the weather like in San Francisco?\u0026#34;) .tools(MethodToolCallback.builder() .toolDefinition(ToolDefinition.builder(toolMethod) .description(\u0026#34;Get the weather in location\u0026#34;) .build()) .toolMethod(toolMethod) .build()) .call() .content(); 或者使用声明式方法：\nString response = ChatClient.create(chatModel) .prompt() .user(\u0026#34;What\u0026#39;s the weather like in San Francisco?\u0026#34;) .tools(new WeatherTools()) .call() .content(); 4.选项配置 # 之前：\nFunctionCallingOptions.builder() .model(modelName) .function(\u0026#34;weatherFunction\u0026#34;) .build() 之后：\nToolCallingChatOptions.builder() .model(modelName) .toolNames(\u0026#34;weatherFunction\u0026#34;) .build() 5.ChatClient Builder中的默认函数 # 之前：\nChatClient.builder(chatModel) .defaultFunctions(FunctionCallback.builder() .function(\u0026#34;getCurrentWeather\u0026#34;, new MockWeatherService()) .description(\u0026#34;Get the weather in location\u0026#34;) .inputType(MockWeatherService.Request.class) .build()) .build() 之后：\nChatClient.builder(chatModel) .defaultTools(FunctionToolCallback.builder(\u0026#34;getCurrentWeather\u0026#34;, new MockWeatherService()) .description(\u0026#34;Get the weather in location\u0026#34;) .inputType(MockWeatherService.Request.class) .build()) .build() 6.Spring Bean配置 # 之前：\n@Bean public FunctionCallback weatherFunctionInfo() { return FunctionCallback.builder() .function(\u0026#34;WeatherInfo\u0026#34;, new MockWeatherService()) .description(\u0026#34;Get the current weather\u0026#34;) .inputType(MockWeatherService.Request.class) .build(); } 之后：\n@Bean public ToolCallback weatherFunctionInfo() { return FunctionToolCallback.builder(\u0026#34;WeatherInfo\u0026#34;, new MockWeatherService()) .description(\u0026#34;Get the current weather\u0026#34;) .inputType(MockWeatherService.Request.class) .build(); } 正在中断更改 # 不推荐的方法 # 以下方法已弃用，并将在未来的版本中删除：\n聊天客户端。Builder.defaultFunctions（字符串…​) 聊天客户端。Builder.defaultFunctions（FunctionCallback…​) 聊天客户端。RequestSpec.functions（） 请使用对应的工具。 使用@Tool的声明性规范 # 现在，您可以使用方法级注释（@Tool）将工具注册到Spring AI：\nclass Home { @Tool(description = \u0026#34;Turn light On or Off in a room.\u0026#34;) void turnLight(String roomName, boolean on) { // ... logger.info(\u0026#34;Turn light in room: {} to: {}\u0026#34;, roomName, on); } } String response = ChatClient.create(this.chatModel).prompt() .user(\u0026#34;Turn the light in the living room On.\u0026#34;) .tools(new Home()) .call() .content(); 其他注释 # 时间表 # 不推荐使用的方法将在当前里程碑版本中维护向后兼容性，但将在下一个里程碑版本中删除。建议尽快迁移到新API。\n"},{"id":14,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/%E4%BA%9A%E9%A9%AC%E9%80%8A%E5%9F%BA%E5%B2%A9/%E7%9B%B8%E5%B9%B2cohere/","title":"内聚层嵌入","section":"亚马逊基岩","content":" 内聚层嵌入 # 提供基岩粘结嵌入模型。 AWS Bedrick Cohere模型页面和Amazon Bedrick用户指南包含有关如何使用AWS托管模型的详细信息。\n前提条件 # 有关设置API访问，请参阅Amazon Bedrock上的Spring AI文档。\n添加存储库和BOM表 # Spring AI工件发布在Maven Central和Spring Snapshot 存储库中。 为了帮助进行依赖关系管理，Spring AI提供了BOM（物料清单），以确保在整个项目中使用一致版本的Spring AI。请参阅依赖项管理部分，将Spring AI BOM添加到构建系统中。\n自动配置 # 将spring ai starter模型基岩依赖项添加到项目的Maven pom.xml文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-bedrock\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-bedrock\u0026#39; } 启用一致性嵌入支持 # 默认情况下，Cohere模型被禁用。\nexport SPRING_AI_MODEL_EMBEDDING=bedrock-cohere 嵌入属性 # 前缀spring.ai.bedrock.aws是用于配置与aws bedrock的连接的属性前缀。 前缀spring.ai.bedrock.cohere.embedding（在BedrockCohereEmbeddingProperties中定义）是配置cohere嵌入模型实现的属性前缀。 查看CohereEmbeddingModel以了解其他模型ID。\n运行时选项 # BedrockCohereEmbeddingOptions.java提供模型配置，如输入类型或截断。 启动时，可以使用BedrockCohereEmbeddingModel（api，options）构造函数或spring.ai.kistern.cohere.embedding.options.*属性配置默认选项。 在运行时，可以通过向EmbeddingRequest调用添加新的特定于请求的选项来覆盖默认选项。\nEmbeddingResponse embeddingResponse = embeddingModel.call( new EmbeddingRequest(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;), BedrockCohereEmbeddingOptions.builder() .withInputType(InputType.SEARCH_DOCUMENT) .build())); 样本控制器 # 创建一个新的SpringBoot项目，并将Springaistarter模型基础添加到pom（或gradle）依赖项中。 在src/main/resources目录下添加application.properties文件，以启用和配置Cohere Embedding模型：\nspring.ai.bedrock.aws.region=eu-central-1 spring.ai.bedrock.aws.access-key=${AWS_ACCESS_KEY_ID} spring.ai.bedrock.aws.secret-key=${AWS_SECRET_ACCESS_KEY} spring.ai.model.embedding=bedrock-cohere spring.ai.bedrock.cohere.embedding.options.input-type=search-document 这将创建一个BedrockCohereEmbeddingModel实现，您可以将其注入到类中。\n@RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(\u0026#34;/ai/embedding\u0026#34;) public Map embed(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { EmbeddingResponse embeddingResponse = this.embeddingModel.embedForResponse(List.of(message)); return Map.of(\u0026#34;embedding\u0026#34;, embeddingResponse); } } 手动配置 # BedrockCohereEmbeddingModel实现了嵌入模型，并使用低级CohereEmbeddingBedrockApi客户端连接到Bedrock Cohere服务。 将spring ai基岩依赖项添加到项目的Maven pom.xml文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-bedrock\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-bedrock\u0026#39; } 接下来，创建BedrockCohereEmbeddingModel，并将其用于文本嵌入：\nvar cohereEmbeddingApi =new CohereEmbeddingBedrockApi( CohereEmbeddingModel.COHERE_EMBED_MULTILINGUAL_V1.id(), EnvironmentVariableCredentialsProvider.create(), Region.US_EAST_1.id(), new ObjectMapper()); var embeddingModel = new BedrockCohereEmbeddingModel(this.cohereEmbeddingApi); EmbeddingResponse embeddingResponse = this.embeddingModel .embedForResponse(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;)); 低级别CohereEmbeddingBedrockApi客户端 # CohereEmbeddingBedrockApi提供的是AWS Bedrock Cohere命令模型之上的轻量级Java客户端。 下图说明了CohereEmbeddingBedrockApi接口和构建块： CohereEmbeddingBedrockApi支持用于单个和批嵌入计算的cohere.embed-english-v3和cohere.embed-multipular-v3模型。 下面是如何以编程方式使用api的简单片段：\nCohereEmbeddingBedrockApi api = new CohereEmbeddingBedrockApi( CohereEmbeddingModel.COHERE_EMBED_MULTILINGUAL_V1.id(), EnvironmentVariableCredentialsProvider.create(), Region.US_EAST_1.id(), new ObjectMapper()); CohereEmbeddingRequest request = new CohereEmbeddingRequest( List.of(\u0026#34;I like to eat apples\u0026#34;, \u0026#34;I like to eat oranges\u0026#34;), CohereEmbeddingRequest.InputType.search_document, CohereEmbeddingRequest.Truncate.NONE); CohereEmbeddingResponse response = this.api.embedding(this.request); "},{"id":15,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/%E5%BC%80%E6%94%BE%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/openai%E5%87%BD%E6%95%B0%E8%B0%83%E7%94%A8%E4%B8%8D%E6%8E%A8%E8%8D%90/","title":"函数调用","section":"OpenAI聊天","content":" 函数调用 # 您可以使用OpenAiChatModel注册自定义Java函数，并让OpenAI模型智能地选择输出包含参数的JSON对象，以调用一个或多个已注册的函数。 OpenAI API不直接调用函数；相反，模型生成JSON，您可以使用它来调用代码中的函数，并将结果返回给模型以完成对话。 Spring AI提供灵活且用户友好的方法来注册和调用自定义函数。 作为开发人员，您需要实现一个函数，该函数接受从AI模型发送的函数调用参数，并将结果返回给模型。您的函数可以反过来调用其他第三方服务来提供结果。 SpringAI使得这一点就像定义一个返回java.util的@Bean定义一样简单。函数，并在调用ChatModel时提供bean名称作为选项。 在引擎盖下，Spring用适当的适配器代码包装POJO（函数），该适配器代码支持与AI模型的交互，从而避免编写冗长的样板代码。\n它是如何工作的 # 假设我们希望人工智能模型用它没有的信息来响应，例如，给定位置的当前温度。 我们可以为AI模型提供关于我们自己函数的元数据，它可以在处理您的提示时使用这些元数据来检索该信息。 例如，如果在处理提示期间，AI模型确定它需要关于给定位置的温度的额外信息，则它将启动服务器端生成的请求/响应交互。人工智能模型调用客户端函数。 模型客户端交互在 Spring AI函数调用流程图中进行了说明。 SpringAI大大简化了您需要编写的代码，以支持函数调用。\n快速入门 # 让我们创建一个聊天机器人，通过调用自己的函数来回答问题。 当模型需要回答“波士顿的天气怎么样？”这样的问题时，人工智能模型将调用客户端，提供位置值作为传递给函数的参数。这种类似RPC的数据作为JSON传递。 我们的函数调用一些基于SaaS的天气服务API，并将天气响应返回给模型以完成对话。在这个例子中，我们将使用一个名为MockWeatherService的简单实现，该实现对不同位置的温度进行硬编码。 以下MockWeatherService.java表示天气服务API：\npublic class MockWeatherService implements Function\u0026lt;Request, Response\u0026gt; { public enum Unit { C, F } public record Request(String location, Unit unit) {} public record Response(double temp, Unit unit) {} public Response apply(Request request) { return new Response(30.0, Unit.C); } } 将函数注册为Bean # 使用 OpenAiChatModel自动配置，您可以使用多种方法在Spring上下文中将自定义函数注册为bean。 我们首先描述最友好的POJO选项。\n普通Java函数 # 在这种方法中，您可以像定义任何其他Spring托管对象一样，在应用程序上下文中定义@Bean。 在内部，Spring AI ChatModel将创建ToolCallback的实例，该实例添加了通过AI模型调用它的逻辑。\n@Configuration static class Config { @Bean @Description(\u0026#34;Get the weather in location\u0026#34;) // function description public Function\u0026lt;MockWeatherService.Request, MockWeatherService.Response\u0026gt; currentWeather() { return new MockWeatherService(); } } @Description注释是可选的，它提供了一个函数描述，帮助模型理解何时调用函数。它是一个重要的属性，可以帮助AI模型确定要调用的客户端函数。 提供函数描述的另一个选项是在MockWeatherService上使用@JsonClassDescription注释。请求：\n@Configuration static class Config { @Bean public Function\u0026lt;Request, Response\u0026gt; currentWeather() { // bean name as function name return new MockWeatherService(); } } @JsonClassDescription(\u0026#34;Get the weather in location\u0026#34;) // // function description public record Request(String location, Unit unit) {} 最好的做法是用信息注释请求对象，以便该函数的生成JSON模式尽可能具有描述性，以帮助AI模型选择要调用的正确函数。 FunctionCallbackWithPlainFunctionBeanIT.java演示了这种方法。\nFunctionToolCallback包装 # 注册函数的另一种方法是创建ToolCallback，如下所示：\n@Configuration static class Config { @Bean public FunctionToolCallback weatherFunctionInfo() { return FunctionToolCallback.builder(\u0026#34;CurrentWeather\u0026#34;, new MockWeatherService()) // (1) function name and instance .description(\u0026#34;Get the weather in location\u0026#34;) // (2) function description .inputType(MockWeatherService.Request.class) // (3) function input type .build(); } } 它包装了第三方MockWeatherService函数，并将其注册为OpenAiChatModel的CurrentWeather函数。\n在聊天选项中指定功能 # 要让模型知道并调用CurrentWeather函数，您需要在提示请求中启用它：\nOpenAiChatModel chatModel = ... UserMessage userMessage = new UserMessage(\u0026#34;What\u0026#39;s the weather like in San Francisco, Tokyo, and Paris?\u0026#34;); ChatResponse response = this.chatModel.call(new Prompt(this.userMessage, OpenAiChatOptions.builder().tools(\u0026#34;CurrentWeather\u0026#34;).build())); // Enable the function logger.info(\u0026#34;Response: {}\u0026#34;, response); 上述用户问题将触发3次对CurrentWeather功能的调用（每个城市一次），最终响应如下： OpenAiFunctionCallbackIT.java测试演示了这种方法。\n具有提示选项的注册/调用函数 # 除了自动配置外，您还可以使用提示请求动态注册回调函数：\nOpenAiChatModel chatModel = ... UserMessage userMessage = new UserMessage(\u0026#34;What\u0026#39;s the weather like in San Francisco, Tokyo, and Paris?\u0026#34;); var promptOptions = OpenAiChatOptions.builder() .toolCallbacks(List.of(FunctionToolCallback.builder(\u0026#34;CurrentWeather\u0026#34;, new MockWeatherService()) // (1) function name and instance .description(\u0026#34;Get the weather in location\u0026#34;) // (2) function description .inputType(MockWeatherService.Request.class) // (3) function input type .build())) // function code .build(); ChatResponse response = this.chatModel.call(new Prompt(this.userMessage, this.promptOptions)); 这种方法允许根据用户输入动态选择要调用的不同函数。 FunctionCallbackInPromptIT.java集成测试提供了一个完整的示例，说明如何使用OpenAiChatModel注册函数，并在提示请求中使用它。\n工具上下文支持 # Spring AI现在支持通过工具上下文向函数回调传递额外的上下文信息。此功能允许您提供可以在函数执行中使用的额外数据，从而增强函数调用的灵活性和功能。 作为java.util的第二个参数传入的上下文信息。双功能。ToolContext包含一个不可变的Map\u0026lt;String，Object\u0026gt;，允许您访问键值对。\n如何使用工具上下文 # 您可以在构建聊天选项时设置工具上下文，并将BiFunction用于回调：\nBiFunction\u0026lt;MockWeatherService.Request, ToolContext, MockWeatherService.Response\u0026gt; weatherFunction = (request, toolContext) -\u0026gt; { String sessionId = (String) toolContext.getContext().get(\u0026#34;sessionId\u0026#34;); String userId = (String) toolContext.getContext().get(\u0026#34;userId\u0026#34;); // Use sessionId and userId in your function logic double temperature = 0; if (request.location().contains(\u0026#34;Paris\u0026#34;)) { temperature = 15; } else if (request.location().contains(\u0026#34;Tokyo\u0026#34;)) { temperature = 10; } else if (request.location().contains(\u0026#34;San Francisco\u0026#34;)) { temperature = 30; } return new MockWeatherService.Response(temperature, 15, 20, 2, 53, 45, MockWeatherService.Unit.C); }; OpenAiChatOptions options = OpenAiChatOptions.builder() .model(OpenAiApi.ChatModel.GPT_4_O.getValue()) .toolCallbacks(List.of(FunctionToolCallback.builder(\u0026#34;getCurrentWeather\u0026#34;, this.weatherFunction) .description(\u0026#34;Get the weather in location\u0026#34;) .inputType(MockWeatherService.Request.class) .build())) .toolContext(Map.of(\u0026#34;sessionId\u0026#34;, \u0026#34;123\u0026#34;, \u0026#34;userId\u0026#34;, \u0026#34;user456\u0026#34;)) .build(); 在这个例子中，weatherFunction被定义为一个BiFunction，它将请求和工具上下文都作为参数。这允许您直接在函数逻辑中访问上下文。 然后，您可以在呼叫聊天模型时使用这些选项：\nUserMessage userMessage = new UserMessage(\u0026#34;What\u0026#39;s the weather like in San Francisco, Tokyo, and Paris?\u0026#34;); ChatResponse response = chatModel.call(new Prompt(List.of(this.userMessage), options)); 这种方法允许您将特定于会话或特定于用户的信息传递给功能，从而实现更上下文化和个性化的响应。\n附录： # Spring AI函数调用流 # 下图说明了OpenAiChatModel函数调用的流程：\nOpenAI API函数调用流 # 下图说明了OpenAI API 函数调用的流程： OpenAiApiToolFunctionCallIT.java提供了一个关于如何使用OpenAI API函数调用的完整示例。\n"},{"id":16,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/%E6%9C%80%E5%B0%8F%E6%9C%80%E5%A4%A7%E5%80%BC/minmaxfunction%E8%B0%83%E7%94%A8/","title":"函数调用","section":"MiniMax聊天","content":" 函数调用 # 您可以使用MiniMaxChatModel注册自定义Java函数，并让MiniMax模型智能地选择输出包含参数的JSON对象，以调用一个或多个已注册的函数。 MiniMax API不直接调用函数；相反，模型生成JSON，您可以使用它来调用代码中的函数，并将结果返回给模型以完成对话。 Spring AI提供灵活且用户友好的方法来注册和调用自定义函数。 作为开发人员，您需要实现一个函数，该函数接受从AI模型发送的函数调用参数，并将结果返回给模型。您的函数可以反过来调用其他第三方服务来提供结果。 SpringAI使得这一点就像定义一个返回java.util的@Bean定义一样简单。函数，并在调用ChatModel时提供bean名称作为选项。 在引擎盖下，Spring用适当的适配器代码包装POJO（函数），该适配器代码支持与AI模型的交互，从而避免编写冗长的样板代码。\n它是如何工作的 # 假设我们希望人工智能模型用它没有的信息来响应，例如给定位置的当前温度。 我们可以为AI模型提供关于我们自己函数的元数据，它可以在处理您的提示时使用这些元数据来检索该信息。 例如，如果在处理提示期间，AI模型确定它需要关于给定位置的温度的额外信息，则它将启动服务器端生成的请求/响应交互。人工智能模型调用客户端函数。 模型客户端交互在[spring-ai函数调用流]图中进行了说明。 SpringAI大大简化了您需要编写的代码，以支持函数调用。\n快速入门 # 让我们创建一个聊天机器人，通过调用自己的函数来回答问题。 当对模型提示的响应需要回答诸如“波士顿的天气怎么样？”这样的问题时，人工智能模型将调用客户端，提供位置值作为要传递给函数的参数。这种类似RPC的数据作为JSON传递。 我们的函数调用一些基于SaaS的天气服务API，并将天气响应返回给模型以完成对话。在这个例子中，我们将使用一个名为MockWeatherService的简单实现，该实现对不同位置的温度进行硬编码。 以下MockWeatherService.java表示天气服务API：\npublic class MockWeatherService implements Function\u0026lt;Request, Response\u0026gt; { public enum Unit { C, F } public record Request(String location, Unit unit) {} public record Response(double temp, Unit unit) {} public Response apply(Request request) { return new Response(30.0, Unit.C); } } 将函数注册为Bean # 使用 MiniMaxChatModel自动配置，您可以使用多种方法在Spring上下文中将自定义函数注册为bean。 我们首先描述最友好的POJO选项。\n普通Java函数 # 在这种方法中，您可以像定义任何其他Spring托管对象一样，在应用程序上下文中定义@Beans。 在内部，Spring AI ChatModel将创建ToolCallback实例的实例，该实例添加了通过AI模型调用它的逻辑。\n@Configuration static class Config { @Bean @Description(\u0026#34;Get the weather in location\u0026#34;) // function description public Function\u0026lt;MockWeatherService.Request, MockWeatherService.Response\u0026gt; weatherFunction1() { return new MockWeatherService(); } ... } @Description注释是可选的，它提供了一个函数描述（2），帮助模型理解何时调用函数。它是一个重要的属性，可以帮助AI模型确定要调用的客户端函数。 提供函数描述的另一个选项是MockWeatherService上的@JacksonDescription注释。请求提供功能描述：\n@Configuration static class Config { @Bean public Function\u0026lt;Request, Response\u0026gt; currentWeather3() { // (1) bean name as function name. return new MockWeatherService(); } ... } @JsonClassDescription(\u0026#34;Get the weather in location\u0026#34;) // (2) function description public record Request(String location, Unit unit) {} 最好的做法是用信息注释请求对象，以便该函数的生成JSON模式尽可能具有描述性，以帮助AI模型选择要调用的正确函数。 FunctionCallbackWithPlainFunctionBeanIT.java演示了这种方法。\n工具回调包装 # 注册函数的另一种方法是创建ToolCallback实例，如下所示：\n@Configuration static class Config { @Bean public FunctionToolCallback weatherFunctionInfo() { return FunctionToolCallback.builder() .function(\u0026#34;CurrentWeather\u0026#34;, new MockWeatherService()) // (1) function name and instance .description(\u0026#34;Get the weather in location\u0026#34;) // (2) function description .inputType(MockWeatherService.Request.class) // (3) function signature .build(); } ... } 它包装了第三方MockWeatherService函数，并将其注册为MiniMaxChatModel的CurrentWeather函数。\n在聊天选项中指定功能 # 要让模型知道并调用CurrentWeather函数，您需要在提示请求中启用它：\nMiniMaxChatModel chatModel = ... UserMessage userMessage = new UserMessage(\u0026#34;What\u0026#39;s the weather like in San Francisco, Tokyo, and Paris?\u0026#34;); ChatResponse response = this.chatModel.call(new Prompt(List.of(this.userMessage), MiniMaxChatOptions.builder().function(\u0026#34;CurrentWeather\u0026#34;).build())); // (1) Enable the function logger.info(\u0026#34;Response: {}\u0026#34;, response); 上述用户问题将触发3次对CurrentWeather功能的调用（每个城市一次），最终响应如下： MiniMaxFunctionCallbackIT.java测试演示了这种方法。\n具有提示选项的注册/调用函数 # 除了自动配置外，您还可以使用提示请求动态注册回调函数：\nMiniMaxChatModel chatModel = ... UserMessage userMessage = new UserMessage(\u0026#34;What\u0026#39;s the weather like in San Francisco, Tokyo, and Paris?\u0026#34;); var promptOptions = MiniMaxChatOptions.builder() .toolCallbacks(List.of(FunctionToolCallback.builder(\u0026#34;CurrentWeather\u0026#34;, new MockWeatherService()) // (1) function name and instance .description(\u0026#34;Get the weather in location\u0026#34;) // (2) function description .inputType(MockWeatherService.Request.class) // (3) function signature .build())) // function code .build(); ChatResponse response = this.chatModel.call(new Prompt(List.of(this.userMessage), this.promptOptions)); 这种方法允许根据用户输入动态选择要调用的不同函数。 FunctionCallbackInPromptIT.java集成测试提供了一个完整的示例，说明如何使用MiniMaxChatModel注册函数，并在提示请求中使用它。\n"},{"id":17,"href":"/docs/%E6%A6%82%E8%BF%B0/","title":"引言","section":"Docs","content":" 引言 # Spring AI项目旨在简化包含人工智能功能的应用程序的开发，而不会产生不必要的复杂性。 该项目从著名的Python项目（如LangChain和LlamaIndex）中获得灵感，但SpringAI不是这些项目的直接端口。 Spring AI提供了抽象，作为开发AI应用程序的基础。 Spring AI提供以下功能：\n跨聊天、文本到图像和嵌入模型的AI提供商的可移植API支持。同步和流式API选项都受支持。还可以访问模型特定的功能。 支持所有主要的人工智能模型提供商，如人类、OpenAI、微软、亚马逊、谷歌和Ollama。支持的模型类型包括： 聊天完成 嵌入 文本到图像 音频转录 文本到语音转换 适度性 结构化输出-将人工智能模型输出映射到POJO。 支持所有主要的矢量数据库提供商，如Apache Cassandra、Azure Cosmos DB、Azure Vector Search、Chroma、Elasticsearch、GemFire、MariaDB、Milvus、MongoDB Atlas、Neo4j、OpenSearch，Oracle、PostgreSQL/PGVector、PineCone、Qdrant、Redis、SAP Hana、Typesense和Weaviate。 跨向量存储提供程序的可移植API，包括一个新颖的类似SQL的元数据过滤器API。 工具/函数调用-允许模型请求执行客户端工具和函数，从而根据需要访问必要的实时信息并采取行动。 可观察性-提供对人工智能相关操作的见解。 用于数据工程的文档摄取ETL框架。 AI模型评估-帮助评估生成的内容并防止幻觉响应的实用程序。 用于AI模型和向量存储的Spring Boot自动配置和启动器。 ChatClient API-用于与AI聊天模型通信的Fluent API，习惯上类似于WebClient和RestClient APIs。 Advisors API-封装重复发生的生成AI模式，转换发送到语言模型（LLM）和从语言模型发送到LLM的数据，并提供跨各种模型和用例的可移植性。 支持聊天对话记忆和检索增强生成（RAG）。 此功能集允许您实现常见用例，如“文档问答”或“与文档聊天” 概念部分提供了人工智能概念及其在SpringAI中的表示的高级概述。 入门部分向您展示如何创建第一个AI应用程序。 "},{"id":18,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/","title":"聊天模型API","section":"弹簧AI API","content":" 聊天模型API # 聊天模型API为开发人员提供了将人工智能支持的聊天完成功能集成到其应用程序中的能力。它利用预先训练的语言模型，如GPT（Generative pre-trained Transformer），对自然语言中的用户输入生成类似人类的响应。 API通常通过向人工智能模型发送提示或部分对话来工作，人工智能模型随后根据其训练数据和对自然语言模式的理解生成对话的完成或继续。然后将完成的响应返回给应用程序，应用程序可以将其呈现给用户或用于进一步处理。 Spring AI Chat Model API被设计为一个简单、可移植的界面，用于与各种AI模型交互，允许开发人员在不同的模型之间切换，只需最少的代码更改。 此外，在输入封装提示和输出处理ChatResponse等伴生类的帮助下，Chat Model API统一了与AI模型的通信。 您可以在available implementations部分中找到有关可用实现的更多信息，并在Chat Models comparison部分中找到详细的比较。\nAPI概述 # 本节提供了Spring AI Chat Model API接口和相关类的指南。\nChat模型 # 下面是ChatModel接口定义：\npublic interface ChatModel extends Model\u0026lt;Prompt, ChatResponse\u0026gt; { default String call(String message) {...} @Override ChatResponse call(Prompt prompt); } 带有String参数的call（）方法简化了初始使用，避免了更复杂的Prompt和ChatResponse类的复杂性。\n流式聊天模型 # 下面是StreamingChatModel接口定义：\npublic interface StreamingChatModel extends StreamingModel\u0026lt;Prompt, ChatResponse\u0026gt; { default Flux\u0026lt;String\u0026gt; stream(String message) {...} @Override Flux\u0026lt;ChatResponse\u0026gt; stream(Prompt prompt); } stream（）方法采用类似于ChatModel的String或Prompt参数，但它使用反应式Flux API来流式处理响应。\n提示 # Prompt是一个ModelRequest，它封装了 消息对象和可选模型请求选项的列表。\npublic class Prompt implements ModelRequest\u0026lt;List\u0026lt;Message\u0026gt;\u0026gt; { private final List\u0026lt;Message\u0026gt; messages; private ChatOptions modelOptions; @Override public ChatOptions getOptions() {...} @Override public List\u0026lt;Message\u0026gt; getInstructions() {...} // constructors and utility methods omitted } 消息 # 消息接口封装了提示文本内容、元数据属性的集合和名为MessageType的分类。 接口定义如下：\npublic interface Content { String getText(); Map\u0026lt;String, Object\u0026gt; getMetadata(); } public interface Message extends Content { MessageType getMessageType(); } 多模式消息类型还实现了提供媒体内容对象列表的``MediaContent接口。\npublic interface MediaContent extends Content { Collection\u0026lt;Media\u0026gt; getMedia(); } 消息接口具有各种实现，这些实现对应于人工智能模型可以处理的消息类别： 聊天完成端点，根据会话角色区分消息类别，由MessageType有效映射。 例如，OpenAI识别不同对话角色（如系统、用户、功能或助手）的消息类别。 虽然术语MessageType可能意味着特定的消息格式，但在这种情况下，它有效地指定了消息在对话中发挥的作用。 对于不使用特定角色的AI模型，UserMessage``实现充当标准类别，通常表示用户生成的查询或指令。\n聊天室选项 # 表示可以传递给AI模型的选项。ChatOptions类是ModelOptions的一个子类，用于定义几个可以传递给AI模型的可移植选项。\npublic interface ChatOptions extends ModelOptions { String getModel(); Float getFrequencyPenalty(); Integer getMaxTokens(); Float getPresencePenalty(); List\u0026lt;String\u0026gt; getStopSequences(); Float getTemperature(); Integer getTopK(); Float getTopP(); ChatOptions copy(); } 此外，每个特定于模型的ChatModel/StreamingChatModel实现都可以有自己的选项，可以传递给AI模型。例如，OpenAI聊天完成模型有自己的选项，如logitBias、种子和用户。 这是一个强大的功能，允许开发人员在启动应用程序时使用模型特定的选项，然后在运行时使用Prompt请求覆盖它们。 Spring AI为配置和使用聊天模型提供了一个复杂的系统。 下面的流程图说明了Spring AI如何结合启动和运行时选项来处理聊天模型的配置和执行： 启动和运行时选项的分离允许全局配置和特定于请求的调整。\n聊天室响应 # ChatResponse类的结构如下：\npublic class ChatResponse implements ModelResponse\u0026lt;Generation\u0026gt; { private final ChatResponseMetadata chatResponseMetadata; private final List\u0026lt;Generation\u0026gt; generations; @Override public ChatResponseMetadata getMetadata() {...} @Override public List\u0026lt;Generation\u0026gt; getResults() {...} // other methods omitted } ChatResponse类保存AI模型的输出，每个生成实例包含单个提示产生的潜在多个输出之一。 ChatResponse类还携带关于人工智能模型响应的ChatResponseMetadata元数据。\n生成 # 最后，Generation类从ModelResult扩展来表示模型输出（辅助消息）和相关元数据：\npublic class Generation implements ModelResult\u0026lt;AssistantMessage\u0026gt; { private final AssistantMessage assistantMessage; private ChatGenerationMetadata chatGenerationMetadata; @Override public AssistantMessage getOutput() {...} @Override public ChatGenerationMetadata getMetadata() {...} // other methods omitted } 可用的实现 # 该图说明了统一的接口ChatModel和StreamingChatModel.用于与来自不同提供商的各种AI聊天模型交互，允许在不同AI服务之间轻松集成和切换，同时维护客户端应用程序的一致API。 OpenAI聊天完成（流媒体、多模态和功能调用支持） Microsoft Azure Open AI聊天完成（流媒体和函数调用支持） Ollama聊天完成（流媒体、多模态和功能调用支持） 拥抱面部聊天完成（无流媒体支持） 谷歌Vertex AI Gemini聊天完成（流媒体、多模态和功能调用支持） 亚马逊基岩 Mistral AI聊天完成（流媒体和功能调用支持） 人工聊天完成（流媒体和功能调用支持） 聊天模型API # SpringAIChatModelAPI构建在SpringAIGenericModelAPI之上，提供特定于聊天的抽象和实现。 "},{"id":19,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B%E6%AF%94%E8%BE%83/","title":"聊天模型比较","section":"聊天模型API","content":" 聊天模型比较 # 该表比较了Spring AI支持的各种聊天模型，详细介绍了它们的功能：\n多模态：模型可以处理的输入类型（例如，文本、图像、音频、视频）。 工具/函数调用：模型是否支持函数调用或工具使用。 流式处理：如果模型提供流式响应。 重试：支持重试机制。 可观察性：用于监视和调试的功能。 内置JSON：对JSON输出的本机支持。 本地部署：模型是否可以在本地运行。 OpenAI API兼容性：如果模型与OpenAI的API兼容。 "},{"id":20,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E9%9F%B3%E9%A2%91%E5%9E%8B%E5%8F%B7/%E8%BD%AC%E5%BD%95api/","title":"转录API","section":"弹簧AI API","content":" 转录API # Spring AI为OpenAI的Transcription API提供支持。\n"},{"id":21,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E9%80%82%E5%BA%A6%E5%8C%96%E6%A8%A1%E5%9E%8B/%E5%BC%80%E6%94%BE%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/","title":"适度性","section":"弹簧AI API","content":" 适度性 # 引言 # Spring AI支持OpenAI的Moderation模型，该模型允许您检测文本中潜在的有害或敏感内容。\n前提条件 # 自动配置 # Spring AI为OpenAI文本到语音客户端提供Spring Boot自动配置。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件：\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-openai\u0026#39; } 仲裁属性 # 连接属性 # 前缀spring.ai.openai用作允许连接到openai的属性前缀。\n配置属性 # 前缀spring.ai.openai.moderation用作配置openai调节模型的属性前缀。\n运行时选项 # OpenAiModerationOptions类提供在发出审核请求时使用的选项。 例如：\nOpenAiModerationOptions moderationOptions = OpenAiModerationOptions.builder() .model(\u0026#34;text-moderation-latest\u0026#34;) .build(); ModerationPrompt moderationPrompt = new ModerationPrompt(\u0026#34;Text to be moderated\u0026#34;, this.moderationOptions); ModerationResponse response = openAiModerationModel.call(this.moderationPrompt); // Access the moderation results Moderation moderation = moderationResponse.getResult().getOutput(); // Print general information System.out.println(\u0026#34;Moderation ID: \u0026#34; + moderation.getId()); System.out.println(\u0026#34;Model used: \u0026#34; + moderation.getModel()); // Access the moderation results (there\u0026#39;s usually only one, but it\u0026#39;s a list) for (ModerationResult result : moderation.getResults()) { System.out.println(\u0026#34;\\nModeration Result:\u0026#34;); System.out.println(\u0026#34;Flagged: \u0026#34; + result.isFlagged()); // Access categories Categories categories = this.result.getCategories(); System.out.println(\u0026#34;\\nCategories:\u0026#34;); System.out.println(\u0026#34;Sexual: \u0026#34; + categories.isSexual()); System.out.println(\u0026#34;Hate: \u0026#34; + categories.isHate()); System.out.println(\u0026#34;Harassment: \u0026#34; + categories.isHarassment()); System.out.println(\u0026#34;Self-Harm: \u0026#34; + categories.isSelfHarm()); System.out.println(\u0026#34;Sexual/Minors: \u0026#34; + categories.isSexualMinors()); System.out.println(\u0026#34;Hate/Threatening: \u0026#34; + categories.isHateThreatening()); System.out.println(\u0026#34;Violence/Graphic: \u0026#34; + categories.isViolenceGraphic()); System.out.println(\u0026#34;Self-Harm/Intent: \u0026#34; + categories.isSelfHarmIntent()); System.out.println(\u0026#34;Self-Harm/Instructions: \u0026#34; + categories.isSelfHarmInstructions()); System.out.println(\u0026#34;Harassment/Threatening: \u0026#34; + categories.isHarassmentThreatening()); System.out.println(\u0026#34;Violence: \u0026#34; + categories.isViolence()); // Access category scores CategoryScores scores = this.result.getCategoryScores(); System.out.println(\u0026#34;\\nCategory Scores:\u0026#34;); System.out.println(\u0026#34;Sexual: \u0026#34; + scores.getSexual()); System.out.println(\u0026#34;Hate: \u0026#34; + scores.getHate()); System.out.println(\u0026#34;Harassment: \u0026#34; + scores.getHarassment()); System.out.println(\u0026#34;Self-Harm: \u0026#34; + scores.getSelfHarm()); System.out.println(\u0026#34;Sexual/Minors: \u0026#34; + scores.getSexualMinors()); System.out.println(\u0026#34;Hate/Threatening: \u0026#34; + scores.getHateThreatening()); System.out.println(\u0026#34;Violence/Graphic: \u0026#34; + scores.getViolenceGraphic()); System.out.println(\u0026#34;Self-Harm/Intent: \u0026#34; + scores.getSelfHarmIntent()); System.out.println(\u0026#34;Self-Harm/Instructions: \u0026#34; + scores.getSelfHarmInstructions()); System.out.println(\u0026#34;Harassment/Threatening: \u0026#34; + scores.getHarassmentThreatening()); System.out.println(\u0026#34;Violence: \u0026#34; + scores.getViolence()); } 手动配置 # 将spring ai openai依赖项添加到项目的Maven pom.xml文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件：\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-openai\u0026#39; } 接下来，创建OpenAiModerationModel：\nOpenAiModerationApi openAiModerationApi = new OpenAiModerationApi(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;)); OpenAiModerationModel openAiModerationModel = new OpenAiModerationModel(this.openAiModerationApi); OpenAiModerationOptions moderationOptions = OpenAiModerationOptions.builder() .model(\u0026#34;text-moderation-latest\u0026#34;) .build(); ModerationPrompt moderationPrompt = new ModerationPrompt(\u0026#34;Text to be moderated\u0026#34;, this.moderationOptions); ModerationResponse response = this.openAiModerationModel.call(this.moderationPrompt); 示例代码 # OpenAiModerationModelIT测试提供了一些如何使用库的一般示例。您可以参考此测试以了解更详细的使用示例。\n"},{"id":22,"href":"/docs/%E8%81%8A%E5%A4%A9%E5%AE%A2%E6%88%B7%E7%AB%AFapi/%E9%A1%BE%E9%97%AE/","title":"顾问API","section":"聊天客户端API","content":" 顾问API # Spring AI Advisors API提供了一种灵活而强大的方法来拦截、修改和增强Spring应用程序中的AI驱动交互。通过利用Advisors API，开发人员可以创建更复杂、可重用和可维护的AI组件。 关键好处包括封装重复的生成人工智能模式，转换发送到大型语言模型（LLM）的数据，以及提供跨各种模型和用例的可移植性。 您可以使用 ChatClient API配置现有顾问，如下例所示： 建议在构建时使用构建器的defaultAdvisors（）方法注册顾问。 顾问还参与Observability堆栈，因此您可以查看与其执行相关的度量和跟踪。 了解问答顾问\n核心组件 # 该API由用于非流式方案的CallAroundAdvisor和CallArroundAdvisorChain组成，以及用于流式场景的StreamAroundadisor和StreamArroundadvisorChain。它还包括AdvisedRequest来表示未密封的提示请求，AdvisedResponse用于聊天完成响应。两者都持有建议上下文，以便在顾问链中共享状态。 nextAroundCall（）和nextArroundStream（）是关键的advisor方法，通常执行操作，如检查未密封的提示数据、自定义和增强提示数据、调用advisor链中的下一个实体、可选地阻止请求、检查聊天完成响应以及抛出异常以指示处理错误。 此外，getOrder（）方法确定链中的顾问顺序，而getName（）提供唯一的顾问名称。 由Spring AI框架创建的Advisor链允许按其getOrder（）值排序的多个顾问的顺序调用。首先执行较低的值。自动添加的最后一个advisor将请求发送到LLM。 以下流程图说明了顾问链和聊天模型之间的交互： 顾问订单 # 链中顾问的执行顺序由getOrder（）方法确定。理解要点：\n首先执行具有较低阶值的顾问。 advisor链作为堆栈运行： 链中的第一个顾问是第一个处理请求的顾问。 它也是最后一个处理响应的。 控制执行顺序： 将订单设置为接近Ordered。HIGHEST_PRECEDENCE，确保在链中首先执行顾问（首先用于请求处理，最后用于响应处理）。 将订单设置为接近Ordered。LOWEST_PRECEDENCE，确保advisor在链中的最后一个执行（最后一个用于请求处理，首先用于响应处理）。 较高的值被解释为较低的优先级。 如果多个顾问具有相同的顺序值，则不能保证其执行顺序。 作为提醒，这里是Spring Ordered接口的语义： public interface Ordered { /** * Constant for the highest precedence value. * @see java.lang.Integer#MIN_VALUE */ int HIGHEST_PRECEDENCE = Integer.MIN_VALUE; /** * Constant for the lowest precedence value. * @see java.lang.Integer#MAX_VALUE */ int LOWEST_PRECEDENCE = Integer.MAX_VALUE; /** * Get the order value of this object. * \u0026lt;p\u0026gt;Higher values are interpreted as lower priority. As a consequence, * the object with the lowest value has the highest priority (somewhat * analogous to Servlet {@code load-on-startup} values). * \u0026lt;p\u0026gt;Same order values will result in arbitrary sort positions for the * affected objects. * @return the order value * @see #HIGHEST_PRECEDENCE * @see #LOWEST_PRECEDENCE */ int getOrder(); } API概述 # 主Advisor接口位于包org.springframework.ai.chat.client.Advisor.api中。以下是创建自己的顾问时将遇到的关键界面：\npublic interface Advisor extends Ordered { String getName(); } 同步和反应式Advisor的两个子接口是\npublic interface CallAroundAdvisor extends Advisor { /** * Around advice that wraps the ChatModel#call(Prompt) method. * @param advisedRequest the advised request * @param chain the advisor chain * @return the response */ AdvisedResponse aroundCall(AdvisedRequest advisedRequest, CallAroundAdvisorChain chain); } 和\npublic interface StreamAroundAdvisor extends Advisor { /** * Around advice that wraps the invocation of the advised request. * @param advisedRequest the advised request * @param chain the chain of advisors to execute * @return the result of the advised request */ Flux\u0026lt;AdvisedResponse\u0026gt; aroundStream(AdvisedRequest advisedRequest, StreamAroundAdvisorChain chain); } 要继续Advice链，请在Advice实现中使用CallAroundAdvisorChain和StreamAroundAgisorChain: 接口包括\npublic interface CallAroundAdvisorChain { AdvisedResponse nextAroundCall(AdvisedRequest advisedRequest); } 和\npublic interface StreamAroundAdvisorChain { Flux\u0026lt;AdvisedResponse\u0026gt; nextAroundStream(AdvisedRequest advisedRequest); } 实施顾问 # 要创建advisor，请实现CallAroundAdvisor或StreamAroundAppisor（或两者）。要实现的关键方法是用于非流式处理的nextAroundCall（），或用于流式处理顾问的nexdAroundStream（）。\n示例 # 我们将提供几个动手的例子来说明如何实现用于观察和增强用例的顾问。\n日志记录顾问 # 我们可以实现一个简单的日志记录顾问，在调用链中的下一个顾问之前记录AdvisedRequest，在调用之后记录AdviseResponse。请注意，顾问仅观察请求和响应，而不会修改它们。该实现支持非流式和流式方案。\npublic class SimpleLoggerAdvisor implements CallAroundAdvisor, StreamAroundAdvisor { private static final Logger logger = LoggerFactory.getLogger(SimpleLoggerAdvisor.class); @Override public String getName() { (1) return this.getClass().getSimpleName(); } @Override public int getOrder() { (2) return 0; } @Override public AdvisedResponse aroundCall(AdvisedRequest advisedRequest, CallAroundAdvisorChain chain) { logger.debug(\u0026#34;BEFORE: {}\u0026#34;, advisedRequest); AdvisedResponse advisedResponse = chain.nextAroundCall(advisedRequest); logger.debug(\u0026#34;AFTER: {}\u0026#34;, advisedResponse); return advisedResponse; } @Override public Flux\u0026lt;AdvisedResponse\u0026gt; aroundStream(AdvisedRequest advisedRequest, StreamAroundAdvisorChain chain) { logger.debug(\u0026#34;BEFORE: {}\u0026#34;, advisedRequest); Flux\u0026lt;AdvisedResponse\u0026gt; advisedResponses = chain.nextAroundStream(advisedRequest); return new MessageAggregator().aggregateAdvisedResponse(advisedResponses, advisedResponse -\u0026gt; logger.debug(\u0026#34;AFTER: {}\u0026#34;, advisedResponse)); (3) } } 重新阅读（Re2）顾问 # “重读改进大型语言模型中的推理”一文介绍了一种称为重读（Re2）的技术，该技术提高了大型语言模型的推理能力。Re2技术需要像这样增加输入提示： 实现将Re2技术应用于用户输入查询的顾问可以这样做：\npublic class ReReadingAdvisor implements CallAroundAdvisor, StreamAroundAdvisor { private AdvisedRequest before(AdvisedRequest advisedRequest) { (1) Map\u0026lt;String, Object\u0026gt; advisedUserParams = new HashMap\u0026lt;\u0026gt;(advisedRequest.userParams()); advisedUserParams.put(\u0026#34;re2_input_query\u0026#34;, advisedRequest.userText()); return AdvisedRequest.from(advisedRequest) .userText(\u0026#34;\u0026#34;\u0026#34; {re2_input_query} Read the question again: {re2_input_query} \u0026#34;\u0026#34;\u0026#34;) .userParams(advisedUserParams) .build(); } @Override public AdvisedResponse aroundCall(AdvisedRequest advisedRequest, CallAroundAdvisorChain chain) { (2) return chain.nextAroundCall(this.before(advisedRequest)); } @Override public Flux\u0026lt;AdvisedResponse\u0026gt; aroundStream(AdvisedRequest advisedRequest, StreamAroundAdvisorChain chain) { (3) return chain.nextAroundStream(this.before(advisedRequest)); } @Override public int getOrder() { (4) return 0; } @Override public String getName() { (5) return this.getClass().getSimpleName(); } } Spring AI内置顾问 # Spring AI框架提供了几个内置的顾问来增强您的AI交互。下面是可用顾问的概述：\n聊天记忆顾问 # 这些顾问在聊天记忆存储中管理对话历史记录：\n消息聊天记忆顾问 PromptChatMemoryAdvisor（提示聊天记忆顾问） VectorStoreChartMemoryAdvisor 问答顾问 # 问题解答顾问 内容安全顾问 # 安全防护顾问 流媒体与非流媒体 # 非流式处理顾问使用完整的请求和响应。 流顾问使用反应式编程概念（例如，响应的Flux）将请求和响应作为连续流处理。 @Override public Flux\u0026lt;AdvisedResponse\u0026gt; aroundStream(AdvisedRequest advisedRequest, StreamAroundAdvisorChain chain) { return Mono.just(advisedRequest) .publishOn(Schedulers.boundedElastic()) .map(request -\u0026gt; { // This can be executed by blocking and non-blocking Threads. // Advisor before next section }) .flatMapMany(request -\u0026gt; chain.nextAroundStream(request)) .map(response -\u0026gt; { // Advisor after next section }); } 最佳实践 # 向后兼容性 # 中断API更改 # Spring AI Advisor链经历了从1.0 M2到1.0 M3的重大变化。以下是关键修改：\nAdvisor接口 # 在1.0 M2中，有单独的RequestAdvisor和ResponseAdvisor接口。 在ChatModel.call和ChatModel.stream方法之前调用了RequestAdvisor。 在这些方法之后调用了ResponseAdvisor。 在1.0 M3中，这些接口已替换为： 呼叫顾问 StreamAroundAdvisor StreamResponseMode（以前是Response Advisor的一部分）已被删除。 上下文映射处理 # 1.0平方米： 上下文映射是一个单独的方法参数。 地图是可变的，沿着链条传递。 在1.0 M3中： 上下文映射现在是AdvisedRequest和AdvisedResponse记录的一部分。 地图是不可变的。 要更新上下文，请使用updateContext方法，该方法使用更新的内容创建一个新的不可修改的映射。 更新1.0 M3中的上下文的示例： @Override public AdvisedResponse aroundCall(AdvisedRequest advisedRequest, CallAroundAdvisorChain chain) { this.advisedRequest = advisedRequest.updateContext(context -\u0026gt; { context.put(\u0026#34;aroundCallBefore\u0026#34; + getName(), \u0026#34;AROUND_CALL_BEFORE \u0026#34; + getName()); // Add multiple key-value pairs context.put(\u0026#34;lastBefore\u0026#34;, getName()); // Add a single key-value pair return context; }); // Method implementation continues... } "},{"id":23,"href":"/docs/%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/azure-cosmos%E6%95%B0%E6%8D%AE%E5%BA%93/","title":"Azure Cosmos数据库","section":"向量数据库","content":" Azure Cosmos数据库 # 本节将指导您设置CosmosDBVectorStore以存储文档嵌入并执行相似性搜索。\n什么是Azure Cosmos数据库？ # Azure Cosmos DB是微软的全球分布式云本机数据库服务，专为关键任务应用程序设计。\n什么是DiskANN？ # DiskANN（基于磁盘的近似最近邻搜索）是Azure Cosmos DB中使用的一种创新技术，用于提高向量搜索的性能。 DiskANN具有以下优势：\n效率：与传统方法相比，通过利用基于磁盘的结构，DiskANN显著减少了查找最近邻居所需的时间。 可扩展性：它可以处理超过内存容量的大型数据集，使其适合各种应用，包括机器学习和人工智能驱动的解决方案。 低延迟：DiskANN最大限度地减少了搜索操作期间的延迟，确保应用程序即使有大量数据也可以快速检索结果。 在Azure Cosmos DB的Spring AI上下文中，向量搜索将创建并利用DiskANN索引，以确保相似性查询的最佳性能。 使用自动配置设置Azure Cosmos DB Vector Store # 以下代码演示如何使用自动配置设置CosmosDBVectorStore：\npackage com.example.demo; import io.micrometer.observation.ObservationRegistry; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import org.springframework.ai.document.Document; import org.springframework.ai.vectorstore.SearchRequest; import org.springframework.ai.vectorstore.VectorStore; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.autoconfigure.EnableAutoConfiguration; import org.springframework.boot.CommandLineRunner; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Lazy; import java.util.List; import java.util.Map; import java.util.UUID; import static org.assertj.core.api.Assertions.assertThat; @SpringBootApplication @EnableAutoConfiguration public class DemoApplication implements CommandLineRunner { private static final Logger log = LoggerFactory.getLogger(DemoApplication.class); @Lazy @Autowired private VectorStore vectorStore; public static void main(String[] args) { SpringApplication.run(DemoApplication.class, args); } @Override public void run(String... args) throws Exception { Document document1 = new Document(UUID.randomUUID().toString(), \u0026#34;Sample content1\u0026#34;, Map.of(\u0026#34;key1\u0026#34;, \u0026#34;value1\u0026#34;)); Document document2 = new Document(UUID.randomUUID().toString(), \u0026#34;Sample content2\u0026#34;, Map.of(\u0026#34;key2\u0026#34;, \u0026#34;value2\u0026#34;)); this.vectorStore.add(List.of(document1, document2)); List\u0026lt;Document\u0026gt; results = this.vectorStore.similaritySearch(SearchRequest.builder().query(\u0026#34;Sample content\u0026#34;).topK(1).build()); log.info(\u0026#34;Search results: {}\u0026#34;, results); // Remove the documents from the vector store this.vectorStore.delete(List.of(document1.getId(), document2.getId())); } @Bean public ObservationRegistry observationRegistry() { return ObservationRegistry.create(); } } 自动配置 # 将以下依赖项添加到Maven项目：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-azure-cosmos-db\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 配置属性 # 以下配置属性可用于Cosmos DB矢量存储：\n使用筛选器的复杂搜索 # 可以使用Cosmos DB向量存储中的过滤器执行更复杂的搜索。\nMap\u0026lt;String, Object\u0026gt; metadata1 = new HashMap\u0026lt;\u0026gt;(); metadata1.put(\u0026#34;country\u0026#34;, \u0026#34;UK\u0026#34;); metadata1.put(\u0026#34;year\u0026#34;, 2021); metadata1.put(\u0026#34;city\u0026#34;, \u0026#34;London\u0026#34;); Map\u0026lt;String, Object\u0026gt; metadata2 = new HashMap\u0026lt;\u0026gt;(); metadata2.put(\u0026#34;country\u0026#34;, \u0026#34;NL\u0026#34;); metadata2.put(\u0026#34;year\u0026#34;, 2022); metadata2.put(\u0026#34;city\u0026#34;, \u0026#34;Amsterdam\u0026#34;); Document document1 = new Document(\u0026#34;1\u0026#34;, \u0026#34;A document about the UK\u0026#34;, this.metadata1); Document document2 = new Document(\u0026#34;2\u0026#34;, \u0026#34;A document about the Netherlands\u0026#34;, this.metadata2); vectorStore.add(List.of(document1, document2)); FilterExpressionBuilder builder = new FilterExpressionBuilder(); List\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch(SearchRequest.builder().query(\u0026#34;The World\u0026#34;) .topK(10) .filterExpression((this.builder.in(\u0026#34;country\u0026#34;, \u0026#34;UK\u0026#34;, \u0026#34;NL\u0026#34;)).build()).build()); 在不自动配置的情况下设置Azure Cosmos DB矢量存储 # 下面的代码演示如何在不依赖自动配置的情况下设置CosmosDBVectorStore。建议使用[DefaultAzureCredential]（learn.microsoft.com/azure/developer/java/sdk/authentication/credential-chains#DefaultAzureCredential overview）对azure Cosmos DB进行身份验证。\n@Bean public VectorStore vectorStore(ObservationRegistry observationRegistry) { // Create the Cosmos DB client CosmosAsyncClient cosmosClient = new CosmosClientBuilder() .endpoint(System.getenv(\u0026#34;COSMOSDB_AI_ENDPOINT\u0026#34;)) .credential(new DefaultAzureCredentialBuilder().build()) .userAgentSuffix(\u0026#34;SpringAI-CDBNoSQL-VectorStore\u0026#34;) .gatewayMode() .buildAsyncClient(); // Create and configure the vector store return CosmosDBVectorStore.builder(cosmosClient, embeddingModel) .databaseName(\u0026#34;test-database\u0026#34;) .containerName(\u0026#34;test-container\u0026#34;) // Configure metadata fields for filtering .metadataFields(List.of(\u0026#34;country\u0026#34;, \u0026#34;year\u0026#34;, \u0026#34;city\u0026#34;)) // Set the partition key path (optional) .partitionKeyPath(\u0026#34;/id\u0026#34;) // Configure performance settings .vectorStoreThroughput(1000) .vectorDimensions(1536) // Match your embedding model\u0026#39;s dimensions // Add custom batching strategy (optional) .batchingStrategy(new TokenCountBatchingStrategy()) // Add observation registry for metrics .observationRegistry(observationRegistry) .build(); } @Bean public EmbeddingModel embeddingModel() { return new TransformersEmbeddingModel(); } 此配置显示所有可用的生成器选项：\ndatabaseName：Cosmos DB数据库的名称 containerName：数据库中容器的名称 partitionKeyPath：分区键的路径（例如，“/id”） metadataFields：将用于筛选的元数据字段的列表 vectorStoreThroughput：向量存储容器的吞吐量（RU/s） vectorDimensions：向量的维数（应该与嵌入模型匹配） batchingStrategy：批处理文档操作的策略（可选） 手动相关性设置 # 在Maven项目中添加以下依赖项：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-azure-cosmos-db-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 访问本机客户端 # Azure Cosmos DB Vector Store实现通过getNativeClient（）方法提供对底层本机Azure Commos DB客户端（CosmosClient）的访问：\nCosmosDBVectorStore vectorStore = context.getBean(CosmosDBVectorStore.class); Optional\u0026lt;CosmosClient\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { CosmosClient client = nativeClient.get(); // Use the native client for Azure Cosmos DB-specific operations } 本机客户端允许您访问Azure Cosmos DB特定的功能和操作，这些功能和操作可能不会通过VectorStore界面公开。\n"},{"id":24,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/azure-openai%E8%BD%AF%E4%BB%B6/","title":"Azure OpenAI嵌入","section":"嵌入模型API","content":" Azure OpenAI嵌入 # Azure的OpenAI扩展了OpenAI功能，为各种任务提供安全的文本生成和嵌入计算模型： Azure OpenAI嵌入依赖于余弦相似性来计算文档和查询之间的相似性。\n前提条件 # Azure OpenAI客户端提供三种连接选项：使用Azure API密钥或使用OpenAI API密钥，或使用Microsoft Entra ID。\nAzure API密钥和终结点 # 从 Azure门户上的Azure OpenAI服务部分获取Azure OpenAI端点和api密钥。 Spring AI定义了两个配置属性： 可以通过导出环境变量来设置这些配置属性：\nexport SPRING_AI_AZURE_OPENAI_API_KEY=\u0026lt;INSERT AZURE KEY HERE\u0026gt; export SPRING_AI_AZURE_OPENAI_ENDPOINT=\u0026lt;INSERT ENDPOINT URL HERE\u0026gt; OpenAI密钥 # 要使用OpenAI服务（而不是Azure）进行身份验证，请提供OpenAI API密钥。这将自动将端点设置为 api.openai.com/v1。 使用此方法时，将spring.ai.azure.openai.chat.options.deployment-name属性设置为要使用的openai模型的名称。\nexport SPRING_AI_AZURE_OPENAI_OPENAI_API_KEY=\u0026lt;INSERT OPENAI KEY HERE\u0026gt; Microsoft条目ID # 对于使用Microsoft Entra ID（以前称为Azure Active Directory）的无密钥身份验证，请仅设置spring.ai.Azure.openai.endpoint配置属性，而不是上面提到的api密钥属性。 仅查找端点属性，应用程序将评估用于检索凭据的几个不同选项，并将使用令牌凭据创建OpenAIClient实例。\n添加存储库和BOM表 # Spring AI工件发布在Maven Central和Spring Snapshot 存储库中。 为了帮助进行依赖关系管理，Spring AI提供了BOM（物料清单），以确保在整个项目中使用一致版本的Spring AI。请参阅依赖项管理部分，将Spring AI BOM添加到构建系统中。\n自动配置 # Spring AI为Azure OpenAI嵌入模型提供Spring Boot自动配置。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-azure-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-azure-openai\u0026#39; } 嵌入属性 # 前缀spring.ai.azure.openai是配置与azure openai的连接的属性前缀。 前缀spring.ai.azure.openai.embedding是配置azure openai的EmbeddingModel实现的属性前缀\n运行时选项 # AzureOpenAiEmbeddingOptions提供嵌入请求的配置信息。 在启动时，使用AzureOpenAiEmbeddingModel构造函数设置用于所有嵌入请求的默认选项。 例如，要覆盖特定请求的默认模型名称：\nEmbeddingResponse embeddingResponse = embeddingModel.call( new EmbeddingRequest(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;), AzureOpenAiEmbeddingOptions.builder() .model(\u0026#34;Different-Embedding-Model-Deployment-Name\u0026#34;) .build())); 示例代码 # 这将创建一个可以注入到类中的EmbeddingModel实现。\nspring.ai.azure.openai.api-key=YOUR_API_KEY spring.ai.azure.openai.endpoint=YOUR_ENDPOINT spring.ai.azure.openai.embedding.options.model=text-embedding-ada-002 @RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(\u0026#34;/ai/embedding\u0026#34;) public Map embed(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { EmbeddingResponse embeddingResponse = this.embeddingModel.embedForResponse(List.of(message)); return Map.of(\u0026#34;embedding\u0026#34;, embeddingResponse); } } 手动配置 # 如果您不喜欢使用Spring Boot自动配置，则可以在应用程序中手动配置AzureOpenAiEmbeddingModel。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-azure-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-azure-openai\u0026#39; } 接下来，创建AzureOpenAiEmbeddingModel实例，并使用它计算两个输入文本之间的相似性：\nvar openAIClient = OpenAIClientBuilder() .credential(new AzureKeyCredential(System.getenv(\u0026#34;AZURE_OPENAI_API_KEY\u0026#34;))) .endpoint(System.getenv(\u0026#34;AZURE_OPENAI_ENDPOINT\u0026#34;)) .buildClient(); var embeddingModel = new AzureOpenAiEmbeddingModel(this.openAIClient) .withDefaultOptions(AzureOpenAiEmbeddingOptions.builder() .model(\u0026#34;text-embedding-ada-002\u0026#34;) .user(\u0026#34;user-6\u0026#34;) .build()); EmbeddingResponse embeddingResponse = this.embeddingModel .embedForResponse(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;)); "},{"id":25,"href":"/docs/%E6%A8%A1%E5%9E%8B%E4%B8%8A%E4%B8%8B%E6%96%87%E5%8D%8F%E8%AE%AEmcp/mcp%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%90%AF%E5%8A%A8%E5%90%AF%E5%8A%A8%E7%A8%8B%E5%BA%8F/","title":"MCP服务器引导启动程序","section":"模型上下文协议（MCP）","content":" MCP服务器引导启动程序 # Spring AI MCP（模型上下文协议）服务器引导启动程序为在Spring Boot应用程序中设置MCP服务器提供自动配置。它能够将MCP服务器功能与Spring Boot的自动配置系统无缝集成。 MCP Server Boot Starter提供：\n启动器 # 根据您的运输要求，选择以下启动器之一：\n标准MCP服务器 # 完整的MCP服务器功能支持STDIO服务器传输。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-mcp-server-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 适用于命令行和桌面工具 不需要其他web依赖项 启动器激活McpServerAutoConfiguration自动配置，负责： 配置基本服务器组件 处理工具、资源和提示规范 管理服务器功能和更改通知 提供同步和异步服务器实现 WebMVC服务器传输 # 完整的MCP服务器功能支持基于Spring MVC的SSE（服务器发送事件）服务器传输和可选的STDIO传输。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-mcp-server-webmvc\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 启动器激活McpWebMvcServerAutoConfiguration和McpServerAutoConfiguration自动配置，以提供：\n使用Spring MVC（WebMvcSseServerTransportProvider）进行基于HTTP的传输 自动配置的SSE终结点 可选STDIO传输（通过设置spring.ai.mcp.server.STDIO=true启用） 包括spring boot starter web和mcp spring webmvc依赖项 WebFlux服务器传输 # 完整的MCP服务器功能支持基于Spring WebFlux的SSE（服务器发送事件）服务器传输和可选的STDIO传输。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-mcp-server-webflux\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 启动器激活McpWebFluxServerAutoConfiguration和McpServerAutoConfiguration自动配置，以提供：\n使用Spring WebFlux的响应式传输（WebFluxSseServerTransportProvider） 自动配置的被动SSE终结点 可选STDIO传输（通过设置spring.ai.mcp.server.STDIO=true启用） 包括spring boot starter webflux和mcp spring webflux.依赖项 配置属性 # 所有属性都以spring.ai.mcp.server为前缀：\n同步/异步服务器类型 # 同步服务器-使用McpSyncServer实现的默认服务器类型。 异步服务器-异步服务器实现使用McpAsyncServer，并针对非阻塞操作进行了优化。 运输选项 # MCP服务器支持三种传输机制，每个传输机制都有其专用的启动器：\n标准输入/输出（STDIO）-spring ai starter mcp服务器 Spring MVC（服务器发送事件）-Spring ai starter mcp Server webmvc Spring WebFlux（反应式SSE）-Spring ai starter mcp服务器WebFlux 功能和功能 # MCP Server Boot Starter允许服务器向客户端公开工具、资源和提示。\n工具 # 允许服务器公开可以由语言模型调用的工具。MCP Server Boot Starter提供：\n更改通知支持 工具会根据服务器类型自动转换为同步/异步规范 通过Spring beans的自动工具规格： @Bean public ToolCallbackProvider myTools(...) { List\u0026lt;ToolCallback\u0026gt; tools = ... return ToolCallbackProvider.from(tools); } 或使用低级API：\n@Bean public List\u0026lt;McpServerFeatures.SyncToolSpecification\u0026gt; myTools(...) { List\u0026lt;McpServerFeatures.SyncToolSpecification\u0026gt; tools = ... return tools; } 资源管理 # 为服务器向客户端公开资源提供标准化的方法。\n静态和动态资源规范 可选更改通知 支持资源模板 在同步/异步资源规范之间自动转换 通过Spring beans自动指定资源： @Bean public List\u0026lt;McpServerFeatures.SyncResourceSpecification\u0026gt; myResources(...) { var systemInfoResource = new McpSchema.Resource(...); var resourceSpecification = new McpServerFeatures.SyncResourceSpecification(systemInfoResource, (exchange, request) -\u0026gt; { try { var systemInfo = Map.of(...); String jsonContent = new ObjectMapper().writeValueAsString(systemInfo); return new McpSchema.ReadResourceResult( List.of(new McpSchema.TextResourceContents(request.uri(), \u0026#34;application/json\u0026#34;, jsonContent))); } catch (Exception e) { throw new RuntimeException(\u0026#34;Failed to generate system info\u0026#34;, e); } }); return List.of(resourceSpecification); } 提示管理 # 为服务器向客户端公开提示模板提供标准化的方法。\n更改通知支持 模板版本控制 在同步/异步提示规范之间自动转换 通过Spring beans自动提示规范： @Bean public List\u0026lt;McpServerFeatures.SyncPromptSpecification\u0026gt; myPrompts() { var prompt = new McpSchema.Prompt(\u0026#34;greeting\u0026#34;, \u0026#34;A friendly greeting prompt\u0026#34;, List.of(new McpSchema.PromptArgument(\u0026#34;name\u0026#34;, \u0026#34;The name to greet\u0026#34;, true))); var promptSpecification = new McpServerFeatures.SyncPromptSpecification(prompt, (exchange, getPromptRequest) -\u0026gt; { String nameArgument = (String) getPromptRequest.arguments().get(\u0026#34;name\u0026#34;); if (nameArgument == null) { nameArgument = \u0026#34;friend\u0026#34;; } var userMessage = new PromptMessage(Role.USER, new TextContent(\u0026#34;Hello \u0026#34; + nameArgument + \u0026#34;! How can I assist you today?\u0026#34;)); return new GetPromptResult(\u0026#34;A personalized greeting message\u0026#34;, List.of(userMessage)); }); return List.of(promptSpecification); } 根更改使用者 # 当根更改时，支持listChanged的客户端发送根更改通知。\n支持监视根更改 自动转换为反应式应用程序的异步使用者 通过SpringBeans进行可选注册 @Bean public BiConsumer\u0026lt;McpSyncServerExchange, List\u0026lt;McpSchema.Root\u0026gt;\u0026gt; rootsChangeHandler() { return (exchange, roots) -\u0026gt; { logger.info(\u0026#34;Registering root resources: {}\u0026#34;, roots); }; } 使用示例 # 标准STDIO服务器配置 # # Using spring-ai-starter-mcp-server spring: ai: mcp: server: name: stdio-mcp-server version: 1.0.0 type: SYNC WebMVC服务器配置 # # Using spring-ai-starter-mcp-server-webmvc spring: ai: mcp: server: name: webmvc-mcp-server version: 1.0.0 type: SYNC sse-message-endpoint: /mcp/messages WebFlux服务器配置 # # Using spring-ai-starter-mcp-server-webflux spring: ai: mcp: server: name: webflux-mcp-server version: 1.0.0 type: ASYNC # Recommended for reactive applications sse-message-endpoint: /mcp/messages 使用MCP服务器创建Spring Boot应用程序 # @Service public class WeatherService { @Tool(description = \u0026#34;Get weather information by city name\u0026#34;) public String getWeather(String cityName) { // Implementation } } @SpringBootApplication public class McpServerApplication { private static final Logger logger = LoggerFactory.getLogger(McpServerApplication.class); public static void main(String[] args) { SpringApplication.run(McpServerApplication.class, args); } @Bean public ToolCallbackProvider weatherTools(WeatherService weatherService) { return MethodToolCallbackProvider.builder().toolObjects(weatherService).build(); } } 自动配置将自动将工具回调注册为MCP工具。\n示例应用程序 # 天气服务器（WebFlux）-Spring AI MCP Server Boot Starter with WebFlux。 天气服务器（STDIO）-带STDIO传输的Spring AI MCP服务器启动程序。 天气服务器手动配置-Spring AI MCP Server Boot Starter，不使用自动配置，而是使用Java SDK手动配置服务器。 其他资源 # Spring AI文档 模型上下文协议规范 弹簧防尘套自动配置 "},{"id":26,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E5%9B%BE%E5%83%8F%E6%A8%A1%E5%9E%8B/%E5%BC%80%E6%94%BE%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/","title":"OpenAI图像生成","section":"映像模型API","content":" OpenAI图像生成 # Spring AI支持DALL-E，这是OpenAI的图像生成模型。\n前提条件 # 您需要使用OpenAI创建API密钥来访问ChatGPT模型。\nexport SPRING_AI_OPENAI_API_KEY=\u0026lt;INSERT KEY HERE\u0026gt; 自动配置 # Spring AI为OpenAI Image Generation Client提供Spring Boot自动配置。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-openai\u0026#39; } 图像生成属性 # 连接属性 # 前缀spring.ai.openai用作允许连接到openai的属性前缀。\n重试属性 # 前缀spring.ai.retry用作属性前缀，允许您配置OpenAI Image客户端的重试机制。\n配置属性 # 前缀spring.ai.openai.image是属性前缀，允许您为openai配置ImageModel实现。\n运行时选项 # OpenAiImageOptions.java提供模型配置，例如要使用的模型、质量、大小等。 启动时，可以使用OpenAiImageModel（OpenAiImage Pi openAiImage API）构造函数和withDefaultOptions（OpenAiImageOptions defaultOptions）方法配置默认选项。或者，使用前面描述的spring.ai.openai.image.options.*属性。 在运行时，可以通过向ImagePrompt调用添加新的特定于请求的选项来覆盖默认选项。\nImageResponse response = openaiImageModel.call( new ImagePrompt(\u0026#34;A light cream colored mini golden doodle\u0026#34;, OpenAiImageOptions.builder() .quality(\u0026#34;hd\u0026#34;) .N(4) .height(1024) .width(1024).build()) ); "},{"id":27,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/%E4%BA%9A%E9%A9%AC%E9%80%8A%E5%9F%BA%E5%B2%A9/%E6%B3%B0%E5%9D%A6/","title":"Titan嵌入","section":"亚马逊基岩","content":" Titan嵌入 # 提供Bedrock Titan嵌入模型。 AWS Bedrick Titan模型页面和Amazon Bedrick用户指南包含有关如何使用AWS托管模型的详细信息。\n前提条件 # 有关设置API访问，请参阅Amazon Bedrock上的Spring AI文档。\n添加存储库和BOM表 # Spring AI工件发布在Maven Central和Spring Snapshot 存储库中。 为了帮助进行依赖关系管理，Spring AI提供了BOM（物料清单），以确保在整个项目中使用一致版本的Spring AI。请参阅依赖项管理部分，将Spring AI BOM添加到构建系统中。\n自动配置 # 将spring ai starter模型基岩依赖项添加到项目的Maven pom.xml文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-bedrock\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-bedrock\u0026#39; } 启用Titan嵌入支持 # 默认情况下，禁用Titan嵌入模型。\nexport SPRING_AI_MODEL_EMBEDDING=bedrock-titan 嵌入属性 # 前缀spring.ai.bedrock.aws是用于配置与aws bedrock的连接的属性前缀。 前缀spring.ai.bedrock.titan.embedding（在BedrockTitanEmbeddingProperties中定义）是配置titan嵌入模型实现的属性前缀。 支持的值为：amazon.titan-embed-image-v1、amazon.titan-embet-text-v1和amazon.tian-embed-text-v2:0。\n运行时选项 # BedrockTitanEmbeddingOptions.java提供模型配置，如输入类型。 在运行时，可以通过向EmbeddingRequest调用添加新的特定于请求的选项来覆盖默认选项。\nEmbeddingResponse embeddingResponse = embeddingModel.call( new EmbeddingRequest(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;), BedrockTitanEmbeddingOptions.builder() .withInputType(InputType.TEXT) .build())); 样本控制器 # 创建一个新的SpringBoot项目，并将Springaistarter模型基础添加到pom（或gradle）依赖项中。 在src/main/resources目录下添加application.properties文件，以启用和配置Titan Embedding模型：\nspring.ai.bedrock.aws.region=eu-central-1 spring.ai.bedrock.aws.access-key=${AWS_ACCESS_KEY_ID} spring.ai.bedrock.aws.secret-key=${AWS_SECRET_ACCESS_KEY} spring.ai.model.embedding=bedrock-titan 这将创建一个可以注入类中的EmbeddingController实现。\n@RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(\u0026#34;/ai/embedding\u0026#34;) public Map embed(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { EmbeddingResponse embeddingResponse = this.embeddingModel.embedForResponse(List.of(message)); return Map.of(\u0026#34;embedding\u0026#34;, embeddingResponse); } } 手动配置 # BedrockTitanEmbeddingModel实现了Embedding模型，并使用低级TitanEmbuddingBedrockapi客户端连接到Bedrock Titan服务。 将spring ai基岩依赖项添加到项目的Maven pom.xml文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-bedrock\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-bedrock\u0026#39; } 接下来，创建BedrockTitanEmbeddingModel，并将其用于文本嵌入：\nvar titanEmbeddingApi = new TitanEmbeddingBedrockApi( TitanEmbeddingModel.TITAN_EMBED_IMAGE_V1.id(), Region.US_EAST_1.id()); var embeddingModel = new BedrockTitanEmbeddingModel(this.titanEmbeddingApi); EmbeddingResponse embeddingResponse = this.embeddingModel .embedForResponse(List.of(\u0026#34;Hello World\u0026#34;)); // NOTE titan does not support batch embedding. 低级TitanEmbeddingBedrockApi客户端 # TitanEmbeddingBedrockApi提供的是AWS Bedrock Titan Embedding模型之上的轻量级Java客户端。 下图说明了TitanEmbeddingBedrockApi接口和构建块： TitanEmbeddingBedrockApi支持用于单个和批嵌入计算的amazon.titan-embed-image-v1和amazon.titan-embed-image-v1模型。 下面是如何以编程方式使用api的简单片段：\nTitanEmbeddingBedrockApi titanEmbedApi = new TitanEmbeddingBedrockApi( TitanEmbeddingModel.TITAN_EMBED_TEXT_V1.id(), Region.US_EAST_1.id()); TitanEmbeddingRequest request = TitanEmbeddingRequest.builder() .withInputText(\u0026#34;I like to eat apples.\u0026#34;) .build(); TitanEmbeddingResponse response = this.titanEmbedApi.embedding(this.request); 要嵌入图像，需要将其转换为base64格式：\nTitanEmbeddingBedrockApi titanEmbedApi = new TitanEmbeddingBedrockApi( TitanEmbeddingModel.TITAN_EMBED_IMAGE_V1.id(), Region.US_EAST_1.id()); byte[] image = new DefaultResourceLoader() .getResource(\u0026#34;classpath:/spring_framework.png\u0026#34;) .getContentAsByteArray(); TitanEmbeddingRequest request = TitanEmbeddingRequest.builder() .withInputImage(Base64.getEncoder().encodeToString(this.image)) .build(); TitanEmbeddingResponse response = this.titanEmbedApi.embedding(this.request); "},{"id":28,"href":"/docs/%E5%85%A5%E9%97%A8/","title":"入门","section":"Docs","content":" 入门 # 本节提供了如何开始使用Spring AI的起点。 您应该根据需要遵循以下每个部分中的步骤。\n弹簧初始化器 # 转到start.spring.io，然后选择要在新应用程序中使用的AI模型和向量存储。\n工件存储库 # 里程碑-使用Maven Central # 从1.0.0-M6开始，Maven Central中提供了版本。\n快照-添加快照存储库 # 要使用Snapshot（和1.0.0-M6之前的里程碑）版本，需要在构建文件中添加以下快照存储库。 将以下存储库定义添加到Maven或Gradle构建文件中： 注意：将Maven与Spring AI快照一起使用时，请注意Maven镜像配置。如果您在settings.xml中配置了镜像，如下所示：\n\u0026lt;mirror\u0026gt; \u0026lt;id\u0026gt;my-mirror\u0026lt;/id\u0026gt; \u0026lt;mirrorOf\u0026gt;*\u0026lt;/mirrorOf\u0026gt; \u0026lt;url\u0026gt;https://my-company-repository.com/maven\u0026lt;/url\u0026gt; \u0026lt;/mirror\u0026gt; 通配符*将所有存储库请求重定向到镜像，从而阻止对Spring快照存储库的访问。要解决此问题，请修改mirrorOf配置以排除Spring存储库：\n\u0026lt;mirror\u0026gt; \u0026lt;id\u0026gt;my-mirror\u0026lt;/id\u0026gt; \u0026lt;mirrorOf\u0026gt;*,!spring-snapshots,!central-portal-snapshots\u0026lt;/mirrorOf\u0026gt; \u0026lt;url\u0026gt;https://my-company-repository.com/maven\u0026lt;/url\u0026gt; \u0026lt;/mirror\u0026gt; 该配置允许Maven直接访问Spring快照存储库，同时仍然使用镜像来处理其他依赖项。\n依赖管理 # Spring AI物料清单（BOM）声明了给定版本的Spring AI.使用的所有依赖项的推荐版本。 将BOM表添加到项目中：\n为特定组件添加依赖项 # 文档中的以下每个部分都显示了需要添加到项目生成系统中的依赖项。\n聊天模型 嵌入模型 图像生成模型 转录模型 文本到语音（TTS）模型 向量数据库 春季AI样本 # 有关Spring AI的更多资源和示例，请参阅此页面。\n"},{"id":29,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/%E4%BA%9A%E9%A9%AC%E9%80%8A%E5%9F%BA%E5%B2%A9%E5%8C%A1%E5%A8%81/","title":"基岩匡威API","section":"聊天模型API","content":" 基岩匡威API # Amazon Bedrock Converse API为对话人工智能模型提供了统一的接口，具有增强的功能，包括功能/工具调用、多模态输入和流式响应。 Bedrock Converse API具有以下高级功能：\n前提条件 # 请参阅 Amazon Bedrock入门，以设置API访问\n获取AWS凭据：如果您尚未配置AWS帐户和AWS CLI，则本视频指南可以帮助您配置它：AWS CLI和SDK安装在4分钟内！。您应该能够获得访问密钥和安全密钥。 启用要使用的模型：转到Amazon Bedrock，从左侧的模型访问菜单中，配置对要使用的模块的访问。 自动配置 # 将spring ai starter模型基岩逆向依赖项添加到项目的Maven pom.xml或Gradle build.Gradle构建文件中：\n聊天室属性 # 前缀spring.ai.bedrock.aws是用于配置与aws bedrock的连接的属性前缀。 前缀spring.ai.bedrock.converse.chat是配置converse API的聊天模型实现的属性前缀。\n运行时选项 # 使用便携式ChatOptions或ToolCallingChatOptions便携式生成器创建模型配置，如温度、maxToken、topP等。 启动时，可以使用BedrockConverseProxyChatModel（api，options）构造函数或spring.ai.kistern.converse.chat.options.*属性配置默认选项。 在运行时，可以通过向Prompt调用添加新的特定于请求的选项来覆盖默认选项：\nvar options = ToolCallingChatOptions.builder() .model(\u0026#34;anthropic.claude-3-5-sonnet-20240620-v1:0\u0026#34;) .temperature(0.6) .maxTokens(300) .toolCallbacks(List.of(FunctionToolCallback.builder(\u0026#34;getCurrentWeather\u0026#34;, new WeatherService()) .description(\u0026#34;Get the weather in location. Return temperature in 36°F or 36°C format. Use multi-turn if needed.\u0026#34;) .inputType(WeatherService.Request.class) .build())) .build(); String response = ChatClient.create(this.chatModel) .prompt(\u0026#34;What is current weather in Amsterdam?\u0026#34;) .options(options) .call() .content(); 工具调用 # Bedrock Converse API支持工具调用功能，允许模型在对话期间使用工具。\npublic class WeatherService { @Tool(description = \u0026#34;Get the weather in location\u0026#34;) public String weatherByLocation(@ToolParam(description= \u0026#34;City or state name\u0026#34;) String location) { ... } } String response = ChatClient.create(this.chatModel) .prompt(\u0026#34;What\u0026#39;s the weather like in Boston?\u0026#34;) .tools(new WeatherService()) .call() .content(); 您也可以将java.util.function bean用作工具：\n@Bean @Description(\u0026#34;Get the weather in location. Return temperature in 36°F or 36°C format.\u0026#34;) public Function\u0026lt;Request, Response\u0026gt; weatherFunction() { return new MockWeatherService(); } String response = ChatClient.create(this.chatModel) .prompt(\u0026#34;What\u0026#39;s the weather like in Boston?\u0026#34;) .tools(\u0026#34;weatherFunction\u0026#34;) .inputType(Request.class) .call() .content(); 在 工具文档中查找更多信息。\n多模态 # 多模态是指模型同时理解和处理来自各种来源的信息的能力，包括文本、图像、视频、pdf、doc、html、md和更多数据格式。 Bedrock Converse API支持多模态输入，包括文本和图像输入，并可以基于组合输入生成文本响应。 您需要一个支持多模态输入的模型，如Arthropic Claude或Amazon Nova模型。\n图像 # 对于支持视觉多模态的模型，如Amazon Nova、人类学Claude、Llama 3.2，Bedrock Converse API Amazon允许您在有效载荷中包含多个图像。这些模型可以分析传递的图像并回答问题，对图像进行分类，以及根据提供的指令对图像进行汇总。 目前，Bedrock Converse支持image/jpeg、image/png、image/gif和image/webp mime类型的base64编码图像。 Spring AI的消息接口通过引入媒体类型支持多模态AI模型。 下面是一个简单的代码示例，演示用户文本与图像的组合。\nString response = ChatClient.create(chatModel) .prompt() .user(u -\u0026gt; u.text(\u0026#34;Explain what do you see on this picture?\u0026#34;) .media(Media.Format.IMAGE_PNG, new ClassPathResource(\u0026#34;/test.png\u0026#34;))) .call() .content(); logger.info(response); 它将test.png图像作为输入： 以及文本消息“解释您在这张图片上看到了什么？”，并生成类似于以下内容的响应：\n视频 # AmazonNova模型允许您在有效负载中包含单个视频，可以以base64格式或通过AmazonS3URI提供。 目前，Bedrock Nova支持视频/x-matros、视频/quicktime、视频/mp4、视频/video/webm、视频/x-flv、视频/mpeg、视频/x-ms-wmv和图像/3gpp mime类型的图像。 Spring AI的消息接口通过引入Media类型支持多模态AI模型。 下面是一个简单的代码示例，演示用户文本与视频的组合。\nString response = ChatClient.create(chatModel) .prompt() .user(u -\u0026gt; u.text(\u0026#34;Explain what do you see in this video?\u0026#34;) .media(Media.Format.VIDEO_MP4, new ClassPathResource(\u0026#34;/test.video.mp4\u0026#34;))) .call() .content(); logger.info(response); 它将test.video.mp4图像作为输入： 以及文本消息“解释您在该视频中看到了什么？”，并生成类似于以下内容的响应：\n文件 # 对于某些模型，Bedrock允许您通过Converse API文档支持将文档包含在有效负载中，该支持可以以字节为单位提供。\n文本文档类型（txt、csv、html、md等），其中重点是文本理解。这些用例包括基于文档的文本元素的应答。 媒体文档类型（pdf、docx、xlsx），其中重点是基于视觉的理解来回答问题。这些用例包括基于图表、图形等回答问题。 目前，人类 PDF支持（测试版）和Amazon Bedrock Nova模型支持文档多模态。 下面是一个简单的代码示例，演示用户文本与媒体文档的组合。 String response = ChatClient.create(chatModel) .prompt() .user(u -\u0026gt; u.text( \u0026#34;You are a very professional document summarization specialist. Please summarize the given document.\u0026#34;) .media(Media.Format.DOC_PDF, new ClassPathResource(\u0026#34;/spring-ai-reference-overview.pdf\u0026#34;))) .call() .content(); logger.info(response); 它将spring-ai-reference-overview.pdf文档作为输入： 以及文本消息“您是一名非常专业的文档摘要专家。请总结给定的文档。”，并生成类似于以下内容的响应：\n样本控制器 # 创建一个新的SpringBoot项目，并将Springaistarter模型基岩converse添加到您的依赖项中。 在src/main/resources下添加application.properties文件：\nspring.ai.bedrock.aws.region=eu-central-1 spring.ai.bedrock.aws.timeout=10m spring.ai.bedrock.aws.access-key=${AWS_ACCESS_KEY_ID} spring.ai.bedrock.aws.secret-key=${AWS_SECRET_ACCESS_KEY} # session token is only required for temporary credentials spring.ai.bedrock.aws.session-token=${AWS_SESSION_TOKEN} spring.ai.bedrock.converse.chat.options.temperature=0.8 spring.ai.bedrock.converse.chat.options.top-k=15 下面是使用聊天模型的控制器示例：\n@RestController public class ChatController { private final ChatClient chatClient; @Autowired public ChatController(ChatClient.Builder builder) { this.chatClient = builder.build(); } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, this.chatClient.prompt(message).call().content()); } @GetMapping(\u0026#34;/ai/generateStream\u0026#34;) public Flux\u0026lt;ChatResponse\u0026gt; generateStream(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return this.chatClient.prompt(message).stream().content(); } } "},{"id":30,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/","title":"嵌入模型API","section":"弹簧AI API","content":" 嵌入模型API # 嵌入是文本、图像或视频的数字表示，用于捕获输入之间的关系。 嵌入的工作原理是将文本、图像和视频转换为浮点数数组，称为向量。这些向量旨在捕获文本、图像和视频的含义。嵌入数组的长度称为向量的维数。 通过计算两段文本的向量表示之间的数字距离，应用程序可以确定用于生成嵌入向量的对象之间的相似性。 EmbeddingModel接口旨在与人工智能和机器学习中的嵌入模型直接集成。 EmbeddingModel接口的设计围绕两个主要目标：\nAPI概述 # 嵌入模型API构建在通用SpringAIModelAPI之上，该API是SpringAI库的一部分。 更高级别的组件反过来使用嵌入API来实现特定嵌入模型的嵌入模型，如OpenAI、Titan、Azure OpenAI和Ollie等。 下图说明了嵌入API及其与Spring AI模型API和嵌入模型的关系：\n嵌入模型 # 本节提供了EmbeddingModel接口和相关类的指南。\npublic interface EmbeddingModel extends Model\u0026lt;EmbeddingRequest, EmbeddingResponse\u0026gt; { @Override EmbeddingResponse call(EmbeddingRequest request); /** * Embeds the given document\u0026#39;s content into a vector. * @param document the document to embed. * @return the embedded vector. */ float[] embed(Document document); /** * Embeds the given text into a vector. * @param text the text to embed. * @return the embedded vector. */ default float[] embed(String text) { Assert.notNull(text, \u0026#34;Text must not be null\u0026#34;); return this.embed(List.of(text)).iterator().next(); } /** * Embeds a batch of texts into vectors. * @param texts list of texts to embed. * @return list of list of embedded vectors. */ default List\u0026lt;float[]\u0026gt; embed(List\u0026lt;String\u0026gt; texts) { Assert.notNull(texts, \u0026#34;Texts must not be null\u0026#34;); return this.call(new EmbeddingRequest(texts, EmbeddingOptions.EMPTY)) .getResults() .stream() .map(Embedding::getOutput) .toList(); } /** * Embeds a batch of texts into vectors and returns the {@link EmbeddingResponse}. * @param texts list of texts to embed. * @return the embedding response. */ default EmbeddingResponse embedForResponse(List\u0026lt;String\u0026gt; texts) { Assert.notNull(texts, \u0026#34;Texts must not be null\u0026#34;); return this.call(new EmbeddingRequest(texts, EmbeddingOptions.EMPTY)); } /** * @return the number of dimensions of the embedded vectors. It is generative * specific. */ default int dimensions() { return embed(\u0026#34;Test String\u0026#34;).size(); } } 嵌入方法提供了各种选项，用于将文本转换为嵌入，适应单个字符串、结构化文档对象或成批文本。 提供了多个用于嵌入文本的快捷方式方法，包括embed（String text）方法，该方法接受单个字符串并返回相应的嵌入向量。 通常，嵌入返回浮点列表，以数字向量格式表示嵌入。 embedForResponse方法提供了更全面的输出，可能包括关于嵌入的附加信息。 维数方法是开发人员快速确定嵌入向量大小的便捷工具，这对于理解嵌入空间和后续处理步骤非常重要。\n嵌入请求 # EmbeddingRequest是一个ModelRequest，它接受文本对象列表和可选的嵌入请求选项。\npublic class EmbeddingRequest implements ModelRequest\u0026lt;List\u0026lt;String\u0026gt;\u0026gt; { private final List\u0026lt;String\u0026gt; inputs; private final EmbeddingOptions options; // other methods omitted } 嵌入响应 # EmbeddingResponse类的结构如下：\npublic class EmbeddingResponse implements ModelResponse\u0026lt;Embedding\u0026gt; { private List\u0026lt;Embedding\u0026gt; embeddings; private EmbeddingResponseMetadata metadata = new EmbeddingResponseMetadata(); // other methods omitted } ``EmbeddingResponse类保存AI模型的输出，每个Embedding实例包含来自单个文本输入的结果向量数据。 EmbeddingResponse类还携带关于AI模型响应的Embedding ResponseMetadata元数据。\n嵌入 # 嵌入表示单个嵌入向量。\npublic class Embedding implements ModelResult\u0026lt;float[]\u0026gt; { private float[] embedding; private Integer index; private EmbeddingResultMetadata metadata; // other methods omitted } 可用的实现 # 在内部，各种EmbeddingModel实现使用不同的低级库和API来执行嵌入任务。以下是EmbeddingModel实现的一些可用实现：\n春季AI OpenAI嵌入 春季AI Azure OpenAI嵌入 春季AI Ollama嵌入 弹簧AI变压器（ONNX）嵌入 Spring AI PostgresML嵌入 弹簧AI基岩粘结嵌入 春季AI基岩泰坦嵌入 弹簧AI VertexAI嵌入 春季AI Mistral AI嵌入 Spring AI Oracle云基础架构GenAI嵌入 "},{"id":31,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E9%9F%B3%E9%A2%91%E5%9E%8B%E5%8F%B7/%E6%96%87%E6%9C%AC%E5%88%B0%E8%AF%AD%E9%9F%B3ttsapi/","title":"文本到语音（TTS）API","section":"弹簧AI API","content":" 文本到语音（TTS）API # Spring AI支持OpenAI的语音API。\n"},{"id":32,"href":"/docs/%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/apache-cassandra%E7%9F%A2%E9%87%8F%E5%AD%98%E5%82%A8/","title":"Apache Cassandra矢量存储","section":"向量数据库","content":" Apache Cassandra矢量存储 # 本节将指导您设置CassandraVectorStore以存储文档嵌入并执行相似性搜索。\n什么是Apache Cassandra？ # Apache Cassandra®是一个真正的开源分布式数据库，以线性可扩展性、经验证的容错性和低延迟而闻名，使其成为任务关键型事务数据的完美平台。 其向量相似性搜索（VSS）基于JVector库，确保最佳的类内性能和相关性。 在Apache Cassandra中进行向量搜索的过程如下所示：\nSELECT content FROM table ORDER BY content_vector ANN OF query_embedding; 更多关于这方面的文档可以 在这里阅读。 这款Spring AI Vector Store旨在为全新的RAG应用程序工作，并能够在现有数据和表的基础上进行改造。 该存储也可以用于现有数据库中的非RAG用例，例如语义搜索、地理邻近性搜索等。 存储将根据其配置根据需要自动创建或增强架构。如果不希望修改架构，请使用disallowSchemaChanges配置存储。 当使用spring boot自动配置时，根据SpringBoot标准，disallowSchemaChanges默认为true，并且您必须通过设置…​在application.properties文件中初始化schema=true。\n什么是JVector？ # JVector是一个纯Java嵌入式向量搜索引擎。 它从其他HNSW矢量相似性搜索实现中脱颖而出，因为：\n算法速度快。JVector使用受DiskANN和相关研究启发的最先进的图形算法，提供高召回和低延迟。 快速实施。JVector使用巴拿马SIMD API来加速索引构建和查询。 内存有效。JVector使用乘积量化压缩向量，以便在搜索期间它们可以留在内存中。 磁盘感知。JVector的磁盘布局旨在在查询时执行最少的必要IOP。 并发。索引构建线性扩展到至少32个线程。线程加倍，构建时间减半。 增量。在构建索引时查询它。在添加向量和能够在搜索结果中找到它之间没有延迟。 易于嵌入。API设计为易于嵌入，供人们在生产中使用。 前提条件 # 依赖关系 # 将这些依赖项添加到项目中：\n仅对于Cassandra Vector Store： \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-cassandra-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者，对于RAG应用程序中所需的一切（使用默认的ONNX嵌入模型）： \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-cassandra\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 配置属性 # 您可以在SpringBoot配置中使用以下属性来定制ApacheCassandra向量存储。\n使用 # 基本用法 # 将CassandraVectorStore实例创建为Spring Bean：\n@Bean public VectorStore vectorStore(CqlSession session, EmbeddingModel embeddingModel) { return CassandraVectorStore.builder(embeddingModel) .session(session) .keyspace(\u0026#34;my_keyspace\u0026#34;) .table(\u0026#34;my_vectors\u0026#34;) .build(); } 获得向量存储实例后，可以添加文档并执行搜索：\n// Add documents vectorStore.add(List.of( new Document(\u0026#34;1\u0026#34;, \u0026#34;content1\u0026#34;, Map.of(\u0026#34;key1\u0026#34;, \u0026#34;value1\u0026#34;)), new Document(\u0026#34;2\u0026#34;, \u0026#34;content2\u0026#34;, Map.of(\u0026#34;key2\u0026#34;, \u0026#34;value2\u0026#34;)) )); // Search with filters List\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch( SearchRequest.query(\u0026#34;search text\u0026#34;) .withTopK(5) .withSimilarityThreshold(0.7f) .withFilterExpression(\u0026#34;metadata.key1 == \u0026#39;value1\u0026#39;\u0026#34;) ); 高级配置 # 对于更复杂的用例，您可以在Spring Bean中配置其他设置：\n@Bean public VectorStore vectorStore(CqlSession session, EmbeddingModel embeddingModel) { return CassandraVectorStore.builder(embeddingModel) .session(session) .keyspace(\u0026#34;my_keyspace\u0026#34;) .table(\u0026#34;my_vectors\u0026#34;) // Configure primary keys .partitionKeys(List.of( new SchemaColumn(\u0026#34;id\u0026#34;, DataTypes.TEXT), new SchemaColumn(\u0026#34;category\u0026#34;, DataTypes.TEXT) )) .clusteringKeys(List.of( new SchemaColumn(\u0026#34;timestamp\u0026#34;, DataTypes.TIMESTAMP) )) // Add metadata columns with optional indexing .addMetadataColumns( new SchemaColumn(\u0026#34;category\u0026#34;, DataTypes.TEXT, SchemaColumnTags.INDEXED), new SchemaColumn(\u0026#34;score\u0026#34;, DataTypes.DOUBLE) ) // Customize column names .contentColumnName(\u0026#34;text\u0026#34;) .embeddingColumnName(\u0026#34;vector\u0026#34;) // Performance tuning .fixedThreadPoolExecutorSize(32) // Schema management .disallowSchemaChanges(false) // Custom batching strategy .batchingStrategy(new TokenCountBatchingStrategy()) .build(); } 连接配置 # 配置与Cassandra的连接有两种方法：\n使用注入的CqlSession（推荐）： @Bean public VectorStore vectorStore(CqlSession session, EmbeddingModel embeddingModel) { return CassandraVectorStore.builder(embeddingModel) .session(session) .keyspace(\u0026#34;my_keyspace\u0026#34;) .table(\u0026#34;my_vectors\u0026#34;) .build(); } 直接在生成器中使用连接详细信息： @Bean public VectorStore vectorStore(EmbeddingModel embeddingModel) { return CassandraVectorStore.builder(embeddingModel) .contactPoint(new InetSocketAddress(\u0026#34;localhost\u0026#34;, 9042)) .localDatacenter(\u0026#34;datacenter1\u0026#34;) .keyspace(\u0026#34;my_keyspace\u0026#34;) .build(); } 元数据筛选 # 您可以使用CassandraVectorStore来利用通用的可移植元数据过滤器。对于可搜索的元数据列，它们必须是主键或SAI索引。要将非主键列编入索引，请使用SchemaColumnTags配置元数据列。索引。 例如，可以使用文本表达式语言：\nvectorStore.similaritySearch( SearchRequest.builder().query(\u0026#34;The World\u0026#34;) .topK(5) .filterExpression(\u0026#34;country in [\u0026#39;UK\u0026#39;, \u0026#39;NL\u0026#39;] \u0026amp;\u0026amp; year \u0026gt;= 2020\u0026#34;).build()); 或以编程方式使用表达式DSL:\nFilter.Expression f = new FilterExpressionBuilder() .and( f.in(\u0026#34;country\u0026#34;, \u0026#34;UK\u0026#34;, \u0026#34;NL\u0026#34;), f.gte(\u0026#34;year\u0026#34;, 2020) ).build(); vectorStore.similaritySearch( SearchRequest.builder().query(\u0026#34;The World\u0026#34;) .topK(5) .filterExpression(f).build()); 可移植的筛选器表达式会自动转换为 CQL查询。\n高级示例：维基百科数据集上的向量存储 # 下面的示例演示如何在现有架构上使用存储。在这里，我们使用来自 github.com/datastax-labs/colbert-wikipedia-data项目的模式，该项目附带了为您矢量化的完整wikipedia数据集。 首先，在Cassandra数据库中创建模式：\nwget https://s.apache.org/colbert-wikipedia-schema-cql -O colbert-wikipedia-schema.cql cqlsh -f colbert-wikipedia-schema.cql 然后使用生成器模式配置存储：\n@Bean public VectorStore vectorStore(CqlSession session, EmbeddingModel embeddingModel) { List\u0026lt;SchemaColumn\u0026gt; partitionColumns = List.of( new SchemaColumn(\u0026#34;wiki\u0026#34;, DataTypes.TEXT), new SchemaColumn(\u0026#34;language\u0026#34;, DataTypes.TEXT), new SchemaColumn(\u0026#34;title\u0026#34;, DataTypes.TEXT) ); List\u0026lt;SchemaColumn\u0026gt; clusteringColumns = List.of( new SchemaColumn(\u0026#34;chunk_no\u0026#34;, DataTypes.INT), new SchemaColumn(\u0026#34;bert_embedding_no\u0026#34;, DataTypes.INT) ); List\u0026lt;SchemaColumn\u0026gt; extraColumns = List.of( new SchemaColumn(\u0026#34;revision\u0026#34;, DataTypes.INT), new SchemaColumn(\u0026#34;id\u0026#34;, DataTypes.INT) ); return CassandraVectorStore.builder() .session(session) .embeddingModel(embeddingModel) .keyspace(\u0026#34;wikidata\u0026#34;) .table(\u0026#34;articles\u0026#34;) .partitionKeys(partitionColumns) .clusteringKeys(clusteringColumns) .contentColumnName(\u0026#34;body\u0026#34;) .embeddingColumnName(\u0026#34;all_minilm_l6_v2_embedding\u0026#34;) .indexName(\u0026#34;all_minilm_l6_v2_ann\u0026#34;) .disallowSchemaChanges(true) .addMetadataColumns(extraColumns) .primaryKeyTranslator((List\u0026lt;Object\u0026gt; primaryKeys) -\u0026gt; { if (primaryKeys.isEmpty()) { return \u0026#34;test§¶0\u0026#34;; } return String.format(\u0026#34;%s§¶%s\u0026#34;, primaryKeys.get(2), primaryKeys.get(3)); }) .documentIdTranslator((id) -\u0026gt; { String[] parts = id.split(\u0026#34;§¶\u0026#34;); String title = parts[0]; int chunk_no = parts.length \u0026gt; 1 ? Integer.parseInt(parts[1]) : 0; return List.of(\u0026#34;simplewiki\u0026#34;, \u0026#34;en\u0026#34;, title, chunk_no, 0); }) .build(); } @Bean public EmbeddingModel embeddingModel() { // default is ONNX all-MiniLM-L6-v2 which is what we want return new TransformersEmbeddingModel(); } 加载完整的Wikipedia数据集 # 要加载完整的维基百科数据集，请执行以下操作：\n访问本机客户端 # Cassandra Vector Store实现通过getNativeClient（）方法提供对底层本机Cassandra客户端（CqlSession）的访问：\nCassandraVectorStore vectorStore = context.getBean(CassandraVectorStore.class); Optional\u0026lt;CqlSession\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { CqlSession session = nativeClient.get(); // Use the native client for Cassandra-specific operations } 本机客户端允许您访问可能无法通过VectorStore界面公开的Cassandra特定功能和操作。\n"},{"id":33,"href":"/docs/%E6%A8%A1%E5%9E%8B%E4%B8%8A%E4%B8%8B%E6%96%87%E5%8D%8F%E8%AE%AEmcp/mcp%E5%AE%9E%E7%94%A8%E7%A8%8B%E5%BA%8F/","title":"MCP实用程序","section":"模型上下文协议（MCP）","content":" MCP实用程序 # MCP实用程序为将模型上下文协议与Spring AI应用程序集成提供了基础支持。\n工具回调实用程序 # 工具回调适配器 # 使MCP工具适应Spring AI的工具接口，具有同步和异步执行支持。\n工具回调提供程序 # 从MCP客户端发现并提供MCP工具。\nMcpToolUtils（McpToolUtility） # 工具规范的工具回调 # 将Spring AI工具回调转换为MCP工具规范：\nMCP客户端到工具回调 # 从MCP客户端获取工具回调\n本机映像支持 # McpHints类为MCP架构类提供GraalVM本机映像提示。\n"},{"id":34,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/mistral%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/","title":"Mistral AI嵌入","section":"嵌入模型API","content":" Mistral AI嵌入 # Spring AI支持Mistral AI的文本嵌入模型。\n前提条件 # 您需要使用MistralAI创建一个API来访问MistralAI嵌入模型。 在 MistralAI注册页面创建帐户，并在 API密钥页面上生成令牌。\nexport SPRING_AI_MISTRALAI_API_KEY=\u0026lt;INSERT KEY HERE\u0026gt; 添加存储库和BOM表 # Spring AI工件发布在Maven Central和Spring Snapshot 存储库中。 为了帮助进行依赖关系管理，Spring AI提供了BOM（物料清单），以确保在整个项目中使用一致版本的Spring AI。请参阅依赖项管理部分，将Spring AI BOM添加到构建系统中。\n自动配置 # Spring AI为MistralAI嵌入模型提供Spring Boot自动配置。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-mistral-ai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-mistral-ai\u0026#39; } 嵌入属性 # 重试属性 # 前缀spring.ai.retry用作属性前缀，允许您配置Mistral ai嵌入模型的重试机制。\n连接属性 # 前缀spring.ai.mistralai用作允许连接到mistralai的属性前缀。\n配置属性 # 前缀spring.ai.mistalai.mbedding是属性前缀，用于配置mistralai的EmbeddingModel实现。\n运行时选项 # MistralAiEmbeddingOptions.java提供MistralAI配置，如要使用的模型等。 也可以使用spring.ai.mistalai.mbedding.options属性配置默认选项。 在启动时，使用MistralAiEmbeddingModel构造函数设置用于所有嵌入请求的默认选项。 例如，要覆盖特定请求的默认模型名称：\nEmbeddingResponse embeddingResponse = embeddingModel.call( new EmbeddingRequest(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;), MistralAiEmbeddingOptions.builder() .withModel(\u0026#34;Different-Embedding-Model-Deployment-Name\u0026#34;) .build())); 样本控制器 # 这将创建一个可以注入到类中的EmbeddingModel实现。\nspring.ai.mistralai.api-key=YOUR_API_KEY spring.ai.mistralai.embedding.options.model=mistral-embed @RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(\u0026#34;/ai/embedding\u0026#34;) public Map embed(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { var embeddingResponse = this.embeddingModel.embedForResponse(List.of(message)); return Map.of(\u0026#34;embedding\u0026#34;, embeddingResponse); } } 手动配置 # 如果不使用Spring Boot，则可以手动配置OpenAI嵌入模型。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-mistral-ai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-mistral-ai\u0026#39; } 接下来，创建MistralAiEmbeddingModel实例，并使用它计算两个输入文本之间的相似性：\nvar mistralAiApi = new MistralAiApi(System.getenv(\u0026#34;MISTRAL_AI_API_KEY\u0026#34;)); var embeddingModel = new MistralAiEmbeddingModel(this.mistralAiApi, MistralAiEmbeddingOptions.builder() .withModel(\u0026#34;mistral-embed\u0026#34;) .withEncodingFormat(\u0026#34;float\u0026#34;) .build()); EmbeddingResponse embeddingResponse = this.embeddingModel .embedForResponse(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;)); MistralAiEmbeddingOptions提供嵌入请求的配置信息。\n"},{"id":35,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/%E4%BA%BA%E7%B1%BB3/","title":"人类3聊天","section":"聊天模型API","content":" 人类3聊天 # 人形克劳德是一系列基础人工智能模型，可以用于各种应用。 Spring AI支持用于同步和流式文本生成的人工消息API。\n前提条件 # 您需要在人类门户上创建API密钥。\nexport SPRING_AI_ANTHROPIC_API_KEY=\u0026lt;INSERT KEY HERE\u0026gt; 添加存储库和BOM表 # Spring AI工件发布在Maven Central和Spring Snapshot 存储库中。 为了帮助进行依赖关系管理，Spring AI提供了BOM（物料清单），以确保在整个项目中使用一致版本的Spring AI。请参阅依赖项管理部分，将Spring AI BOM添加到构建系统中。\n自动配置 # Spring AI为人类聊天客户端提供Spring Boot自动配置。\n聊天室属性 # 重试属性 # 前缀spring.ai.retry用作属性前缀，允许您为人工聊天模型配置重试机制。\n连接属性 # 前缀spring.ai.anthropic用作允许连接到anthropic的特性前缀。\n配置属性 # 前缀spring.ai.anthropic.chat是属性前缀，允许您为anthropic配置聊天模型实现。\n运行时选项 # AnthropicChatOptions.java提供模型配置，例如要使用的模型、温度、最大令牌计数等。 启动时，可以使用AnthropicChatModel（api，options）构造函数或spring.ai.anthropic.chat.options.*属性配置默认选项。 在运行时，可以通过向Prompt调用添加新的特定于请求的选项来覆盖默认选项。\nChatResponse response = chatModel.call( new Prompt( \u0026#34;Generate the names of 5 famous pirates.\u0026#34;, AnthropicChatOptions.builder() .model(\u0026#34;claude-3-7-sonnet-latest\u0026#34;) .temperature(0.4) .build() )); 工具/函数调用 # 您可以使用AnthropicChatModel注册自定义Java工具，并让Anthropic Claude模型智能地选择输出包含参数的JSON对象，以调用一个或多个已注册的函数。\n多模态 # 多模态是指模型同时理解和处理来自各种来源的信息的能力，包括文本、pdf、图像、数据格式。\n图像 # 目前，人类克劳德3支持图像的base64源类型，以及image/jpeg、image/png、image/gif和image/webp媒体类型。 Spring AI的消息接口通过引入媒体类型支持多模态AI模型。 下面是一个从AnthropicChatModelIT.java中提取的简单代码示例，演示了用户文本与图像的组合。\nvar imageData = new ClassPathResource(\u0026#34;/multimodal.test.png\u0026#34;); var userMessage = new UserMessage(\u0026#34;Explain what do you see on this picture?\u0026#34;, List.of(new Media(MimeTypeUtils.IMAGE_PNG, this.imageData))); ChatResponse response = chatModel.call(new Prompt(List.of(this.userMessage))); logger.info(response.getResult().getOutput().getContent()); 它将multimal.test.png图像作为输入： 以及文本消息“解释您在这张图片上看到了什么？”，并生成类似于以下内容的响应：\nPDF格式 # 从Sonnet 3.5开始，提供了 PDF支持（测试版）。\nvar pdfData = new ClassPathResource(\u0026#34;/spring-ai-reference-overview.pdf\u0026#34;); var userMessage = new UserMessage( \u0026#34;You are a very professional document summarization specialist. Please summarize the given document.\u0026#34;, List.of(new Media(new MimeType(\u0026#34;application\u0026#34;, \u0026#34;pdf\u0026#34;), pdfData))); var response = this.chatModel.call(new Prompt(List.of(userMessage))); 样本控制器 # 创建一个新的SpringBoot项目，并将Spring-ai-starter模型anthropic添加到pom（或gradle）依赖项中。 在src/main/resources目录下添加application.properties文件，以启用和配置人工聊天模型：\nspring.ai.anthropic.api-key=YOUR_API_KEY spring.ai.anthropic.chat.options.model=claude-3-5-sonnet-latest spring.ai.anthropic.chat.options.temperature=0.7 spring.ai.anthropic.chat.options.max-tokens=450 这将创建一个可以注入到类中的AnthropicChatModel实现。\n@RestController public class ChatController { private final AnthropicChatModel chatModel; @Autowired public ChatController(AnthropicChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, this.chatModel.call(message)); } @GetMapping(\u0026#34;/ai/generateStream\u0026#34;) public Flux\u0026lt;ChatResponse\u0026gt; generateStream(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { Prompt prompt = new Prompt(new UserMessage(message)); return this.chatModel.stream(prompt); } } 手动配置 # AnthropicChatModel实现了查特模型和流式查特模型，并使用 低级AnthropicApi客户端连接到Anthropic服务。 将spring ai人类依赖项添加到项目的Maven pom.xml文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-anthropic\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-anthropic\u0026#39; } 接下来，创建一个人类聊天模型，并将其用于文本生成：\nvar anthropicApi = new AnthropicApi(System.getenv(\u0026#34;ANTHROPIC_API_KEY\u0026#34;)); var chatModel = new AnthropicChatModel(this.anthropicApi, AnthropicChatOptions.builder() .model(\u0026#34;claude-3-opus-20240229\u0026#34;) .temperature(0.4) .maxTokens(200) .build()); ChatResponse response = this.chatModel.call( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); // Or with streaming responses Flux\u0026lt;ChatResponse\u0026gt; response = this.chatModel.stream( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); AnthropicChatOptions提供聊天请求的配置信息。\n低级AnthropicApi客户端 # AnthropicApi为Anthropic Message API提供了一个轻量级Java客户端。 下面的类图说明了AnthropicApi聊天界面和构建块： 下面是如何以编程方式使用api的简单片段：\nAnthropicApi anthropicApi = new AnthropicApi(System.getenv(\u0026#34;ANTHROPIC_API_KEY\u0026#34;)); AnthropicMessage chatCompletionMessage = new AnthropicMessage( List.of(new ContentBlock(\u0026#34;Tell me a Joke?\u0026#34;)), Role.USER); // Sync request ResponseEntity\u0026lt;ChatCompletionResponse\u0026gt; response = this.anthropicApi .chatCompletionEntity(new ChatCompletionRequest(AnthropicApi.ChatModel.CLAUDE_3_OPUS.getValue(), List.of(this.chatCompletionMessage), null, 100, 0.8, false)); // Streaming request Flux\u0026lt;StreamResponse\u0026gt; response = this.anthropicApi .chatCompletionStream(new ChatCompletionRequest(AnthropicApi.ChatModel.CLAUDE_3_OPUS.getValue(), List.of(this.chatCompletionMessage), null, 100, 0.8, true)); 有关更多信息，请参阅AnthropicApi.java的JavaDoc。\n低级API示例 # java测试提供了一些如何使用轻量级库的通用示例。 "},{"id":36,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E5%9B%BE%E5%83%8F%E6%A8%A1%E5%9E%8B/","title":"映像模型API","section":"弹簧AI API","content":" 映像模型API # Spring Image Model API被设计为一个简单、可移植的接口，用于与专门用于图像生成的各种AI模型交互，允许开发人员在不同的图像相关模型之间切换，只需最少的代码更改。 此外，通过支持配套类，如用于输入封装的ImagePrompt和用于输出处理的ImageResponse，Image Model API统一了与专用于图像生成的AI模型的通信。 Spring Image Model API构建在Spring AI通用模型API之上，提供特定于图像的抽象和实现。\nAPI概述 # 本节提供SpringImageModelAPI接口和相关类的指南。\n图像模型（Image Model） # 下面是ImageModel接口定义：\n@FunctionalInterface public interface ImageModel extends Model\u0026lt;ImagePrompt, ImageResponse\u0026gt; { ImageResponse call(ImagePrompt request); } 图像提示 # ImagePrompt是一个ModelRequest，它封装了ImageMessage对象和可选模型请求选项的列表。\npublic class ImagePrompt implements ModelRequest\u0026lt;List\u0026lt;ImageMessage\u0026gt;\u0026gt; { private final List\u0026lt;ImageMessage\u0026gt; messages; private ImageOptions imageModelOptions; @Override public List\u0026lt;ImageMessage\u0026gt; getInstructions() {...} @Override public ImageOptions getOptions() {...} // constructors and utility methods omitted } 图像消息 # ImageMessage类封装要使用的文本以及文本在影响生成的图像时应具有的权重。对于支持权重的模型，它们可以是正的，也可以是负的。\npublic class ImageMessage { private String text; private Float weight; public String getText() {...} public Float getWeight() {...} // constructors and utility methods omitted } 图像选项 # 表示可以传递给图像生成模型的选项。ImageOptions接口扩展了ModelOptions接口，并用于定义几个可以传递给AI模型的可移植选项。 ImageOptions接口定义如下：\npublic interface ImageOptions extends ModelOptions { Integer getN(); String getModel(); Integer getWidth(); Integer getHeight(); String getResponseFormat(); // openai - url or base64 : stability ai byte[] or base64 } 此外，每个特定于模型的ImageModel实现都可以有自己的选项，可以传递给AI模型。例如，OpenAI图像生成模型有自己的选项，如质量、样式等。 这是一个强大的功能，允许开发人员在启动应用程序时使用模型特定的选项，然后在运行时使用ImagePrompt覆盖它们。\n图像响应 # ImageResponse类的结构如下：\npublic class ImageResponse implements ModelResponse\u0026lt;ImageGeneration\u0026gt; { private final ImageResponseMetadata imageResponseMetadata; private final List\u0026lt;ImageGeneration\u0026gt; imageGenerations; @Override public ImageGeneration getResult() { // get the first result } @Override public List\u0026lt;ImageGeneration\u0026gt; getResults() {...} @Override public ImageResponseMetadata getMetadata() {...} // other methods omitted } ImageResponse类保存AI模型的输出，每个ImageGeneration实例包含单个提示产生的潜在多个输出之一。 ImageResponse类还携带ImageRespenseMetadata对象，该对象保存关于AI模型响应的元数据。\n图像生成 # 最后，ImageGeneration类从ModelResult扩展来表示关于该结果的输出响应和相关元数据：\npublic class ImageGeneration implements ModelResult\u0026lt;Image\u0026gt; { private ImageGenerationMetadata imageGenerationMetadata; private Image image; @Override public Image getOutput() {...} @Override public ImageGenerationMetadata getMetadata() {...} // other methods omitted } 可用的实现 # ImageModel实现是为以下模型提供程序提供的：\nOpenAI图像生成 Azure OpenAI映像生成 千帆影像生成 StabilityAI图像生成 ZhiPuAI图像生成 API文档 # 您可以 在这里找到Javadoc。\n反馈和贡献 # 项目的 GitHub讨论是发送反馈的好地方。\n"},{"id":37,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E5%9B%BE%E5%83%8F%E6%A8%A1%E5%9E%8B/%E7%A8%B3%E5%AE%9A%E6%80%A7/","title":"稳定性AI图像生成","section":"映像模型API","content":" 稳定性AI图像生成 # Spring AI支持Stability AI的 文本到图像生成模型。\n前提条件 # 您需要使用Stability AI创建API密钥来访问他们的AI模型，请遵循他们的 入门文档。 Spring AI项目定义了一个名为Spring.AI.stabilityai.api-key的配置属性，您应该将其设置为从Stability AI获得的api key的值。\nexport SPRING_AI_STABILITYAI_API_KEY=\u0026lt;INSERT KEY HERE\u0026gt; 自动配置 # Spring AI为稳定性AI映像生成客户端提供Spring Boot自动配置。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-stability-ai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-stability-ai\u0026#39; } 图像生成属性 # 前缀spring.ai.stabilityai用作属性前缀，用于连接到Stability ai。 前缀spring.ai.stabilityai.image是属性前缀，用于配置稳定性ai的ImageModel实现。\n运行时选项 # StabilityAiImageOptions.java提供模型配置，例如要使用的模型、样式、大小等。 启动时，可以使用Stability AiImageModel（StabilityAiApi StabilityAiApi，Stability AiImageOptions选项）构造函数配置默认选项。或者，使用前面描述的spring.ai.openai.image.options.*属性。 在运行时，可以通过向ImagePrompt调用添加新的特定于请求的选项来覆盖默认选项。\nImageResponse response = stabilityaiImageModel.call( new ImagePrompt(\u0026#34;A light cream colored mini golden doodle\u0026#34;, StabilityAiImageOptions.builder() .stylePreset(\u0026#34;cinematic\u0026#34;) .N(4) .height(1024) .width(1024).build()) ); "},{"id":38,"href":"/docs/%E8%81%8A%E5%A4%A9%E5%AE%A2%E6%88%B7%E7%AB%AFapi/","title":"聊天客户端API","section":"Docs","content":" 聊天客户端API # ChatClient提供了一个流畅的API，用于与人工智能模型通信。 fluent API具有用于构建Prompt的组成部分的方法，该Prompt作为输入传递给AI模型。 人工智能模型处理两种主要类型的消息：用户消息（来自用户的直接输入）和系统消息（由系统生成以指导对话）。 这些消息通常包含占位符，这些占位符在运行时根据用户输入进行替换，以定制AI模型对用户输入的响应。 还可以指定提示选项，例如要使用的AI模型的名称，以及控制生成输出的随机性或创造性的温度设置。\n创建ChatClient # 使用ChatClient创建ChatClient。Builder对象。\n使用自动配置的ChatClient。建筑商 # 在最简单的用例中，SpringAI提供了SpringBoot自动配置，创建了一个原型ChatClient。生成器bean，供您注入到类中。\n@RestController class MyController { private final ChatClient chatClient; public MyController(ChatClient.Builder chatClientBuilder) { this.chatClient = chatClientBuilder.build(); } @GetMapping(\u0026#34;/ai\u0026#34;) String generation(String userInput) { return this.chatClient.prompt() .user(userInput) .call() .content(); } } 在这个简单的示例中，用户输入设置用户消息的内容。\n以编程方式创建ChatClient # 您可以禁用ChatClient。通过设置属性spring.ai.chat.client.enabled=false来构建自动配置。\nChatModel myChatModel = ... // usually autowired ChatClient.Builder builder = ChatClient.builder(this.myChatModel); // or create a ChatClient with the default builder settings: ChatClient chatClient = ChatClient.create(this.myChatModel); ChatClient Fluent API # ChatClient fluent API允许您使用重载提示方法以三种不同的方式创建提示，以启动fluent API:\nprompt（）：这个没有参数的方法允许您开始使用fluent API，允许您构建提示符的用户、系统和其他部分。 prompt（prompt-prompt）：该方法接受prompt参数，允许您传入使用prompt的非流畅API创建的prompt实例。 prompt（Stringcontent）：这是一种与前面的重载类似的方便方法。它接受用户的文本内容。 ChatClient响应 # ChatClient API提供了几种使用fluent API格式化来自AI模型的响应的方法。\n返回ChatResponse # 来自AI模型的响应是由类型ChatResponse定义的丰富结构。 下面显示了一个返回包含元数据的ChatResponse对象的示例，方法是在call（）方法之后调用chatRespons（）。\nChatResponse chatResponse = chatClient.prompt() .user(\u0026#34;Tell me a joke\u0026#34;) .call() .chatResponse(); 返回实体 # 您通常希望返回从返回的String映射的实体类。 例如，给定Java记录：\nrecord ActorFilms(String actor, List\u0026lt;String\u0026gt; movies) {} 您可以使用entity（）方法轻松地将AI模型的输出映射到此记录，如下所示：\nActorFilms actorFilms = chatClient.prompt() .user(\u0026#34;Generate the filmography for a random actor.\u0026#34;) .call() .entity(ActorFilms.class); 还有一个具有签名实体（ParameterizedTypeReference类型）的重载实体方法，允许您指定类型，如泛型Lists：\nList\u0026lt;ActorFilms\u0026gt; actorFilms = chatClient.prompt() .user(\u0026#34;Generate the filmography of 5 movies for Tom Hanks and Bill Murray.\u0026#34;) .call() .entity(new ParameterizedTypeReference\u0026lt;List\u0026lt;ActorFilms\u0026gt;\u0026gt;() {}); 流式处理响应 # stream（）方法允许您获得异步响应，如下所示：\nFlux\u0026lt;String\u0026gt; output = chatClient.prompt() .user(\u0026#34;Tell me a joke\u0026#34;) .stream() .content(); 您还可以使用方法Flux\u0026lt;ChatResponse\u0026gt;chatRespons（）来流式传输ChatRespense。 在将来，我们将提供一种方便的方法，允许您使用reactive stream（）方法返回Java实体。\nvar converter = new BeanOutputConverter\u0026lt;\u0026gt;(new ParameterizedTypeReference\u0026lt;List\u0026lt;ActorsFilms\u0026gt;\u0026gt;() {}); Flux\u0026lt;String\u0026gt; flux = this.chatClient.prompt() .user(u -\u0026gt; u.text(\u0026#34;\u0026#34;\u0026#34; Generate the filmography for a random actor. {format} \u0026#34;\u0026#34;\u0026#34;) .param(\u0026#34;format\u0026#34;, this.converter.getFormat())) .stream() .content(); String content = this.flux.collectList().block().stream().collect(Collectors.joining()); List\u0026lt;ActorFilms\u0026gt; actorFilms = this.converter.convert(this.content); 提示模板 # ChatClient fluent API允许您将用户和系统文本作为模板提供，其中包含在运行时替换的变量。\nString answer = ChatClient.create(chatModel).prompt() .user(u -\u0026gt; u .text(\u0026#34;Tell me the names of 5 movies whose soundtrack was composed by {composer}\u0026#34;) .param(\u0026#34;composer\u0026#34;, \u0026#34;John Williams\u0026#34;)) .call() .content(); 在内部，ChatClient使用PromptTemplate类来处理用户和系统文本，并根据给定的TemplateRenderer实现，用运行时提供的值替换变量。 Spring AI还为不需要模板处理的情况提供NoOpTemplateRenderer。 Spring AI还提供NoOpTemplateRenderer。 如果希望使用不同的模板引擎，可以直接向ChatClient提供TemplateRenderer接口的自定义实现。您也可以继续使用默认的StTemplateRenderer，但需要自定义配置。 例如，默认情况下，模板变量由{}语法标识。如果您计划在提示符中包含JSON，您可能希望使用不同的语法来避免与JSON语法冲突。例如，可以使用\u0026lt;和\u0026gt;分隔符。\nString answer = ChatClient.create(chatModel).prompt() .user(u -\u0026gt; u .text(\u0026#34;Tell me the names of 5 movies whose soundtrack was composed by \u0026lt;composer\u0026gt;\u0026#34;) .param(\u0026#34;composer\u0026#34;, \u0026#34;John Williams\u0026#34;)) .templateRenderer(StTemplateRenderer.builder().startDelimiterToken(\u0026#39;\u0026lt;\u0026#39;).endDelimiterToken(\u0026#39;\u0026gt;\u0026#39;).build()) .call() .content(); call（）返回值 # 在ChatClient上指定call（）方法后，响应类型有几个不同的选项。\nString content（）：返回响应的String内容 ChatResponse chatRespons（）：返回包含多个生成的ChatRespense对象，以及关于响应的元数据，例如，用于创建响应的令牌数量。 ChatClientResponse chatClientRespense（）：返回包含ChatResponse对象和ChatClient执行上下文的ChatClientResponse对象，使您可以访问执行顾问期间使用的其他数据（例如，在RAG流中检索的相关文档）。 entity（）返回Java类型 entity（ParameterizedTypeReference类型）：用于返回实体类型的集合。 实体（类类型）：用于返回特定的实体类型。 实体（StructuredOutputConverter）：用于指定Structured Outputconverter的实例，以将String转换为实体类型。 您还可以调用stream（）方法，而不是call（）。 stream（）返回值 # 在ChatClient上指定stream（）方法后，响应类型有几个选项：\nFluxcontent（）：返回AI模型生成的字符串的Flux。 FluxchatRespons（）：返回ChatRespense对象的Flux，其中包含关于响应的其他元数据。 FluxchatClientRespense（）：返回包含ChatResponse对象和ChatClient执行上下文的ChatClientRepose对象的Flux，使您可以访问执行顾问期间使用的其他数据（例如，在RAG流中检索的相关文档）。 使用默认值 # 在@Configuration类中创建具有默认系统文本的ChatClient可以简化运行时代码。\n默认系统文本 # 在下面的示例中，我们将配置系统文本，使其始终以海盗的声音回复。\n@Configuration class Config { @Bean ChatClient chatClient(ChatClient.Builder builder) { return builder.defaultSystem(\u0026#34;You are a friendly chat bot that answers question in the voice of a Pirate\u0026#34;) .build(); } } 和@RestController来调用它：\n@RestController class AIController { private final ChatClient chatClient; AIController(ChatClient chatClient) { this.chatClient = chatClient; } @GetMapping(\u0026#34;/ai/simple\u0026#34;) public Map\u0026lt;String, String\u0026gt; completion(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;completion\u0026#34;, this.chatClient.prompt().user(message).call().content()); } } 通过curl调用应用程序端点时，结果是：\n❯ curl localhost:8080/ai/simple {\u0026#34;completion\u0026#34;:\u0026#34;Why did the pirate go to the comedy club? To hear some arrr-rated jokes! Arrr, matey!\u0026#34;} 带参数的默认系统文本 # 在下面的示例中，我们将在系统文本中使用占位符来指定运行时而不是设计时的完成声音。\n@Configuration class Config { @Bean ChatClient chatClient(ChatClient.Builder builder) { return builder.defaultSystem(\u0026#34;You are a friendly chat bot that answers question in the voice of a {voice}\u0026#34;) .build(); } } @RestController class AIController { private final ChatClient chatClient; AIController(ChatClient chatClient) { this.chatClient = chatClient; } @GetMapping(\u0026#34;/ai\u0026#34;) Map\u0026lt;String, String\u0026gt; completion(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message, String voice) { return Map.of(\u0026#34;completion\u0026#34;, this.chatClient.prompt() .system(sp -\u0026gt; sp.param(\u0026#34;voice\u0026#34;, voice)) .user(message) .call() .content()); } } 通过httpie调用应用程序端点时，结果是：\nhttp localhost:8080/ai voice==\u0026#39;Robert DeNiro\u0026#39; { \u0026#34;completion\u0026#34;: \u0026#34;You talkin\u0026#39; to me? Okay, here\u0026#39;s a joke for ya: Why couldn\u0026#39;t the bicycle stand up by itself? Because it was two tired! Classic, right?\u0026#34; } 其他默认值 # 在ChatClient上。生成器级别，可以指定默认提示配置。\ndefaultOptions（ChatOptions ChatOptions）：传入在ChatOptions类中定义的可移植选项或特定于模型的选项，如OpenAiChatOptions。有关特定于模型的ChatOptions实现的更多信息，请参阅JavaDocs。 defaultFunction（字符串名称，字符串描述，java.util.function.function\u0026lt;I，O\u0026gt;function）：名称用于引用用户文本中的函数。该描述解释了函数的用途，并帮助AI模型选择正确的函数以获得准确的响应。函数参数是模型将在必要时执行的Java函数实例。 defaultFunctions（字符串…​ functionNames）：`java.util的bean名称。函数在应用程序上下文中定义。 defaultUser（String text）、defaultUser（Resource text）和默认用户（ConsumeruserSpecConsumer）：这些方法允许您定义用户文本。Consumer允许您使用lambda来指定用户文本和任何默认参数。 defaultAdvisors（顾问…​ advisor）：advisor允许修改用于创建提示的数据。QuestionAnswerAdvisor实现通过在提示中附加与用户文本相关的上下文信息来启用检索增强生成模式。 defaultAdvisors（ConsumeradvisorSpecConsumer）：此方法允许您定义一个Consumer，以使用AdvisorSpec配置多个顾问。顾问可以修改用于创建最终提示的数据。Consumer允许您指定lambda来添加顾问，如QuestionAnswerAdvisor，它通过在提示中附加基于用户文本的相关上下文信息来支持检索增强生成。 您可以在运行时使用没有默认前缀的相应方法覆盖这些默认值。 选项（聊天室选项聊天室选项） 函数（字符串名称，字符串描述， 函数（字符串…​ 函数名称） user（字符串文本），user（资源文本），用户（ConsumeruserSpecConsumer） 顾问（顾问…​ 顾问） 顾问（ConsumeradvisorSpecConsumer） 顾问 # Advisors API提供了一种灵活而强大的方法来拦截、修改和增强Spring应用程序中的AI驱动交互。 使用用户文本调用AI模型时的一个常见模式是使用上下文数据附加或增强提示。 此上下文数据可以是不同的类型。常见类型包括：\n您自己的数据：这是人工智能模型尚未训练的数据。即使模型看到类似的数据，附加的上下文数据在生成响应时优先。 对话历史：聊天模型的API是无状态的。如果你告诉人工智能模型你的名字，它将不会在随后的交互中记住它。会话历史记录必须与每个请求一起发送，以确保在生成响应时考虑以前的交互。 ChatClient中的Advisor配置 # ChatClient fluent API提供用于配置顾问的AdvisorSpec接口。该接口提供了用于添加参数、一次设置多个参数以及向链添加一个或多个顾问的方法。\ninterface AdvisorSpec { AdvisorSpec param(String k, Object v); AdvisorSpec params(Map\u0026lt;String, Object\u0026gt; p); AdvisorSpec advisors(Advisor... advisors); AdvisorSpec advisors(List\u0026lt;Advisor\u0026gt; advisors); } ChatClient.builder(chatModel) .build() .prompt() .advisors( new MessageChatMemoryAdvisor(chatMemory), new QuestionAnswerAdvisor(vectorStore) ) .user(userText) .call() .content(); 在此配置中，将首先执行MessageChatMemoryAdvisor，将对话历史添加到提示符中。然后，QuestionAnswerAdvisor将根据用户的问题和添加的对话历史记录执行搜索，可能会提供更相关的结果。 了解问答顾问 以下advisor实现使用ChatMemory接口向提示符提供对话历史的建议，对话历史在如何将内存添加到提示符的细节上有所不同\nMessageChatMemoryAdvisor:检索内存，并将其作为消息集合添加到提示符 PromptChatMemoryAdvisor：检索内存并将其添加到提示符的系统文本中。 VectorStoreChatMemoryAdvisor:构造函数VectorStore ChatMemoriyAdvisor（VectorStore-VectorStore，String defaultConversationId，int chatHistoryWindowSize，int order）此构造函数允许您： VectorStoreChatMemoryAdvisor.builder（）方法允许您指定默认对话ID、聊天历史窗口大小和要检索的聊天历史的顺序。 下面显示了使用多个顾问的@Service实现示例。 import static org.springframework.ai.chat.client.advisor.AbstractChatMemoryAdvisor.CHAT_MEMORY_CONVERSATION_ID_KEY; import static org.springframework.ai.chat.client.advisor.AbstractChatMemoryAdvisor.CHAT_MEMORY_RETRIEVE_SIZE_KEY; @Service public class CustomerSupportAssistant { private final ChatClient chatClient; public CustomerSupportAssistant(ChatClient.Builder builder, VectorStore vectorStore, ChatMemory chatMemory) { this.chatClient = builder .defaultSystem(\u0026#34;\u0026#34;\u0026#34; You are a customer chat support agent of an airline named \u0026#34;Funnair\u0026#34;. Respond in a friendly, helpful, and joyful manner. Before providing information about a booking or cancelling a booking, you MUST always get the following information from the user: booking number, customer first name and last name. Before changing a booking you MUST ensure it is permitted by the terms. If there is a charge for the change, you MUST ask the user to consent before proceeding. \u0026#34;\u0026#34;\u0026#34;) .defaultAdvisors( new MessageChatMemoryAdvisor(chatMemory), // CHAT MEMORY new QuestionAnswerAdvisor(vectorStore), // RAG new SimpleLoggerAdvisor()) .defaultFunctions(\u0026#34;getBookingDetails\u0026#34;, \u0026#34;changeBooking\u0026#34;, \u0026#34;cancelBooking\u0026#34;) // FUNCTION CALLING .build(); } public Flux\u0026lt;String\u0026gt; chat(String chatId, String userMessageContent) { return this.chatClient.prompt() .user(userMessageContent) .advisors(a -\u0026gt; a .param(CHAT_MEMORY_CONVERSATION_ID_KEY, chatId) .param(CHAT_MEMORY_RETRIEVE_SIZE_KEY, 100)) .stream().content(); } } 了解问答顾问\n检索增强生成 # 请参阅《 检索增强生成》指南。\n日志记录 # SimpleLoggerAdvisor是记录ChatClient的请求和响应数据的顾问。 要启用日志记录，请在创建ChatClient时将SimpleLoggerAdvisor添加到advisor链。\nChatResponse response = ChatClient.create(chatModel).prompt() .advisors(new SimpleLoggerAdvisor()) .user(\u0026#34;Tell me a joke?\u0026#34;) .call() .chatResponse(); 要查看日志，请将advisor包的日志记录级别设置为DEBUG： 将此添加到application.properties或application.yaml文件中。 您可以使用以下构造函数自定义记录来自AdvisedRequest和ChatResponse的数据：\nSimpleLoggerAdvisor( Function\u0026lt;AdvisedRequest, String\u0026gt; requestToString, Function\u0026lt;ChatResponse, String\u0026gt; responseToString ) 示例用法：\nSimpleLoggerAdvisor customLogger = new SimpleLoggerAdvisor( request -\u0026gt; \u0026#34;Custom request: \u0026#34; + request.userText, response -\u0026gt; \u0026#34;Custom response: \u0026#34; + response.getResult() ); 这允许您根据特定需要定制记录的信息。\n聊天记忆（不推荐） # ChatMemory接口表示聊天室会话历史记录的存储。它提供了向对话中添加消息、从对话中检索消息以及清除对话历史记录的方法。 目前有四种实现：CassandraChatMemory、Neo4jChatMemority和JdbcChatMemorary，它们分别在内存中为聊天对话历史记录提供存储，在Cassandra中保留生存时间，并在Neo4j和Jdbc中保留没有生存时间。\nCassandra聊天存储器 # 要创建具有生存时间的CassandraChatMemory，请执行以下操作：\nCassandraChatMemory.create(CassandraChatMemoryConfig.builder().withTimeToLive(Duration.ofDays(1)).build()); Neo4j聊天记忆 # Neo4j聊天室存储器支持以下配置参数：\nJdbc聊天记忆 # "},{"id":39,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/azure-openai%E8%BD%AF%E4%BB%B6/","title":"Azure OpenAI聊天","section":"聊天模型API","content":" Azure OpenAI聊天 # Azure的OpenAI产品由ChatGPT提供支持，超越了传统的OpenAI.功能，提供了具有增强功能的人工智能驱动的文本生成。Azure提供了额外的人工智能安全和负责任的人工智能功能，如他们最近的更新所强调的。 Azure通过将人工智能与一系列Azure服务集成，为Java开发人员提供了充分利用人工智能潜力的机会，这些服务包括与人工智能相关的资源，如Azure上的向量存储。\n前提条件 # Azure OpenAI客户端提供三种连接选项：使用Azure API密钥或使用OpenAI API密钥，或使用Microsoft Entra ID。\nAzure API密钥和终结点 # 要使用API密钥访问模型，请从 Azure门户上的Azure OpenAI服务部分获取Azure OpenAI端点和API密钥。 Spring AI定义了两个配置属性： 可以通过导出环境变量来设置这些配置属性：\nexport SPRING_AI_AZURE_OPENAI_API_KEY=\u0026lt;INSERT AZURE KEY HERE\u0026gt; export SPRING_AI_AZURE_OPENAI_ENDPOINT=\u0026lt;INSERT ENDPOINT URL HERE\u0026gt; OpenAI密钥 # 要使用OpenAI服务（而不是Azure）进行身份验证，请提供OpenAI API密钥。这将自动将端点设置为 api.openai.com/v1。 使用此方法时，将spring.ai.azure.openai.chat.options.deployment-name属性设置为要使用的openai模型的名称。\nexport SPRING_AI_AZURE_OPENAI_OPENAI_API_KEY=\u0026lt;INSERT OPENAI KEY HERE\u0026gt; Microsoft条目ID # 对于使用Microsoft Entra ID（以前称为Azure Active Directory）的无密钥身份验证，请仅设置spring.ai.Azure.openai.endpoint配置属性，而不是上面提到的api密钥属性。 仅查找端点属性，应用程序将评估用于检索凭据的几个不同选项，并将使用令牌凭据创建OpenAIClient实例。\n部署名称 # 要使用Azure AI应用程序，您需要通过Azure人工智能门户创建Azure人工智部署。 要开始，请按照以下步骤创建具有默认设置的展开： 此Azure配置与Spring Boot Azure AI Starter及其自动配置功能的默认配置一致。\nspring.ai.azure.openai.chat.options.deployment-name=\u0026lt;my deployment name\u0026gt; Azure OpenAI和OpenAI的不同部署结构导致Azure OpenAI客户端库中名为deploymentOrModelName的属性。\n访问OpenAI模型 # 您可以将客户端配置为直接使用OpenAI，而不是Azure OpenAI部署的模型。\n添加存储库和BOM表 # Spring AI工件发布在Maven Central和Spring Snapshot 存储库中。 为了帮助进行依赖关系管理，Spring AI提供了BOM（物料清单），以确保在整个项目中使用一致版本的Spring AI。请参阅依赖项管理部分，将Spring AI BOM添加到构建系统中。\n自动配置 # Spring AI为Azure OpenAI聊天客户端提供Spring Boot自动配置。 Azure OpenAI聊天客户端是使用Azure SDK提供的OpenAIClientBuilder创建的。Spring AI允许通过提供AzureOpenAIClientBuilderCustomizer bean来定制生成器。 例如，可以使用自定义程序来更改默认响应超时：\n@Configuration public class AzureOpenAiConfig { @Bean public AzureOpenAIClientBuilderCustomizer responseTimeoutCustomizer() { return openAiClientBuilder -\u0026gt; { HttpClientOptions clientOptions = new HttpClientOptions() .setResponseTimeout(Duration.ofMinutes(5)); openAiClientBuilder.httpClient(HttpClient.createDefault(clientOptions)); }; } } 聊天室属性 # 前缀spring.ai.azure.openai是配置与azure openai的连接的属性前缀。 前缀spring.ai.azure.openai.chat是为azure openai配置ChatModel实现的属性前缀。\n运行时选项 # AzureOpenAiChatOptions.java提供模型配置，例如要使用的模型、温度、频率惩罚等。 启动时，可以使用AzureOpenAiChatModel（api，options）构造函数或spring.ai.azure.openai.chat.options.*属性配置默认选项。 在运行时，可以通过向Prompt调用添加新的特定于请求的选项来覆盖默认选项。\nChatResponse response = chatModel.call( new Prompt( \u0026#34;Generate the names of 5 famous pirates.\u0026#34;, AzureOpenAiChatOptions.builder() .deploymentName(\u0026#34;gpt-4o\u0026#34;) .temperature(0.4) .build() )); 函数调用 # 您可以使用AzureOpenAiChatModel注册自定义Java函数，并让模型智能地选择输出包含参数的JSON对象，以调用一个或多个已注册的函数。\n多模态 # 多模态是指模型同时理解和处理来自各种来源的信息的能力，包括文本、图像、音频和其他数据格式。 Azure OpenAI可以将base64编码图像或图像URL的列表与 消息合并。 下面是摘自 OpenAiChatModelIT.java的代码示例，说明了使用GPT_4_O模型将用户文本与图像融合。\nURL url = new URL(\u0026#34;https://docs.spring.io/spring-ai/reference/_images/multimodal.test.png\u0026#34;); String response = ChatClient.create(chatModel).prompt() .options(AzureOpenAiChatOptions.builder().deploymentName(\u0026#34;gpt-4o\u0026#34;).build()) .user(u -\u0026gt; u.text(\u0026#34;Explain what do you see on this picture?\u0026#34;).media(MimeTypeUtils.IMAGE_PNG, this.url)) .call() .content(); 它将multimal.test.png图像作为输入： 以及文本消息“解释您在这张图片上看到了什么？”，并生成如下响应： 您还可以传入类路径资源，而不是URL，如下例所示\nResource resource = new ClassPathResource(\u0026#34;multimodality/multimodal.test.png\u0026#34;); String response = ChatClient.create(chatModel).prompt() .options(AzureOpenAiChatOptions.builder() .deploymentName(\u0026#34;gpt-4o\u0026#34;).build()) .user(u -\u0026gt; u.text(\u0026#34;Explain what do you see on this picture?\u0026#34;) .media(MimeTypeUtils.IMAGE_PNG, this.resource)) .call() .content(); 样本控制器 # 创建一个新的SpringBoot项目，并将Spring-ai-starter模型azure-openai添加到pom（或gradle）依赖项中。 在src/main/resources目录下添加application.properties文件，以启用和配置OpenAi聊天模型：\nspring.ai.azure.openai.api-key=YOUR_API_KEY spring.ai.azure.openai.endpoint=YOUR_ENDPOINT spring.ai.azure.openai.chat.options.deployment-name=gpt-4o spring.ai.azure.openai.chat.options.temperature=0.7 这将创建一个AzureOpenAiChatModel实现，您可以将其注入到类中。\n@RestController public class ChatController { private final AzureOpenAiChatModel chatModel; @Autowired public ChatController(AzureOpenAiChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, this.chatModel.call(message)); } @GetMapping(\u0026#34;/ai/generateStream\u0026#34;) public Flux\u0026lt;ChatResponse\u0026gt; generateStream(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { Prompt prompt = new Prompt(new UserMessage(message)); return this.chatModel.stream(prompt); } } 手动配置 # AzureOpenAiChatModel实现ChatModel.流式聊天模型，并使用 Azure OpenAI Java客户端。 要启用它，请将spring ai azure openai依赖项添加到项目的Maven pom.xml文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-azure-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-azure-openai\u0026#39; } 接下来，创建AzureOpenAiChatModel实例，并使用它生成文本响应：\nvar openAIClientBuilder = new OpenAIClientBuilder() .credential(new AzureKeyCredential(System.getenv(\u0026#34;AZURE_OPENAI_API_KEY\u0026#34;))) .endpoint(System.getenv(\u0026#34;AZURE_OPENAI_ENDPOINT\u0026#34;)); var openAIChatOptions = AzureOpenAiChatOptions.builder() .deploymentName(\u0026#34;gpt-4o\u0026#34;) .temperature(0.4) .maxTokens(200) .build(); var chatModel = AzureOpenAiChatModel.builder() .openAIClientBuilder(openAIClientBuilder) .defaultOptions(openAIChatOptions) .build(); ChatResponse response = chatModel.call( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); // Or with streaming responses Flux\u0026lt;ChatResponse\u0026gt; streamingResponses = chatModel.stream( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); "},{"id":40,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/%E6%9C%80%E5%B0%8F%E6%9C%80%E5%A4%A7%E5%80%BC/","title":"MiniMax聊天","section":"嵌入模型API","content":" MiniMax聊天 # Spring AI支持MiniMax的各种人工智能语言模型。您可以与MiniMax语言模型交互，并基于MiniMax模型创建多语言对话助手。\n前提条件 # 您需要使用MiniMax创建API来访问MiniMax语言模型。 在 MiniMax注册页面创建帐户，并在[ API密钥页面]( https://www.minimaxi.com/user-center/basic-information/interface-key)上生成令牌。\nexport SPRING_AI_MINIMAX_API_KEY=\u0026lt;INSERT KEY HERE\u0026gt; 添加存储库和BOM表 # Spring AI工件发布在Maven Central和Spring Snapshot 存储库中。 为了帮助进行依赖关系管理，Spring AI提供了BOM（物料清单），以确保在整个项目中使用一致版本的Spring AI。请参阅依赖项管理部分，将Spring AI BOM添加到构建系统中。\n自动配置 # Spring AI为Azure MiniMax嵌入模型提供Spring Boot自动配置。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-minimax\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-minimax\u0026#39; } 嵌入属性 # 重试属性 # 前缀spring.ai.retry用作属性前缀，允许您为MiniMax嵌入模型配置重试机制。\n连接属性 # 前缀spring.ai.minimax用作允许连接到minimax的属性前缀。\n配置属性 # 前缀spring.ai.minimax.embedding是为minimax配置EmbeddingModel实现的属性前缀。\n运行时选项 # MiniMaxEmbeddingOptions.java提供MiniMax配置，如要使用的模型等。 也可以使用spring.ai.miminax.embedding.options属性配置默认选项。 在启动时，使用MiniMaxEmbeddingModel构造函数设置用于所有嵌入请求的默认选项。 例如，要覆盖特定请求的默认模型名称：\nEmbeddingResponse embeddingResponse = embeddingModel.call( new EmbeddingRequest(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;), MiniMaxEmbeddingOptions.builder() .model(\u0026#34;Different-Embedding-Model-Deployment-Name\u0026#34;) .build())); 样本控制器 # 这将创建一个可以注入到类中的EmbeddingModel实现。\nspring.ai.minimax.api-key=YOUR_API_KEY spring.ai.minimax.embedding.options.model=embo-01 @RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(\u0026#34;/ai/embedding\u0026#34;) public Map embed(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { EmbeddingResponse embeddingResponse = this.embeddingModel.embedForResponse(List.of(message)); return Map.of(\u0026#34;embedding\u0026#34;, embeddingResponse); } } 手动配置 # 如果不使用Spring Boot，则可以手动配置MiniMax嵌入模型。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-minimax\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-minimax\u0026#39; } 接下来，创建MiniMaxEmbeddingModel实例，并使用它计算两个输入文本之间的相似性：\nvar miniMaxApi = new MiniMaxApi(System.getenv(\u0026#34;MINIMAX_API_KEY\u0026#34;)); var embeddingModel = new MiniMaxEmbeddingModel(minimaxApi, MetadataMode.EMBED, MiniMaxEmbeddingOptions.builder().model(\u0026#34;embo-01\u0026#34;).build()); EmbeddingResponse embeddingResponse = this.embeddingModel .embedForResponse(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;)); MiniMaxEmbeddingOptions提供嵌入请求的配置信息。\n"},{"id":41,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E5%9B%BE%E5%83%8F%E6%A8%A1%E5%9E%8B/%E5%BF%97%E6%99%AE%E8%89%BE/","title":"ZhiPuAI图像生成","section":"映像模型API","content":" ZhiPuAI图像生成 # Spring AI支持ZhiPuAI的图像生成模型CogView。\n前提条件 # 您需要使用ZhiPuAI创建API来访问ZhiPu AI语言模型。 在智浦AI注册页面创建账户，并在API Keys页面上生成代币。\nexport SPRING_AI_ZHIPU_AI_API_KEY=\u0026lt;INSERT KEY HERE\u0026gt; 添加存储库和BOM表 # Spring AI工件发布在Maven Central和Spring Snapshot 存储库中。 为了帮助进行依赖关系管理，Spring AI提供了BOM（物料清单），以确保在整个项目中使用一致版本的Spring AI。请参阅依赖项管理部分，将Spring AI BOM添加到构建系统中。\n自动配置 # Spring AI为智普AI聊天客户端提供Spring Boot自动配置。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-zhipuai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-zhipuai\u0026#39; } 图像生成属性 # 前缀spring.ai.zhipuai.image是属性前缀，允许您为zhipuai配置ImageModel实现。\n连接属性 # 前缀spring.ai.zhipuai用作允许连接到Zipuai的属性前缀。\n配置属性 # 重试属性 # 前缀spring.ai.retry用作属性前缀，允许您配置ZhiPuAI Image客户端的重试机制。\n运行时选项 # ZhiPuAiImageOptions.java提供模型配置，如要使用的模型、质量、大小等。 启动时，可以使用ZhiPuAiImageModel（ZhiPuAiImageApi ZhiPuAlImageAPI）构造函数和withDefaultOptions（ZhiPuAiImageOptions defaultOptions）方法配置默认选项。或者，使用前面描述的spring.ai.zhipuai.image.options.*属性。 在运行时，可以通过向ImagePrompt调用添加新的特定于请求的选项来覆盖默认选项。\nImageResponse response = zhiPuAiImageModel.call( new ImagePrompt(\u0026#34;A light cream colored mini golden doodle\u0026#34;, ZhiPuAiImageOptions.builder() .quality(\u0026#34;hd\u0026#34;) .N(4) .height(1024) .width(1024).build()) ); "},{"id":42,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/","title":"弹簧AI API","section":"Docs","content":" 弹簧AI API # 引言 # Spring AI API涵盖了广泛的功能。\nAI模型API # 可移植模型API跨聊天、文本到图像、音频转录、文本到语音和嵌入模型的人工智能提供商。 支持OpenAI、微软、亚马逊、谷歌、亚马逊Bedrock、Hugging Face等的人工智能模型。 向量存储API # 跨多个提供程序的可移植向量存储API，包括一个新颖的类似SQL的元数据过滤器API，该API也是可移植的。支持14个矢量数据库。\n工具调用API # SpringAI使人工智能模型很容易以@Tool注释方法或POJOjava.util的形式调用服务。函数对象。 检查Spring AI Tool Calling文档。\n自动配置 # 用于AI模型和向量存储的Spring Boot自动配置和启动器。\nETL数据工程 # 数据工程的ETL框架。这为将数据加载到向量数据库中提供了基础，有助于实现检索增强生成模式，该模式使您能够将数据带到AI模型中，以纳入其响应。 反馈和贡献 # 项目的 GitHub讨论是发送反馈的好地方。\n"},{"id":43,"href":"/docs/%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/%E8%89%B2%E5%BA%A6chroma/","title":"色度（Chroma）","section":"向量数据库","content":" 色度（Chroma） # 本节将指导您设置Chroma VectorStore以存储文档嵌入并执行相似性搜索。 Chroma是开源嵌入数据库。它为您提供了存储文档嵌入、内容和元数据以及搜索这些嵌入的工具，包括元数据过滤。\n前提条件 # 启动时，如果尚未配置所需的集合，则ChromaVectorStore会创建该集合。\n自动配置 # Spring AI为Chroma Vector Store提供Spring Boot自动配置。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-chroma\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-vector-store-chroma\u0026#39; } 向量存储实现可以为您初始化必要的模式，但您必须通过在适当的构造函数中指定initializeSchema布尔值或通过设置…​在application.properties文件中初始化schema=true。 此外，您还需要一个已配置的EmbeddingModelbean。有关更多信息，请参阅EmbeddingModel部分。 下面是所需bean的示例：\n@Bean public EmbeddingModel embeddingModel() { // Can be any other EmbeddingModel implementation. return new OpenAiEmbeddingModel(new OpenAiApi(System.getenv(\u0026#34;SPRING_AI_OPENAI_API_KEY\u0026#34;))); } 要连接到Chroma，您需要提供实例的访问详细信息。\n# Chroma Vector Store connection properties spring.ai.vectorstore.chroma.client.host=\u0026lt;your Chroma instance host\u0026gt; spring.ai.vectorstore.chroma.client.port=\u0026lt;your Chroma instance port\u0026gt; spring.ai.vectorstore.chroma.client.key-token=\u0026lt;your access token (if configure)\u0026gt; spring.ai.vectorstore.chroma.client.username=\u0026lt;your username (if configure)\u0026gt; spring.ai.vectorstore.chroma.client.password=\u0026lt;your password (if configure)\u0026gt; # Chroma Vector Store collection properties spring.ai.vectorstore.chroma.initialize-schema=\u0026lt;true or false\u0026gt; spring.ai.vectorstore.chroma.collection-name=\u0026lt;your collection name\u0026gt; # Chroma Vector Store configuration properties # OpenAI API key if the OpenAI auto-configuration is used. spring.ai.openai.api.key=\u0026lt;OpenAI Api-key\u0026gt; 请查看矢量存储的 配置参数列表，以了解默认值和配置选项。 现在，您可以在应用程序中自动连接Chroma Vector Store并使用它\n@Autowired VectorStore vectorStore; // ... List \u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); // Add the documents vectorStore.add(documents); // Retrieve documents similar to a query List\u0026lt;Document\u0026gt; results = this.vectorStore.similaritySearch(SearchRequest.builder().query(\u0026#34;Spring\u0026#34;).topK(5).build()); 配置属性 # 您可以在SpringBoot配置中使用以下属性来定制向量存储。\n元数据筛选 # 您也可以将通用的、可移植的元数据过滤器与ChromaVector存储一起使用。 例如，可以使用文本表达式语言：\nvectorStore.similaritySearch( SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(\u0026#34;author in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; article_type == \u0026#39;blog\u0026#39;\u0026#34;).build()); 或以编程方式使用筛选器。表达式DSL:\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(b.and( b.in(\u0026#34;john\u0026#34;, \u0026#34;jill\u0026#34;), b.eq(\u0026#34;article_type\u0026#34;, \u0026#34;blog\u0026#34;)).build()).build()); 例如，此可移植筛选器表达式：\nauthor in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; article_type == \u0026#39;blog\u0026#39; 转换为专有的Chroma格式\n{\u0026#34;$and\u0026#34;:[ {\u0026#34;author\u0026#34;: {\u0026#34;$in\u0026#34;: [\u0026#34;john\u0026#34;, \u0026#34;jill\u0026#34;]}}, {\u0026#34;article_type\u0026#34;:{\u0026#34;$eq\u0026#34;:\u0026#34;blog\u0026#34;}}] } 手动配置 # 如果您喜欢手动配置Chroma Vector Store，可以通过在Spring Boot应用程序中创建ChromaVectorStore bean来完成。 将这些依赖项添加到项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-chroma-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; OpenAI：计算嵌入时需要。您可以使用任何其他嵌入模型实现。 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 示例代码 # 创建RestClient。使用适当的ChromaDB授权配置构建器实例，并使用它创建ChromaApi实例：\n@Bean public RestClient.Builder builder() { return RestClient.builder().requestFactory(new SimpleClientHttpRequestFactory()); } @Bean public ChromaApi chromaApi(RestClient.Builder restClientBuilder) { String chromaUrl = \u0026#34;http://localhost:8000\u0026#34;; ChromaApi chromaApi = new ChromaApi(chromaUrl, restClientBuilder); return chromaApi; } 通过将Spring Boot OpenAI启动器添加到您的项目中来集成OpenAI的嵌入。这为您提供了嵌入式客户端的实现：\n@Bean public VectorStore chromaVectorStore(EmbeddingModel embeddingModel, ChromaApi chromaApi) { return ChromaVectorStore.builder(chromaApi, embeddingModel) .collectionName(\u0026#34;TestCollection\u0026#34;) .initializeSchema(true) .build(); } 在主代码中，创建一些文档：\nList\u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); 将文档添加到向量存储：\nvectorStore.add(documents); 最后，检索类似于查询的文档：\nList\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch(\u0026#34;Spring\u0026#34;); 如果一切顺利，您应该检索包含文本“Spring AI rocks！！”的文档。\n在本地运行Chroma # docker run -it --rm --name chroma -p 8000:8000 ghcr.io/chroma-core/chroma:1.0.0 在localhost:8000/api/v1上启动色度存储\n"},{"id":44,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/deepseek%E6%B7%B1%E5%BA%A6%E6%90%9C%E7%B4%A2/","title":"DeepSeek聊天","section":"聊天模型API","content":" DeepSeek聊天 # Spring AI支持DeepSeek的各种人工智能语言模型。您可以与DeepSeek语言模型交互，并基于DeepSeok模型创建多语言对话助手。\n前提条件 # 您需要使用DeepSeek创建API密钥来访问DeepSeok语言模型。\nexport SPRING_AI_DEEPSEEK_API_KEY=\u0026lt;INSERT KEY HERE\u0026gt; 添加存储库和BOM表 # Spring AI工件发布在Spring Milestone和Snapshot 存储库中。 为了帮助进行依赖关系管理，Spring AI提供了BOM（BOM表），以确保在整个项目中使用一致版本的Spring AI。请参阅依赖项管理部分，将Spring AI BOM添加到构建系统中。\n自动配置 # Spring AI为DeepSeek聊天模型提供Spring Boot自动配置。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-deepseek-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或Gradle build.Gradle文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-deepseek-spring-boot-starter\u0026#39; } 聊天室属性 # 重试属性 # 前缀spring.ai.retry用作属性前缀，允许您配置DeepSeek聊天模型的重试机制。\n连接属性 # 前缀spring.ai.deepseek用作允许连接到deepseek的属性前缀。\n配置属性 # 前缀spring.ai.deepseek.chat是属性前缀，允许您配置deepseek的聊天模型实现。\n运行时选项 # DeepSeekChatOptions.java提供模型配置，例如要使用的模型、温度、频率惩罚等。 启动时，可以使用DeepSeekChatModel（api，options）构造函数或spring.ai.deepseek.chat.options.*属性配置默认选项。 在运行时，可以通过向Prompt调用添加新的特定于请求的选项来覆盖默认选项。\nChatResponse response = chatModel.call( new Prompt( \u0026#34;Generate the names of 5 famous pirates. Please provide the JSON response without any code block markers such as ```json```.\u0026#34;, DeepSeekChatOptions.builder() .withModel(DeepSeekApi.ChatModel.DEEPSEEK_CHAT.getValue()) .withTemperature(0.8f) .build() )); 样本控制器（自动配置） # 创建一个新的SpringBoot项目，并将Spring-aideepseek-springbootstarter添加到pom（或gradle）依赖项中。 在src/main/resources目录下添加application.properties文件，以启用和配置DeepSeek聊天模型：\nspring.ai.deepseek.api-key=YOUR_API_KEY spring.ai.deepseek.chat.options.model=deepseek-chat spring.ai.deepseek.chat.options.temperature=0.8 这将创建一个DeepSeekChatModel实现，您可以将其注入到类中。\n@RestController public class ChatController { private final DeepSeekChatModel chatModel; @Autowired public ChatController(DeepSeekChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, chatModel.call(message)); } @GetMapping(\u0026#34;/ai/generateStream\u0026#34;) public Flux\u0026lt;ChatResponse\u0026gt; generateStream(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { var prompt = new Prompt(new UserMessage(message)); return chatModel.stream(prompt); } } 聊天前缀完成 # 聊天前缀完成遵循聊天完成API，其中用户为模型提供助手的前缀消息，以完成消息的其余部分。 使用前缀完成时，用户必须确保消息列表中的最后一条消息是DeepSeekAssistantMessage。 下面是一个完整的聊天前缀完成Java代码示例。在本例中，我们将助手的前缀消息设置为“``python\\n”，以强制模型输出python代码，并将stop参数设置为[\u0026rsquo;`\u0026rsquo;]，以防止模型进行其他解释。\n@RestController public class CodeGenerateController { private final DeepSeekChatModel chatModel; @Autowired public ChatController(DeepSeekChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generatePythonCode\u0026#34;) public String generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Please write quick sort code\u0026#34;) String message) { UserMessage userMessage = new UserMessage(message); Message assistantMessage = DeepSeekAssistantMessage.prefixAssistantMessage(\u0026#34;```python\\\\n\u0026#34;); Prompt prompt = new Prompt(List.of(userMessage, assistantMessage), ChatOptions.builder().stopSequences(List.of(\u0026#34;```\u0026#34;)).build()); ChatResponse response = chatModel.call(prompt); return response.getResult().getOutput().getText(); } } 推理模型（deepseek reasoner） # deepseek推理机是DeepSeak开发的一种推理模型。在给出最终答案之前，模型首先生成思想链（CoT），以提高其响应的准确性。我们的API为用户提供了对deepseek reasoner生成的CoT内容的访问，使他们能够查看、显示和提取它。 您可以使用DeepSeekAssistantMessage来获取deepseek推理器生成的CoT内容。\npublic void deepSeekReasonerExample() { DeepSeekChatOptions promptOptions = DeepSeekChatOptions.builder() .model(DeepSeekApi.ChatModel.DEEPSEEK_REASONER.getValue()) .build(); Prompt prompt = new Prompt(\u0026#34;9.11 and 9.8, which is greater?\u0026#34;, promptOptions); ChatResponse response = chatModel.call(prompt); // Get the CoT content generated by deepseek-reasoner, only available when using deepseek-reasoner model DeepSeekAssistantMessage deepSeekAssistantMessage = (DeepSeekAssistantMessage) response.getResult().getOutput(); String reasoningContent = deepSeekAssistantMessage.getReasoningContent(); String text = deepSeekAssistantMessage.getText(); } 推理模型多轮对话 # 在每一轮对话中，模型输出CoT（reasoning_content）和最终答案（content）。在下一轮对话中，前几轮的CoT不会连接到上下文中，如下图所示： 请注意，如果reasoning_content字段包含在输入消息序列中，则API将返回400错误。因此，在发出API请求之前，应该从API响应中删除reasoning_content字段，如API示例所示。\npublic String deepSeekReasonerMultiRoundExample() { List\u0026lt;Message\u0026gt; messages = new ArrayList\u0026lt;\u0026gt;(); messages.add(new UserMessage(\u0026#34;9.11 and 9.8, which is greater?\u0026#34;)); DeepSeekChatOptions promptOptions = DeepSeekChatOptions.builder() .model(DeepSeekApi.ChatModel.DEEPSEEK_REASONER.getValue()) .build(); Prompt prompt = new Prompt(messages, promptOptions); ChatResponse response = chatModel.call(prompt); DeepSeekAssistantMessage deepSeekAssistantMessage = (DeepSeekAssistantMessage) response.getResult().getOutput(); String reasoningContent = deepSeekAssistantMessage.getReasoningContent(); String text = deepSeekAssistantMessage.getText(); messages.add(new AssistantMessage(Objects.requireNonNull(text))); messages.add(new UserMessage(\u0026#34;How many Rs are there in the word \u0026#39;strawberry\u0026#39;?\u0026#34;)); Prompt prompt2 = new Prompt(messages, promptOptions); ChatResponse response2 = chatModel.call(prompt2); DeepSeekAssistantMessage deepSeekAssistantMessage2 = (DeepSeekAssistantMessage) response2.getResult().getOutput(); String reasoningContent2 = deepSeekAssistantMessage2.getReasoningContent(); return deepSeekAssistantMessage2.getText(); } 手动配置 # DeepSeekChatModel实现ChatModels和StreamingChatModel.并使用低级DeepSekeApi客户端连接到DeepSeok服务。 将spring ai deepseek依赖项添加到项目的Maven pom.xml文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-deepseek\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或Gradle build.Gradle文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-deepseek\u0026#39; } 接下来，创建DeepSeekChatModel并将其用于文本生成：\nvar deepSeekApi = new DeepSeekApi(System.getenv(\u0026#34;DEEPSEEK_API_KEY\u0026#34;)); var chatModel = new DeepSeekChatModel(deepSeekApi, DeepSeekChatOptions.builder() .withModel(DeepSeekApi.ChatModel.DEEPSEEK_CHAT.getValue()) .withTemperature(0.4f) .withMaxTokens(200) .build()); ChatResponse response = chatModel.call( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); // Or with streaming responses Flux\u0026lt;ChatResponse\u0026gt; streamResponse = chatModel.stream( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); DeepSeekChatOptions提供聊天请求的配置信息。\n低级DeepSeekApi客户端 # DeepSeekApi是DeepSeok API的轻量级Java客户端。 下面是一个简单的片段，演示如何以编程方式使用API：\nDeepSeekApi deepSeekApi = new DeepSeekApi(System.getenv(\u0026#34;DEEPSEEK_API_KEY\u0026#34;)); ChatCompletionMessage chatCompletionMessage = new ChatCompletionMessage(\u0026#34;Hello world\u0026#34;, Role.USER); // Sync request ResponseEntity\u0026lt;ChatCompletion\u0026gt; response = deepSeekApi.chatCompletionEntity( new ChatCompletionRequest(List.of(chatCompletionMessage), DeepSeekApi.ChatModel.DEEPSEEK_CHAT.getValue(), 0.7f, false)); // Streaming request Flux\u0026lt;ChatCompletionChunk\u0026gt; streamResponse = deepSeekApi.chatCompletionStream( new ChatCompletionRequest(List.of(chatCompletionMessage), DeepSeekApi.ChatModel.DEEPSEEK_CHAT.getValue(), 0.7f, true)); 有关更多信息，请参阅DeepSeekApi.java的JavaDoc。\nDeepSeekApi示例 # DeepSeekApiIT.java测试提供了一些如何使用轻量级库的一般示例。 "},{"id":45,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/oci-genai%E5%85%AC%E5%8F%B8/","title":"Oracle云基础架构（OCI）GenAI嵌入","section":"嵌入模型API","content":" Oracle云基础架构（OCI）GenAI嵌入 # OCI GenAI服务提供按需模型或专用AI集群的文本嵌入。 OCI嵌入模型页面和 OCI文本嵌入页面提供了有关在OCI上使用和托管嵌入模型的详细信息。\n前提条件 # 添加存储库和BOM表 # Spring AI工件发布在Maven Central和Spring Snapshot 存储库中。 为了帮助进行依赖关系管理，Spring AI提供了BOM（物料清单），以确保在整个项目中使用一致版本的Spring AI。请参阅依赖项管理部分，将Spring AI BOM添加到构建系统中。\n自动配置 # Spring AI为OCI GenAI嵌入客户端提供Spring Boot自动配置。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-oci-genai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-oci-genai\u0026#39; } 嵌入属性 # 前缀spring.ai.oci.genai是配置与oci genai的连接的属性前缀。 前缀spring.ai.oci.genai.mbedding是为oci genai配置EmbeddingModel实现的属性前缀\n运行时选项 # OCIEmbeddingOptions提供嵌入请求的配置信息。 在启动时，使用OCIEmbeddingOptions构造函数设置用于所有嵌入请求的默认选项。 例如，要覆盖特定请求的默认模型名称：\nEmbeddingResponse embeddingResponse = embeddingModel.call( new EmbeddingRequest(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;), OCIEmbeddingOptions.builder() .model(\u0026#34;my-other-embedding-model\u0026#34;) .build() )); 示例代码 # 这将创建一个可以注入到类中的EmbeddingModel实现。\nspring.ai.oci.genai.embedding.model=\u0026lt;your model\u0026gt; spring.ai.oci.genai.embedding.compartment=\u0026lt;your model compartment\u0026gt; @RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(\u0026#34;/ai/embedding\u0026#34;) public Map embed(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { EmbeddingResponse embeddingResponse = this.embeddingModel.embedForResponse(List.of(message)); return Map.of(\u0026#34;embedding\u0026#34;, embeddingResponse); } } 手动配置 # 如果您不喜欢使用SpringBoot自动配置，则可以在应用程序中手动配置OCIEmbeddingModel。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-oci-genai-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-oci-genai-openai\u0026#39; } 接下来，创建OCIEmbeddingModel实例，并使用它计算两个输入文本之间的相似性：\nfinal String EMBEDDING_MODEL = \u0026#34;cohere.embed-english-light-v2.0\u0026#34;; final String CONFIG_FILE = Paths.get(System.getProperty(\u0026#34;user.home\u0026#34;), \u0026#34;.oci\u0026#34;, \u0026#34;config\u0026#34;).toString(); final String PROFILE = \u0026#34;DEFAULT\u0026#34;; final String REGION = \u0026#34;us-chicago-1\u0026#34;; final String COMPARTMENT_ID = System.getenv(\u0026#34;OCI_COMPARTMENT_ID\u0026#34;); var authProvider = new ConfigFileAuthenticationDetailsProvider( this.CONFIG_FILE, this.PROFILE); var aiClient = GenerativeAiInferenceClient.builder() .region(Region.valueOf(this.REGION)) .build(this.authProvider); var options = OCIEmbeddingOptions.builder() .model(this.EMBEDDING_MODEL) .compartment(this.COMPARTMENT_ID) .servingMode(\u0026#34;on-demand\u0026#34;) .build(); var embeddingModel = new OCIEmbeddingModel(this.aiClient, this.options); List\u0026lt;Double\u0026gt; embedding = this.embeddingModel.embed(new Document(\u0026#34;How many provinces are in Canada?\u0026#34;)); "},{"id":46,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E5%9B%BE%E5%83%8F%E6%A8%A1%E5%9E%8B/%E5%8D%83%E5%B8%86/","title":"千帆形象","section":"映像模型API","content":" 千帆形象 # 此功能已移至Spring AI社区存储库。 有关最新版本，请访问github.com/spring-ai-community/chanfan。\n"},{"id":47,"href":"/docs/%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/","title":"向量数据库","section":"Docs","content":" 向量数据库 # 向量数据库是一种特殊类型的数据库，在人工智能应用中发挥着重要作用。 在向量数据库中，查询不同于传统的关系数据库。 向量数据库用于将数据与人工智能模型集成。 下面的部分描述了用于使用多个向量数据库实现和一些高级示例用法的SpringAI接口。 最后一节旨在揭开向量数据库中相似性搜索的底层方法的神秘面纱。\nAPI概述 # 本节作为Spring AI框架中VectorStore接口及其相关类的指南。 Spring AI提供了一个抽象的API，用于通过VectorStore接口与向量数据库交互。 下面是VectorStore接口定义：\npublic interface VectorStore extends DocumentWriter { default String getName() { return this.getClass().getSimpleName(); } void add(List\u0026lt;Document\u0026gt; documents); void delete(List\u0026lt;String\u0026gt; idList); void delete(Filter.Expression filterExpression); default void delete(String filterExpression) { ... }; List\u0026lt;Document\u0026gt; similaritySearch(String query); List\u0026lt;Document\u0026gt; similaritySearch(SearchRequest request); default \u0026lt;T\u0026gt; Optional\u0026lt;T\u0026gt; getNativeClient() { return Optional.empty(); } } 和相关的SearchRequest生成器：\npublic class SearchRequest { public static final double SIMILARITY_THRESHOLD_ACCEPT_ALL = 0.0; public static final int DEFAULT_TOP_K = 4; private String query = \u0026#34;\u0026#34;; private int topK = DEFAULT_TOP_K; private double similarityThreshold = SIMILARITY_THRESHOLD_ACCEPT_ALL; @Nullable private Filter.Expression filterExpression; public static Builder from(SearchRequest originalSearchRequest) { return builder().query(originalSearchRequest.getQuery()) .topK(originalSearchRequest.getTopK()) .similarityThreshold(originalSearchRequest.getSimilarityThreshold()) .filterExpression(originalSearchRequest.getFilterExpression()); } public static class Builder { private final SearchRequest searchRequest = new SearchRequest(); public Builder query(String query) { Assert.notNull(query, \u0026#34;Query can not be null.\u0026#34;); this.searchRequest.query = query; return this; } public Builder topK(int topK) { Assert.isTrue(topK \u0026gt;= 0, \u0026#34;TopK should be positive.\u0026#34;); this.searchRequest.topK = topK; return this; } public Builder similarityThreshold(double threshold) { Assert.isTrue(threshold \u0026gt;= 0 \u0026amp;\u0026amp; threshold \u0026lt;= 1, \u0026#34;Similarity threshold must be in [0,1] range.\u0026#34;); this.searchRequest.similarityThreshold = threshold; return this; } public Builder similarityThresholdAll() { this.searchRequest.similarityThreshold = 0.0; return this; } public Builder filterExpression(@Nullable Filter.Expression expression) { this.searchRequest.filterExpression = expression; return this; } public Builder filterExpression(@Nullable String textExpression) { this.searchRequest.filterExpression = (textExpression != null) ? new FilterExpressionTextParser().parse(textExpression) : null; return this; } public SearchRequest build() { return this.searchRequest; } } public String getQuery() {...} public int getTopK() {...} public double getSimilarityThreshold() {...} public Filter.Expression getFilterExpression() {...} } 要将数据插入向量数据库，请将其封装在Document对象中。 在插入到向量数据库中时，使用嵌入模型将文本内容转换为数字数组或浮点[]，称为向量嵌入。嵌入模型，如Word2Vec、GLoVE和BERT，或OpenAI的文本嵌入-ada-002，用于将单词、句子或段落转换为这些向量嵌入。 向量数据库的作用是存储和促进这些嵌入的相似性搜索。它本身不生成嵌入。为了创建向量嵌入，应该使用EmbeddingModel。 接口中的相似性Search方法允许检索类似于给定查询字符串的文档。可以使用以下参数微调这些方法：\nk： 一个整数，指定要返回的最大相似文档数。这通常被称为“top K”搜索，或“K最近邻居”（KNN）。 阈值：从0到1的双精度值，其中接近1的值表示更高的相似性。默认情况下，如果将阈值设置为0.75，则仅返回相似性高于该值的文档。 过滤器。表达式：用于传递流畅DSL（领域特定语言）表达式的类，该表达式的功能类似于SQL中的“where”子句，但它仅适用于文档的元数据键值对。 filterExpression：基于ANTLR4的外部DSL，接受过滤器表达式作为字符串。例如，对于诸如country、year和isActive的元数据键，可以使用如下表达式：country==\u0026lsquo;UK\u0026rsquo;\u0026amp;\u0026amp;year\u0026gt;=2020\u0026amp;\u0026amp;isActive==true。 查找有关筛选器的详细信息。 元数据筛选器部分中的表达式。 架构初始化 # 一些向量存储要求在使用之前初始化其后端架构。\n批处理策略 # 在使用向量存储时，通常需要嵌入大量文档。 为了解决这个令牌限制，SpringAI实现了一个批处理策略。 Spring AI通过BatchingStrategy接口提供了此功能，该接口允许根据令牌计数以子批次处理文档。 核心BatchingStrategy接口定义如下：\npublic interface BatchingStrategy { List\u0026lt;List\u0026lt;Document\u0026gt;\u0026gt; batch(List\u0026lt;Document\u0026gt; documents); } 该接口定义了单个方法batch，该方法获取文档列表并返回文档批次列表。\n默认实现 # Spring AI提供了一个名为TokenCountBatchingStrategy的默认实现。 TokenCountBatchingStrategy的关键功能： 该策略估计每个文档的令牌计数，在不超过最大输入令牌计数的情况下将它们分组为批，并在单个文档超过此限制时引发异常。 您还可以自定义TokenCountBatchingStrategy，以更好地满足您的特定要求。这可以通过在SpringBoot@Configuration类中创建具有自定义参数的新实例来完成。 下面是如何创建自定义TokenCountBatchingStrategy bean的示例：\n@Configuration public class EmbeddingConfig { @Bean public BatchingStrategy customTokenCountBatchingStrategy() { return new TokenCountBatchingStrategy( EncodingType.CL100K_BASE, // Specify the encoding type 8000, // Set the maximum input token count 0.1 // Set the reserve percentage ); } } 在此配置中： 默认情况下，此构造函数使用Document。DEFAULT_CONTENT_FORMATTER用于内容格式和MetadataMode。无用于元数据处理。如果需要自定义这些参数，可以使用带附加参数的完整构造函数。 一旦定义，该自定义TokenCountBatchingStrategy bean将由应用程序中的EmbeddingModel实现自动使用，替换默认策略。 TokenCountBatchingStrategy在内部使用TokenCountEstimator（特别是JTokkitTokenContountEstimitor）来计算令牌计数以进行有效的批处理。这确保基于指定的编码类型进行准确的令牌估计。 此外，TokenCountBatchingStrategy允许您传入自己的TokenContountEstimator接口实现，从而提供了灵活性。此功能使您能够使用根据特定需求定制的自定义令牌计数策略。例如：\nTokenCountEstimator customEstimator = new YourCustomTokenCountEstimator(); TokenCountBatchingStrategy strategy = new TokenCountBatchingStrategy( this.customEstimator, 8000, // maxInputTokenCount 0.1, // reservePercentage Document.DEFAULT_CONTENT_FORMATTER, MetadataMode.NONE ); 自定义实现 # 虽然TokenCountBatchingStrategy提供了一个健壮的默认实现，但您可以自定义批处理策略以满足您的特定需求。 要自定义批处理策略，请在Spring Boot应用程序中定义BatchingStrategy bean：\n@Configuration public class EmbeddingConfig { @Bean public BatchingStrategy customBatchingStrategy() { return new CustomBatchingStrategy(); } } 然后，应用程序中的EmbeddingModel实现将自动使用此自定义BatchingStrategy。\nVectorStore实现 # 以下是VectorStore接口的可用实现：\nAzure矢量搜索-Azure矢量存储。 Apache Cassandra-Apache卡桑德拉矢量存储。 Chroma Vector Store-色度向量存储。 Elasticsearch向量存储-Elasticsearch向量存储。 GemFire矢量存储-GemFire.矢量存储。 MariaDB向量存储-MariaDB矢量存储。 Milvus矢量存储-Milvus向量存储。 MongoDB Atlas矢量存储-MongoDB阿特拉斯矢量存储。 Neo4j矢量存储-Neo4j矢量存储。 OpenSearch Vector Store-开放搜索向量存储。 Oracle矢量存储—Oracle数据库矢量存储。 PgVector存储-PostgreSQL/PgVector矢量存储。 Pinecone矢量存储-Pinecone矢量存储。 Qdrant Vector Store-矢量存储。 Redis Vector Store-Redis向量存储。 SAP Hana矢量存储-SAP Hana矢量存储。 Typesense向量存储-Typesense矢量存储。 Weaviate Vector Store-弱化向量存储。 SimpleVectorStore-持久向量存储的简单实现，非常适合教育目的。 在未来的版本中可能支持更多的实现。 如果您有一个需要Spring AI支持的向量数据库，请在GitHub上打开一个问题，或者更好地，提交一个带有实现的pull请求。 有关每个VectorStore实现的信息可以在本章的小节中找到。 示例用法 # 要计算向量数据库的嵌入，您需要选择与所使用的更高级别AI模型匹配的嵌入模型。 例如，在OpenAI的ChatGPT中，我们使用OpenAiEmbeddingModel和一个名为text-embedding-ada-002的模型。 SpringBootstarter的OpenAI自动配置使得EmbeddingModel的实现可以在Spring应用程序上下文中用于依赖注入。 将数据加载到向量存储中的一般用法是在类似批处理的作业中完成的，方法是首先将数据加载至Spring AI的Document类，然后调用save方法。 给定对源文件的String引用，该源文件表示一个JSON文件，其中包含我们想要加载到向量数据库中的数据，我们使用Spring AI的JsonReader加载JSON中的特定字段，该字段将它们拆分为小块，然后将这些小块传递给向量存储实现。\n@Autowired VectorStore vectorStore; void load(String sourceFile) { JsonReader jsonReader = new JsonReader(new FileSystemResource(sourceFile), \u0026#34;price\u0026#34;, \u0026#34;name\u0026#34;, \u0026#34;shortDescription\u0026#34;, \u0026#34;description\u0026#34;, \u0026#34;tags\u0026#34;); List\u0026lt;Document\u0026gt; documents = jsonReader.get(); this.vectorStore.add(documents); } 后来，当用户问题被传递到AI模型中时，进行相似性搜索以检索相似的文档，然后将这些文档“插入”到提示符中，作为用户问题的上下文。\nString question = \u0026lt;question from user\u0026gt; List\u0026lt;Document\u0026gt; similarDocuments = store.similaritySearch(this.question); 可以将其他选项传递到similaritySearch方法中，以定义要检索的文档数量和相似性搜索的阈值。\n元数据筛选器 # 本节介绍可以对查询结果使用的各种筛选器。\n筛选器字符串 # 可以将类似SQL的筛选器表达式作为String传递给其中一个类似的Search重载。 考虑以下示例：\n“国家==\u0026lsquo;BG\u0026rsquo;” “类型==\u0026lsquo;戏剧\u0026rsquo;\u0026amp;\u0026amp;year\u0026gt;=2020” “[\u0026lsquo;comedy\u0026rsquo;，\u0026lsquo;纪录片\u0026rsquo;，\u0026lsquo;戏剧\u0026rsquo;]中的流派” 过滤器。表达式 # 可以创建Filter的实例。具有公开流畅API的FilterExpressionBuilder的表达式。\nFilterExpressionBuilder b = new FilterExpressionBuilder(); Expression expression = this.b.eq(\u0026#34;country\u0026#34;, \u0026#34;BG\u0026#34;).build(); 可以使用以下运算符构建复杂的表达式：\nEQUALS: \u0026#39;==\u0026#39; MINUS : \u0026#39;-\u0026#39; PLUS: \u0026#39;+\u0026#39; GT: \u0026#39;\u0026gt;\u0026#39; GE: \u0026#39;\u0026gt;=\u0026#39; LT: \u0026#39;\u0026lt;\u0026#39; LE: \u0026#39;\u0026lt;=\u0026#39; NE: \u0026#39;!=\u0026#39; 可以使用以下运算符组合表达式：\nAND: \u0026#39;AND\u0026#39; | \u0026#39;and\u0026#39; | \u0026#39;\u0026amp;\u0026amp;\u0026#39;; OR: \u0026#39;OR\u0026#39; | \u0026#39;or\u0026#39; | \u0026#39;||\u0026#39;; 考虑以下示例：\nExpression exp = b.and(b.eq(\u0026#34;genre\u0026#34;, \u0026#34;drama\u0026#34;), b.gte(\u0026#34;year\u0026#34;, 2020)).build(); 您还可以使用以下运算符：\nIN: \u0026#39;IN\u0026#39; | \u0026#39;in\u0026#39;; NIN: \u0026#39;NIN\u0026#39; | \u0026#39;nin\u0026#39;; NOT: \u0026#39;NOT\u0026#39; | \u0026#39;not\u0026#39;; 考虑以下示例：\nExpression exp = b.and(b.in(\u0026#34;genre\u0026#34;, \u0026#34;drama\u0026#34;, \u0026#34;documentary\u0026#34;), b.not(b.lt(\u0026#34;year\u0026#34;, 2020))).build(); 从Vector Store中删除文档 # Vector Store接口提供了多种删除文档的方法，允许您通过特定的文档ID或使用筛选器表达式删除数据。\n按文档ID删除 # 删除文档的最简单方法是提供文档ID列表：\nvoid delete(List\u0026lt;String\u0026gt; idList); 此方法删除ID与提供的列表中的ID匹配的所有文档。\n// Create and add document Document document = new Document(\u0026#34;The World is Big\u0026#34;, Map.of(\u0026#34;country\u0026#34;, \u0026#34;Netherlands\u0026#34;)); vectorStore.add(List.of(document)); // Delete document by ID vectorStore.delete(List.of(document.getId())); 按筛选器删除表达式 # 对于更复杂的删除条件，可以使用过滤器表达式：\nvoid delete(Filter.Expression filterExpression); 此方法接受筛选器。定义应删除文档的条件的Expression对象。\n// Create test documents with different metadata Document bgDocument = new Document(\u0026#34;The World is Big\u0026#34;, Map.of(\u0026#34;country\u0026#34;, \u0026#34;Bulgaria\u0026#34;)); Document nlDocument = new Document(\u0026#34;The World is Big\u0026#34;, Map.of(\u0026#34;country\u0026#34;, \u0026#34;Netherlands\u0026#34;)); // Add documents to the store vectorStore.add(List.of(bgDocument, nlDocument)); // Delete documents from Bulgaria using filter expression Filter.Expression filterExpression = new Filter.Expression( Filter.ExpressionType.EQ, new Filter.Key(\u0026#34;country\u0026#34;), new Filter.Value(\u0026#34;Bulgaria\u0026#34;) ); vectorStore.delete(filterExpression); // Verify deletion with search SearchRequest request = SearchRequest.builder() .query(\u0026#34;World\u0026#34;) .filterExpression(\u0026#34;country == \u0026#39;Bulgaria\u0026#39;\u0026#34;) .build(); List\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch(request); // results will be empty as Bulgarian document was deleted 按字符串筛选器表达式删除 # 为了方便起见，您还可以使用基于字符串的筛选器表达式删除文档：\nvoid delete(String filterExpression); 此方法将提供的字符串筛选器转换为筛选器。表达式对象内部。\n// Create and add documents Document bgDocument = new Document(\u0026#34;The World is Big\u0026#34;, Map.of(\u0026#34;country\u0026#34;, \u0026#34;Bulgaria\u0026#34;)); Document nlDocument = new Document(\u0026#34;The World is Big\u0026#34;, Map.of(\u0026#34;country\u0026#34;, \u0026#34;Netherlands\u0026#34;)); vectorStore.add(List.of(bgDocument, nlDocument)); // Delete Bulgarian documents using string filter vectorStore.delete(\u0026#34;country == \u0026#39;Bulgaria\u0026#39;\u0026#34;); // Verify remaining documents SearchRequest request = SearchRequest.builder() .query(\u0026#34;World\u0026#34;) .topK(5) .build(); List\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch(request); // results will only contain the Netherlands document 调用Delete API时的错误处理 # 出现错误时，所有删除方法都可能引发异常： 最佳实践是将删除操作包装在try-catch块中：\ntry { vectorStore.delete(\u0026#34;country == \u0026#39;Bulgaria\u0026#39;\u0026#34;); } catch (Exception e) { logger.error(\u0026#34;Invalid filter expression\u0026#34;, e); } 文档版本控制用例 # 一种常见的场景是管理文档版本，其中需要上载文档的新版本，同时删除旧版本。下面是如何使用筛选器表达式处理此问题：\n// Create initial document (v1) with version metadata Document documentV1 = new Document( \u0026#34;AI and Machine Learning Best Practices\u0026#34;, Map.of( \u0026#34;docId\u0026#34;, \u0026#34;AIML-001\u0026#34;, \u0026#34;version\u0026#34;, \u0026#34;1.0\u0026#34;, \u0026#34;lastUpdated\u0026#34;, \u0026#34;2024-01-01\u0026#34; ) ); // Add v1 to the vector store vectorStore.add(List.of(documentV1)); // Create updated version (v2) of the same document Document documentV2 = new Document( \u0026#34;AI and Machine Learning Best Practices - Updated\u0026#34;, Map.of( \u0026#34;docId\u0026#34;, \u0026#34;AIML-001\u0026#34;, \u0026#34;version\u0026#34;, \u0026#34;2.0\u0026#34;, \u0026#34;lastUpdated\u0026#34;, \u0026#34;2024-02-01\u0026#34; ) ); // First, delete the old version using filter expression Filter.Expression deleteOldVersion = new Filter.Expression( Filter.ExpressionType.AND, Arrays.asList( new Filter.Expression( Filter.ExpressionType.EQ, new Filter.Key(\u0026#34;docId\u0026#34;), new Filter.Value(\u0026#34;AIML-001\u0026#34;) ), new Filter.Expression( Filter.ExpressionType.EQ, new Filter.Key(\u0026#34;version\u0026#34;), new Filter.Value(\u0026#34;1.0\u0026#34;) ) ) ); vectorStore.delete(deleteOldVersion); // Add the new version vectorStore.add(List.of(documentV2)); // Verify only v2 exists SearchRequest request = SearchRequest.builder() .query(\u0026#34;AI and Machine Learning\u0026#34;) .filterExpression(\u0026#34;docId == \u0026#39;AIML-001\u0026#39;\u0026#34;) .build(); List\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch(request); // results will contain only v2 of the document 也可以使用字符串筛选器表达式来完成相同的操作：\n// Delete old version using string filter vectorStore.delete(\u0026#34;docId == \u0026#39;AIML-001\u0026#39; AND version == \u0026#39;1.0\u0026#39;\u0026#34;); // Add new version vectorStore.add(List.of(documentV2)); 删除文档时的性能注意事项 # 当您确切知道要删除哪些文档时，按ID列表删除通常更快。 基于过滤器的删除可能需要扫描索引以查找匹配的文档；然而，这是特定于向量存储实现的。 大型删除操作应成批进行，以避免压倒系统。 根据文档属性删除时，请考虑使用筛选器表达式，而不是首先收集ID。 了解向量 # 了解向量\n"},{"id":48,"href":"/docs/%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/%E6%B2%99%E5%8F%91%E5%BA%95%E5%BA%A7/","title":"沙发底座","section":"向量数据库","content":" 沙发底座 # 本节将指导您设置CouchbaseSearchVectorStore，以存储文档嵌入，并使用Couchbase执行相似性搜索。 Couchbase是一个分布式JSON文档数据库，具有关系数据库管理系统所需的所有功能。除其他功能外，它允许用户使用基于向量的存储和检索来查询信息。\n前提条件 # 正在运行的Couchbase实例。以下选项可用：\n自动配置 # Spring AI为Couchbase Vector Store提供Spring Boot自动配置。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-couchbase\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-couchbase-store-spring-boot-starter\u0026#39; } 向量存储实现可以使用默认选项为您初始化配置的桶、范围、集合和搜索索引，但您必须通过在适当的构造函数中指定initializeSchema布尔值来选择。 请查看矢量存储的 配置参数列表，以了解默认值和配置选项。 此外，您还需要一个已配置的EmbeddingModelbean。有关更多信息，请参阅EmbeddingModel部分。 现在，您可以在应用程序中自动将CouchbaseSearchVectorStore连接为向量存储。\n@Autowired VectorStore vectorStore; // ... List \u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); // Add the documents to Qdrant vectorStore.add(documents); // Retrieve documents similar to a query List\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch(SearchRequest.query(\u0026#34;Spring\u0026#34;).withTopK(5)); 配置属性 # 要连接到Couchbase并使用CouchbaseSearchVectorStore，您需要提供实例的访问详细信息。 环境变量，\nexport SPRING_COUCHBASE_CONNECTION_STRINGS=\u0026lt;couchbase connection string like couchbase://localhost\u0026gt; export SPRING_COUCHBASE_USERNAME=\u0026lt;couchbase username\u0026gt; export SPRING_COUCHBASE_PASSWORD=\u0026lt;couchbase password\u0026gt; # API key if needed, e.g. OpenAI export SPRING_AI_OPENAI_API_KEY=\u0026lt;api-key\u0026gt; 或者可以是这些的混合。 SpringBoot的Couchbase集群自动配置功能将创建一个bean实例，供CouchbaseSearchVectorStore使用。 以Spring.couchbase.*开头的Spring Boot属性用于配置couchbase集群实例： 以spring.ai.vectorstore.couchbase.*前缀开头的属性用于配置CouchbaseSearchVectorStore。 以下相似性函数可用：\nl2_形式 点_产品 以下索引优化可用： 回忆 延迟，延迟 Couchbase文档中关于向量搜索的每一项的更多详细信息。 元数据筛选 # 您可以在Couchbase存储中利用通用的、可移植的元数据过滤器。 例如，可以使用文本表达式语言：\nvectorStore.similaritySearch( SearchRequest.defaults() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .filterExpression(\u0026#34;author in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; article_type == \u0026#39;blog\u0026#39;\u0026#34;)); 或以编程方式使用筛选器。表达式DSL:\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.defaults() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .filterExpression(b.and( b.in(\u0026#34;author\u0026#34;,\u0026#34;john\u0026#34;, \u0026#34;jill\u0026#34;), b.eq(\u0026#34;article_type\u0026#34;, \u0026#34;blog\u0026#34;)).build())); 手动配置 # 您可以手动配置Couchbase向量存储，而不是使用SpringBoot自动配置。为此，您需要将spring ai couchbase商店添加到您的项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-couchbase-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-couchbase-store\u0026#39; } 创建Couchbase集群bean。\n@Bean public Cluster cluster() { return Cluster.connect(\u0026#34;couchbase://localhost\u0026#34;, \u0026#34;username\u0026#34;, \u0026#34;password\u0026#34;); } 然后使用构建器模式创建CouchbaseSearchVectorStore bean：\n@Bean public VectorStore couchbaseSearchVectorStore(Cluster cluster, EmbeddingModel embeddingModel, Boolean initializeSchema) { return CouchbaseSearchVectorStore .builder(cluster, embeddingModel) .bucketName(\u0026#34;test\u0026#34;) .scopeName(\u0026#34;test\u0026#34;) .collectionName(\u0026#34;test\u0026#34;) .initializeSchema(initializeSchema) .build(); } // This can be any EmbeddingModel implementation. @Bean public EmbeddingModel embeddingModel() { return new OpenAiEmbeddingModel(OpenAiApi.builder().apiKey(this.openaiKey).build()); } 限制 # "},{"id":49,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/docker%E6%A8%A1%E5%9E%8B%E8%BF%90%E8%A1%8C%E5%99%A8/","title":"Docker Model Runner聊天","section":"聊天模型API","content":" Docker Model Runner聊天 # Docker Model Runner是一个人工智能推理引擎，提供来自不同提供商的各种模型。 Spring AI通过重用现有的OpenAI支持的ChatClient与Docker Model Runner集成。 检查 DockerModelRunnerWithOpenAiChatModelIT.java测试\n先决条件 # 下载Mac 4.40.0的Docker Desktop。 选择以下选项之一以启用Model Runner： 选项1： Enable Model Runner docker desktop启用Model Runner\u0026ndash;tcp 12434。 将基本url设置为localhost:12434/engines 选项2： Enable Model Runner docker desktop启用模型运行器。 使用Testcontainers并按如下所示设置基url： @Container private static final SocatContainer socat = new SocatContainer().withTarget(80, \u0026#34;model-runner.docker.internal\u0026#34;); @Bean public OpenAiApi chatCompletionApi() { var baseUrl = \u0026#34;http://%s:%d/engines\u0026#34;.formatted(socat.getHost(), socat.getMappedPort(80)); return OpenAiApi.builder().baseUrl(baseUrl).apiKey(\u0026#34;test\u0026#34;).build(); } 您可以通过阅读Run LLMs Locally with Docker博客文章来了解更多关于Docker Model Runner的信息。\n自动配置 # Spring AI为OpenAI聊天客户端提供Spring Boot自动配置。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或将以下内容添加到Gradle build.Gradle构建文件中。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-openai\u0026#39; } 聊天室属性 # 重试属性 # 前缀spring.ai.retry用作属性前缀，允许您为OpenAI聊天模型配置重试机制。\n连接属性 # 前缀spring.ai.openai用作允许连接到openai的属性前缀。\n配置属性 # 前缀spring.ai.openai.chat是属性前缀，允许您配置openai的聊天模型实现。\n运行时选项 # OpenAiChatOptions.java提供模型配置，例如要使用的模型、温度、频率惩罚等。 启动时，可以使用OpenAiChatModel（api，options）构造函数或spring.ai.openai.chat.options.*属性配置默认选项。 在运行时，可以通过向Prompt调用添加新的特定于请求的选项来覆盖默认选项。\nChatResponse response = chatModel.call( new Prompt( \u0026#34;Generate the names of 5 famous pirates.\u0026#34;, OpenAiChatOptions.builder() .model(\u0026#34;ai/gemma3:4B-F16\u0026#34;) .build() )); 函数调用 # Docker Model Runner在选择支持它的模型时支持工具/函数调用。 您可以使用ChatModel注册自定义Java函数，并让提供的模型智能地选择输出包含参数的JSON对象，以调用一个或多个已注册的函数。\n工具示例 # 下面是一个如何使用Spring AI调用Docker Model Runner函数的简单示例：\nspring.ai.openai.api-key=test spring.ai.openai.base-url=http://localhost:12434/engines spring.ai.openai.chat.options.model=ai/gemma3:4B-F16 @SpringBootApplication public class DockerModelRunnerLlmApplication { public static void main(String[] args) { SpringApplication.run(DockerModelRunnerLlmApplication.class, args); } @Bean CommandLineRunner runner(ChatClient.Builder chatClientBuilder) { return args -\u0026gt; { var chatClient = chatClientBuilder.build(); var response = chatClient.prompt() .user(\u0026#34;What is the weather in Amsterdam and Paris?\u0026#34;) .functions(\u0026#34;weatherFunction\u0026#34;) // reference by bean name. .call() .content(); System.out.println(response); }; } @Bean @Description(\u0026#34;Get the weather in location\u0026#34;) public Function\u0026lt;WeatherRequest, WeatherResponse\u0026gt; weatherFunction() { return new MockWeatherService(); } public static class MockWeatherService implements Function\u0026lt;WeatherRequest, WeatherResponse\u0026gt; { public record WeatherRequest(String location, String unit) {} public record WeatherResponse(double temp, String unit) {} @Override public WeatherResponse apply(WeatherRequest request) { double temperature = request.location().contains(\u0026#34;Amsterdam\u0026#34;) ? 20 : 25; return new WeatherResponse(temperature, request.unit); } } } 在本例中，当模型需要天气信息时，它将自动调用weatherFunctionbean，然后bean可以获取实时天气数据。 阅读有关OpenAI 函数调用的更多信息。\n样本控制器 # 创建一个新的SpringBoot项目，并将Springaistarter模型openai添加到pom（或gradle）依赖项中。 在src/main/resources目录下添加application.properties文件，以启用和配置OpenAi聊天模型：\nspring.ai.openai.api-key=test spring.ai.openai.base-url=http://localhost:12434/engines spring.ai.openai.chat.options.model=ai/gemma3:4B-F16 # Docker Model Runner doesn\u0026#39;t support embeddings, so we need to disable them. spring.ai.openai.embedding.enabled=false 下面是一个简单的@Controller类的示例，该类使用聊天模型生成文本。\n@RestController public class ChatController { private final OpenAiChatModel chatModel; @Autowired public ChatController(OpenAiChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, this.chatModel.call(message)); } @GetMapping(\u0026#34;/ai/generateStream\u0026#34;) public Flux\u0026lt;ChatResponse\u0026gt; generateStream(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { Prompt prompt = new Prompt(new UserMessage(message)); return this.chatModel.stream(prompt); } } "},{"id":50,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/%E5%A5%A5%E6%8B%89%E9%A9%ACollama/","title":"Ollama嵌入","section":"嵌入模型API","content":" Ollama嵌入 # 使用Ollama，您可以在本地运行各种 人工智能模型，并从中生成嵌入。 OllamaEmbeddingModel实现利用Ollama-Embeddings API端点。\n前提条件 # 首先需要访问Ollama实例。有几个选项，包括：\n在本地计算机上下载并安装Ollama。 通过Testcontainers配置和运行Ollama。 通过Kubernetes服务绑定绑定到Ollama实例。 您可以从 Ollama模型库中提取要在应用程序中使用的模型： ollama pull \u0026lt;model-name\u0026gt; 您还可以拉动数千个免费 GGUF拥抱面部模型中的任何一个：\nollama pull hf.co/\u0026lt;username\u0026gt;/\u0026lt;model-repository\u0026gt; 或者，可以启用该选项以自动下载任何所需的模型：自动拉动模型。\n自动配置 # Spring AI为Azure Ollama嵌入模型提供Spring Boot自动配置。\n基本属性 # 前缀spring.ai.ollama是配置与ollama的连接的属性前缀 下面是用于初始化Ollama集成和自动提取模型的属性。\n嵌入属性 # 前缀spring.ai.ollama.embedding.options``是配置ollama嵌入模型的属性前缀。 以下是Ollama嵌入模型的高级请求参数： 其余选项属性基于 Ollama有效参数和值以及 Ollama类型。默认值基于： Ollama类型默认值。\n运行时选项 # OllamaOptions.java提供Ollama配置，如要使用的模型、低级GPU和CPU调整等。 也可以使用spring.ai.ollama.embedding.options属性配置默认选项。 在启动时，使用OllamaEmbeddingModel（OllamaApi OllamaApi，OllamaOptions defaultOptions）配置用于所有嵌入请求的默认选项。 例如，要覆盖特定请求的默认模型名称：\nEmbeddingResponse embeddingResponse = embeddingModel.call( new EmbeddingRequest(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;), OllamaOptions.builder() .model(\u0026#34;Different-Embedding-Model-Deployment-Name\u0026#34;)) .truncates(false) .build()); 自动牵引型号 # 当模型在Ollama实例中不可用时，Spring AI Ollama可以自动拉取模型。 拉动模型有三种策略：\nalways（在PullModelStrategy.always中定义）：始终拉动模型，即使它已经可用。用于确保使用模型的最新版本。 when_missing（在PullModelStrategy.when_missing中定义）：仅在模型不可用时拉取模型。这可能导致使用较旧版本的模型。 never（在PullModelStrategy.never中定义）：从不自动拉取模型。 通过配置属性和默认选项定义的所有模型都可以在启动时自动提取。 spring: ai: ollama: init: pull-model-strategy: always timeout: 60s max-retries: 1 您可以在启动时初始化其他模型，这对于在运行时动态使用的模型非常有用：\nspring: ai: ollama: init: pull-model-strategy: always embedding: additional-models: - mxbai-embed-large - nomic-embed-text 如果要将拉取策略仅应用于特定类型的模型，则可以从初始化任务中排除嵌入模型：\nspring: ai: ollama: init: pull-model-strategy: always embedding: include: false 该配置将把拉策略应用于除嵌入模型之外的所有模型。\nHuggingFace模型 # Ollama可以立即访问所有GGUF拥抱面部嵌入模型。\nspring.ai.ollama.embedding.options.model=hf.co/mixedbread-ai/mxbai-embed-large-v1 spring.ai.ollama.init.pull-model-strategy=always spring.ai.ollama.embedding.options.model：指定要使用的拥抱面GGUF模型。 spring.ai.ollama.init.pull-model-strategy=always:（可选）在启动时启用自动模型提取。 样本控制器 # 这将创建一个可以注入到类中的EmbeddingModel实现。\n@RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(\u0026#34;/ai/embedding\u0026#34;) public Map embed(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { EmbeddingResponse embeddingResponse = this.embeddingModel.embedForResponse(List.of(message)); return Map.of(\u0026#34;embedding\u0026#34;, embeddingResponse); } } 手动配置 # 如果不使用Spring Boot，则可以手动配置OllamaEmbeddingModel。 接下来，创建一个OllamaEmbeddingModel实例，并使用它来计算两个输入文本的嵌入，使用专用的色度/all-minilm-l6-v2-f32嵌入模型：\nvar ollamaApi = OllamaApi.builder().build(); var embeddingModel = new OllamaEmbeddingModel(this.ollamaApi, OllamaOptions.builder() .model(OllamaModel.MISTRAL.id()) .build()); EmbeddingResponse embeddingResponse = this.embeddingModel.call( new EmbeddingRequest(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;), OllamaOptions.builder() .model(\u0026#34;chroma/all-minilm-l6-v2-f32\u0026#34;)) .truncate(false) .build()); OllamaOptions为所有嵌入请求提供配置信息。\n"},{"id":51,"href":"/docs/%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%BC%B9%E6%80%A7%E6%90%9C%E7%B4%A2/","title":"弹性搜索","section":"向量数据库","content":" 弹性搜索 # 本节将指导您设置Elasticsearch VectorStore，以存储文档嵌入并执行相似性搜索。 Elasticsearch是一个基于Apache Lucene库的开源搜索和分析引擎。\n前提条件 # 正在运行的Elasticsearch实例。以下选项可用：\n码头工人 自我管理的弹性搜索 弹性云（Elastic Cloud） 自动配置 # Spring AI为Elasticsearch Vector Store提供Spring Boot自动配置。 向量存储实现可以为您初始化必要的模式，但您必须通过在适当的构造函数中指定initializeSchema布尔值或通过设置…​在application.properties文件中初始化schema=true。 请查看矢量存储的 配置参数列表，以了解默认值和配置选项。 此外，您还需要一个已配置的EmbeddingModelbean。有关更多信息，请参阅EmbeddingModel部分。 现在，您可以在应用程序中自动将ElasticsearchVectorStore连接为向量存储。\n@Autowired VectorStore vectorStore; // ... List \u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); // Add the documents to Elasticsearch vectorStore.add(documents); // Retrieve documents similar to a query List\u0026lt;Document\u0026gt; results = this.vectorStore.similaritySearch(SearchRequest.builder().query(\u0026#34;Spring\u0026#34;).topK(5).build()); 配置属性 # 要连接到Elasticsearch并使用ElasticsearchVectorStore，您需要提供实例的访问详细信息。\nspring: elasticsearch: uris: \u0026lt;elasticsearch instance URIs\u0026gt; username: \u0026lt;elasticsearch username\u0026gt; password: \u0026lt;elasticsearch password\u0026gt; ai: vectorstore: elasticsearch: initialize-schema: true index-name: custom-index dimensions: 1536 similarity: cosine 以Spring.lasticsearch.*开头的Spring Boot属性用于配置elasticsearch客户端： 以spring.ai.vectorstore.lasticsearch.*开头的属性用于配置ElasticsearchVectorStore: 以下相似性函数可用：\n余弦-默认值，适用于大多数用例。测量向量之间的余弦相似性。 l2_orm-向量之间的欧氏距离。较低的值表示较高的相似性。 dot_product-归一化向量的最佳性能（例如，OpenAI嵌入）。 Elasticsearch文档中关于密集向量的每一个的详细信息。 元数据筛选 # 您也可以将通用的、可移植的元数据过滤器与Elasticsearch结合使用。 例如，可以使用文本表达式语言：\nvectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(\u0026#34;author in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; \u0026#39;article_type\u0026#39; == \u0026#39;blog\u0026#39;\u0026#34;).build()); 或以编程方式使用筛选器。表达式DSL:\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(b.and( b.in(\u0026#34;author\u0026#34;, \u0026#34;john\u0026#34;, \u0026#34;jill\u0026#34;), b.eq(\u0026#34;article_type\u0026#34;, \u0026#34;blog\u0026#34;)).build()).build()); 例如，此可移植筛选器表达式：\nauthor in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; \u0026#39;article_type\u0026#39; == \u0026#39;blog\u0026#39; 转换为专有的Elasticsearch过滤器格式：\n(metadata.author:john OR jill) AND metadata.article_type:blog 手动配置 # 您可以手动配置Elasticsearch向量存储，而不是使用SpringBoot自动配置。为此，您需要将spring ai弹性搜索存储添加到您的项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-elasticsearch-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-elasticsearch-store\u0026#39; } 创建Elasticsearch RestClient bean。\n@Bean public RestClient restClient() { return RestClient.builder(new HttpHost(\u0026#34;\u0026lt;host\u0026gt;\u0026#34;, 9200, \u0026#34;http\u0026#34;)) .setDefaultHeaders(new Header[]{ new BasicHeader(\u0026#34;Authorization\u0026#34;, \u0026#34;Basic \u0026lt;encoded username and password\u0026gt;\u0026#34;) }) .build(); } 然后使用构建器模式创建ElasticsearchVectorStore bean：\n@Bean public VectorStore vectorStore(RestClient restClient, EmbeddingModel embeddingModel) { ElasticsearchVectorStoreOptions options = new ElasticsearchVectorStoreOptions(); options.setIndexName(\u0026#34;custom-index\u0026#34;); // Optional: defaults to \u0026#34;spring-ai-document-index\u0026#34; options.setSimilarity(COSINE); // Optional: defaults to COSINE options.setDimensions(1536); // Optional: defaults to model dimensions or 1536 return ElasticsearchVectorStore.builder(restClient, embeddingModel) .options(options) // Optional: use custom options .initializeSchema(true) // Optional: defaults to false .batchingStrategy(new TokenCountBatchingStrategy()) // Optional: defaults to TokenCountBatchingStrategy .build(); } // This can be any EmbeddingModel implementation @Bean public EmbeddingModel embeddingModel() { return new OpenAiEmbeddingModel(new OpenAiApi(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;))); } 访问本机客户端 # Elasticsearch Vector Store实现通过getNativeClient（）方法提供对底层本机Elasticsearch客户端（ElasticsearchClient）的访问：\nElasticsearchVectorStore vectorStore = context.getBean(ElasticsearchVectorStore.class); Optional\u0026lt;ElasticsearchClient\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { ElasticsearchClient client = nativeClient.get(); // Use the native client for Elasticsearch-specific operations } 本机客户端为您提供对Elasticsearch特定功能和操作的访问，这些功能和操作可能不会通过VectorStore界面公开。\n"},{"id":52,"href":"/docs/%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90rag/","title":"检索增强生成","section":"Docs","content":" 检索增强生成 # 检索增强生成（RAG）是一种有助于克服大型语言模型局限性的技术 Spring AI通过提供模块化架构来支持RAG，该架构允许您自己构建定制RAG流\n顾问 # Spring AI使用Advisor API为常见RAG流提供开箱即用的支持。 要使用QuestionAnswerAdvisor或RetrievalAugmentationAdvisor，您需要将spring-ai-advisors向量存储依赖项添加到项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-advisors-vector-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 问题解答顾问 # 向量数据库存储人工智能模型不知道的数据。当用户问题发送到AI模型时，QuestionAnswerAdvisor查询向量数据库中与用户问题相关的文档。 来自向量数据库的响应被附加到用户文本中，以为AI模型生成响应提供上下文。 假设您已经将数据加载到VectorStore中，则可以通过向ChatClient提供QuestionAnswerAdvisor的实例来执行检索增强生成（RAG）。\nChatResponse response = ChatClient.builder(chatModel) .build().prompt() .advisors(new QuestionAnswerAdvisor(vectorStore)) .user(userText) .call() .chatResponse(); 在本例中，QuestionAnswerAdvisor将对Vector Database中的所有文档执行相似性搜索。为了限制搜索的文档类型，SearchRequest采用类似SQL的筛选器表达式，该表达式可移植到所有VectorStore。 此筛选器表达式可以在创建QuestionAnswerAdvisor时配置，因此将始终应用于所有ChatClient请求，或者可以在每个请求的运行时提供。 下面是如何创建阈值为0.8的QuestionAnswerAdvisor实例，并返回前6个结果。\nvar qaAdvisor = QuestionAnswerAdvisor.builder(vectorStore) .searchRequest(SearchRequest.builder().similarityThreshold(0.8d).topK(6).build()) .build(); 动态过滤器表达式 # 使用filter_expression advisor上下文参数在运行时更新SearchRequest筛选器表达式：\nChatClient chatClient = ChatClient.builder(chatModel) .defaultAdvisors(QuestionAnswerAdvisor.builder(vectorStore) .searchRequest(SearchRequest.builder().build()) .build()) .build(); // Update filter expression at runtime String content = this.chatClient.prompt() .user(\u0026#34;Please answer my question XYZ\u0026#34;) .advisors(a -\u0026gt; a.param(QuestionAnswerAdvisor.FILTER_EXPRESSION, \u0026#34;type == \u0026#39;Spring\u0026#39;\u0026#34;)) .call() .content(); FILTER_EXPRESSION参数允许您基于提供的表达式动态筛选搜索结果。\n自定义模板 # QuestionAnswerAdvisor使用默认模板用检索到的文档来增加用户问题。您可以通过.PromptTemplate（）构建器方法提供自己的PromptTemplate对象来定制此行为。 自定义PromptTemplate可以使用任何TemplateRenderer实现（默认情况下，它使用基于StringTemplate引擎的StPrompt模板）。重要的要求是模板必须包含以下两个占位符：\n用于接收用户问题的查询占位符。 questionanswercontext占位符以接收检索的上下文。 PromptTemplate customPromptTemplate = PromptTemplate.builder() .renderer(StTemplateRenderer.builder().startDelimiterToken(\u0026#39;\u0026lt;\u0026#39;).endDelimiterToken(\u0026#39;\u0026gt;\u0026#39;).build()) .template(\u0026#34;\u0026#34;\u0026#34; \u0026lt;query\u0026gt; Context information is below. --------------------- \u0026lt;question_answer_context\u0026gt; --------------------- Given the context information and no prior knowledge, answer the query. Follow these rules: 1. If the answer is not in the context, just say that you don\u0026#39;t know. 2. Avoid statements like \u0026#34;Based on the context...\u0026#34; or \u0026#34;The provided information...\u0026#34;. \u0026#34;\u0026#34;\u0026#34;) .build(); String question = \u0026#34;Where does the adventure of Anacletus and Birba take place?\u0026#34;; QuestionAnswerAdvisor qaAdvisor = QuestionAnswerAdvisor.builder(vectorStore) .promptTemplate(customPromptTemplate) .build(); String response = ChatClient.builder(chatModel).build() .prompt(question) .advisors(qaAdvisor) .call() .content(); 检索增强顾问 # SpringAI包括一个 RAG模块库，您可以使用它来构建自己的RAG流。\n顺序RAG流 # 天真的RAG # Advisor retrievalAugmentationAdvisor = RetrievalAugmentationAdvisor.builder() .documentRetriever(VectorStoreDocumentRetriever.builder() .similarityThreshold(0.50) .vectorStore(vectorStore) .build()) .build(); String answer = chatClient.prompt() .advisors(retrievalAugmentationAdvisor) .user(question) .call() .content(); 默认情况下，RetrievalAugmentationAdvisor不允许检索的上下文为空。当这种情况发生时，\nAdvisor retrievalAugmentationAdvisor = RetrievalAugmentationAdvisor.builder() .documentRetriever(VectorStoreDocumentRetriever.builder() .similarityThreshold(0.50) .vectorStore(vectorStore) .build()) .queryAugmenter(ContextualQueryAugmenter.builder() .allowEmptyContext(true) .build()) .build(); String answer = chatClient.prompt() .advisors(retrievalAugmentationAdvisor) .user(question) .call() .content(); VectorStoreDocumentRetriever接受FilterExpression以基于元数据过滤搜索结果。\nAdvisor retrievalAugmentationAdvisor = RetrievalAugmentationAdvisor.builder() .documentRetriever(VectorStoreDocumentRetriever.builder() .similarityThreshold(0.50) .vectorStore(vectorStore) .build()) .build(); String answer = chatClient.prompt() .advisors(retrievalAugmentationAdvisor) .advisors(a -\u0026gt; a.param(VectorStoreDocumentRetriever.FILTER_EXPRESSION, \u0026#34;type == \u0026#39;Spring\u0026#39;\u0026#34;)) .user(question) .call() .content(); 有关详细信息，请参见 VectorStoreDocumentRetriever。\n高级RAG # Advisor retrievalAugmentationAdvisor = RetrievalAugmentationAdvisor.builder() .queryTransformers(RewriteQueryTransformer.builder() .chatClientBuilder(chatClientBuilder.build().mutate()) .build()) .documentRetriever(VectorStoreDocumentRetriever.builder() .similarityThreshold(0.50) .vectorStore(vectorStore) .build()) .build(); String answer = chatClient.prompt() .advisors(retrievalAugmentationAdvisor) .user(question) .call() .content(); 您还可以使用DocumentPostProcessor API在将检索到的文档传递到模型之前对其进行后处理。例如，您可以使用这样的接口来根据检索到的文档与查询的相关性对其进行重新排序，删除不相关或冗余的文档，或者压缩每个文档的内容以减少噪声和冗余。\n模块 # Spring AI实现了一个模块化RAG架构，该架构受到了本文中详细描述的模块化概念的启发\n预检索 # 预检索模块负责处理用户查询，以实现尽可能最佳的检索结果。\n查询转换 # 一个用于转换输入查询以使其更有效地执行检索任务的组件，解决了挑战\n压缩查询变压器 # CompressionQueryTransformer使用大型语言模型来压缩对话历史记录和后续查询 当对话历史较长且后续查询相关时，此转换器非常有用\nQuery query = Query.builder() .text(\u0026#34;And what is its second largest city?\u0026#34;) .history(new UserMessage(\u0026#34;What is the capital of Denmark?\u0026#34;), new AssistantMessage(\u0026#34;Copenhagen is the capital of Denmark.\u0026#34;)) .build(); QueryTransformer queryTransformer = CompressionQueryTransformer.builder() .chatClientBuilder(chatClientBuilder) .build(); Query transformedQuery = queryTransformer.transform(query); 该组件使用的提示可以通过生成器中可用的promptTemplate（）方法进行自定义。\n重写QueryTransformer # RewriteQueryTransformer使用大型语言模型重写用户查询，以便在以下情况下提供更好的结果： 当用户查询冗长、不明确或包含不相关的信息时，此转换器非常有用\nQuery query = new Query(\u0026#34;I\u0026#39;m studying machine learning. What is an LLM?\u0026#34;); QueryTransformer queryTransformer = RewriteQueryTransformer.builder() .chatClientBuilder(chatClientBuilder) .build(); Query transformedQuery = queryTransformer.transform(query); 该组件使用的提示可以通过生成器中可用的promptTemplate（）方法进行自定义。\n转换查询变压器 # TranslationQueryTransformer使用大型语言模型将查询转换为支持的目标语言 当嵌入模型在特定语言和用户查询上训练时，该转换器非常有用\nQuery query = new Query(\u0026#34;Hvad er Danmarks hovedstad?\u0026#34;); QueryTransformer queryTransformer = TranslationQueryTransformer.builder() .chatClientBuilder(chatClientBuilder) .targetLanguage(\u0026#34;english\u0026#34;) .build(); Query transformedQuery = queryTransformer.transform(query); 该组件使用的提示可以通过生成器中可用的promptTemplate（）方法进行自定义。\n查询扩展 # 一个用于将输入查询扩展为查询列表的组件，解决诸如格式不佳的查询之类的挑战\nMultiQueryExpander（MultiQuery扩展程序） # MultiQueryExpander使用大型语言模型将查询扩展为多个语义不同的变体\nMultiQueryExpander queryExpander = MultiQueryExpander.builder() .chatClientBuilder(chatClientBuilder) .numberOfQueries(3) .build(); List\u0026lt;Query\u0026gt; queries = queryExpander.expand(new Query(\u0026#34;How to run a Spring Boot app?\u0026#34;)); 默认情况下，MultiQueryExpander在展开的查询列表中包含原始查询。您可以禁用此行为\nMultiQueryExpander queryExpander = MultiQueryExpander.builder() .chatClientBuilder(chatClientBuilder) .includeOriginal(false) .build(); 该组件使用的提示可以通过生成器中可用的promptTemplate（）方法进行自定义。\n检索 # 检索模块负责查询向量存储等数据系统，并检索最相关的文档。\n文档搜索 # 负责从底层数据源（如搜索引擎、向量存储、，\nVectorStoreDocumentRetriever # VectorStoreDocumentRetriever从向量存储中检索语义类似于输入的文档\nDocumentRetriever retriever = VectorStoreDocumentRetriever.builder() .vectorStore(vectorStore) .similarityThreshold(0.73) .topK(5) .filterExpression(new FilterExpressionBuilder() .eq(\u0026#34;genre\u0026#34;, \u0026#34;fairytale\u0026#34;) .build()) .build(); List\u0026lt;Document\u0026gt; documents = retriever.retrieve(new Query(\u0026#34;What is the main character of the story?\u0026#34;)); 筛选器表达式可以是静态的，也可以是动态的。对于动态过滤器表达式，可以传递供应商。\nDocumentRetriever retriever = VectorStoreDocumentRetriever.builder() .vectorStore(vectorStore) .filterExpression(() -\u0026gt; new FilterExpressionBuilder() .eq(\u0026#34;tenant\u0026#34;, TenantContextHolder.getTenantIdentifier()) .build()) .build(); List\u0026lt;Document\u0026gt; documents = retriever.retrieve(new Query(\u0026#34;What are the KPIs for the next semester?\u0026#34;)); 您还可以使用filter_expression参数通过查询API提供特定于请求的筛选器表达式。\nQuery query = Query.builder() .text(\u0026#34;Who is Anacletus?\u0026#34;) .context(Map.of(VectorStoreDocumentRetriever.FILTER_EXPRESSION, \u0026#34;location == \u0026#39;Whispering Woods\u0026#39;\u0026#34;)) .build(); List\u0026lt;Document\u0026gt; retrievedDocuments = documentRetriever.retrieve(query); 文档连接 # 一个组件，用于将基于多个查询和从多个数据源检索的文档组合到\n级联DocumentJoiner # ConcatenationDocumentJoiner组合基于多个查询和从多个数据源检索的文档\nMap\u0026lt;Query, List\u0026lt;List\u0026lt;Document\u0026gt;\u0026gt;\u0026gt; documentsForQuery = ... DocumentJoiner documentJoiner = new ConcatenationDocumentJoiner(); List\u0026lt;Document\u0026gt; documents = documentJoiner.join(documentsForQuery); 后期检索 # 后期检索模块负责处理检索的文档，以实现尽可能最佳的生成结果。\n单据过账处理 # 一种用于基于查询对检索到的文档进行后处理的组件，解决诸如中间丢失、模型的上下文长度限制以及减少检索到的信息中的噪声和冗余的需要等挑战。 例如，它可以根据文档与查询的相关性对文档进行排序，删除不相关或冗余的文档，或者压缩每个文档的内容以减少噪声和冗余。\n生成 # 生成模块负责根据用户查询和检索的文档生成最终响应。\n查询增强功能 # 用于用附加数据增强输入查询的组件，用于提供大型语言模型\n上下文QueryAugmenter # ContextualQueryAugmenter使用来自所提供文档内容的上下文数据来增强用户查询。\nQueryAugmenter queryAugmenter = ContextualQueryAugmenter.builder().build(); 默认情况下，ContextualQueryAugmenter不允许检索的上下文为空。当这种情况发生时， 您可以启用allowEmptyContext选项，以允许模型生成响应，即使检索的上下文为空。\nQueryAugmenter queryAugmenter = ContextualQueryAugmenter.builder() .allowEmptyContext(true) .build(); 该组件使用的提示可以通过promptTemplate（）和emptyContextPromptTempate（）方法进行定制\n"},{"id":53,"href":"/docs/%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/gemfire%E5%85%AC%E5%8F%B8/","title":"GemFire矢量存储","section":"向量数据库","content":" GemFire矢量存储 # 本节将指导您设置GemFireVectorStore以存储文档嵌入并执行相似性搜索。 GemFire是一个分布式内存键值存储，以极快的速度执行读写操作。它提供了高可用性并行消息队列、连续可用性和事件驱动的体系结构，您可以在不停机的情况下动态扩展。随着数据大小需求的增加，以支持高性能、实时应用程序，GemFire可以轻松地线性扩展。 GemFireVectorDB扩展了GemFire的功能，作为一个通用的向量数据库，有效地存储、检索和执行向量相似性搜索。\n前提条件 # 自动配置 # 将GemFire VectorStore Spring Boot starter添加到项目的Maven构建文件pom.xml中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-gemfire\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle文件\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-vector-store-gemfire\u0026#39; } 配置属性 # 您可以在SpringBoot配置中使用以下属性来进一步配置GemFireVectorStore。\n手动配置 # 要仅使用GemFireVectorStore，请在没有Spring Boot的自动配置的情况下将以下依赖项添加到项目的Maven pom.xml：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-gemfire-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 对于Gradle用户，将以下内容添加到依赖项块下的build.Gradle文件中，以仅使用GemFireVectorStore：\n使用 # 下面是一个创建GemfireVectorStore实例而不是使用AutoConfiguration的示例\n@Bean public GemFireVectorStore vectorStore(EmbeddingModel embeddingModel) { return GemFireVectorStore.builder(embeddingModel) .host(\u0026#34;localhost\u0026#34;) .port(7071) .indexName(\u0026#34;my-vector-index\u0026#34;) .initializeSchema(true) .build(); } 在应用程序中，创建几个文档： List\u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;country\u0026#34;, \u0026#34;UK\u0026#34;, \u0026#34;year\u0026#34;, 2020)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;, Map.of()), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;country\u0026#34;, \u0026#34;NL\u0026#34;, \u0026#34;year\u0026#34;, 2023))); 将文档添加到向量存储： vectorStore.add(documents); 要使用相似性搜索检索文档，请执行以下操作： List\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch( SearchRequest.builder().query(\u0026#34;Spring\u0026#34;).topK(5).build()); 您应该检索包含文本“Spring AI rocks！！”的文档。 您还可以使用相似性阈值来限制结果的数量：\nList\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch( SearchRequest.builder().query(\u0026#34;Spring\u0026#34;).topK(5) .similarityThreshold(0.5d).build()); "},{"id":54,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/onnx%E5%8F%98%E5%8E%8B%E5%99%A8/","title":"变压器（ONNX）嵌入","section":"嵌入模型API","content":" 变压器（ONNX）嵌入 # TransformersEmbeddingModel``是一个Embedding Model实现，它使用选定的句子转换器本地计算 句子嵌入。 可以使用任何 HuggingFace嵌入模型。 它使用 预先训练的变压器模型，序列化为开放神经网络交换（ONNX）格式。 应用Deep Java Library和Microsoft ONNX Java Runtime库来运行ONNX模型并计算Java中的嵌入。\n前提条件 # 要在Java中运行，我们需要将Tokenizer和Transformer Model序列化为ONNX格式。 用 最佳cli序列化-实现这一点的一种快速方法是使用 最佳cli命令行工具。\npython3 -m venv venv source ./venv/bin/activate (venv) pip install --upgrade pip (venv) pip install optimum onnx onnxruntime sentence-transformers (venv) optimum-cli export onnx --model sentence-transformers/all-MiniLM-L6-v2 onnx-output-folder 片段将句子转换器/all-MiniLM-L6-v2转换器导出到onnx输出文件夹文件夹中。后者包括嵌入模型使用的tokenizer.json和model.onnx文件。 代替所有MiniLM-L6-v2，您可以选择任何拥抱面变压器标识符或提供直接文件路径。\n自动配置 # Spring AI为ONNX变压器嵌入模型提供Spring Boot自动配置。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-transformers\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-transformers\u0026#39; } 要配置它，请使用spring.ai.embedding.transformer.*属性。 例如，将其添加到application.properties文件中，以使用intfloat/e5-small-v2文本嵌入模型配置客户端： 支持的属性的完整列表为：\n嵌入属性 # 错误和特殊情况 # 手动配置 # 如果不使用Spring Boot，则可以手动配置Onnx Transformers嵌入模型。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-transformers\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 然后创建一个新的TransformersEmbeddingModel实例，并使用setTokenizerResource（tokenizerJsonUri）和setModelResource。（支持类路径：、文件：或https:URI架构）。 如果未显式设置模型，TransformersEmbeddingModel默认为句子转换器/all-MiniLM-L6-v2： 以下代码段说明了如何手动使用TransformersEmbeddingModel：\nTransformersEmbeddingModel embeddingModel = new TransformersEmbeddingModel(); // (optional) defaults to classpath:/onnx/all-MiniLM-L6-v2/tokenizer.json embeddingModel.setTokenizerResource(\u0026#34;classpath:/onnx/all-MiniLM-L6-v2/tokenizer.json\u0026#34;); // (optional) defaults to classpath:/onnx/all-MiniLM-L6-v2/model.onnx embeddingModel.setModelResource(\u0026#34;classpath:/onnx/all-MiniLM-L6-v2/model.onnx\u0026#34;); // (optional) defaults to ${java.io.tmpdir}/spring-ai-onnx-model // Only the http/https resources are cached by default. embeddingModel.setResourceCacheDirectory(\u0026#34;/tmp/onnx-zoo\u0026#34;); // (optional) Set the tokenizer padding if you see an errors like: // \u0026#34;ai.onnxruntime.OrtException: Supplied array is ragged, ...\u0026#34; embeddingModel.setTokenizerOptions(Map.of(\u0026#34;padding\u0026#34;, \u0026#34;true\u0026#34;)); embeddingModel.afterPropertiesSet(); List\u0026lt;List\u0026lt;Double\u0026gt;\u0026gt; embeddings = this.embeddingModel.embed(List.of(\u0026#34;Hello world\u0026#34;, \u0026#34;World is big\u0026#34;)); 第一个embed（）调用下载大型ONNX模型，并将其缓存在本地文件系统上。 将TransformersEmbeddingModel创建为Bean更方便（也是首选）。\n@Bean public EmbeddingModel embeddingModel() { return new TransformersEmbeddingModel(); } "},{"id":55,"href":"/docs/%E7%BB%93%E6%9E%84%E5%8C%96%E8%BE%93%E5%87%BA/","title":"结构化输出转换器","section":"Docs","content":" 结构化输出转换器 # LLM生成结构化输出的能力对于依赖于可靠解析输出值的下游应用程序非常重要。 Spring AI结构化输出转换器有助于将LLM输出转换为结构化格式。 使用通用完成API从大型语言模型（LLM）生成结构化输出需要仔细处理输入和输出。结构化输出转换器在LLM调用之前和之后发挥关键作用，确保实现所需的输出结构。 在LLM调用之前，转换器将格式指令附加到提示符，为模型提供生成所需输出结构的明确指导。这些指令充当蓝图，塑造模型的响应以符合指定的格式。 在LLM调用之后，转换器获取模型的输出文本，并将其转换为结构化类型的实例。该转换过程涉及解析原始文本输出，并将其映射到相应的结构化数据表示，如JSON、XML或特定于域的数据结构。\n结构化输出API # StructuredOutputConverter接口允许您获得结构化输出，例如将输出映射到Java类或基于文本的AI模型输出的值数组。\npublic interface StructuredOutputConverter\u0026lt;T\u0026gt; extends Converter\u0026lt;String, T\u0026gt;, FormatProvider { } 它结合了Spring Converter\u0026lt;String，T\u0026gt;接口和FormatProvider接口\npublic interface FormatProvider { String getFormat(); } 下图显示了使用结构化输出API时的数据流。 FormatProvider为AI模型提供特定的格式指南，使其能够生成文本输出，这些文本输出可以使用转换器转换为指定的目标类型T。下面是这种格式说明的示例： 格式指令通常使用PromptTemplate追加到用户输入的末尾，如下所示：\nStructuredOutputConverter outputConverter = ... String userInputTemplate = \u0026#34;\u0026#34;\u0026#34; ... user text input .... {format} \u0026#34;\u0026#34;\u0026#34;; // user input with a \u0026#34;format\u0026#34; placeholder. Prompt prompt = new Prompt( new PromptTemplate( this.userInputTemplate, Map.of(..., \u0026#34;format\u0026#34;, outputConverter.getFormat()) // replace the \u0026#34;format\u0026#34; placeholder with the converter\u0026#39;s format. ).createMessage()); Converter\u0026lt;String，T\u0026gt;负责将模型的输出文本转换为指定类型T的实例。\n可用的转换器 # 目前，Spring AI提供AbstractConversionServiceOutputConverter、AbstractMessageOutputConverter、BeanOutputInverter、MapOutputTransverter和ListOutput转换器实现： AbstractConversionServiceOutputConverter-提供预配置的GenericConversionService，用于将LLM输出转换为所需的格式。未提供默认的FormatProvider实现。 AbstractMessageOutputConverter-提供预配置的MessageConverter，用于将LLM输出转换为所需格式。未提供默认的FormatProvider实现。 BeanOutputConverter-使用指定的Java类（例如，Bean）或ParameterizedTypeReference配置，该转换器采用FormatProvider实现，该实现指示AI模型生成符合从指定Java类派生的DRAFT_2020 _12 JSON架构的JSON响应。随后，它利用ObjectMapper将JSON输出反序列化为目标类的Java对象实例。 MapOutputConverter-使用FormatProvider实现扩展AbstractMessageOutputConverter的功能，该实现指导AI模型生成符合RFC8259的JSON响应。此外，它还集成了一个转换器实现，该实现利用提供的MessageConverter将JSON有效负载转换为java.util。映射\u0026lt;String，Object\u0026gt;instance。 ListOutputConverter-扩展AbstractConversionServiceOutputConverter，并包括为逗号分隔的列表输出定制的FormatProvider实现。转换器实现使用提供的ConversionService将模型文本输出转换为java.util。列表。 使用转换器 # 以下部分提供了如何使用可用转换器生成结构化输出的指南。\nBean输出转换器 # 下面的示例演示如何使用BeanOutputConverter为演员生成胶片摄影。 代表演员电影制作的目标记录：\nrecord ActorsFilms(String actor, List\u0026lt;String\u0026gt; movies) { } 下面是如何使用高级、流畅的ChatClient API应用BeanOutputConverter：\nActorsFilms actorsFilms = ChatClient.create(chatModel).prompt() .user(u -\u0026gt; u.text(\u0026#34;Generate the filmography of 5 movies for {actor}.\u0026#34;) .param(\u0026#34;actor\u0026#34;, \u0026#34;Tom Hanks\u0026#34;)) .call() .entity(ActorsFilms.class); 或直接使用低级ChatModel API：\nBeanOutputConverter\u0026lt;ActorsFilms\u0026gt; beanOutputConverter = new BeanOutputConverter\u0026lt;\u0026gt;(ActorsFilms.class); String format = this.beanOutputConverter.getFormat(); String actor = \u0026#34;Tom Hanks\u0026#34;; String template = \u0026#34;\u0026#34;\u0026#34; Generate the filmography of 5 movies for {actor}. {format} \u0026#34;\u0026#34;\u0026#34;; Generation generation = chatModel.call( new PromptTemplate(this.template, Map.of(\u0026#34;actor\u0026#34;, this.actor, \u0026#34;format\u0026#34;, this.format)).create()).getResult(); ActorsFilms actorsFilms = this.beanOutputConverter.convert(this.generation.getOutput().getText()); 生成的架构中的属性排序 # BeanOutputConverter通过@JsonPropertyOrder注释支持生成的JSON模式中的自定义属性排序。 例如，为了确保ActorsFilms记录中属性的特定顺序：\n@JsonPropertyOrder({\u0026#34;actor\u0026#34;, \u0026#34;movies\u0026#34;}) record ActorsFilms(String actor, List\u0026lt;String\u0026gt; movies) {} 该注释同时适用于记录和常规Java类。\n通用Bean类型 # 使用ParameterizedTypeReference构造函数指定更复杂的目标类结构。\nList\u0026lt;ActorsFilms\u0026gt; actorsFilms = ChatClient.create(chatModel).prompt() .user(\u0026#34;Generate the filmography of 5 movies for Tom Hanks and Bill Murray.\u0026#34;) .call() .entity(new ParameterizedTypeReference\u0026lt;List\u0026lt;ActorsFilms\u0026gt;\u0026gt;() {}); 或直接使用低级ChatModel API：\nBeanOutputConverter\u0026lt;List\u0026lt;ActorsFilms\u0026gt;\u0026gt; outputConverter = new BeanOutputConverter\u0026lt;\u0026gt;( new ParameterizedTypeReference\u0026lt;List\u0026lt;ActorsFilms\u0026gt;\u0026gt;() { }); String format = this.outputConverter.getFormat(); String template = \u0026#34;\u0026#34;\u0026#34; Generate the filmography of 5 movies for Tom Hanks and Bill Murray. {format} \u0026#34;\u0026#34;\u0026#34;; Prompt prompt = new PromptTemplate(this.template, Map.of(\u0026#34;format\u0026#34;, this.format)).create(); Generation generation = chatModel.call(this.prompt).getResult(); List\u0026lt;ActorsFilms\u0026gt; actorsFilms = this.outputConverter.convert(this.generation.getOutput().getText()); 地图输出转换器 # 下面的片段演示如何使用MapOutputConverter将模型输出转换为地图中的数字列表。\nMap\u0026lt;String, Object\u0026gt; result = ChatClient.create(chatModel).prompt() .user(u -\u0026gt; u.text(\u0026#34;Provide me a List of {subject}\u0026#34;) .param(\u0026#34;subject\u0026#34;, \u0026#34;an array of numbers from 1 to 9 under they key name \u0026#39;numbers\u0026#39;\u0026#34;)) .call() .entity(new ParameterizedTypeReference\u0026lt;Map\u0026lt;String, Object\u0026gt;\u0026gt;() {}); 或直接使用低级ChatModel API：\nMapOutputConverter mapOutputConverter = new MapOutputConverter(); String format = this.mapOutputConverter.getFormat(); String template = \u0026#34;\u0026#34;\u0026#34; Provide me a List of {subject} {format} \u0026#34;\u0026#34;\u0026#34;; Prompt prompt = new PromptTemplate(this.template, Map.of(\u0026#34;subject\u0026#34;, \u0026#34;an array of numbers from 1 to 9 under they key name \u0026#39;numbers\u0026#39;\u0026#34;, \u0026#34;format\u0026#34;, this.format)).create(); Generation generation = chatModel.call(this.prompt).getResult(); Map\u0026lt;String, Object\u0026gt; result = this.mapOutputConverter.convert(this.generation.getOutput().getText()); 列出输出转换器 # 下面的片段展示了如何使用ListOutputConverter将模型输出转换为冰淇淋口味列表。\nList\u0026lt;String\u0026gt; flavors = ChatClient.create(chatModel).prompt() .user(u -\u0026gt; u.text(\u0026#34;List five {subject}\u0026#34;) .param(\u0026#34;subject\u0026#34;, \u0026#34;ice cream flavors\u0026#34;)) .call() .entity(new ListOutputConverter(new DefaultConversionService())); 或直接使用低级ChatModel API：\nListOutputConverter listOutputConverter = new ListOutputConverter(new DefaultConversionService()); String format = this.listOutputConverter.getFormat(); String template = \u0026#34;\u0026#34;\u0026#34; List five {subject} {format} \u0026#34;\u0026#34;\u0026#34;; Prompt prompt = new PromptTemplate(this.template, Map.of(\u0026#34;subject\u0026#34;, \u0026#34;ice cream flavors\u0026#34;, \u0026#34;format\u0026#34;, this.format)).create(); Generation generation = this.chatModel.call(this.prompt).getResult(); List\u0026lt;String\u0026gt; list = this.listOutputConverter.convert(this.generation.getOutput().getText()); 支持的人工智能模型 # 以下人工智能模型已经过测试，以支持列表、映射和Bean结构化输出。\n内置JSON模式 # 一些AI模型提供专用的配置选项来生成结构化（通常是JSON）输出。\nOpenAI结构化输出可以确保您的模型生成严格符合您提供的JSON模式的响应。您可以在JSON_OBJECT（确保模型生成的消息是有效的JSON）或JSON_SCHEMA（提供的模式确保模型将生成与您提供的模式匹配的响应）之间进行选择（spring.ai.openai.chat.options.responseFormat选项）。 Azure OpenAI-提供spring.ai.Azure.OpenAI.chat.options.responseFormat选项，指定模型必须输出的格式。设置为{“type”：“json_object”}将启用json模式，这确保模型生成的消息是有效的json。 Ollama-提供spring.ai.Ollama.chat.options.format选项来指定返回响应的格式。目前，唯一接受的值是json。 Mistral AI-提供spring.AI.mistralai.chat.options.responseFormat选项来指定返回响应的格式。将其设置为{“类型”：“json_object”}将启用json模式，这确保模型生成的消息是有效的json。 "},{"id":56,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/%E8%B0%B7%E6%AD%8Cvertexai/","title":"谷歌VertexAI API","section":"聊天模型API","content":" 谷歌VertexAI API # VertexAI API提供了高质量的定制机器学习模型，具有最少的机器学习专业知识和工作量。 Spring AI通过以下客户端提供与VertexAI API的集成：\nVertexAI双子座聊天 "},{"id":57,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/%E6%A0%BC%E7%BD%97%E5%85%8B/","title":"Groq聊天","section":"聊天模型API","content":" Groq聊天 # Groq是一个非常快速的、基于LPU™的人工智能推理引擎，支持各种 人工智能模型， Spring AI通过重用现有的OpenAI客户端与Groq集成。 检查GroqWithOpenAiChatModelIT.java测试\n前提条件 # 创建API密钥。 设置Groq URL。 选择Groq模型。 导出环境变量是设置该配置属性的一种方法： export SPRING_AI_OPENAI_API_KEY=\u0026lt;INSERT GROQ API KEY HERE\u0026gt; export SPRING_AI_OPENAI_BASE_URL=https://api.groq.com/openai export SPRING_AI_OPENAI_CHAT_MODEL=llama3-70b-8192 添加存储库和BOM表 # Spring AI工件发布在Maven Central和Spring Snapshot 存储库中。 为了帮助进行依赖关系管理，Spring AI提供了BOM（物料清单），以确保在整个项目中使用一致版本的Spring AI。请参阅依赖项管理部分，将Spring AI BOM添加到构建系统中。\n自动配置 # Spring AI为OpenAI聊天客户端提供Spring Boot自动配置。\n聊天室属性 # 重试属性 # 前缀spring.ai.retry用作属性前缀，允许您为OpenAI聊天模型配置重试机制。\n连接属性 # 前缀spring.ai.openai用作允许连接到openai的属性前缀。\n配置属性 # 前缀spring.ai.openai.chat是属性前缀，允许您配置openai的聊天模型实现。\n运行时选项 # OpenAiChatOptions.java提供模型配置，例如要使用的模型、温度、频率惩罚等。 启动时，可以使用OpenAiChatModel（api，options）构造函数或spring.ai.openai.chat.options.*属性配置默认选项。 在运行时，可以通过向Prompt调用添加新的特定于请求的选项来覆盖默认选项。\nChatResponse response = chatModel.call( new Prompt( \u0026#34;Generate the names of 5 famous pirates.\u0026#34;, OpenAiChatOptions.builder() .model(\u0026#34;mixtral-8x7b-32768\u0026#34;) .temperature(0.4) .build() )); 函数调用 # 当选择一个工具/函数支持模型时，Groq API端点支持 工具/函数调用。 您可以使用ChatModel注册自定义Java函数，并让提供的Groq模型智能地选择输出包含参数的JSON对象，以调用一个或多个已注册的函数。\n工具示例 # 下面是如何在Spring AI中使用Groq函数调用的简单示例：\n@SpringBootApplication public class GroqApplication { public static void main(String[] args) { SpringApplication.run(GroqApplication.class, args); } @Bean CommandLineRunner runner(ChatClient.Builder chatClientBuilder) { return args -\u0026gt; { var chatClient = chatClientBuilder.build(); var response = chatClient.prompt() .user(\u0026#34;What is the weather in Amsterdam and Paris?\u0026#34;) .functions(\u0026#34;weatherFunction\u0026#34;) // reference by bean name. .call() .content(); System.out.println(response); }; } @Bean @Description(\u0026#34;Get the weather in location\u0026#34;) public Function\u0026lt;WeatherRequest, WeatherResponse\u0026gt; weatherFunction() { return new MockWeatherService(); } public static class MockWeatherService implements Function\u0026lt;WeatherRequest, WeatherResponse\u0026gt; { public record WeatherRequest(String location, String unit) {} public record WeatherResponse(double temp, String unit) {} @Override public WeatherResponse apply(WeatherRequest request) { double temperature = request.location().contains(\u0026#34;Amsterdam\u0026#34;) ? 20 : 25; return new WeatherResponse(temperature, request.unit); } } } 在本例中，当模型需要天气信息时，它将自动调用weatherFunctionbean，然后bean可以获取实时天气数据。 阅读有关OpenAI 函数调用的更多信息。\n多模态 # 样本控制器 # 创建一个新的SpringBoot项目，并将Springaistarter模型openai添加到pom（或gradle）依赖项中。 在src/main/resources目录下添加application.properties文件，以启用和配置OpenAi聊天模型：\nspring.ai.openai.api-key=\u0026lt;GROQ_API_KEY\u0026gt; spring.ai.openai.base-url=https://api.groq.com/openai spring.ai.openai.chat.options.model=llama3-70b-8192 spring.ai.openai.chat.options.temperature=0.7 这将创建一个OpenAiChatModel实现，您可以将其注入到类中。\n@RestController public class ChatController { private final OpenAiChatModel chatModel; @Autowired public ChatController(OpenAiChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, this.chatModel.call(message)); } @GetMapping(\u0026#34;/ai/generateStream\u0026#34;) public Flux\u0026lt;ChatResponse\u0026gt; generateStream(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { Prompt prompt = new Prompt(new UserMessage(message)); return this.chatModel.stream(prompt); } } 手动配置 # OpenAiChatModel实现了ChatModel.和StreamingChatModel.[low level api]连接到OpenAI服务。 将spring ai openai依赖项添加到项目的Maven pom.xml文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-openai\u0026#39; } 接下来，创建OpenAiChatModel并将其用于文本生成：\nvar openAiApi = new OpenAiApi(\u0026#34;https://api.groq.com/openai\u0026#34;, System.getenv(\u0026#34;GROQ_API_KEY\u0026#34;)); var openAiChatOptions = OpenAiChatOptions.builder() .model(\u0026#34;llama3-70b-8192\u0026#34;) .temperature(0.4) .maxTokens(200) .build(); var chatModel = new OpenAiChatModel(this.openAiApi, this.openAiChatOptions); ChatResponse response = this.chatModel.call( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); // Or with streaming responses Flux\u0026lt;ChatResponse\u0026gt; response = this.chatModel.stream( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); OpenAiChatOptions提供聊天请求的配置信息。\n"},{"id":58,"href":"/docs/%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/mariadb%E7%9F%A2%E9%87%8F%E5%AD%98%E5%82%A8/","title":"MariaDB矢量存储","section":"向量数据库","content":" MariaDB矢量存储 # 本节将指导您完成设置MariaDBVectorStore以存储文档嵌入和执行相似性搜索。 MariaDB Vector是MariaDB 11.7的一部分，支持在机器学习生成的嵌入上存储和搜索。\n前提条件 # 正在运行的MariaDB（11.7+）实例。以下选项可用： Docker图像 MariaDB服务器 MariaDB SkySQL 如果需要，EmbeddingModel的API密钥，用于生成由MariaDBVectorStore存储的嵌入。 自动配置 # Spring AI为MariaDB Vector Store提供Spring Boot自动配置。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-mariadb\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-vector-store-mariadb\u0026#39; } 向量存储实现可以为您初始化所需的模式，但您必须通过在适当的构造函数中指定initializeSchema布尔值或通过设置…​在application.properties文件中初始化schema=true。 此外，您还需要一个已配置的EmbeddingModelbean。有关更多信息，请参阅EmbeddingModel部分。 例如，要使用OpenAI EmbeddingModel，请添加以下依赖项：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 现在，您可以在应用程序中自动连接MariaDBVectorStore：\n@Autowired VectorStore vectorStore; // ... List\u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); // Add the documents to MariaDB vectorStore.add(documents); // Retrieve documents similar to a query List\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch(SearchRequest.builder().query(\u0026#34;Spring\u0026#34;).topK(5).build()); 配置属性 # 要连接到MariaDB并使用MariaDBVectorStore，您需要提供实例的访问详细信息。\nspring: datasource: url: jdbc:mariadb://localhost/db username: myUser password: myPassword ai: vectorstore: mariadb: initialize-schema: true distance-type: COSINE dimensions: 1536 以spring.ai.vectorstore.mariadb.*开头的属性用于配置MariaDBVectorStore:\n手动配置 # 您可以手动配置MariaDB向量存储，而不是使用SpringBoot自动配置。为此，您需要将以下依赖项添加到项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-jdbc\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.mariadb.jdbc\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mariadb-java-client\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;runtime\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-mariadb-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 然后使用构建器模式创建MariaDBVectorStore bean：\n@Bean public VectorStore vectorStore(JdbcTemplate jdbcTemplate, EmbeddingModel embeddingModel) { return MariaDBVectorStore.builder(jdbcTemplate, embeddingModel) .dimensions(1536) // Optional: defaults to 1536 .distanceType(MariaDBDistanceType.COSINE) // Optional: defaults to COSINE .schemaName(\u0026#34;mydb\u0026#34;) // Optional: defaults to null .vectorTableName(\u0026#34;custom_vectors\u0026#34;) // Optional: defaults to \u0026#34;vector_store\u0026#34; .contentFieldName(\u0026#34;text\u0026#34;) // Optional: defaults to \u0026#34;content\u0026#34; .embeddingFieldName(\u0026#34;embedding\u0026#34;) // Optional: defaults to \u0026#34;embedding\u0026#34; .idFieldName(\u0026#34;doc_id\u0026#34;) // Optional: defaults to \u0026#34;id\u0026#34; .metadataFieldName(\u0026#34;meta\u0026#34;) // Optional: defaults to \u0026#34;metadata\u0026#34; .initializeSchema(true) // Optional: defaults to false .schemaValidation(true) // Optional: defaults to false .removeExistingVectorStoreTable(false) // Optional: defaults to false .maxDocumentBatchSize(10000) // Optional: defaults to 10000 .build(); } // This can be any EmbeddingModel implementation @Bean public EmbeddingModel embeddingModel() { return new OpenAiEmbeddingModel(new OpenAiApi(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;))); } 元数据筛选 # 您可以将通用的、可移植的元数据过滤器与MariaDB Vector store结合使用。 例如，可以使用文本表达式语言：\nvectorStore.similaritySearch( SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(\u0026#34;author in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; article_type == \u0026#39;blog\u0026#39;\u0026#34;).build()); 或以编程方式使用筛选器。表达式DSL:\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(b.and( b.in(\u0026#34;author\u0026#34;, \u0026#34;john\u0026#34;, \u0026#34;jill\u0026#34;), b.eq(\u0026#34;article_type\u0026#34;, \u0026#34;blog\u0026#34;)).build()).build()); 访问本机客户端 # MariaDB Vector Store实现通过getNativeClient（）方法提供对底层本机JDBC客户端（JdbcTemplate）的访问：\nMariaDBVectorStore vectorStore = context.getBean(MariaDBVectorStore.class); Optional\u0026lt;JdbcTemplate\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { JdbcTemplate jdbc = nativeClient.get(); // Use the native client for MariaDB-specific operations } 本机客户端为您提供对MariaDB特定功能和操作的访问，这些功能和操作可能不会通过VectorStore界面公开。\n"},{"id":59,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/%E5%BC%80%E6%94%BE%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/","title":"OpenAI嵌入","section":"嵌入模型API","content":" OpenAI嵌入 # Spring AI支持OpenAI的文本嵌入模型。\n前提条件 # 您需要使用OpenAI创建一个API来访问OpenAI嵌入模型。 在 OpenAI注册页面创建帐户，并在 API密钥页面上生成令牌。\nexport SPRING_AI_OPENAI_API_KEY=\u0026lt;INSERT KEY HERE\u0026gt; 添加存储库和BOM表 # Spring AI工件发布在Maven Central和Spring Snapshot 存储库中。 为了帮助进行依赖关系管理，Spring AI提供了BOM（物料清单），以确保在整个项目中使用一致版本的Spring AI。请参阅依赖项管理部分，将Spring AI BOM添加到构建系统中。\n自动配置 # Spring AI为OpenAI嵌入模型提供Spring Boot自动配置。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-openai\u0026#39; } 嵌入属性 # 重试属性 # 前缀spring.ai.retry用作属性前缀，允许您为OpenAI嵌入模型配置重试机制。\n连接属性 # 前缀spring.ai.openai用作允许连接到openai的属性前缀。\n配置属性 # 前缀spring.ai.openai.embedding是为openai配置EmbeddingModel实现的属性前缀。\n运行时选项 # OpenAiEmbeddingOptions.java提供OpenAI配置，如要使用的模型等。 也可以使用spring.ai.openai.embedding.options属性配置默认选项。 在启动时，使用OpenAiEmbeddingModel构造函数设置用于所有嵌入请求的默认选项。 例如，要覆盖特定请求的默认模型名称：\nEmbeddingResponse embeddingResponse = embeddingModel.call( new EmbeddingRequest(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;), OpenAiEmbeddingOptions.builder() .model(\u0026#34;Different-Embedding-Model-Deployment-Name\u0026#34;) .build())); 样本控制器 # 这将创建一个可以注入到类中的EmbeddingModel实现。\nspring.ai.openai.api-key=YOUR_API_KEY spring.ai.openai.embedding.options.model=text-embedding-ada-002 @RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(\u0026#34;/ai/embedding\u0026#34;) public Map embed(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { EmbeddingResponse embeddingResponse = this.embeddingModel.embedForResponse(List.of(message)); return Map.of(\u0026#34;embedding\u0026#34;, embeddingResponse); } } 手动配置 # 如果不使用Spring Boot，则可以手动配置OpenAI嵌入模型。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-openai\u0026#39; } 接下来，创建OpenAiEmbeddingModel实例，并使用它计算两个输入文本之间的相似性：\nvar openAiApi = OpenAiApi.builder() .apiKey(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;)) .build(); var embeddingModel = new OpenAiEmbeddingModel( this.openAiApi, MetadataMode.EMBED, OpenAiEmbeddingOptions.builder() .model(\u0026#34;text-embedding-ada-002\u0026#34;) .user(\u0026#34;user-6\u0026#34;) .build(), RetryUtils.DEFAULT_RETRY_TEMPLATE); EmbeddingResponse embeddingResponse = this.embeddingModel .embedForResponse(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;)); OpenAiEmbeddingOptions提供嵌入请求的配置信息。\n"},{"id":60,"href":"/docs/%E8%81%8A%E5%A4%A9%E5%AE%A4%E5%AD%98%E5%82%A8%E5%99%A8/","title":"聊天室存储器","section":"Docs","content":" 聊天室存储器 # 大型语言模型（LLM）是无状态的，这意味着它们不保留关于先前交互的信息。当您希望跨多个交互维护上下文或状态时，这可能是一个限制。为了解决这个问题，SpringAI提供了聊天记忆功能，允许您在与LLM的多个交互中存储和检索信息。 ChatMemory抽象允许您实现各种类型的内存，以支持不同的用例。消息的底层存储由ChatMemoryRepository处理，其唯一职责是存储和检索消息。由ChatMemory实现决定保留哪些消息以及何时删除它们。策略的示例可以包括保持最后N条消息、将消息保持特定时间段或将消息保持在特定令牌限制以内。 在选择记忆类型之前，务必了解聊天室记忆和聊天历史记录之间的区别。 ChatMemory抽象旨在管理聊天室内存。它允许您存储和检索与当前对话上下文相关的消息。然而，它不是存储聊天历史记录的最佳选择。如果您需要维护所有交换消息的完整记录，则应该考虑使用不同的方法，例如依赖SpringData来有效地存储和检索完整的聊天历史。\n快速入门 # SpringAI自动配置ChatMemoryBean，您可以在应用程序中直接使用。默认情况下，它使用内存中存储库来存储消息（InMemoryChatMemoryRepository），使用MessageWindowChatMemority实现来管理对话历史记录。如果已经配置了不同的存储库（例如，Cassandra、JDBC或Neo4j），Spring AI将使用该存储库。\n@Autowired ChatMemory chatMemory; 下面的部分将进一步描述SpringAI中可用的不同内存类型和存储库。\n存储器类型 # ChatMemory抽象允许您实现各种类型的内存，以适应不同的用例。内存类型的选择会显著影响应用程序的性能和行为。本节描述Spring AI提供的内置内存类型及其特性。\n信息窗口聊天存储器 # MessageWindowChatMemory维护一个消息窗口，最大为指定的最大大小。当消息数超过最大值时，将删除较旧的消息，同时保留系统消息。默认窗口大小为20条消息。\nMessageWindowChatMemory memory = MessageWindowChatMemory.builder() .maxMessages(10) .build(); 这是SpringAI用于自动配置ChatMemoryBean的默认消息类型。\n存储器存储器 # Spring AI提供ChatMemoryRepository抽象来存储聊天内存。本节描述了SpringAI提供的内置存储库以及如何使用它们，但如果需要，您也可以实现自己的存储库。\n内存中存储库 # InMemoryChatMemoryRepository使用ConcurrentHashMap将消息存储在内存中。 默认情况下，如果尚未配置其他存储库，Spring AI会自动配置InMemoryChatMemoryRepository类型的ChatMemorityBean，您可以在应用程序中直接使用它。\n@Autowired ChatMemoryRepository chatMemoryRepository; 如果您希望手动创建InMemoryChatMemoryRepository，可以执行以下操作：\nChatMemoryRepository repository = new InMemoryChatMemoryRepository(); JdbcChatMemoryRepository数据库 # JdbcChatMemoryRepository是一个内置实现，使用JDBC将消息存储在关系数据库中。它适用于需要永久存储聊天室存储器的应用程序。 首先，将以下依赖项添加到项目中： Spring AI为JdbcChatMemoryRepository提供自动配置，您可以在应用程序中直接使用。\n@Autowired JdbcChatMemoryRepository chatMemoryRepository; ChatMemory chatMemory = MessageWindowChatMemory.builder() .chatMemoryRepository(chatMemoryRepository) .maxMessages(10) .build(); 如果您希望手动创建JdbcChatMemoryRepository，可以通过提供JdbcTemplate实例来实现：\nChatMemoryRepository chatMemoryRepository = JdbcChatMemoryRepository.builder() .jdbcTemplate(jdbcTemplate) .build(); ChatMemory chatMemory = MessageWindowChatMemory.builder() .chatMemoryRepository(chatMemoryRepository) .maxMessages(10) .build(); 配置属性 # 架构初始化 # 自动配置将使用JDBC驱动程序自动创建ai_chat_memory表。目前仅支持PostgreSQL和MariaDB。 可以通过将属性spring.ai.chat.memory.repository.jdbc.initialize-schema设置为false来禁用架构初始化。 如果项目使用Flyway或Liquibase等工具来管理数据库模式，则可以禁用模式初始化，并参考 这些SQL脚本来配置这些工具以创建ai_chat_memory表。\nNeo4j聊天记忆库 # Neo4jChatMemoryRepository是一个内置实现，它使用Neo4j将聊天消息存储为属性图数据库中的节点和关系。它适用于希望利用Neo4j的图形功能进行聊天内存持久化的应用程序。 首先，将以下依赖项添加到项目中： Spring AI为Neo4jChatMemoryRepository提供自动配置，您可以在应用程序中直接使用。\n@Autowired Neo4jChatMemoryRepository chatMemoryRepository; ChatMemory chatMemory = MessageWindowChatMemory.builder() .chatMemoryRepository(chatMemoryRepository) .maxMessages(10) .build(); 如果您希望手动创建Neo4jChatMemoryRepository，可以通过提供Neo4j驱动程序实例来实现：\nChatMemoryRepository chatMemoryRepository = Neo4jChatMemoryRepository.builder() .driver(driver) .build(); ChatMemory chatMemory = MessageWindowChatMemory.builder() .chatMemoryRepository(chatMemoryRepository) .maxMessages(10) .build(); 配置属性 # 索引初始化 # Neo4j存储库将自动确保为会话ID和消息索引创建索引，以优化性能。如果使用自定义标签，则也将为这些标签创建索引。不需要架构初始化，但您应该确保应用程序可以访问Neo4j实例。\n聊天客户端内存 # 使用ChatClient API时，可以提供ChatMemory实现来维护多个交互之间的对话上下文。 Spring AI提供了一些内置的Advisor，您可以根据需要使用它们来配置ChatClient的内存行为。\n消息聊天记忆顾问。该顾问使用提供的ChatMemory实现管理对话内存。在每次交互时，它从内存中检索对话历史记录，并将其作为消息集合包含在提示符中。 PromptChatMemoryAdvisor。该顾问使用提供的ChatMemory实现管理对话内存。在每次交互时，它从内存中检索对话历史记录，并将其作为纯文本附加到系统提示符。 VectorStoreChartMemoryAdvisor。该顾问使用提供的VectorStore实现管理对话内存。在每次交互中，它从向量存储中检索对话历史记录，并将其作为纯文本附加到系统消息中。 例如，如果要将MessageWindowChatMemory与MessageChatMemoridAdvisor一起使用，则可以如下配置： ChatMemory chatMemory = MessageWindowChatMemory.builder().build(); ChatClient chatClient = ChatClient.builder(chatModel) .defaultAdvisors(MessageChatMemoryAdvisor.builder(chatMemory).build()) .build(); 当执行对ChatClient的调用时，内存将由MessageChatMemoryAdvisor自动管理。将基于指定的对话ID从内存中检索对话历史记录：\nString conversationId = \u0026#34;007\u0026#34;; chatClient.prompt() .user(\u0026#34;Do I have license to code?\u0026#34;) .advisors(a -\u0026gt; a.param(AbstractChatMemoryAdvisor.CHAT_MEMORY_CONVERSATION_ID_KEY, conversationId)) .call() .content(); PromptChatMemoryAdvisor（提示聊天记忆顾问） # 自定义模板 # PromptChatMemoryAdvisor使用默认模板用检索到的对话内存来增加系统消息。您可以通过.PromptTemplate（）构建器方法提供自己的PromptTemplate对象来定制此行为。 自定义PromptTemplate可以使用任何TemplateRenderer实现（默认情况下，它使用基于StringTemplate引擎的StPrompt模板）。重要的要求是模板必须包含以下两个占位符：\n用于接收原始系统消息的指令占位符。 存储器占位符，用于接收检索到的会话存储器。 VectorStoreChartMemoryAdvisor # 自定义模板 # VectorStoreChatMemoryAdvisor使用默认模板用检索到的对话内存来增加系统消息。您可以通过.PromptTemplate（）构建器方法提供自己的PromptTemplate对象来定制此行为。 自定义PromptTemplate可以使用任何TemplateRenderer实现（默认情况下，它使用基于StringTemplate引擎的StPrompt模板）。重要的要求是模板必须包含以下两个占位符：\n用于接收原始系统消息的指令占位符。 long _termmemory占位符，用于接收检索到的会话内存。 聊天记忆模式 # 如果直接使用ChatModel而不是ChatClient，则可以显式管理内存：\n// Create a memory instance ChatMemory chatMemory = MessageWindowChatMemory.builder().build(); String conversationId = \u0026#34;007\u0026#34;; // First interaction UserMessage userMessage1 = new UserMessage(\u0026#34;My name is James Bond\u0026#34;); chatMemory.add(conversationId, userMessage1); ChatResponse response1 = chatModel.call(new Prompt(chatMemory.get(conversationId))); chatMemory.add(conversationId, response1.getResult().getOutput()); // Second interaction UserMessage userMessage2 = new UserMessage(\u0026#34;What is my name?\u0026#34;); chatMemory.add(conversationId, userMessage2); ChatResponse response2 = chatModel.call(new Prompt(chatMemory.get(conversationId))); chatMemory.add(conversationId, response2.getResult().getOutput()); // The response will contain \u0026#34;James Bond\u0026#34; "},{"id":61,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/%E9%82%AE%E6%94%BFml/","title":"PostgresML嵌入","section":"嵌入模型API","content":" PostgresML嵌入 # Spring AI支持PostgresML文本嵌入模型。 嵌入是文本的数字表示。 许多预先训练的LLM可以用于从PostgresML中的文本生成嵌入。\n添加存储库和BOM表 # Spring AI工件发布在Maven Central和Spring Snapshot 存储库中。 为了帮助进行依赖关系管理，Spring AI提供了BOM（物料清单），以确保在整个项目中使用一致版本的Spring AI。请参阅依赖项管理部分，将Spring AI BOM添加到构建系统中。\n自动配置 # Spring AI为Azure PostgresML嵌入模型提供Spring Boot自动配置。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-postgresml-embedding\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-postgresml-embedding\u0026#39; } 使用spring.ai.postgresml.embedding.options.*属性配置PostgresMlEmbeddingModel。链接\n嵌入属性 # 前缀spring.ai.postgresml.embedding是属性前缀，用于配置postgresml嵌入的EmbeddingModel实现。\n运行时选项 # 使用PostgresMlEmbeddingOptions.java配置带有选项的PostgresMLEmbedding模型，如要使用的模型等。 启动时，可以将PostgresMlEmbeddingOptions传递给PostgresMLEmbedding Model构造函数，以配置用于所有嵌入请求的默认选项。 在运行时，可以在EmbeddingRequest中使用PostgresMlEmbedding选项覆盖默认选项。 例如，要覆盖特定请求的默认模型名称：\nEmbeddingResponse embeddingResponse = embeddingModel.call( new EmbeddingRequest(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;), PostgresMlEmbeddingOptions.builder() .transformer(\u0026#34;intfloat/e5-small\u0026#34;) .vectorType(VectorType.PG_ARRAY) .kwargs(Map.of(\u0026#34;device\u0026#34;, \u0026#34;gpu\u0026#34;)) .build())); 样本控制器 # 这将创建一个可以注入到类中的EmbeddingModel实现。\nspring.ai.postgresml.embedding.options.transformer=distilbert-base-uncased spring.ai.postgresml.embedding.options.vectorType=PG_ARRAY spring.ai.postgresml.embedding.options.metadataMode=EMBED spring.ai.postgresml.embedding.options.kwargs.device=cpu @RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(\u0026#34;/ai/embedding\u0026#34;) public Map embed(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { EmbeddingResponse embeddingResponse = this.embeddingModel.embedForResponse(List.of(message)); return Map.of(\u0026#34;embedding\u0026#34;, embeddingResponse); } } 手动配置 # 您可以手动创建PostgresMlEmbeddingModel，而不是使用SpringBoot自动配置。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-postgresml\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-postgresml\u0026#39; } 接下来，创建PostgresMlEmbeddingModel实例，并使用它计算两个输入文本之间的相似性：\nvar jdbcTemplate = new JdbcTemplate(dataSource); // your posgresml data source PostgresMlEmbeddingModel embeddingModel = new PostgresMlEmbeddingModel(this.jdbcTemplate, PostgresMlEmbeddingOptions.builder() .transformer(\u0026#34;distilbert-base-uncased\u0026#34;) // huggingface transformer model name. .vectorType(VectorType.PG_VECTOR) //vector type in PostgreSQL. .kwargs(Map.of(\u0026#34;device\u0026#34;, \u0026#34;cpu\u0026#34;)) // optional arguments. .metadataMode(MetadataMode.EMBED) // Document metadata mode. .build()); embeddingModel.afterPropertiesSet(); // initialize the jdbc template and database. EmbeddingResponse embeddingResponse = this.embeddingModel .embedForResponse(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;)); @Bean public EmbeddingModel embeddingModel(JdbcTemplate jdbcTemplate) { return new PostgresMlEmbeddingModel(jdbcTemplate, PostgresMlEmbeddingOptions.builder() .... .build()); } "},{"id":62,"href":"/docs/%E5%B7%A5%E5%85%B7%E8%B0%83%E7%94%A8/","title":"工具调用","section":"Docs","content":" 工具调用 # 工具调用（也称为函数调用）是人工智能应用程序中的一种常见模式，允许模型与一组API或工具交互，以增强其功能。 工具主要用于： 尽管我们通常将工具调用称为模型功能，但实际上由客户端应用程序提供工具调用逻辑。模型只能请求工具调用并提供输入参数，而应用程序负责从输入参数执行工具调用并返回结果。该模型永远无法访问作为工具提供的任何API，这是一个关键的安全考虑事项。 SpringAI提供了方便的API来定义工具、解决来自模型的工具调用请求和执行工具调用。以下部分概述了Spring AI中的工具调用功能。\n快速入门 # 让我们看看如何开始在SpringAI中使用工具调用。我们将实现两个简单的工具：一个用于信息检索，另一个用于采取行动。信息检索工具将用于获取用户时区中的当前日期和时间。操作工具将用于设置指定时间的报警。\n信息检索 # 人工智能模型无法访问实时信息。任何假设知道当前日期或天气预报等信息的问题都不能由模型回答。然而，我们可以提供一个可以检索该信息的工具，并让模型在需要访问实时信息时调用该工具。 让我们实现一个工具，在DateTimeTools类中获取用户时区中的当前日期和时间。该工具不需要参数。Spring框架中的LocaleContextHolder可以提供用户的时区。该工具将被定义为用@tool注释的方法。为了帮助模型理解是否以及何时调用该工具，我们将提供工具功能的详细描述。\nimport java.time.LocalDateTime; import org.springframework.ai.tool.annotation.Tool; import org.springframework.context.i18n.LocaleContextHolder; class DateTimeTools { @Tool(description = \u0026#34;Get the current date and time in the user\u0026#39;s timezone\u0026#34;) String getCurrentDateTime() { return LocalDateTime.now().atZone(LocaleContextHolder.getTimeZone().toZoneId()).toString(); } } 接下来，让我们为模型提供该工具。在这个例子中，我们将使用ChatClient与模型交互。我们将通过tools（）方法传递DateTimeTools的实例来为模型提供工具。当模型需要知道当前日期和时间时，它将请求调用该工具。在内部，ChatClient将调用工具并将结果返回给模型，然后模型将使用工具调用结果来生成对原始问题的最终响应。\nChatModel chatModel = ... String response = ChatClient.create(chatModel) .prompt(\u0026#34;What day is tomorrow?\u0026#34;) .tools(new DateTimeTools()) .call() .content(); System.out.println(response); 输出将类似于：\nTomorrow is 2015-10-21. 您可以重新尝试询问相同的问题。这一次，不要为模型提供工具。输出将类似于：\nI am an AI and do not have access to real-time information. Please provide the current date so I can accurately determine what day tomorrow will be. 如果没有该工具，模型就不知道如何回答这个问题，因为它不能确定当前的日期和时间。\n采取行动 # 人工智能模型可用于生成实现特定目标的计划。例如，模型可以生成预订丹麦旅行的计划。然而，模型没有执行计划的能力。这就是工具的作用：它们可以用于执行模型生成的计划。 在前面的示例中，我们使用了一个工具来确定当前日期和时间。在本例中，我们将定义第二个工具，用于在特定时间设置报警。目标是从现在开始设置10分钟的警报，因此我们需要为模型提供这两个工具来完成这项任务。 我们将一如既往地将新工具添加到相同的DateTimeTools类中。新工具将采用单个参数，即ISO-8601格式的时间。然后，该工具将向控制台打印一条消息，指示已为给定时间设置报警。与前面一样，该工具被定义为用@tool注释的方法，我们也使用它来提供详细的描述，以帮助模型理解何时以及如何使用该工具。\nimport java.time.LocalDateTime; import java.time.format.DateTimeFormatter; import org.springframework.ai.tool.annotation.Tool; import org.springframework.context.i18n.LocaleContextHolder; class DateTimeTools { @Tool(description = \u0026#34;Get the current date and time in the user\u0026#39;s timezone\u0026#34;) String getCurrentDateTime() { return LocalDateTime.now().atZone(LocaleContextHolder.getTimeZone().toZoneId()).toString(); } @Tool(description = \u0026#34;Set a user alarm for the given time, provided in ISO-8601 format\u0026#34;) void setAlarm(String time) { LocalDateTime alarmTime = LocalDateTime.parse(time, DateTimeFormatter.ISO_DATE_TIME); System.out.println(\u0026#34;Alarm set for \u0026#34; + alarmTime); } } 接下来，让我们将这两个工具都提供给模型。我们将使用ChatClient与模型交互。我们将通过tools（）方法传递DateTimeTools的实例来为模型提供工具。当我们要求在10分钟后设置警报时，模型首先需要知道当前的日期和时间。然后，它将使用当前日期和时间来计算报警时间。最后，它将使用报警工具来设置报警。在内部，ChatClient将处理来自模型的任何工具调用请求，并将任何工具调用执行结果发送回它，以便模型可以生成最终响应。\nChatModel chatModel = ... String response = ChatClient.create(chatModel) .prompt(\u0026#34;Can you set an alarm 10 minutes from now?\u0026#34;) .tools(new DateTimeTools()) .call() .content(); System.out.println(response); 在应用程序日志中，您可以检查警报是否已在正确的时间设置。\n概述 # SpringAI通过一组灵活的抽象来支持工具调用，这些抽象允许您以一致的方式定义、解析和执行工具。本节概述了Spring AI中工具调用的主要概念和组件。 工具是工具调用的构建块，它们由ToolCallback接口建模。Spring AI提供了从方法和函数指定ToolCallback的内置支持，但您始终可以定义自己的ToolCallbak实现来支持更多用例。 ChatModel实现透明地将工具调用请求分派到相应的ToolCallback实现，并将工具调用结果发送回模型，该模型最终将生成最终响应。它们使用ToolCallingManager接口来执行此操作，该接口负责管理工具执行生命周期。 ChatClient和ChatModel都接受ToolCallback对象的列表，以使工具可用于模型和最终将执行它们的ToolCallingManager。 除了直接传递ToolCallback对象外，还可以传递工具名称列表，该列表将使用ToolCallbickResolver接口动态解析。 下面的部分将详细介绍所有这些概念和API，包括如何定制和扩展它们以支持更多用例。\n方法作为工具 # Spring AI以两种方式提供了从方法中指定工具（即ToolCallback）的内置支持：\n声明性地，使用@Tool注释 以编程方式，使用低级MethodToolCallback实现。 声明性规范：@Tool # 通过使用@tool对方法进行注释，可以将其转换为工具。\nclass DateTimeTools { @Tool(description = \u0026#34;Get the current date and time in the user\u0026#39;s timezone\u0026#34;) String getCurrentDateTime() { return LocalDateTime.now().atZone(LocaleContextHolder.getTimeZone().toZoneId()).toString(); } } @Tool注释允许您提供有关工具的关键信息：\nname：工具的名称。如果未提供，将使用方法名称。人工智能模型在调用工具时使用这个名称来标识它。因此，不允许在同一个类中有两个同名的工具。对于特定的聊天请求，该名称在模型可用的所有工具中都必须是唯一的。 description:工具的描述，模型可以使用该描述来了解何时以及如何调用工具。如果未提供，则方法名称将用作工具描述。然而，强烈建议提供详细的描述，因为这对于模型理解工具的用途和如何使用它是至关重要的。未能提供良好的描述可能会导致模型在应该使用该工具时不使用该工具，或者错误地使用该工具。 returnDirect：工具结果应该直接返回给客户端还是传递回模型。有关更多详细信息，请参阅直接退货。 resultConverter：用于将工具调用的结果转换为String对象以发送回AI模型的ToolCallResultConvertor实现。有关详细信息，请参见结果转换。 该方法可以是静态的，也可以是实例的，并且它可以具有任何可见性（public、protected、package private或private）。包含该方法的类可以是顶级类或嵌套类，并且它也可以具有任何可见性（只要在计划实例化它的位置可以访问）。 可以为大多数类型（原语、POJO、枚举、列表、数组、映射等）的方法定义任意数量的参数（包括无参数）。类似地，该方法可以返回大多数类型，包括void。如果方法返回值，则返回类型必须是可序列化类型，因为结果将被序列化并发送回模型。 Spring AI将自动为@Tool注释方法的输入参数生成JSON模式。模型使用模式来理解如何调用工具和准备工具请求。@ToolParam注释可用于提供有关输入参数的附加信息，例如描述或参数是必需的还是可选的。默认情况下，所有输入参数都被认为是必需的。 import java.time.LocalDateTime; import java.time.format.DateTimeFormatter; import org.springframework.ai.tool.annotation.Tool; import org.springframework.ai.tool.annotation.ToolParam; class DateTimeTools { @Tool(description = \u0026#34;Set a user alarm for the given time\u0026#34;) void setAlarm(@ToolParam(description = \u0026#34;Time in ISO-8601 format\u0026#34;) String time) { LocalDateTime alarmTime = LocalDateTime.parse(time, DateTimeFormatter.ISO_DATE_TIME); System.out.println(\u0026#34;Alarm set for \u0026#34; + alarmTime); } } @ToolParam注释允许您提供有关工具参数的关键信息：\ndescription:参数的描述，模型可以使用它来更好地理解如何使用它。例如，参数应该采用什么格式，允许使用什么值，等等。 required：参数是必需的还是可选的。默认情况下，所有参数都被认为是必需的。 如果参数被注释为@Nullable，则它将被视为可选的，除非使用@ToolParam注释根据需要显式标记。 除了@ToolParam注释外，您还可以使用Swager的@Schema注释或Jackson的@JsonProperty注释。有关更多详细信息，请参阅JSON模式。 将工具添加到ChatClient # 使用声明性规范方法时，可以在调用ChatClient时将工具类实例传递给tools（）方法。此类工具仅适用于它们添加到的特定聊天请求。\nChatClient.create(chatModel) .prompt(\u0026#34;What day is tomorrow?\u0026#34;) .tools(new DateTimeTools()) .call() .content(); 在幕后，ChatClient将从工具类实例中的每个@Tool注释方法生成一个ToolCallback，并将它们传递给模型。如果希望自己生成ToolCallback，则可以使用ToolCallbaks实用程序类。\nToolCallback[] dateTimeTools = ToolCallbacks.from(new DateTimeTools()); 将默认工具添加到ChatClient # 使用声明性规范方法时，可以将默认工具添加到ChatClient。通过将工具类实例传递给defaultTools（）方法来生成生成器。\nChatModel chatModel = ... ChatClient chatClient = ChatClient.builder(chatModel) .defaultTools(new DateTimeTools()) .build(); 将工具添加到ChatModel # 使用声明性规范方法时，可以将工具类实例传递给用于调用ChatModel的ToolCallingChatOptions的toolCallback（）方法。此类工具仅适用于它们添加到的特定聊天请求。\nChatModel chatModel = ... ToolCallback[] dateTimeTools = ToolCallbacks.from(new DateTimeTools()); ChatOptions chatOptions = ToolCallingChatOptions.builder() .toolCallbacks(dateTimeTools) .build(); Prompt prompt = new Prompt(\u0026#34;What day is tomorrow?\u0026#34;, chatOptions); chatModel.call(prompt); 将默认工具添加到ChatModel # 使用声明性规范方法时，可以通过将工具类实例传递给用于创建ChatModel的ToolCallingChatOptions实例的toolCallback（）方法，在构造时将默认工具添加到ChatModel。\nToolCallback[] dateTimeTools = ToolCallbacks.from(new DateTimeTools()); ChatModel chatModel = OllamaChatModel.builder() .ollamaApi(OllamaApi.builder().build()) .defaultOptions(ToolCallingChatOptions.builder() .toolCallbacks(dateTimeTools) .build()) .build(); 编程规范：MethodToolCallback # 通过以编程方式构建MethodToolCallback，可以将方法转换为工具。\nclass DateTimeTools { String getCurrentDateTime() { return LocalDateTime.now().atZone(LocaleContextHolder.getTimeZone().toZoneId()).toString(); } } MethodToolCallback。Builder允许您构建MethodToolCallback实例，并提供有关该工具的关键信息：\ntoolDefinition：定义工具名称、描述和输入架构的toolDefinition实例。可以使用ToolDefinition构建它。生成器类。必需。 toolMetadata：toolMetadata实例，定义其他设置，例如是否应将结果直接返回到客户端，以及要使用的结果转换器。可以使用ToolMetadata构建它。生成器类。 toolMethod：表示工具方法的Method实例。必需。 toolObject：包含工具方法的对象实例。如果方法是静态的，则可以省略此参数。 toolCallResultConverter：用于将工具调用的结果转换为要发送回AI模型的String对象的ToolCallResaultConverter实例。如果未提供，则将使用默认转换器（DefaultToolCallResultConverter）。 工具定义。Builder允许您构建ToolDefinition实例，并定义工具名称、描述和输入架构： name：工具的名称。如果未提供，将使用方法名称。人工智能模型在调用工具时使用这个名称来标识它。因此，不允许在同一个类中有两个同名的工具。对于特定的聊天请求，该名称在模型可用的所有工具中都必须是唯一的。 description:工具的描述，模型可以使用该描述来了解何时以及如何调用工具。如果未提供，则方法名称将用作工具描述。然而，强烈建议提供详细的描述，因为这对于模型理解工具的用途和如何使用它是至关重要的。未能提供良好的描述可能会导致模型在应该使用该工具时不使用该工具，或者错误地使用该工具。 inputSchema：工具输入参数的JSON模式。如果未提供，则将基于方法参数自动生成架构。您可以使用@ToolParam注释来提供有关输入参数的其他信息，例如描述或参数是必需的还是可选的。默认情况下，所有输入参数都被认为是必需的。有关更多详细信息，请参阅JSON模式。 ToolMetadata。Builder允许您构建ToolMetadata实例，并定义工具的其他设置： returnDirect：工具结果应该直接返回给客户端还是传递回模型。有关更多详细信息，请参阅直接退货。 Method method = ReflectionUtils.findMethod(DateTimeTools.class, \u0026#34;getCurrentDateTime\u0026#34;); ToolCallback toolCallback = MethodToolCallback.builder() .toolDefinition(ToolDefinition.builder(method) .description(\u0026#34;Get the current date and time in the user\u0026#39;s timezone\u0026#34;) .build()) .toolMethod(method) .toolObject(new DateTimeTools()) .build(); 该方法可以是静态的，也可以是实例的，并且它可以具有任何可见性（public、protected、package private或private）。包含该方法的类可以是顶级类或嵌套类，并且它也可以具有任何可见性（只要在计划实例化它的位置可以访问）。 可以为大多数类型（原语、POJO、枚举、列表、数组、映射等）的方法定义任意数量的参数（包括无参数）。类似地，该方法可以返回大多数类型，包括void。如果方法返回值，则返回类型必须是可序列化类型，因为结果将被序列化并发送回模型。 如果方法是静态的，则可以省略toolObject（）方法，因为不需要它。\nclass DateTimeTools { static String getCurrentDateTime() { return LocalDateTime.now().atZone(LocaleContextHolder.getTimeZone().toZoneId()).toString(); } } Method method = ReflectionUtils.findMethod(DateTimeTools.class, \u0026#34;getCurrentDateTime\u0026#34;); ToolCallback toolCallback = MethodToolCallback.builder() .toolDefinition(ToolDefinition.builder(method) .description(\u0026#34;Get the current date and time in the user\u0026#39;s timezone\u0026#34;) .build()) .toolMethod(method) .build(); Spring AI将自动为方法的输入参数生成JSON模式。模型使用模式来理解如何调用工具和准备工具请求。@ToolParam注释可用于提供有关输入参数的附加信息，例如描述或参数是必需的还是可选的。默认情况下，所有输入参数都被认为是必需的。\nimport java.time.LocalDateTime; import java.time.format.DateTimeFormatter; import org.springframework.ai.tool.annotation.ToolParam; class DateTimeTools { void setAlarm(@ToolParam(description = \u0026#34;Time in ISO-8601 format\u0026#34;) String time) { LocalDateTime alarmTime = LocalDateTime.parse(time, DateTimeFormatter.ISO_DATE_TIME); System.out.println(\u0026#34;Alarm set for \u0026#34; + alarmTime); } } @ToolParam注释允许您提供有关工具参数的关键信息：\ndescription:参数的描述，模型可以使用它来更好地理解如何使用它。例如，参数应该采用什么格式，允许使用什么值，等等。 required：参数是必需的还是可选的。默认情况下，所有参数都被认为是必需的。 如果参数被注释为@Nullable，则它将被视为可选的，除非使用@ToolParam注释根据需要显式标记。 除了@ToolParam注释外，您还可以使用Swager的@Schema注释或Jackson的@JsonProperty注释。有关更多详细信息，请参阅JSON模式。 将工具添加到ChatClient和ChatModel # 使用编程规范方法时，可以将MethodToolCallback实例传递给ChatClient的tools（）方法。\nToolCallback toolCallback = ... ChatClient.create(chatModel) .prompt(\u0026#34;What day is tomorrow?\u0026#34;) .tools(toolCallback) .call() .content(); 将默认工具添加到ChatClient # 使用编程规范方法时，可以将默认工具添加到ChatClient。通过将MethodToolCallback实例传递给defaultTools（）方法来构建生成器。\nChatModel chatModel = ... ToolCallback toolCallback = ... ChatClient chatClient = ChatClient.builder(chatModel) .defaultTools(toolCallback) .build(); 将工具添加到ChatModel # 使用编程规范方法时，可以将MethodToolCallback实例传递给用于调用ChatModel的ToolCallingChatOptions的Toolcallback（）方法。该工具仅适用于其添加到的特定聊天请求。\nChatModel chatModel = ... ToolCallback toolCallback = ... ChatOptions chatOptions = ToolCallingChatOptions.builder() .toolCallbacks(toolCallback) .build(): Prompt prompt = new Prompt(\u0026#34;What day is tomorrow?\u0026#34;, chatOptions); chatModel.call(prompt); 将默认工具添加到ChatModel # 使用编程规范方法时，可以通过将MethodToolCallback实例传递给用于创建ChatModel的ToolCallingChatOptions实例的ToolCallbark（）方法，在构造时将默认工具添加到ChatModel。\nToolCallback toolCallback = ... ChatModel chatModel = OllamaChatModel.builder() .ollamaApi(OllamaApi.builder().build()) .defaultOptions(ToolCallingChatOptions.builder() .toolCallbacks(toolCallback) .build()) .build(); 方法工具限制 # 以下类型当前不支持作为用作工具的方法的参数或返回类型：\n可选 异步类型（例如CompletableFuture、Future） 无功类型（例如Flow、Mono、Flux） 功能类型（例如功能、供应商、消费者）。 使用基于函数的工具规范方法支持函数类型。有关详细信息，请参阅“ 作为工具的功能”。 作为工具的功能 # Spring AI提供了从函数中指定工具的内置支持，可以通过编程方式使用低级FunctionToolCallback实现，也可以在运行时动态解析为@Bean。\n编程规范：FunctionToolCallback # 通过以编程方式构建FunctionToolCallback，可以将函数类型（Function、Supplier、Consumer或BiFunction）转换为工具。\npublic class WeatherService implements Function\u0026lt;WeatherRequest, WeatherResponse\u0026gt; { public WeatherResponse apply(WeatherRequest request) { return new WeatherResponse(30.0, Unit.C); } } public enum Unit { C, F } public record WeatherRequest(String location, Unit unit) {} public record WeatherResponse(double temp, Unit unit) {} FunctionToolCallback。Builder允许您构建FunctionToolCallback实例，并提供有关该工具的关键信息：\nname：工具的名称。人工智能模型在调用工具时使用这个名称来标识它。因此，不允许在同一上下文中有两个同名的工具。对于特定的聊天请求，该名称在模型可用的所有工具中都必须是唯一的。必需。 toolFunction：表示工具方法（Function、Supplier、Consumer或BiFunction）的函数对象。必需。 description:工具的描述，模型可以使用该描述来了解何时以及如何调用工具。如果未提供，则方法名称将用作工具描述。然而，强烈建议提供详细的描述，因为这对于模型理解工具的用途和如何使用它是至关重要的。未能提供良好的描述可能会导致模型在应该使用该工具时不使用该工具，或者错误地使用该工具。 inputType：函数输入的类型。必需。 inputSchema：工具输入参数的JSON模式。如果未提供，则将基于inputType自动生成模式。您可以使用@ToolParam注释来提供有关输入参数的其他信息，例如描述或参数是必需的还是可选的。默认情况下，所有输入参数都被认为是必需的。有关更多详细信息，请参阅JSON模式。 toolMetadata：toolMetadata实例，定义其他设置，例如是否应将结果直接返回到客户端，以及要使用的结果转换器。可以使用ToolMetadata构建它。生成器类。 toolCallResultConverter：用于将工具调用的结果转换为要发送回AI模型的String对象的ToolCallResaultConverter实例。如果未提供，则将使用默认转换器（DefaultToolCallResultConverter）。 ToolMetadata。Builder允许您构建ToolMetadata实例，并定义工具的其他设置： returnDirect：工具结果应该直接返回给客户端还是传递回模型。有关更多详细信息，请参阅直接退货。 ToolCallback toolCallback = FunctionToolCallback .builder(\u0026#34;currentWeather\u0026#34;, new WeatherService()) .description(\u0026#34;Get the weather in location\u0026#34;) .inputType(WeatherRequest.class) .build(); 功能输入和输出可以是Void或POJO。输入和输出POJO必须是可序列化的，因为结果将被序列化并发送回模型。函数以及输入和输出类型必须是公共的。\n将工具添加到ChatClient # 使用编程规范方法时，可以将FunctionToolCallback实例传递给ChatClient的tools（）方法。该工具仅适用于其添加到的特定聊天请求。\nToolCallback toolCallback = ... ChatClient.create(chatModel) .prompt(\u0026#34;What\u0026#39;s the weather like in Copenhagen?\u0026#34;) .tools(toolCallback) .call() .content(); 将默认工具添加到ChatClient # 使用编程规范方法时，可以将默认工具添加到ChatClient。通过将FunctionToolCallback实例传递给defaultTools（）方法来构建生成器。\nChatModel chatModel = ... ToolCallback toolCallback = ... ChatClient chatClient = ChatClient.builder(chatModel) .defaultTools(toolCallback) .build(); 将工具添加到ChatModel # 使用编程规范方法时，可以将FunctionToolCallback实例传递给ToolCallingChatOptions的toolCallbacks（）方法。该工具仅适用于其添加到的特定聊天请求。\nChatModel chatModel = ... ToolCallback toolCallback = ... ChatOptions chatOptions = ToolCallingChatOptions.builder() .toolCallbacks(toolCallback) .build(): Prompt prompt = new Prompt(\u0026#34;What\u0026#39;s the weather like in Copenhagen?\u0026#34;, chatOptions); chatModel.call(prompt); 将默认工具添加到ChatModel # 使用编程规范方法时，可以通过将FunctionToolCallback实例传递给用于创建ChatModel的ToolCallingChatOptions实例的ToolCallbick（）方法，在构造时将默认工具添加到ChatModel。\nToolCallback toolCallback = ... ChatModel chatModel = OllamaChatModel.builder() .ollamaApi(OllamaApi.builder().build()) .defaultOptions(ToolCallingChatOptions.builder() .toolCallbacks(toolCallback) .build()) .build(); 动态规范：@Bean # 您可以将工具定义为Springbean，并让SpringAI使用ToolCallbackResolver接口（通过SpringBeanToolCallbickResolver实现）在运行时动态解析它们，而不是以编程方式指定工具。该选项允许您使用任何Function、Supplier、Consumer或BiFunctionbean作为工具。bean名称将用作工具名称，Spring Framework中的@Description注释可用于提供该工具的描述，模型使用该注释来理解何时以及如何调用该工具。如果不提供描述，则方法名称将用作工具描述。然而，强烈建议提供详细的描述，因为这对于模型理解工具的用途和如何使用它是至关重要的。未能提供良好的描述可能会导致模型在应该使用该工具时不使用该工具，或者错误地使用该工具。\n@Configuration(proxyBeanMethods = false) class WeatherTools { WeatherService weatherService = new WeatherService(); @Bean @Description(\u0026#34;Get the weather in location\u0026#34;) Function\u0026lt;WeatherRequest, WeatherResponse\u0026gt; currentWeather() { return weatherService; } } 将自动生成工具输入参数的JSON模式。您可以使用@ToolParam注释来提供有关输入参数的其他信息，例如描述或参数是必需的还是可选的。默认情况下，所有输入参数都被认为是必需的。有关更多详细信息，请参阅JSON模式。\nrecord WeatherRequest(@ToolParam(description = \u0026#34;The name of a city or a country\u0026#34;) String location, Unit unit) {} 这种工具规范方法的缺点是不能保证类型安全，因为工具解析是在运行时完成的。为了减轻这种情况，可以使用@Bean注释显式地指定工具名称，并将值存储在常量中，以便可以在聊天请求中使用它，而不是硬编码工具名称。\n@Configuration(proxyBeanMethods = false) class WeatherTools { public static final String CURRENT_WEATHER_TOOL = \u0026#34;currentWeather\u0026#34;; @Bean(CURRENT_WEATHER_TOOL) @Description(\u0026#34;Get the weather in location\u0026#34;) Function\u0026lt;WeatherRequest, WeatherResponse\u0026gt; currentWeather() { ... } } 将工具添加到ChatClient # 使用动态规范方法时，可以将工具名称（即函数bean名称）传递给ChatClient的tools（）方法。\nChatClient.create(chatModel) .prompt(\u0026#34;What\u0026#39;s the weather like in Copenhagen?\u0026#34;) .tools(\u0026#34;currentWeather\u0026#34;) .call() .content(); 将默认工具添加到ChatClient # 使用动态规范方法时，可以将默认工具添加到ChatClient。通过将工具名称传递给defaultTools（）方法来构建生成器。\nChatModel chatModel = ... ChatClient chatClient = ChatClient.builder(chatModel) .defaultTools(\u0026#34;currentWeather\u0026#34;) .build(); 将工具添加到ChatModel # 使用动态规范方法时，可以将工具名称传递给用于调用ChatModel的ToolCallingChatOptions的toolNames（）方法。该工具仅适用于其添加到的特定聊天请求。\nChatModel chatModel = ... ChatOptions chatOptions = ToolCallingChatOptions.builder() .toolNames(\u0026#34;currentWeather\u0026#34;) .build(): Prompt prompt = new Prompt(\u0026#34;What\u0026#39;s the weather like in Copenhagen?\u0026#34;, chatOptions); chatModel.call(prompt); 将默认工具添加到ChatModel # 使用动态规范方法时，可以通过将工具名称传递给用于创建ChatModel的ToolCallingChatOptions实例的toolNames（）方法，在构造时将默认工具添加到ChatModel。\nChatModel chatModel = OllamaChatModel.builder() .ollamaApi(OllamaApi.builder().build()) .defaultOptions(ToolCallingChatOptions.builder() .toolNames(\u0026#34;currentWeather\u0026#34;) .build()) .build(); 功能工具限制 # 以下类型当前不支持作为用作工具的函数的输入或输出类型：\n基本体类型 可选 集合类型（例如列表、映射、数组、集合） 异步类型（例如CompletableFuture、Future） 无功类型（例如流量、单声道、通量）。 使用基于方法的工具规范方法支持基元类型和集合。有关详细信息，请参见作为工具的方法。 工具规格 # 在Spring AI中，工具通过ToolCallback接口建模。在前面的部分中，我们已经看到了如何使用Spring AI提供的内置支持从方法和函数中定义工具（请参见 方法作为工具和函数作为工具）。本节将深入研究工具规范，以及如何定制和扩展它以支持更多用例。\n工具回调 # ToolCallback接口提供了一种定义AI模型可以调用的工具的方法，包括定义和执行逻辑。当您想要从头定义工具时，它是要实现的主界面。例如，可以从MCP客户端（使用模型上下文协议）或ChatClient（构建模块化代理应用程序）定义ToolCallback。 该接口提供以下方法：\npublic interface ToolCallback { /** * Definition used by the AI model to determine when and how to call the tool. */ ToolDefinition getToolDefinition(); /** * Metadata providing additional information on how to handle the tool. */ ToolMetadata getToolMetadata(); /** * Execute tool with the given input and return the result to send back to the AI model. */ String call(String toolInput); /** * Execute tool with the given input and context, and return the result to send back to the AI model. */ String call(String toolInput, ToolContext tooContext); } Spring AI为工具方法（MethodToolCallback）和工具函数（FunctionToolCallbak）提供了内置实现。\n刀具定义 # ToolDefinition接口为AI模型提供所需的信息，以了解工具的可用性，包括工具名称、描述和输入模式。每个ToolCallback实现都必须提供ToolDefinition实例来定义工具。 该接口提供以下方法：\npublic interface ToolDefinition { /** * The tool name. Unique within the tool set provided to a model. */ String name(); /** * The tool description, used by the AI model to determine what the tool does. */ String description(); /** * The schema of the parameters used to call the tool. */ String inputSchema(); } 工具定义。Builder允许使用默认实现（DefaultToolDefinition）构建ToolDefinitionInstance。\nToolDefinition toolDefinition = ToolDefinition.builder() .name(\u0026#34;currentWeather\u0026#34;) .description(\u0026#34;Get the weather in location\u0026#34;) .inputSchema(\u0026#34;\u0026#34;\u0026#34; { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;unit\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;enum\u0026#34;: [\u0026#34;C\u0026#34;, \u0026#34;F\u0026#34;] } }, \u0026#34;required\u0026#34;: [\u0026#34;location\u0026#34;, \u0026#34;unit\u0026#34;] } \u0026#34;\u0026#34;\u0026#34;) .build(); 方法工具定义 # 从方法生成工具时，将自动为您生成ToolDefinition。如果您希望自己生成ToolDefinition，则可以使用此方便的生成器。\nMethod method = ReflectionUtils.findMethod(DateTimeTools.class, \u0026#34;getCurrentDateTime\u0026#34;); ToolDefinition toolDefinition = ToolDefinition.from(method); 从方法生成的ToolDefinition包括作为工具名称的方法名、作为工具描述的方法名以及方法输入参数的JSON模式。如果使用@Tool对方法进行注释，则工具名称和描述将从注释中获取（如果设置）。 如果希望显式提供部分或全部属性，则可以使用ToolDefinition。生成器以生成自定义ToolDefinition实例。\nMethod method = ReflectionUtils.findMethod(DateTimeTools.class, \u0026#34;getCurrentDateTime\u0026#34;); ToolDefinition toolDefinition = ToolDefinition.builder(method) .name(\u0026#34;currentDateTime\u0026#34;) .description(\u0026#34;Get the current date and time in the user\u0026#39;s timezone\u0026#34;) .inputSchema(JsonSchemaGenerator.generateForMethodInput(method)) .build(); 功能工具定义 # 从函数构建工具时，将自动为您生成工具定义。使用FunctionToolCallback时。生成器以生成FunctionToolCallback实例，则可以提供将用于生成ToolDefinition的工具名称、描述和输入架构。有关详细信息，请参阅“ 作为工具的功能”。\nJSON架构 # 当为AI模型提供工具时，模型需要知道用于调用该工具的输入类型的模式。模式用于理解如何调用工具和准备工具请求。SpringAI通过JsonSchemaGenerator类为工具生成输入类型的JSONSchema提供了内置支持。架构作为ToolDefinition的一部分提供。 JsonSchemaGenerator类在幕后用于为方法或函数的输入参数生成JSON模式，使用Methods as Tools和Functions as Tools中描述的任何策略。JSON模式生成逻辑支持一系列注释，您可以对方法和函数的输入参数使用这些注释来定制结果模式。 本节描述在为工具的输入参数生成JSON模式时可以定制的两个主要选项：描述和所需状态。\n描述 # 除了为工具本身提供描述外，还可以为工具的输入参数提供描述。描述可用于提供有关输入参数的关键信息，例如参数应采用何种格式、允许使用何种值等。这有助于帮助模型理解输入模式以及如何使用它。Spring AI为使用以下注释之一生成输入参数的描述提供了内置支持：\n@Spring AI的ToolParam（description=“…”） @来自Jackson的JsonClassDescription（description=“…”） @来自Jackson的JsonPropertyDescription（description=“…”） @Swagger的架构（description=“…”）。 这种方法适用于方法和函数，并且您可以对嵌套类型递归地使用它。 import java.time.LocalDateTime; import java.time.format.DateTimeFormatter; import org.springframework.ai.tool.annotation.Tool; import org.springframework.ai.tool.annotation.ToolParam; import org.springframework.context.i18n.LocaleContextHolder; class DateTimeTools { @Tool(description = \u0026#34;Set a user alarm for the given time\u0026#34;) void setAlarm(@ToolParam(description = \u0026#34;Time in ISO-8601 format\u0026#34;) String time) { LocalDateTime alarmTime = LocalDateTime.parse(time, DateTimeFormatter.ISO_DATE_TIME); System.out.println(\u0026#34;Alarm set for \u0026#34; + alarmTime); } } 必需/可选 # 默认情况下，每个输入参数都被认为是必需的，这迫使AI模型在调用工具时为其提供值。然而，可以通过使用以下注释之一（按优先级顺序）使输入参数成为可选的：\n@Spring AI的ToolParam（必需=假） @Jackson的JsonProperty（必需=false） @Swagger的架构（必需=假） @从Spring Framework可以为Null。 这种方法适用于方法和函数，并且您可以对嵌套类型递归地使用它。 class CustomerTools { @Tool(description = \u0026#34;Update customer information\u0026#34;) void updateCustomerInfo(Long id, String name, @ToolParam(required = false) String email) { System.out.println(\u0026#34;Updated info for customer with id: \u0026#34; + id); } } 结果转换 # 工具调用的结果使用ToolCallResultConverter序列化，然后发送回AI模型。ToolCallResultConverter接口提供了一种将工具调用的结果转换为String对象的方法。 该接口提供以下方法：\n@FunctionalInterface public interface ToolCallResultConverter { /** * Given an Object returned by a tool, convert it to a String compatible with the * given class type. */ String convert(@Nullable Object result, @Nullable Type returnType); } 结果必须是可序列化类型。默认情况下，使用Jackson（DefaultToolCallResultConverter``）将结果序列化为JSON，但您可以通过提供自己的ToolCallresultConvertor实现来定制序列化过程。 Spring AI在方法和函数工具中都依赖于ToolCallResultConverter。\n方法工具调用结果转换 # 从具有声明性方法的方法构建工具时，可以通过设置@tool注释的resultConverter（）属性来提供用于该工具的自定义ToolCallResultConvertor。\nclass CustomerTools { @Tool(description = \u0026#34;Retrieve customer information\u0026#34;, resultConverter = CustomToolCallResultConverter.class) Customer getCustomerInfo(Long id) { return customerRepository.findById(id); } } 如果使用编程方法，则可以通过设置MethodToolCallback的resultConverter（）属性来提供用于该工具的自定义ToolCallResultConvertor。建筑商。 有关详细信息，请参见作为工具的方法。\n函数工具调用结果转换 # 使用编程方法从函数构建工具时，可以通过设置FunctionToolCallback的resultConverter（）属性来提供用于该工具的自定义ToolCallResultConvertor。建筑商。 有关详细信息，请参阅“ 作为工具的功能”。\n工具上下文 # Spring AI支持通过ToolContext API将额外的上下文信息传递给工具。此功能允许您提供额外的用户提供的数据，这些数据可以与AI模型传递的工具参数一起在工具执行中使用。 class CustomerTools { @Tool(description = \u0026#34;Retrieve customer information\u0026#34;) Customer getCustomerInfo(Long id, ToolContext toolContext) { return customerRepository.findById(id, toolContext.get(\u0026#34;tenantId\u0026#34;)); } } ToolContext由用户在调用ChatClient时提供的数据填充。\nChatModel chatModel = ... String response = ChatClient.create(chatModel) .prompt(\u0026#34;Tell me more about the customer with ID 42\u0026#34;) .tools(new CustomerTools()) .toolContext(Map.of(\u0026#34;tenantId\u0026#34;, \u0026#34;acme\u0026#34;)) .call() .content(); System.out.println(response); 类似地，您可以在直接调用ChatModel时定义工具上下文数据。\nChatModel chatModel = ... ToolCallback[] customerTools = ToolCallbacks.from(new CustomerTools()); ChatOptions chatOptions = ToolCallingChatOptions.builder() .toolCallbacks(customerTools) .toolContext(Map.of(\u0026#34;tenantId\u0026#34;, \u0026#34;acme\u0026#34;)) .build(); Prompt prompt = new Prompt(\u0026#34;Tell me more about the customer with ID 42\u0026#34;, chatOptions); chatModel.call(prompt); 如果在默认选项和运行时选项中都设置了toolContext选项，则生成的toolContext将是这两个选项的合并，\n直接退货 # 默认情况下，工具调用的结果作为响应发送回模型。然后，模型可以使用结果来继续对话。 在某些情况下，您宁愿将结果直接返回给调用者，而不是将其发送回模型。例如，如果构建依赖于RAG工具的代理，则可能希望将结果直接返回给调用者，而不是将其发送回模型进行不必要的后处理。或者，您可能有某些工具可以结束代理的推理循环。 每个ToolCallback实现都可以定义是将工具调用的结果直接返回给调用者，还是发送回模型。默认情况下，结果被发送回模型。但您可以根据工具更改此行为。 ToolCallingManager负责管理工具执行生命周期，负责处理与工具关联的returnDirect属性。如果属性设置为true，则工具调用的结果将直接返回给调用者。否则，结果将发送回模型。 方法直接返回 # 从具有声明性方法的方法构建工具时，可以通过将@tool注释的returnDirect属性设置为true来标记工具以直接将结果返回给调用方。\nclass CustomerTools { @Tool(description = \u0026#34;Retrieve customer information\u0026#34;, returnDirect = true) Customer getCustomerInfo(Long id) { return customerRepository.findById(id); } } 如果使用编程方法，则可以通过ToolMetadata接口设置returnDirect属性，并将其传递给MethodToolCallback。建筑商。\nToolMetadata toolMetadata = ToolMetadata.builder() .returnDirect(true) .build(); 有关详细信息，请参见作为工具的方法。\n函数直接返回 # 使用编程方法从函数构建工具时，可以通过ToolMetadata接口设置returnDirect属性，并将其传递给FunctionToolCallback。建筑商。\nToolMetadata toolMetadata = ToolMetadata.builder() .returnDirect(true) .build(); 有关详细信息，请参阅“ 作为工具的功能”。\n刀具执行 # 工具执行是使用提供的输入参数调用工具并返回结果的过程。工具执行由ToolCallingManager接口处理，该接口负责管理工具执行生命周期。\npublic interface ToolCallingManager { /** * Resolve the tool definitions from the model\u0026#39;s tool calling options. */ List\u0026lt;ToolDefinition\u0026gt; resolveToolDefinitions(ToolCallingChatOptions chatOptions); /** * Execute the tool calls requested by the model. */ ToolExecutionResult executeToolCalls(Prompt prompt, ChatResponse chatResponse); } 如果您正在使用任何Spring AI Spring Boot Starters，`DefaultToolCallingManager```是ToolCalling Manager接口的自动配置实现。您可以通过提供自己的ToolCallingManager``bean来定制工具执行行为。\n@Bean ToolCallingManager toolCallingManager() { return ToolCallingManager.builder().build(); } 默认情况下，Spring AI从每个ChatModel实现中为您透明地管理工具执行生命周期。但您有可能选择退出此行为，并自行控制工具的执行。本节描述这两个场景。\n框架控制的工具执行 # 当使用默认行为时，Spring AI将自动拦截来自模型的任何工具调用请求，调用该工具并将结果返回给模型。所有这些都是通过使用ToolCallingManager的每个ChatModel实现为您透明地完成的。 确定工具调用是否符合执行条件的逻辑由ToolExecutionEligibilityPredicate接口处理。默认情况下，通过检查ToolCallingChatOptions的internalToolExecutionEnabled属性是否设置为true（默认值），以及ChatResponse是否包含任何工具调用，来确定工具执行合格性。\npublic class DefaultToolExecutionEligibilityPredicate implements ToolExecutionEligibilityPredicate { @Override public boolean test(ChatOptions promptOptions, ChatResponse chatResponse) { return ToolCallingChatOptions.isInternalToolExecutionEnabled(promptOptions) \u0026amp;\u0026amp; chatResponse != null \u0026amp;\u0026amp; chatResponse.hasToolCalls(); } } 在创建ChatModelbean时，可以提供ToolExecutionEligibilityPredicate的自定义实现。\n用户控制的工具执行 # 在某些情况下，您宁愿自己控制工具执行生命周期。可以通过将ToolCallingChatOptions的internalToolExecutionEnabled属性设置为false来完成此操作。 使用此选项调用ChatModel时，工具执行将委托给调用者，使您能够完全控制工具执行生命周期。您的责任是检查ChatResponse中的工具调用，并使用ToolCallingManager执行它们。 下面的示例演示了用户控制的工具执行方法的最小实现：\nChatModel chatModel = ... ToolCallingManager toolCallingManager = ToolCallingManager.builder().build(); ChatOptions chatOptions = ToolCallingChatOptions.builder() .toolCallbacks(new CustomerTools()) .internalToolExecutionEnabled(false) .build(); Prompt prompt = new Prompt(\u0026#34;Tell me more about the customer with ID 42\u0026#34;, chatOptions); ChatResponse chatResponse = chatModel.call(prompt); while (chatResponse.hasToolCalls()) { ToolExecutionResult toolExecutionResult = toolCallingManager.executeToolCalls(prompt, chatResponse); prompt = new Prompt(toolExecutionResult.conversationHistory(), chatOptions); chatResponse = chatModel.call(prompt); } System.out.println(chatResponse.getResult().getOutput().getText()); 下面的示例显示了用户控制的工具执行方法与ChatMemory API的使用相结合的最小实现：\nToolCallingManager toolCallingManager = DefaultToolCallingManager.builder().build(); ChatMemory chatMemory = MessageWindowChatMemory.builder().build(); String conversationId = UUID.randomUUID().toString(); ChatOptions chatOptions = ToolCallingChatOptions.builder() .toolCallbacks(ToolCallbacks.from(new MathTools())) .internalToolExecutionEnabled(false) .build(); Prompt prompt = new Prompt( List.of(new SystemMessage(\u0026#34;You are a helpful assistant.\u0026#34;), new UserMessage(\u0026#34;What is 6 * 8?\u0026#34;)), chatOptions); chatMemory.add(conversationId, prompt.getInstructions()); Prompt promptWithMemory = new Prompt(chatMemory.get(conversationId), chatOptions); ChatResponse chatResponse = chatModel.call(promptWithMemory); chatMemory.add(conversationId, chatResponse.getResult().getOutput()); while (chatResponse.hasToolCalls()) { ToolExecutionResult toolExecutionResult = toolCallingManager.executeToolCalls(promptWithMemory, chatResponse); chatMemory.add(conversationId, toolExecutionResult.conversationHistory() .get(toolExecutionResult.conversationHistory().size() - 1)); promptWithMemory = new Prompt(chatMemory.get(conversationId), chatOptions); chatResponse = chatModel.call(promptWithMemory); chatMemory.add(conversationId, chatResponse.getResult().getOutput()); } UserMessage newUserMessage = new UserMessage(\u0026#34;What did I ask you earlier?\u0026#34;); chatMemory.add(conversationId, newUserMessage); ChatResponse newResponse = chatModel.call(new Prompt(chatMemory.get(conversationId))); 异常处理 # 当工具调用失败时，异常将作为ToolExecutionException传播，可以捕获该异常来处理错误。ToolExecutionExceptionProcessor可用于处理ToolExectionException，具有两个结果：要么生成要发送回AI模型的错误消息，要么引发要由调用方处理的异常。\n@FunctionalInterface public interface ToolExecutionExceptionProcessor { /** * Convert an exception thrown by a tool to a String that can be sent back to the AI * model or throw an exception to be handled by the caller. */ String process(ToolExecutionException exception); } 如果您正在使用任何Spring AI Spring Boot Starters，DefaultToolExecutionExceptionProcessor是ToolExectionException处理器接口的自动配置实现。默认情况下，错误消息被发送回模型。`Default`ToolExecutionExceptionProcessor构造函数允许将alwaysThrow属性设置为true或false。如果为true，则将引发异常，而不是将错误消息发送回模型。\n@Bean ToolExecutionExceptionProcessor toolExecutionExceptionProcessor() { return new DefaultToolExecutionExceptionProcessor(true); } ToolExecutionExceptionProcessor由默认ToolCallingManager（DefaultToolCalling Manager）在内部使用，以处理工具执行期间的异常。有关工具执行生命周期的更多详细信息，请参见工具执行。\n工具分辨率 # 将工具传递到模型的主要方法是在调用ChatClient或ChatModel时提供ToolCallback， 然而，Spring AI还支持使用ToolCallbackResolver接口在运行时动态解析工具。\npublic interface ToolCallbackResolver { /** * Resolve the {@link ToolCallback} for the given tool name. */ @Nullable ToolCallback resolve(String toolName); } 使用此方法时：\n在客户端，将工具名称提供给ChatClient或ChatModel，而不是ToolCallback。 在服务器端，ToolCallbackResolver实现负责将工具名称解析为相应的ToolCallbak实例。 默认情况下，Spring AI依赖于DelegatingToolCallbackResolver``，后者将工具解析委托给ToolCallbakResolver实例列表： SpringBeanToolCallbackResolver从Function、Supplier、Consumer或BiFunction类型的SpringBean解析工具。有关详细信息，请参阅动态规范：@Bean。 StaticToolCallbackResolver从ToolCallbak实例的静态列表中解析工具。当使用SpringBoot自动配置时，该解析器自动配置为应用程序上下文中定义的所有ToolCallback类型的bean。 如果依赖于SpringBoot自动配置，则可以通过提供自定义ToolCallbackResolverbean来定制解析逻辑。 @Bean ToolCallbackResolver toolCallbackResolver(List\u0026lt;FunctionCallback\u0026gt; toolCallbacks) { StaticToolCallbackResolver staticToolCallbackResolver = new StaticToolCallbackResolver(toolCallbacks); return new DelegatingToolCallbackResolver(List.of(staticToolCallbackResolver)); } ToolCallbackResolver由ToolCallingManager在内部使用，以在运行时动态解析工具，支持 框架控制的工具执行和用户控制的工具运行。\n可观察性 # 工具调用的检测正在进行中。现在，您可以使用日志功能来跟踪工具调用操作。\n日志记录 # 刀具调用功能的所有主要操作都在DEBUG级别记录。您可以通过将org.springframework.ai包的日志级别设置为DEBUG来启用日志记录。\n"},{"id":63,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/%E6%8B%A5%E6%8A%B1%E7%9A%84%E8%84%B8/","title":"拥抱式面部聊天","section":"聊天模型API","content":" 拥抱式面部聊天 # Hugging Face Text Generation Inference（TGI）是一种专门的部署解决方案，用于在云中服务大型语言模型（LLM），使它们可以通过API访问。TGI通过连续批处理、令牌流和高效内存管理等功能为文本生成任务提供优化的性能。\n前提条件 # 您将需要在拥抱面上创建推断端点，并创建API令牌来访问该端点。\nexport SPRING_AI_HUGGINGFACE_CHAT_API_KEY=\u0026lt;INSERT KEY HERE\u0026gt; export SPRING_AI_HUGGINGFACE_CHAT_URL=\u0026lt;INSERT INFERENCE ENDPOINT URL HERE\u0026gt; 添加存储库和BOM表 # Spring AI工件发布在Maven Central和Spring Snapshot 存储库中。 为了帮助进行依赖关系管理，Spring AI提供了BOM（物料清单），以确保在整个项目中使用一致版本的Spring AI。请参阅依赖项管理部分，将Spring AI BOM添加到构建系统中。\n自动配置 # Spring AI为Hugging Face Chat Client提供Spring Boot自动配置。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-huggingface\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-huggingface\u0026#39; } 聊天室属性 # 前缀spring.ai.huggingface是属性前缀，允许您为Hugging Face配置聊天模型实现。\n样本控制器（自动配置） # 创建一个新的SpringBoot项目，并将Spring-ai-starter模型拥抱面添加到pom（或gradle）依赖项中。 在src/main/resources目录下添加application.properties文件，以启用和配置Hugging Face聊天模型：\nspring.ai.huggingface.chat.api-key=YOUR_API_KEY spring.ai.huggingface.chat.url=YOUR_INFERENCE_ENDPOINT_URL 这将创建一个可以注入到类中的HuggingfaceChatModel实现。\n@RestController public class ChatController { private final HuggingfaceChatModel chatModel; @Autowired public ChatController(HuggingfaceChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, this.chatModel.call(message)); } } 手动配置 # HuggingfaceChatModel实现ChatModel.接口并使用[low level api]连接到Hugging Face推理端点。 将spring-ai-huggingface依赖项添加到项目的Maven pom.xml文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-huggingface\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-huggingface\u0026#39; } 接下来，创建HuggingfaceChatModel并将其用于文本生成：\nHuggingfaceChatModel chatModel = new HuggingfaceChatModel(apiKey, url); ChatResponse response = this.chatModel.call( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); System.out.println(response.getGeneration().getResult().getOutput().getContent()); "},{"id":64,"href":"/docs/%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/%E9%B8%A2%E5%B1%9E/","title":"鸢属","section":"向量数据库","content":" 鸢属 # Milvus是一个开源向量数据库，在数据科学和机器学习领域受到了极大的关注。它的突出特点之一在于它对向量索引和查询的强大支持。Milvus采用最先进的尖端算法来加速搜索过程，使其在检索相似向量时特别高效，即使在处理大量数据集时也是如此。\n前提条件 # 正在运行的Milvus实例。以下选项可用： Milvus独立：Docker、Operator、Helm、DEB/RPM、Docker Compose。 Milvus集群：操作员，头盔。 如果需要，EmbeddingModel的API密钥，用于生成MilvusVectorStore存储的嵌入。 依赖关系 # 然后将Milvus VectorStore引导启动程序依赖项添加到项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-milvus\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-vector-store-milvus\u0026#39; } 向量存储实现可以为您初始化必要的模式，但您必须通过在适当的构造函数中指定initializeSchema布尔值或通过设置…​在application.properties文件中初始化schema=true。 Vector Store还需要一个EmbeddingModel实例来计算文档的嵌入。 要连接和配置MilvusVectorStore，您需要提供实例的访问详细信息。 现在，您可以在应用程序中自动连接Milvus Vector Store并使用它\n@Autowired VectorStore vectorStore; // ... List \u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); // Add the documents to Milvus Vector Store vectorStore.add(documents); // Retrieve documents similar to a query List\u0026lt;Document\u0026gt; results = this.vectorStore.similaritySearch(SearchRequest.builder().query(\u0026#34;Spring\u0026#34;).topK(5).build()); 手动配置 # 您可以手动配置MilvusVectorStore，而不是使用SpringBoot自动配置。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-milvus-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 要在应用程序中配置MilvusVectorStore，可以使用以下设置：\n@Bean public VectorStore vectorStore(MilvusServiceClient milvusClient, EmbeddingModel embeddingModel) { return MilvusVectorStore.builder(milvusClient, embeddingModel) .collectionName(\u0026#34;test_vector_store\u0026#34;) .databaseName(\u0026#34;default\u0026#34;) .indexType(IndexType.IVF_FLAT) .metricType(MetricType.COSINE) .batchingStrategy(new TokenCountBatchingStrategy()) .initializeSchema(true) .build(); } @Bean public MilvusServiceClient milvusClient() { return new MilvusServiceClient(ConnectParam.newBuilder() .withAuthorization(\u0026#34;minioadmin\u0026#34;, \u0026#34;minioadmin\u0026#34;) .withUri(milvusContainer.getEndpoint()) .build()); } 元数据筛选 # 您可以在Milvus存储中利用通用的、可移植的元数据过滤器。 例如，可以使用文本表达式语言：\nvectorStore.similaritySearch( SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(\u0026#34;author in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; article_type == \u0026#39;blog\u0026#39;\u0026#34;).build()); 或以编程方式使用筛选器。表达式DSL:\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(b.and( b.in(\u0026#34;author\u0026#34;,\u0026#34;john\u0026#34;, \u0026#34;jill\u0026#34;), b.eq(\u0026#34;article_type\u0026#34;, \u0026#34;blog\u0026#34;)).build()).build()); 使用MilvusSearchRequest # MilvusSearchRequest扩展了SearchRequest，允许您使用特定于Milvus的搜索参数，如本机表达式和搜索参数JSON。\nMilvusSearchRequest request = MilvusSearchRequest.milvusBuilder() .query(\u0026#34;sample query\u0026#34;) .topK(5) .similarityThreshold(0.7) .nativeExpression(\u0026#34;metadata[\\\u0026#34;age\\\u0026#34;] \u0026gt; 30\u0026#34;) // Overrides filterExpression if both are set .filterExpression(\u0026#34;age \u0026lt;= 30\u0026#34;) // Ignored if nativeExpression is set .searchParamsJson(\u0026#34;{\\\u0026#34;nprobe\\\u0026#34;:128}\u0026#34;) .build(); List results = vectorStore.similaritySearch(request); 这使得在使用Milvus特定的搜索功能时具有更大的灵活性。\nMilvusSearchRequest中nativeExpression和searchParamsJson的重要性 # 这两个参数提高了Milvus搜索精度，并确保最佳查询性能： nativeExpression：使用Milvus的本机过滤表达式启用其他过滤功能。 示例：\nMilvusSearchRequest request = MilvusSearchRequest.milvusBuilder() .query(\u0026#34;sample query\u0026#34;) .topK(5) .nativeExpression(\u0026#34;metadata[\u0026#39;category\u0026#39;] == \u0026#39;science\u0026#39;\u0026#34;) .build(); searchParamsJson：对于在使用IVF_FLAT（Milvus的默认索引）时调整搜索行为至关重要。 默认情况下，IVF_FLAT要求设置nprobe以获得准确的结果。如果未指定，nprobe默认为1，这可能会导致较差的召回，甚至零搜索结果。 示例：\nMilvusSearchRequest request = MilvusSearchRequest.milvusBuilder() .query(\u0026#34;sample query\u0026#34;) .topK(5) .searchParamsJson(\u0026#34;{\\\u0026#34;nprobe\\\u0026#34;:128}\u0026#34;) .build(); 使用nativeExpression可以确保高级过滤，而searchParamsJson可以防止由低默认nprobe值导致的无效搜索。\nMilvus VectorStore属性 # 您可以在SpringBoot配置中使用以下属性来定制Milvus向量存储。\n启动Milvus商店 # 从src/test/resources/文件夹运行中：\ndocker-compose up 清洁环境：\ndocker-compose down; rm -Rf ./volumes 然后连接到上的向量存储 http://localhost:19530或用于管理 http://localhost:9001（用户：minioadmin，传递：minioadmin）\n故障排除 # 如果Docker抱怨资源，则执行：\ndocker system prune --all --force --volumes 访问本机客户端 # Milvus Vector Store实现通过getNativeClient（）方法提供对底层本机Milvus客户端（MilvusServiceClient）的访问：\nMilvusVectorStore vectorStore = context.getBean(MilvusVectorStore.class); Optional\u0026lt;MilvusServiceClient\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { MilvusServiceClient client = nativeClient.get(); // Use the native client for Milvus-specific operations } 本机客户端为您提供对Milvus特定功能和操作的访问，这些功能和操作可能不会通过VectorStore界面公开。\n"},{"id":65,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/mistral%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/","title":"Mistral AI聊天","section":"聊天模型API","content":" Mistral AI聊天 # Spring AI支持Mistral AI的各种AI语言模型。您可以与Mistral人工智能语言模型交互，并基于Mistral模型创建多语言对话助手。\n前提条件 # 您需要使用Mistral AI创建API来访问Mistral人工智能语言模型。\nexport SPRING_AI_MISTRALAI_API_KEY=\u0026lt;INSERT KEY HERE\u0026gt; 添加存储库和BOM表 # Spring AI工件发布在Maven Central和Spring Snapshot 存储库中。 为了帮助进行依赖关系管理，Spring AI提供了BOM（物料清单），以确保在整个项目中使用一致版本的Spring AI。请参阅依赖项管理部分，将Spring AI BOM添加到构建系统中。\n自动配置 # Spring AI为Mistral AI聊天客户端提供Spring Boot自动配置。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-mistral-ai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-mistral-ai\u0026#39; } 聊天室属性 # 重试属性 # 前缀spring.ai.retry用作属性前缀，允许您配置Mistral ai聊天模型的重试机制。\n连接属性 # 前缀spring.ai.mistalai用作允许连接到OpenAI的属性前缀。\n配置属性 # 前缀spring.ai.mistalai.chat是属性前缀，允许您为Mistral ai配置聊天模型实现。\n运行时选项 # MistralAiChatOptions.java提供模型配置，例如要使用的模型、温度、频率惩罚等。 启动时，可以使用MistralAiChatModel（api，options）构造函数或spring.ai.mistralai.chat.options.*属性配置默认选项。 在运行时，可以通过向Prompt调用添加新的特定于请求的选项来覆盖默认选项。\nChatResponse response = chatModel.call( new Prompt( \u0026#34;Generate the names of 5 famous pirates.\u0026#34;, MistralAiChatOptions.builder() .model(MistralAiApi.ChatModel.LARGE.getValue()) .temperature(0.5) .build() )); 函数调用 # 您可以使用MistralAiChatModel注册自定义Java函数，并让Mistral AI模型智能地选择输出包含参数的JSON对象，以调用一个或多个已注册的函数。\n多模态 # 多模态是指模型同时理解和处理来自各种来源的信息的能力，包括文本、图像、音频和其他数据格式。\n愿景 # 提供视觉多模态支持的Mistral AI模型包括最新的pixtral large。 Mistral AI[用户 消息API]( https://docs.mistral.ai/api/#tag/chat/operation/chat_completion_v1_chat_completions_post)可以将base64编码图像或图像URL的列表与 消息合并。 下面是从MistralAiChatModelIT.java中摘录的代码示例，说明了用户文本与图像的融合。\nvar imageResource = new ClassPathResource(\u0026#34;/multimodal.test.png\u0026#34;); var userMessage = new UserMessage(\u0026#34;Explain what do you see on this picture?\u0026#34;, new Media(MimeTypeUtils.IMAGE_PNG, this.imageResource)); ChatResponse response = chatModel.call(new Prompt(this.userMessage, ChatOptions.builder().model(MistralAiApi.ChatModel.PIXTRAL_LARGE.getValue()).build())); 或等效的图像URL：\nvar userMessage = new UserMessage(\u0026#34;Explain what do you see on this picture?\u0026#34;, new Media(MimeTypeUtils.IMAGE_PNG, URI.create(\u0026#34;https://docs.spring.io/spring-ai/reference/_images/multimodal.test.png\u0026#34;))); ChatResponse response = chatModel.call(new Prompt(this.userMessage, ChatOptions.builder().model(MistralAiApi.ChatModel.PIXTRAL_LARGE.getValue()).build())); 该示例显示了一个将multimal.test.png图像作为输入的模型： 以及文本消息“解释您在这张图片上看到了什么？”，并生成如下响应：\nOpenAI API兼容性 # Mistral与OpenAI API兼容，您可以使用Spring AI OpenAI客户端与Mistral对话。 有关在Spring AI OpenAI上使用Mistral的示例，请检查MistralWithOpenAiChatModelIT.java测试。\n样本控制器（自动配置） # 创建一个新的SpringBoot项目，并将Springaistarter模型Mistal-ai添加到pom（或gradle）依赖项中。 在src/main/resources目录下添加application.properties文件，以启用和配置Mistral AI聊天模型：\nspring.ai.mistralai.api-key=YOUR_API_KEY spring.ai.mistralai.chat.options.model=mistral-small spring.ai.mistralai.chat.options.temperature=0.7 这将创建一个MistralAiChatModel实现，您可以将其注入到类中。\n@RestController public class ChatController { private final MistralAiChatModel chatModel; @Autowired public ChatController(MistralAiChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map\u0026lt;String,String\u0026gt; generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, this.chatModel.call(message)); } @GetMapping(\u0026#34;/ai/generateStream\u0026#34;) public Flux\u0026lt;ChatResponse\u0026gt; generateStream(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { var prompt = new Prompt(new UserMessage(message)); return this.chatModel.stream(prompt); } } 手动配置 # MistralAiChatModel实现ChatModels和StreamingChatModel.并使用 低级MistralAiApi客户端连接到Mistral AI服务。 将spring-ai-mistal-ai依赖项添加到项目的Maven pom.xml文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-mistral-ai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-mistral-ai\u0026#39; } 接下来，创建MistralAiChatModel并将其用于文本生成：\nvar mistralAiApi = new MistralAiApi(System.getenv(\u0026#34;MISTRAL_AI_API_KEY\u0026#34;)); var chatModel = new MistralAiChatModel(this.mistralAiApi, MistralAiChatOptions.builder() .model(MistralAiApi.ChatModel.LARGE.getValue()) .temperature(0.4) .maxTokens(200) .build()); ChatResponse response = this.chatModel.call( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); // Or with streaming responses Flux\u0026lt;ChatResponse\u0026gt; response = this.chatModel.stream( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); MistralAiChatOptions提供聊天请求的配置信息。\n低级MistralAiApi客户端 # MistralAiApi为Mistral AI API提供了轻量级Java客户端。 下面是一个简单的片段，演示如何以编程方式使用API：\nMistralAiApi mistralAiApi = new MistralAiApi(System.getenv(\u0026#34;MISTRAL_AI_API_KEY\u0026#34;)); ChatCompletionMessage chatCompletionMessage = new ChatCompletionMessage(\u0026#34;Hello world\u0026#34;, Role.USER); // Sync request ResponseEntity\u0026lt;ChatCompletion\u0026gt; response = this.mistralAiApi.chatCompletionEntity( new ChatCompletionRequest(List.of(this.chatCompletionMessage), MistralAiApi.ChatModel.LARGE.getValue(), 0.8, false)); // Streaming request Flux\u0026lt;ChatCompletionChunk\u0026gt; streamResponse = this.mistralAiApi.chatCompletionStream( new ChatCompletionRequest(List.of(this.chatCompletionMessage), MistralAiApi.ChatModel.LARGE.getValue(), 0.8, true)); 有关更多信息，请参阅MistralAiApi.java的JavaDoc。\nMistralAiApi样本 # MistralAiApiIT.java测试提供了一些如何使用轻量级库的一般示例。 PaymentStatusFunctionCallingIT.java测试展示了如何使用低级API调用工具函数。 "},{"id":66,"href":"/docs/%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/mongodb%E5%9C%B0%E5%9B%BE%E9%9B%86/","title":"MongoDB地图集","section":"向量数据库","content":" MongoDB地图集 # 本节将介绍如何将MongoDB Atlas设置为与Spring AI一起使用的向量存储。\n什么是MongoDB Atlas？ # MongoDB Atlas是MongoDB的完全托管云数据库，在AWS、Azure和GCP中提供。 MongoDB Atlas Vector Search允许您将嵌入存储在MongoDB文档中，创建向量搜索索引，并使用近似最近邻算法（分层可导航小世界）执行KNN搜索。\n前提条件 # 运行MongoDB 6.0.11、7.0.2或更高版本的Atlas集群。要开始使用MongoDB Atlas，您可以按照此处的说明进行操作。确保您的IP地址包含在Atlas项目的访问列表中。 已启用矢量搜索的正在运行的MongoDB Atlas实例 配置了向量搜索索引的集合 具有id（字符串）、内容（字符串），元数据（文档）和嵌入（向量）字段的集合架构 索引和集合操作的适当访问权限 自动配置 # Spring AI为MongoDB Atlas Vector Store提供Spring Boot自动配置。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-mongodb-atlas\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件：\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-vector-store-mongodb-atlas\u0026#39; } 向量存储实现可以为您初始化所需的模式，但您必须通过在application.properties文件中设置spring.ai.vectorstore.mongodb.initialize-schema=true来选择。 请查看矢量存储的 配置参数列表，以了解默认值和配置选项。 此外，您还需要一个已配置的EmbeddingModelbean。有关更多信息，请参阅EmbeddingModel部分。 现在，您可以在应用程序中自动将MongoDBAtlasVectorStore连接为向量存储：\n@Autowired VectorStore vectorStore; // ... List\u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); // Add the documents to MongoDB Atlas vectorStore.add(documents); // Retrieve documents similar to a query List\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch(SearchRequest.builder().query(\u0026#34;Spring\u0026#34;).topK(5).build()); 配置属性 # 要连接到MongoDB Atlas并使用MongoDBAtlasVectorStore，您需要提供实例的访问详细信息。\nspring: data: mongodb: uri: \u0026lt;mongodb atlas connection string\u0026gt; database: \u0026lt;database name\u0026gt; ai: vectorstore: mongodb: initialize-schema: true collection-name: custom_vector_store index-name: custom_vector_index path-name: custom_embedding metadata-fields-to-filter: author,year 以spring.ai.vectorstore.mongodb.*开头的属性用于配置MongoDBAtlasVectorStore:\n手动配置 # 您可以手动配置MongoDBAtlas矢量存储，而不是使用SpringBoot自动配置。为此，您需要将spring ai mongodb atlas store添加到项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-mongodb-atlas-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件：\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-mongodb-atlas-store\u0026#39; } 创建MongoTemplate bean：\n@Bean public MongoTemplate mongoTemplate() { return new MongoTemplate(MongoClients.create(\u0026#34;\u0026lt;mongodb atlas connection string\u0026gt;\u0026#34;), \u0026#34;\u0026lt;database name\u0026gt;\u0026#34;); } 然后使用构建器模式创建MongoDBAtlasVectorStore bean：\n@Bean public VectorStore vectorStore(MongoTemplate mongoTemplate, EmbeddingModel embeddingModel) { return MongoDBAtlasVectorStore.builder(mongoTemplate, embeddingModel) .collectionName(\u0026#34;custom_vector_store\u0026#34;) // Optional: defaults to \u0026#34;vector_store\u0026#34; .vectorIndexName(\u0026#34;custom_vector_index\u0026#34;) // Optional: defaults to \u0026#34;vector_index\u0026#34; .pathName(\u0026#34;custom_embedding\u0026#34;) // Optional: defaults to \u0026#34;embedding\u0026#34; .numCandidates(500) // Optional: defaults to 200 .metadataFieldsToFilter(List.of(\u0026#34;author\u0026#34;, \u0026#34;year\u0026#34;)) // Optional: defaults to empty list .initializeSchema(true) // Optional: defaults to false .batchingStrategy(new TokenCountBatchingStrategy()) // Optional: defaults to TokenCountBatchingStrategy .build(); } // This can be any EmbeddingModel implementation @Bean public EmbeddingModel embeddingModel() { return new OpenAiEmbeddingModel(new OpenAiApi(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;))); } 元数据筛选 # 您也可以将通用的、可移植的元数据过滤器与MongoDB Atlas结合使用。 例如，可以使用文本表达式语言：\nvectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(5) .similarityThreshold(0.7) .filterExpression(\u0026#34;author in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; article_type == \u0026#39;blog\u0026#39;\u0026#34;).build()); 或以编程方式使用筛选器。表达式DSL:\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(5) .similarityThreshold(0.7) .filterExpression(b.and( b.in(\u0026#34;author\u0026#34;, \u0026#34;john\u0026#34;, \u0026#34;jill\u0026#34;), b.eq(\u0026#34;article_type\u0026#34;, \u0026#34;blog\u0026#34;)).build()).build()); 例如，此可移植筛选器表达式：\nauthor in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; article_type == \u0026#39;blog\u0026#39; 转换为专有的MongoDB Atlas过滤器格式：\n{ \u0026#34;$and\u0026#34;: [ { \u0026#34;$or\u0026#34;: [ { \u0026#34;metadata.author\u0026#34;: \u0026#34;john\u0026#34; }, { \u0026#34;metadata.author\u0026#34;: \u0026#34;jill\u0026#34; } ] }, { \u0026#34;metadata.article_type\u0026#34;: \u0026#34;blog\u0026#34; } ] } 教程和代码示例 # 要开始使用Spring AI和MongoDB：\n请参阅Spring AI集成入门指南。 有关使用Spring AI和MongoDB演示检索增强生成（RAG）的综合代码示例，请参阅本详细教程。 访问本机客户端 # MongoDB Atlas Vector Store实现通过getNativeClient（）方法提供对底层本机MongoDB客户端（MongoClient）的访问：\nMongoDBAtlasVectorStore vectorStore = context.getBean(MongoDBAtlasVectorStore.class); Optional\u0026lt;MongoClient\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { MongoClient client = nativeClient.get(); // Use the native client for MongoDB-specific operations } 本机客户端为您提供对MongoDB特定功能和操作的访问，这些功能和操作可能不会通过VectorStore界面公开。\n"},{"id":67,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/%E5%8D%83%E5%B8%86/","title":"千凡嵌件","section":"嵌入模型API","content":" 千凡嵌件 # 此功能已移至Spring AI社区存储库。 有关最新版本，请访问github.com/spring-ai-community/chanfan。\n"},{"id":68,"href":"/docs/%E6%A8%A1%E5%9E%8B%E4%B8%8A%E4%B8%8B%E6%96%87%E5%8D%8F%E8%AE%AEmcp/","title":"模型上下文协议（MCP）","section":"Docs","content":" 模型上下文协议（MCP） # 模型上下文协议（MCP）是一种标准化协议，它使人工智能模型能够以结构化的方式与外部工具和资源交互。 MCP Java SDK提供了模型上下文协议的Java实现，通过同步和异步通信模式实现与AI模型和工具的标准化交互。 Spring AI MCP通过Spring Boot集成扩展了MCP Java SDK，提供了 客户端和 服务器启动器。\nMCP Java SDK架构 # Java MCP实现遵循三层体系结构： 有关使用低级MCP客户端/服务器API的详细实现指南，请参阅 MCP Java SDK文档。\nSpring AI MCP集成 # Spring AI通过以下Spring Boot启动器提供MCP集成：\n客户端启动程序 # spring ai starter mcp客户端-提供STDIO和基于HTTP的SSE支持的核心启动器 spring-ai-starter mcp客户端webflux-基于webflux的SSE传输实现 服务器启动程序 # spring ai starter mcp服务器-具有STDIO传输支持的核心服务器 spring ai starter mcp服务器webmvc-基于spring MVC的SSE传输实现 spring-ai-starter mcp服务器webflux-基于webflux的SSE传输实现 其他资源 # MCP客户端启动程序文档 MCP服务器启动程序文档 MCP实用程序文档 模型上下文协议规范 "},{"id":69,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/%E6%9C%80%E5%B0%8F%E6%9C%80%E5%A4%A7%E5%80%BC/","title":"MiniMax聊天","section":"聊天模型API","content":" MiniMax聊天 # Spring AI支持MiniMax的各种人工智能语言模型。您可以与MiniMax语言模型交互，并基于MiniMax模型创建多语言对话助手。\n前提条件 # 您需要使用MiniMax创建API来访问MiniMax语言模型。 在 MiniMax注册页面创建帐户，并在[ API密钥页面]( https://www.minimaxi.com/user-center/basic-information/interface-key)上生成令牌。\nexport SPRING_AI_MINIMAX_API_KEY=\u0026lt;INSERT KEY HERE\u0026gt; 添加存储库和BOM表 # Spring AI工件发布在Maven Central和Spring Snapshot 存储库中。 为了帮助进行依赖关系管理，Spring AI提供了BOM（物料清单），以确保在整个项目中使用一致版本的Spring AI。请参阅依赖项管理部分，将Spring AI BOM添加到构建系统中。\n自动配置 # Spring AI为MiniMax聊天客户端提供Spring Boot自动配置。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-minimax\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-minimax\u0026#39; } 聊天室属性 # 重试属性 # 前缀spring.ai.retry用作属性前缀，允许您为MiniMax聊天模型配置重试机制。\n连接属性 # 前缀spring.ai.minimax用作允许连接到minimax的属性前缀。\n配置属性 # 前缀spring.ai.miminax.chat是属性前缀，允许您为minimax配置聊天模型实现。\n运行时选项 # MiniMaxChatOptions.java提供模型配置，例如要使用的模型、温度、频率惩罚等。 启动时，可以使用MiniMaxChatModel（api，options）构造函数或spring.ai.minimax.chat.options.*属性配置默认选项。 在运行时，可以通过向Prompt调用添加新的特定于请求的选项来覆盖默认选项。\nChatResponse response = chatModel.call( new Prompt( \u0026#34;Generate the names of 5 famous pirates.\u0026#34;, MiniMaxChatOptions.builder() .model(MiniMaxApi.ChatModel.ABAB_6_5_S_Chat.getValue()) .temperature(0.5) .build() )); 样本控制器 # 创建一个新的SpringBoot项目，并将SpringAIstarter模型minimax添加到pom（或gradle）依赖项中。 在src/main/resources目录下添加application.properties文件，以启用和配置MiniMax聊天模型：\nspring.ai.minimax.api-key=YOUR_API_KEY spring.ai.minimax.chat.options.model=abab6.5g-chat spring.ai.minimax.chat.options.temperature=0.7 这将创建一个可以注入到类中的MiniMaxChatModel实现。\n@RestController public class ChatController { private final MiniMaxChatModel chatModel; @Autowired public ChatController(MiniMaxChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, this.chatModel.call(message)); } @GetMapping(\u0026#34;/ai/generateStream\u0026#34;) public Flux\u0026lt;ChatResponse\u0026gt; generateStream(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { var prompt = new Prompt(new UserMessage(message)); return this.chatModel.stream(prompt); } } 手动配置 # MiniMaxChatModel实现ChatModels和StreamingChatModel.并使用 低级MiniMaxApi客户端连接到MiniMax服务。 将spring ai minimax依赖项添加到项目的Maven pom.xml文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-minimax\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-minimax\u0026#39; } 接下来，创建MiniMaxChatModel并将其用于文本生成：\nvar miniMaxApi = new MiniMaxApi(System.getenv(\u0026#34;MINIMAX_API_KEY\u0026#34;)); var chatModel = new MiniMaxChatModel(this.miniMaxApi, MiniMaxChatOptions.builder() .model(MiniMaxApi.ChatModel.ABAB_6_5_S_Chat.getValue()) .temperature(0.4) .maxTokens(200) .build()); ChatResponse response = this.chatModel.call( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); // Or with streaming responses Flux\u0026lt;ChatResponse\u0026gt; streamResponse = this.chatModel.stream( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); MiniMaxChatOptions提供聊天请求的配置信息。\n低级MiniMaxApi客户端 # MiniMaxApi提供了用于MiniMax API的轻量级Java客户端。 下面是如何以编程方式使用api的简单片段：\nMiniMaxApi miniMaxApi = new MiniMaxApi(System.getenv(\u0026#34;MINIMAX_API_KEY\u0026#34;)); ChatCompletionMessage chatCompletionMessage = new ChatCompletionMessage(\u0026#34;Hello world\u0026#34;, Role.USER); // Sync request ResponseEntity\u0026lt;ChatCompletion\u0026gt; response = this.miniMaxApi.chatCompletionEntity( new ChatCompletionRequest(List.of(this.chatCompletionMessage), MiniMaxApi.ChatModel.ABAB_6_5_S_Chat.getValue(), 0.7f, false)); // Streaming request Flux\u0026lt;ChatCompletionChunk\u0026gt; streamResponse = this.miniMaxApi.chatCompletionStream( new ChatCompletionRequest(List.of(this.chatCompletionMessage), MiniMaxApi.ChatModel.ABAB_6_5_S_Chat.getValue(), 0.7f, true)); 有关更多信息，请参阅MiniMaxApi.java的JavaDoc。\nWebSearch聊天 # MiniMax模型支持web搜索功能。网络搜索功能允许您在网络上搜索信息，并在聊天响应中返回结果。 关于web搜索，请遵循MiniMax ChatCompletion以了解更多信息。 下面是如何使用web搜索的简单片段：\nUserMessage userMessage = new UserMessage( \u0026#34;How many gold medals has the United States won in total at the 2024 Olympics?\u0026#34;); List\u0026lt;Message\u0026gt; messages = new ArrayList\u0026lt;\u0026gt;(List.of(this.userMessage)); List\u0026lt;MiniMaxApi.FunctionTool\u0026gt; functionTool = List.of(MiniMaxApi.FunctionTool.webSearchFunctionTool()); MiniMaxChatOptions options = MiniMaxChatOptions.builder() .model(MiniMaxApi.ChatModel.ABAB_6_5_S_Chat.value) .tools(this.functionTool) .build(); // Sync request ChatResponse response = chatModel.call(new Prompt(this.messages, this.options)); // Streaming request Flux\u0026lt;ChatResponse\u0026gt; streamResponse = chatModel.stream(new Prompt(this.messages, this.options)); MiniMaxApi示例 # java测试提供了一些如何使用轻量级库的一般示例。 MiniMaxApiToolFunctionCallIT.java测试展示了如何使用低级API调用工具函数。\u0026gt; "},{"id":70,"href":"/docs/%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/neo4j%E5%85%AC%E5%8F%B8/","title":"Neo4j公司","section":"向量数据库","content":" Neo4j公司 # 本节将指导您设置Neo4jVectorStore以存储文档嵌入并执行相似性搜索。 Neo4j是一个开源的NoSQL图形数据库。 Neo4j的向量搜索允许用户从大型数据集中查询向量嵌入。\n前提条件 # 正在运行的Neo4j（5.15+）实例。以下选项可用： Docker图像 Neo4j桌面 Neo4j光环 Neo4j服务器实例 如果需要，EmbeddingModel的API键，用于生成Neo4jVectorStore存储的嵌入。 自动配置 # Spring AI为Neo4j Vector Store提供Spring Boot自动配置。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-neo4j\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-vector-store-neo4j\u0026#39; } 请查看矢量存储的 配置参数列表，以了解默认值和配置选项。 向量存储实现可以为您初始化必要的模式，但您必须通过在适当的构造函数中指定initializeSchema布尔值或通过设置…​在application.properties文件中初始化schema=true。 此外，您还需要一个已配置的EmbeddingModelbean。有关更多信息，请参阅EmbeddingModel部分。 现在，您可以在应用程序中自动将Neo4jVectorStore连接为向量存储。\n@Autowired VectorStore vectorStore; // ... List\u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); // Add the documents to Neo4j vectorStore.add(documents); // Retrieve documents similar to a query List\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch(SearchRequest.builder().query(\u0026#34;Spring\u0026#34;).topK(5).build()); 配置属性 # 要连接到Neo4j并使用Neo4jVectorStore，您需要提供实例的访问详细信息。\nspring: neo4j: uri: \u0026lt;neo4j instance URI\u0026gt; authentication: username: \u0026lt;neo4j username\u0026gt; password: \u0026lt;neo4j password\u0026gt; ai: vectorstore: neo4j: initialize-schema: true database-name: neo4j index-name: custom-index embedding-dimension: 1536 distance-type: cosine 以Spring.neo4j开头的Spring Boot属性。*用于配置Neo4j客户端： 以spring.ai.vectorstore.neo4j开头的属性。*用于配置Neo4jVectorStore： 以下距离功能可用：\n余弦-默认值，适用于大多数用例。测量向量之间的余弦相似性。 欧几里德-向量之间的欧几里得距离。较低的值表示较高的相似性。 手动配置 # 您可以手动配置Neo4j向量存储，而不是使用SpringBoot自动配置。为此，您需要将spring-ai-neo4j-store添加到项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-neo4j-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-neo4j-store\u0026#39; } 创建Neo4j驱动程序bean。\n@Bean public Driver driver() { return GraphDatabase.driver(\u0026#34;neo4j://\u0026lt;host\u0026gt;:\u0026lt;bolt-port\u0026gt;\u0026#34;, AuthTokens.basic(\u0026#34;\u0026lt;username\u0026gt;\u0026#34;, \u0026#34;\u0026lt;password\u0026gt;\u0026#34;)); } 然后使用构建器模式创建Neo4jVectorStore bean：\n@Bean public VectorStore vectorStore(Driver driver, EmbeddingModel embeddingModel) { return Neo4jVectorStore.builder(driver, embeddingModel) .databaseName(\u0026#34;neo4j\u0026#34;) // Optional: defaults to \u0026#34;neo4j\u0026#34; .distanceType(Neo4jDistanceType.COSINE) // Optional: defaults to COSINE .embeddingDimension(1536) // Optional: defaults to 1536 .label(\u0026#34;Document\u0026#34;) // Optional: defaults to \u0026#34;Document\u0026#34; .embeddingProperty(\u0026#34;embedding\u0026#34;) // Optional: defaults to \u0026#34;embedding\u0026#34; .indexName(\u0026#34;custom-index\u0026#34;) // Optional: defaults to \u0026#34;spring-ai-document-index\u0026#34; .initializeSchema(true) // Optional: defaults to false .batchingStrategy(new TokenCountBatchingStrategy()) // Optional: defaults to TokenCountBatchingStrategy .build(); } // This can be any EmbeddingModel implementation @Bean public EmbeddingModel embeddingModel() { return new OpenAiEmbeddingModel(new OpenAiApi(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;))); } 元数据筛选 # 您也可以在Neo4j存储中利用通用的、可移植的元数据过滤器。 例如，可以使用文本表达式语言：\nvectorStore.similaritySearch( SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(\u0026#34;author in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; \u0026#39;article_type\u0026#39; == \u0026#39;blog\u0026#39;\u0026#34;).build()); 或以编程方式使用筛选器。表达式DSL:\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(b.and( b.in(\u0026#34;author\u0026#34;, \u0026#34;john\u0026#34;, \u0026#34;jill\u0026#34;), b.eq(\u0026#34;article_type\u0026#34;, \u0026#34;blog\u0026#34;)).build()).build()); 例如，此可移植筛选器表达式：\nauthor in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; \u0026#39;article_type\u0026#39; == \u0026#39;blog\u0026#39; 转换为专有的Neo4j过滤器格式：\nnode.`metadata.author` IN [\u0026#34;john\u0026#34;,\u0026#34;jill\u0026#34;] AND node.`metadata.\u0026#39;article_type\u0026#39;` = \u0026#34;blog\u0026#34; 访问本机客户端 # Neo4j向量存储实现通过getNativeClient（）方法提供对底层本机Neo4j客户端（驱动程序）的访问：\nNeo4jVectorStore vectorStore = context.getBean(Neo4jVectorStore.class); Optional\u0026lt;Driver\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { Driver driver = nativeClient.get(); // Use the native client for Neo4j-specific operations } 本机客户端允许您访问可能无法通过VectorStore界面公开的Neo4j特定功能和操作。\n"},{"id":71,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B/%E6%99%BA%E6%B5%A6ai/","title":"ZhiPuAI嵌入","section":"嵌入模型API","content":" ZhiPuAI嵌入 # Spring AI支持智普AI的文本嵌入模型。\n前提条件 # 您需要使用ZhiPuAI创建API来访问ZhiPu AI语言模型。 在智浦AI注册页面创建账户，并在API Keys页面上生成代币。\nexport SPRING_AI_ZHIPU_AI_API_KEY=\u0026lt;INSERT KEY HERE\u0026gt; 添加存储库和BOM表 # Spring AI工件发布在Maven Central和Spring Snapshot 存储库中。 为了帮助进行依赖关系管理，Spring AI提供了BOM（物料清单），以确保在整个项目中使用一致版本的Spring AI。请参阅依赖项管理部分，将Spring AI BOM添加到构建系统中。\n自动配置 # Spring AI为Azure ZhiPuAI嵌入模型提供Spring Boot自动配置。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-zhipuai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-zhipuai\u0026#39; } 嵌入属性 # 重试属性 # 前缀spring.ai.retry用作属性前缀，允许您配置ZhiPuAI嵌入模型的重试机制。\n连接属性 # 前缀spring.ai.zhipuai用作允许连接到Zipuai的属性前缀。\n配置属性 # 前缀spring.ai.zhipuai.embedding是为zhipuai配置EmbeddingModel实现的属性前缀。\n运行时选项 # ZhiPuAiEmbeddingOptions.java提供了ZhiPuAI配置，如要使用的模型等。 也可以使用spring.ai.zhipuai.embedding.options属性配置默认选项。 在启动时，使用ZhiPuAiEmbeddingModel构造函数设置用于所有嵌入请求的默认选项。 例如，要覆盖特定请求的默认模型名称：\nEmbeddingResponse embeddingResponse = embeddingModel.call( new EmbeddingRequest(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;), ZhiPuAiEmbeddingOptions.builder() .model(\u0026#34;Different-Embedding-Model-Deployment-Name\u0026#34;) .build())); 样本控制器 # 这将创建一个可以注入到类中的EmbeddingModel实现。\nspring.ai.zhipuai.api-key=YOUR_API_KEY spring.ai.zhipuai.embedding.options.model=embedding-2 @RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(\u0026#34;/ai/embedding\u0026#34;) public Map embed(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { EmbeddingResponse embeddingResponse = this.embeddingModel.embedForResponse(List.of(message)); return Map.of(\u0026#34;embedding\u0026#34;, embeddingResponse); } } 手动配置 # 如果不使用Spring Boot，则可以手动配置ZhiPuAI嵌入模型。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-zhipuai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-zhipuai\u0026#39; } 接下来，创建一个ZhiPuAiEmbeddingModel实例，并使用它计算两个输入文本之间的相似性：\nvar zhiPuAiApi = new ZhiPuAiApi(System.getenv(\u0026#34;ZHIPU_AI_API_KEY\u0026#34;)); var embeddingModel = new ZhiPuAiEmbeddingModel(api, MetadataMode.EMBED, ZhiPuAiEmbeddingOptions.builder() .model(\u0026#34;embedding-3\u0026#34;) .dimensions(1536) .build()); EmbeddingResponse embeddingResponse = this.embeddingModel .embedForResponse(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;)); ZhiPuAiEmbeddingOptions为嵌入请求提供配置信息。\n"},{"id":72,"href":"/docs/%E5%A4%9A%E6%A8%A1%E6%80%81/","title":"多模态API","section":"Docs","content":" 多模态API # 人类在多个数据输入模式中同时处理知识。 与这些原则相反，机器学习通常侧重于为处理单个模态而定制的专门模型。 然而，一股新的多模态大型语言模型浪潮开始出现。\nSpring AI多模态 # 多模态是指模型同时理解和处理来自各种来源的信息的能力，包括文本、图像、音频和其他数据格式。 Spring AI消息API提供了所有必要的抽象来支持多模态LLM。 UserMessage的内容字段主要用于文本输入，而可选媒体字段允许添加不同模式的一个或多个附加内容，如图像、音频和视频。 例如，我们可以将下面的图片（multimal.test.png）作为输入，并要求LLM解释它看到的内容。 对于大多数多模式LLM，Spring AI代码将如下所示：\nvar imageResource = new ClassPathResource(\u0026#34;/multimodal.test.png\u0026#34;); var userMessage = new UserMessage( \u0026#34;Explain what do you see in this picture?\u0026#34;, // content new Media(MimeTypeUtils.IMAGE_PNG, this.imageResource)); // media ChatResponse response = chatModel.call(new Prompt(this.userMessage)); 或使用流畅的ChatClient API：\nString response = ChatClient.create(chatModel).prompt() .user(u -\u0026gt; u.text(\u0026#34;Explain what do you see on this picture?\u0026#34;) .media(MimeTypeUtils.IMAGE_PNG, new ClassPathResource(\u0026#34;/multimodal.test.png\u0026#34;))) .call() .content(); 并产生如下响应： Spring AI为以下聊天模型提供多模式支持：\n人类克劳德3 AWS基岩匡威 Azure Open AI（例如GPT-4o模型） Mistral AI（例如，Mistral Pixtral模型） Ollama（例如LLaVA、BakLLaVA和Llama3.2型号） OpenAI（例如GPT-4和GPT-4o模型） Vertex AI双子座（例如，双子座-1.5-pro-001、双子座1.5-flash-001型号） "},{"id":73,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/moonshot%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/","title":"Moonshot AI聊天","section":"聊天模型API","content":" Moonshot AI聊天 # 此功能已移至Spring AI社区存储库。 有关最新版本，请访问 github.com/spring-ai-community/moonshot。\n"},{"id":74,"href":"/docs/%E5%8F%AF%E8%A7%82%E5%AF%9F%E6%80%A7/","title":"可观察性","section":"Docs","content":" 可观察性 # Spring AI以Spring生态系统中的可观察性功能为基础，提供对人工智能相关操作的见解。\n聊天客户端 # 当调用ChatClient call（）或stream（）操作时，将记录spring.ai.chat.client观察。\n提示内容 # ChatClient提示内容通常很大，并且可能包含敏感信息。 Spring AI支持在所有跟踪后端将提示内容导出为span属性/事件。\n输入数据（不推荐） # ChatClient输入数据通常很大，并且可能包含敏感信息。 Spring AI支持在所有跟踪后端将输入数据导出为span属性/事件。\n聊天客户顾问 # 执行advisor时会记录spring.ai.advisor观察结果。\n聊天模型 # 调用ChatModel调用或流方法时记录gen_ai.client.operation观察。\n聊天提示和完成数据 # 聊天提示和完成数据通常很大，并且可能包含敏感信息。 如果使用OpenTelemetry跟踪后端，Spring AI支持将聊天提示和完成数据导出为span事件， 此外，Spring AI支持记录聊天提示和完成数据，这对于故障排除场景非常有用。\n嵌入模型 # gen_ai.client.operation观察值在嵌入模型方法调用时记录。\n图像模型（Image Model） # gen_ai.client.operation观察值记录在图像模型方法调用上。\n图像提示数据 # 图像提示数据通常很大，并且可能包含敏感信息。 如果使用OpenTelemetry跟踪后端，Spring AI支持将图像提示数据导出为span事件，\n向量存储区 # Spring AI中的所有向量存储实现都被检测为通过测微计提供度量和分布式跟踪数据。 当与vector Store交互时，会记录db.vector.client.operation观察值。\n响应数据 # 向量搜索响应数据通常很大，并且可能包含敏感信息。 如果使用OpenTelemetry跟踪后端，Spring AI支持将向量搜索响应数据导出为span事件，\n"},{"id":75,"href":"/docs/%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/%E6%89%93%E5%BC%80%E6%90%9C%E7%B4%A2/","title":"打开搜索","section":"向量数据库","content":" 打开搜索 # 本节将指导您设置OpenSearchVectorStore以存储文档嵌入并执行相似性搜索。 OpenSearch是一个开源搜索和分析引擎，最初源于Elasticsearch，在Apache License 2.0下发布。它通过简化人工智能生成资产的集成和管理来增强人工智能应用程序开发。OpenSearch支持向量、词法和混合搜索功能，利用高级向量数据库功能来促进低延迟查询和相似性搜索，如向量数据库页面上所述。 OpenSearch k-NN功能允许用户从大型数据集中查询向量嵌入。嵌入是数据对象（如文本、图像、音频或文档）的数字表示。嵌入可以存储在索引中，并使用各种相似性函数进行查询。\n前提条件 # 正在运行的OpenSearch实例。以下选项可用： 自我管理的OpenSearch Amazon OpenSearch服务 如果需要，EmbeddingModel的API键，用于生成OpenSearchVectorStore存储的嵌入。 自动配置 # Spring AI为OpenSearch Vector Store提供Spring Boot自动配置。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-opensearch\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件：\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-vector-store-opensearch\u0026#39; } 对于Amazon OpenSearch服务，请改用这些依赖项：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-opensearch\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或用于分级：\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-vector-store-opensearch\u0026#39; } 请查看矢量存储的 配置参数列表，以了解默认值和配置选项。 此外，您还需要一个已配置的EmbeddingModelbean。有关更多信息，请参阅EmbeddingModel部分。 现在，您可以将OpenSearchVectorStore自动关联为应用程序中的向量存储：\n@Autowired VectorStore vectorStore; // ... List\u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); // Add the documents to OpenSearch vectorStore.add(documents); // Retrieve documents similar to a query List\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch(SearchRequest.builder().query(\u0026#34;Spring\u0026#34;).topK(5).build()); 配置属性 # 要连接到OpenSearch并使用OpenSearchVectorStore，您需要提供实例的访问详细信息。\nspring: ai: vectorstore: opensearch: uris: \u0026lt;opensearch instance URIs\u0026gt; username: \u0026lt;opensearch username\u0026gt; password: \u0026lt;opensearch password\u0026gt; index-name: spring-ai-document-index initialize-schema: true similarity-function: cosinesimil aws: # Only for Amazon OpenSearch Service host: \u0026lt;aws opensearch host\u0026gt; service-name: \u0026lt;aws service name\u0026gt; access-key: \u0026lt;aws access key\u0026gt; secret-key: \u0026lt;aws secret key\u0026gt; region: \u0026lt;aws region\u0026gt; 以spring.ai.vectorstore.opensearch.*开头的属性用于配置OpenSearchVectorStore: 以下相似性函数可用：\ncosinesimil-默认，适用于大多数用例。测量向量之间的余弦相似性。 l1-矢量之间的曼哈顿距离。 l2-向量之间的欧氏距离。 linf—向量之间的切比雪夫距离。 手动配置 # 您可以手动配置OpenSearch向量存储，而不是使用SpringBoot自动配置。为此，您需要将spring ai opensearch存储添加到项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-opensearch-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件：\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-opensearch-store\u0026#39; } 创建OpenSearch客户端bean：\n@Bean public OpenSearchClient openSearchClient() { RestClient restClient = RestClient.builder( HttpHost.create(\u0026#34;http://localhost:9200\u0026#34;)) .build(); return new OpenSearchClient(new RestClientTransport( restClient, new JacksonJsonpMapper())); } 然后使用构建器模式创建OpenSearchVectorStore bean：\n@Bean public VectorStore vectorStore(OpenSearchClient openSearchClient, EmbeddingModel embeddingModel) { return OpenSearchVectorStore.builder(openSearchClient, embeddingModel) .index(\u0026#34;custom-index\u0026#34;) // Optional: defaults to \u0026#34;spring-ai-document-index\u0026#34; .similarityFunction(\u0026#34;l2\u0026#34;) // Optional: defaults to \u0026#34;cosinesimil\u0026#34; .initializeSchema(true) // Optional: defaults to false .batchingStrategy(new TokenCountBatchingStrategy()) // Optional: defaults to TokenCountBatchingStrategy .build(); } // This can be any EmbeddingModel implementation @Bean public EmbeddingModel embeddingModel() { return new OpenAiEmbeddingModel(new OpenAiApi(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;))); } 元数据筛选 # 您也可以在OpenSearch中利用通用的、可移植的元数据过滤器。 例如，可以使用文本表达式语言：\nvectorStore.similaritySearch( SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(\u0026#34;author in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; \u0026#39;article_type\u0026#39; == \u0026#39;blog\u0026#39;\u0026#34;).build()); 或以编程方式使用筛选器。表达式DSL:\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(b.and( b.in(\u0026#34;author\u0026#34;, \u0026#34;john\u0026#34;, \u0026#34;jill\u0026#34;), b.eq(\u0026#34;article_type\u0026#34;, \u0026#34;blog\u0026#34;)).build()).build()); 例如，此可移植筛选器表达式：\nauthor in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; \u0026#39;article_type\u0026#39; == \u0026#39;blog\u0026#39; 转换为专有的OpenSearch筛选器格式：\n(metadata.author:john OR jill) AND metadata.article_type:blog 访问本机客户端 # OpenSearch Vector Store实现通过getNativeClient（）方法提供对底层本机OpenSearch客户端（OpenSearchClient）的访问：\nOpenSearchVectorStore vectorStore = context.getBean(OpenSearchVectorStore.class); Optional\u0026lt;OpenSearchClient\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { OpenSearchClient client = nativeClient.get(); // Use the native client for OpenSearch-specific operations } 本机客户端允许您访问OpenSearch特定的功能和操作，这些功能和操作可能不会通过VectorStore界面公开。\n"},{"id":76,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/%E8%8B%B1%E4%BC%9F%E8%BE%BEnvidia/","title":"NVIDIA聊天室","section":"聊天模型API","content":" NVIDIA聊天室 # NVIDIA LLM API是一个代理AI推理引擎，提供来自不同提供商的各种模型。 Spring AI通过重用现有的OpenAI客户端与NVIDIA LLM API集成。 检查NvidiaWithOpenAiChatModelIT.java测试\n先决条件 # 创建具有足够信用的NVIDIA帐户。 选择要使用的LLM模型。例如，下面的屏幕截图中的meta/llama-3.1-70b-指令。 从所选模型的页面中，可以获得用于访问该模型的api键。 自动配置 # Spring AI为OpenAI聊天客户端提供Spring Boot自动配置。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-openai\u0026#39; } 聊天室属性 # 重试属性 # 前缀spring.ai.retry用作属性前缀，允许您为OpenAI聊天模型配置重试机制。\n连接属性 # 前缀spring.ai.openai用作允许连接到openai的属性前缀。\n配置属性 # 前缀spring.ai.openai.chat是属性前缀，允许您配置openai的聊天模型实现。\n运行时选项 # OpenAiChatOptions.java提供模型配置，例如要使用的模型、温度、频率惩罚等。 启动时，可以使用OpenAiChatModel（api，options）构造函数或spring.ai.openai.chat.options.*属性配置默认选项。 在运行时，可以通过向Prompt调用添加新的特定于请求的选项来覆盖默认选项。\nChatResponse response = chatModel.call( new Prompt( \u0026#34;Generate the names of 5 famous pirates.\u0026#34;, OpenAiChatOptions.builder() .model(\u0026#34;mixtral-8x7b-32768\u0026#34;) .temperature(0.4) .build() )); 函数调用 # NVIDIA LLM API在选择支持它的模型时支持工具/函数调用。 您可以使用ChatModel注册自定义Java函数，并让提供的模型智能地选择输出包含参数的JSON对象，以调用一个或多个已注册的函数。\n工具示例 # 下面是如何使用NVIDIA LLM API函数调用Spring AI的简单示例：\nspring.ai.openai.api-key=${NVIDIA_API_KEY} spring.ai.openai.base-url=https://integrate.api.nvidia.com spring.ai.openai.chat.options.model=meta/llama-3.1-70b-instruct spring.ai.openai.chat.options.max-tokens=2048 @SpringBootApplication public class NvidiaLlmApplication { public static void main(String[] args) { SpringApplication.run(NvidiaLlmApplication.class, args); } @Bean CommandLineRunner runner(ChatClient.Builder chatClientBuilder) { return args -\u0026gt; { var chatClient = chatClientBuilder.build(); var response = chatClient.prompt() .user(\u0026#34;What is the weather in Amsterdam and Paris?\u0026#34;) .functions(\u0026#34;weatherFunction\u0026#34;) // reference by bean name. .call() .content(); System.out.println(response); }; } @Bean @Description(\u0026#34;Get the weather in location\u0026#34;) public Function\u0026lt;WeatherRequest, WeatherResponse\u0026gt; weatherFunction() { return new MockWeatherService(); } public static class MockWeatherService implements Function\u0026lt;WeatherRequest, WeatherResponse\u0026gt; { public record WeatherRequest(String location, String unit) {} public record WeatherResponse(double temp, String unit) {} @Override public WeatherResponse apply(WeatherRequest request) { double temperature = request.location().contains(\u0026#34;Amsterdam\u0026#34;) ? 20 : 25; return new WeatherResponse(temperature, request.unit); } } } 在本例中，当模型需要天气信息时，它将自动调用weatherFunctionbean，然后bean可以获取实时天气数据。 阅读有关OpenAI 函数调用的更多信息。\n样本控制器 # 创建一个新的SpringBoot项目，并将Springaistarter模型openai添加到pom（或gradle）依赖项中。 在src/main/resources目录下添加application.properties文件，以启用和配置OpenAi聊天模型：\nspring.ai.openai.api-key=${NVIDIA_API_KEY} spring.ai.openai.base-url=https://integrate.api.nvidia.com spring.ai.openai.chat.options.model=meta/llama-3.1-70b-instruct # The NVIDIA LLM API doesn\u0026#39;t support embeddings, so we need to disable it. spring.ai.openai.embedding.enabled=false # The NVIDIA LLM API requires this parameter to be set explicitly or server internal error will be thrown. spring.ai.openai.chat.options.max-tokens=2048 下面是一个简单的@Controller类的示例，该类使用聊天模型生成文本。\n@RestController public class ChatController { private final OpenAiChatModel chatModel; @Autowired public ChatController(OpenAiChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, this.chatModel.call(message)); } @GetMapping(\u0026#34;/ai/generateStream\u0026#34;) public Flux\u0026lt;ChatResponse\u0026gt; generateStream(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { Prompt prompt = new Prompt(new UserMessage(message)); return this.chatModel.stream(prompt); } } "},{"id":77,"href":"/docs/%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/%E7%94%B2%E9%AA%A8%E6%96%87%E5%85%AC%E5%8F%B8/","title":"Oracle Database 23ai-AI矢量搜索","section":"向量数据库","content":" Oracle Database 23ai-AI矢量搜索 # Oracle Database 23ai（23.4+）的AI向量搜索功能作为Spring AI VectorStore提供，帮助您存储文档嵌入和执行相似性搜索。当然，所有其他功能也可用。\n自动配置 # 首先，将Oracle Vector Store引导启动程序依赖项添加到项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-oracle\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-vector-store-oracle\u0026#39; } 如果需要此向量存储来初始化模式，则需要在适当的构造函数中为initializeSchema布尔参数传递true，或者通过设置…​在application.properties文件中初始化schema=true。 Vector Store还需要一个EmbeddingModel实例来计算文档的嵌入。 例如，要使用OpenAI EmbeddingModel，请将以下依赖项添加到项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-openai\u0026#39; } 要连接和配置OracleVectorStore，您需要提供数据库的访问详细信息。 现在，您可以在应用程序中自动关联OracleVectorStore并使用它：\n@Autowired VectorStore vectorStore; // ... List\u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); // Add the documents to Oracle Vector Store vectorStore.add(documents); // Retrieve documents similar to a query List\u0026lt;Document\u0026gt; results = this.vectorStore.similaritySearch(SearchRequest.builder().query(\u0026#34;Spring\u0026#34;).topK(5).build()); 配置属性 # 您可以在SpringBoot配置中使用以下属性来定制OracleVectorStore。\n元数据筛选 # 您可以通过OracleVectorStore利用通用的可移植元数据过滤器。 例如，可以使用文本表达式语言：\nvectorStore.similaritySearch( SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(\u0026#34;author in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; article_type == \u0026#39;blog\u0026#39;\u0026#34;).build()); 或以编程方式使用筛选器。表达式DSL:\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(b.and( b.in(\u0026#34;author\u0026#34;,\u0026#34;john\u0026#34;, \u0026#34;jill\u0026#34;), b.eq(\u0026#34;article_type\u0026#34;, \u0026#34;blog\u0026#34;)).build()).build()); 手动配置 # 您可以手动配置OracleVectorStore，而不是使用SpringBoot自动配置。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-jdbc\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.oracle.database.jdbc\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;ojdbc11\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;runtime\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-oracle-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 要在应用程序中配置OracleVectorStore，可以使用以下设置：\n@Bean public VectorStore vectorStore(JdbcTemplate jdbcTemplate, EmbeddingModel embeddingModel) { return OracleVectorStore.builder(jdbcTemplate, embeddingModel) .tableName(\u0026#34;my_vectors\u0026#34;) .indexType(OracleVectorStoreIndexType.IVF) .distanceType(OracleVectorStoreDistanceType.COSINE) .dimensions(1536) .searchAccuracy(95) .initializeSchema(true) .build(); } 在本地运行Oracle Database 23ai # 然后，可以使用以下命令连接到数据库：\n访问本机客户端 # Oracle Vector Store实现通过getNativeClient（）方法提供对底层本机Oracle客户端（OracleConnection）的访问：\nOracleVectorStore vectorStore = context.getBean(OracleVectorStore.class); Optional\u0026lt;OracleConnection\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { OracleConnection connection = nativeClient.get(); // Use the native client for Oracle-specific operations } 本机客户端允许您访问可能无法通过VectorStore界面公开的Oracle特定功能和操作。\n"},{"id":78,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/","title":"评估和测试","section":"Docs","content":" 评估和测试 # 测试人工智能应用程序需要评估生成的内容，以确保人工智能模型没有产生幻觉响应。 评估响应的一种方法是使用人工智能模型本身进行评估。选择用于评估的最佳人工智能模型，该模型可能与用于生成响应的模型不同。 用于评估响应的Spring AI接口是Evaluator，定义为： 评估的输入是EvaluationRequest，定义为\n相关性评估器 # 一个实现是相关性评估器，它使用人工智能模型进行评估。在未来的版本中将提供更多的实现。 相关性评估器使用输入（userText）和人工智能模型的输出（chatResponse）来提问：\nYour task is to evaluate if the response for the query is in line with the context information provided.\\n You have two options to answer. Either YES/ NO.\\n Answer - YES, if the response for the query is in line with context information otherwise NO.\\n Query: \\n {query}\\n Response: \\n {response}\\n Context: \\n {context}\\n Answer: \u0026#34; 下面是一个JUnit测试的示例，该测试对加载到向量存储中的PDF文档执行RAG查询，然后评估响应是否与用户文本相关。\n@Test void testEvaluation() { dataController.delete(); dataController.load(); String userText = \u0026#34;What is the purpose of Carina?\u0026#34;; ChatResponse response = ChatClient.builder(chatModel) .build().prompt() .advisors(new QuestionAnswerAdvisor(vectorStore)) .user(userText) .call() .chatResponse(); String responseContent = response.getResult().getOutput().getContent(); var relevancyEvaluator = new RelevancyEvaluator(ChatClient.builder(chatModel)); EvaluationRequest evaluationRequest = new EvaluationRequest(userText, (List\u0026lt;Content\u0026gt;) response.getMetadata().get(QuestionAnswerAdvisor.RETRIEVED_DOCUMENTS), responseContent); EvaluationResponse evaluationResponse = relevancyEvaluator.evaluate(evaluationRequest); assertTrue(evaluationResponse.isPass(), \u0026#34;Response is not relevant to the question\u0026#34;); } 上面的代码来自这里的示例应用程序。\nFactCheckingEvaluator（事实检查评估器） # FactCheckingEvaluator是EvaluatorInterface的另一个实现，旨在根据提供的上下文评估人工智能生成的响应的事实准确性。该计算器通过验证给定的语句（声明）是否由所提供的上下文（文档）逻辑支持来帮助检测和减少人工智能输出中的幻觉。 将“索赔”和“文件”提交给人工智能模型进行评估。可提供专用于此目的的更小、更有效的AI模型，如Bespoke的Minicheck，与GPT-4等旗舰模型相比，它有助于降低执行这些检查的成本。Minicheck也可以通过Ollama使用。\n使用 # FactCheckingEvaluator构造函数采用ChatClient。生成器作为参数：\npublic FactCheckingEvaluator(ChatClient.Builder chatClientBuilder) { this.chatClientBuilder = chatClientBuilder; } 计算器使用以下提示模板进行事实检查：\nDocument: {document} Claim: {claim} 其中{document}是上下文信息，{claim}是要评估的AI模型的响应。\n示例 # 下面是如何将FactCheckingEvaluator与基于Ollama的ChatModel（特别是定制Minicheck模型）一起使用的示例：\n@Test void testFactChecking() { // Set up the Ollama API OllamaApi ollamaApi = new OllamaApi(\u0026#34;http://localhost:11434\u0026#34;); ChatModel chatModel = new OllamaChatModel(ollamaApi, OllamaOptions.builder().model(BESPOKE_MINICHECK).numPredict(2).temperature(0.0d).build()) // Create the FactCheckingEvaluator var factCheckingEvaluator = new FactCheckingEvaluator(ChatClient.builder(chatModel)); // Example context and claim String context = \u0026#34;The Earth is the third planet from the Sun and the only astronomical object known to harbor life.\u0026#34;; String claim = \u0026#34;The Earth is the fourth planet from the Sun.\u0026#34;; // Create an EvaluationRequest EvaluationRequest evaluationRequest = new EvaluationRequest(context, Collections.emptyList(), claim); // Perform the evaluation EvaluationResponse evaluationResponse = factCheckingEvaluator.evaluate(evaluationRequest); assertFalse(evaluationResponse.isPass(), \u0026#34;The claim should not be supported by the context\u0026#34;); } "},{"id":79,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/%E5%A5%A5%E6%8B%89%E9%A9%ACollama/","title":"Ollama聊天","section":"聊天模型API","content":" Ollama聊天 # 使用Ollama，您可以在本地运行各种大型语言模型（LLM），并从中生成文本。\n前提条件 # 首先需要访问Ollama实例。有几个选项，包括：\n在本地计算机上下载并安装Ollama。 通过Testcontainers配置和运行Ollama。 通过Kubernetes服务绑定绑定到Ollama实例。 您可以从 Ollama模型库中提取要在应用程序中使用的模型： ollama pull \u0026lt;model-name\u0026gt; 您还可以拉动数千个免费 GGUF拥抱面部模型中的任何一个：\nollama pull hf.co/\u0026lt;username\u0026gt;/\u0026lt;model-repository\u0026gt; 或者，可以启用该选项以自动下载任何所需的模型：自动拉动模型。\n自动配置 # Spring AI为Ollama聊天集成提供Spring Boot自动配置。\n基本属性 # 前缀spring.ai.ollama是配置与ollama的连接的属性前缀。 下面是用于初始化Ollama集成和自动提取模型的属性。\n聊天室属性 # 前缀spring.ai.ollama.chat.options``是配置ollama聊天模型的属性前缀。 以下是Ollama聊天模型的高级请求参数： 其余选项属性基于 Ollama有效参数和值以及 Ollama类型。默认值基于 Ollama类型默认值。\n运行时选项 # OllamaOptions.java类提供模型配置，例如要使用的模型、温度等。 启动时，可以使用OllamaChatModel（api，options）构造函数或spring.ai.ollama.chat.options.*属性配置默认选项。 在运行时，可以通过向Prompt调用添加新的特定于请求的选项来覆盖默认选项。\nChatResponse response = chatModel.call( new Prompt( \u0026#34;Generate the names of 5 famous pirates.\u0026#34;, OllamaOptions.builder() .model(OllamaModel.LLAMA3_1) .temperature(0.4) .build() )); 自动牵引型号 # 当模型在Ollama实例中不可用时，Spring AI Ollama可以自动拉取模型。 拉动模型有三种策略：\nalways（在PullModelStrategy.always中定义）：始终拉动模型，即使它已经可用。用于确保使用模型的最新版本。 when_missing（在PullModelStrategy.when_missing中定义）：仅在模型不可用时拉取模型。这可能导致使用较旧版本的模型。 never（在PullModelStrategy.never中定义）：从不自动拉取模型。 通过配置属性和默认选项定义的所有模型都可以在启动时自动提取。 spring: ai: ollama: init: pull-model-strategy: always timeout: 60s max-retries: 1 您可以在启动时初始化其他模型，这对于在运行时动态使用的模型非常有用：\nspring: ai: ollama: init: pull-model-strategy: always chat: additional-models: - llama3.2 - qwen2.5 如果要将拉取策略仅应用于特定类型的模型，则可以从初始化任务中排除聊天模型：\nspring: ai: ollama: init: pull-model-strategy: always chat: include: false 该配置将把拉策略应用于除聊天模型外的所有模型。\n函数调用 # 您可以使用OllamaChatModel注册自定义Java函数，并让Ollama模型智能地选择输出包含参数的JSON对象，以调用一个或多个已注册的函数。\n多模态 # 多模态是指模型同时理解和处理来自各种来源的信息的能力，包括文本、图像、音频和其他数据格式。 Ollama中提供的具有多模态支持的一些型号是LLaVA和BakLLaVA（请参阅 完整列表）。 Ollama Message API提供了一个“images”参数，用于将base64编码图像的列表与消息合并。 Spring AI的 消息接口通过引入 媒体类型促进了多模态AI模型。 下面是一个简单的代码示例，摘自OllamaChatModelMultimodalIT.java，演示了用户文本与图像的融合。\nvar imageResource = new ClassPathResource(\u0026#34;/multimodal.test.png\u0026#34;); var userMessage = new UserMessage(\u0026#34;Explain what do you see on this picture?\u0026#34;, new Media(MimeTypeUtils.IMAGE_PNG, this.imageResource)); ChatResponse response = chatModel.call(new Prompt(this.userMessage, OllamaOptions.builder().model(OllamaModel.LLAVA)).build()); 该示例显示了一个将multimal.test.png图像作为输入的模型： 以及文本消息“解释您在这张图片上看到了什么？”，并生成如下响应：\n结构化输出 # Ollama提供定制的 结构化输出API，确保模型生成严格符合所提供的JSON架构的响应。\n配置 # Spring AI允许您使用OllamaOptions构建器以编程方式配置响应格式。\n使用聊天选项生成器 # 您可以使用OllamaOptions构建器以编程方式设置响应格式，如下所示：\nString jsonSchema = \u0026#34;\u0026#34;\u0026#34; { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;steps\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, \u0026#34;items\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;explanation\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;output\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } }, \u0026#34;required\u0026#34;: [\u0026#34;explanation\u0026#34;, \u0026#34;output\u0026#34;], \u0026#34;additionalProperties\u0026#34;: false } }, \u0026#34;final_answer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } }, \u0026#34;required\u0026#34;: [\u0026#34;steps\u0026#34;, \u0026#34;final_answer\u0026#34;], \u0026#34;additionalProperties\u0026#34;: false } \u0026#34;\u0026#34;\u0026#34;; Prompt prompt = new Prompt(\u0026#34;how can I solve 8x + 7 = -23\u0026#34;, OllamaOptions.builder() .model(OllamaModel.LLAMA3_2.getName()) .format(new ObjectMapper().readValue(jsonSchema, Map.class)) .build()); ChatResponse response = this.ollamaChatModel.call(this.prompt); 与BeanOutputConverter实用程序集成 # 您可以利用现有的BeanOutputConverter实用程序从域对象自动生成JSON架构，然后将结构化响应转换为特定于域的实例：\nrecord MathReasoning( @JsonProperty(required = true, value = \u0026#34;steps\u0026#34;) Steps steps, @JsonProperty(required = true, value = \u0026#34;final_answer\u0026#34;) String finalAnswer) { record Steps( @JsonProperty(required = true, value = \u0026#34;items\u0026#34;) Items[] items) { record Items( @JsonProperty(required = true, value = \u0026#34;explanation\u0026#34;) String explanation, @JsonProperty(required = true, value = \u0026#34;output\u0026#34;) String output) { } } } var outputConverter = new BeanOutputConverter\u0026lt;\u0026gt;(MathReasoning.class); Prompt prompt = new Prompt(\u0026#34;how can I solve 8x + 7 = -23\u0026#34;, OllamaOptions.builder() .model(OllamaModel.LLAMA3_2.getName()) .format(outputConverter.getJsonSchemaMap()) .build()); ChatResponse response = this.ollamaChatModel.call(this.prompt); String content = this.response.getResult().getOutput().getText(); MathReasoning mathReasoning = this.outputConverter.convert(this.content); OpenAI API兼容性 # Ollama与OpenAI API兼容，您可以使用Spring AI OpenAI客户端与Ollama对话并使用工具。 有关在Spring AI OpenAI上使用Ollama的示例，请检查OllamaWithOpenAiChatModelIT.java测试。\nHuggingFace模型 # Ollama可以立即访问所有GGUF拥抱面部聊天模型。\nspring.ai.ollama.chat.options.model=hf.co/bartowski/gemma-2-2b-it-GGUF spring.ai.ollama.init.pull-model-strategy=always spring.ai.ollama.chat.options.model：指定要使用的拥抱面GGUF模型。 spring.ai.ollama.init.pull-model-strategy=always:（可选）在启动时启用自动模型提取。 样本控制器 # 创建一个新的SpringBoot项目，并将Spring-ai-starter模型ollama添加到pom（或gradle）依赖项中。 在src/main/resources目录下添加application.yaml文件，以启用和配置Ollama聊天模型：\nspring: ai: ollama: base-url: http://localhost:11434 chat: options: model: mistral temperature: 0.7 这将创建一个OllamaChatModel实现，您可以将其注入到类中。\n@RestController public class ChatController { private final OllamaChatModel chatModel; @Autowired public ChatController(OllamaChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map\u0026lt;String,String\u0026gt; generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, this.chatModel.call(message)); } @GetMapping(\u0026#34;/ai/generateStream\u0026#34;) public Flux\u0026lt;ChatResponse\u0026gt; generateStream(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { Prompt prompt = new Prompt(new UserMessage(message)); return this.chatModel.stream(prompt); } } 手动配置 # 如果不想使用SpringBoot自动配置，可以在应用程序中手动配置OllamaChatModel``。 要使用它，请将spring ai ollama依赖项添加到项目的Maven pom.xml或Gradle build.Gradle构建文件中： 接下来，创建OllamaChatModel实例，并使用它发送文本生成请求：\nvar ollamaApi = OllamaApi.builder().build(); var chatModel = OllamaChatModel.builder() .ollamaApi(ollamaApi) .defaultOptions( OllamaOptions.builder() .model(OllamaModel.MISTRAL) .temperature(0.9) .build()) .build(); ChatResponse response = this.chatModel.call( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); // Or with streaming responses Flux\u0026lt;ChatResponse\u0026gt; response = this.chatModel.stream( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); OllamaOptions为所有聊天请求提供配置信息。\n低级OllamaApi客户端 # OllamaApi为 Ollama聊天完成API Ollama Chat Completion API提供了一个轻量级Java客户端。 下面的类图说明了OllamaApi聊天接口和构建块： 下面是一个简单的片段，演示如何以编程方式使用API：\nOllamaApi ollamaApi = new OllamaApi(\u0026#34;YOUR_HOST:YOUR_PORT\u0026#34;); // Sync request var request = ChatRequest.builder(\u0026#34;orca-mini\u0026#34;) .stream(false) // not streaming .messages(List.of( Message.builder(Role.SYSTEM) .content(\u0026#34;You are a geography teacher. You are talking to a student.\u0026#34;) .build(), Message.builder(Role.USER) .content(\u0026#34;What is the capital of Bulgaria and what is the size? \u0026#34; + \u0026#34;What is the national anthem?\u0026#34;) .build())) .options(OllamaOptions.builder().temperature(0.9).build()) .build(); ChatResponse response = this.ollamaApi.chat(this.request); // Streaming request var request2 = ChatRequest.builder(\u0026#34;orca-mini\u0026#34;) .ttream(true) // streaming .messages(List.of(Message.builder(Role.USER) .content(\u0026#34;What is the capital of Bulgaria and what is the size? \u0026#34; + \u0026#34;What is the national anthem?\u0026#34;) .build())) .options(OllamaOptions.builder().temperature(0.9).build().toMap()) .build(); Flux\u0026lt;ChatResponse\u0026gt; streamingResponse = this.ollamaApi.streamingChat(this.request2); "},{"id":80,"href":"/docs/%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/pg%E8%BD%BD%E4%BD%93/","title":"PG载体","section":"向量数据库","content":" PG载体 # 本节将指导您设置PGvector VectorStore以存储文档嵌入并执行相似性搜索。 PGvector是PostgreSQL的开源扩展，支持在机器学习生成的嵌入上存储和搜索。它提供了不同的功能，使用户可以识别精确的和近似的最近邻居。它旨在与其他PostgreSQL功能无缝协作，包括索引和查询。\n前提条件 # 首先，您需要访问启用了vector、hstore和uuid ossp扩展的PostgreSQL实例。 启动时，PgVectorStore将尝试安装所需的数据库扩展，并使用索引（如果不存在）创建所需的vector_store表。 （可选）您可以手动执行此操作，如下所示： 接下来，如果需要，提供EmbeddingModel的API键，以生成PgVectorStore存储的嵌入。\n自动配置 # 然后将PgVectorStore引导启动程序依赖项添加到项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-pgvector\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-vector-store-pgvector\u0026#39; } 向量存储实现可以为您初始化所需的模式，但您必须通过在适当的构造函数中指定initializeSchema布尔值或通过设置…​在application.properties文件中初始化schema=true。 Vector Store还需要一个EmbeddingModel实例来计算文档的嵌入。 例如，要使用OpenAI EmbeddingModel，请将以下依赖项添加到项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-openai\u0026#39; } 要连接和配置PgVectorStore，您需要提供实例的访问详细信息。 现在，您可以在应用程序中自动关联VectorStore并使用它\n@Autowired VectorStore vectorStore; // ... List\u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); // Add the documents to PGVector vectorStore.add(documents); // Retrieve documents similar to a query List\u0026lt;Document\u0026gt; results = this.vectorStore.similaritySearch(SearchRequest.builder().query(\u0026#34;Spring\u0026#34;).topK(5).build()); 配置属性 # 您可以在SpringBoot配置中使用以下属性来定制PGVector向量存储。\n元数据筛选 # 您可以使用PgVector存储来利用通用的、可移植的元数据过滤器。 例如，可以使用文本表达式语言：\nvectorStore.similaritySearch( SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(\u0026#34;author in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; article_type == \u0026#39;blog\u0026#39;\u0026#34;).build()); 或以编程方式使用筛选器。表达式DSL:\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(b.and( b.in(\u0026#34;author\u0026#34;,\u0026#34;john\u0026#34;, \u0026#34;jill\u0026#34;), b.eq(\u0026#34;article_type\u0026#34;, \u0026#34;blog\u0026#34;)).build()).build()); 手动配置 # 您可以手动配置PgVectorStore，而不是使用SpringBoot自动配置。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-jdbc\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.postgresql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;postgresql\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;runtime\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-pgvector-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 要在应用程序中配置PgVector，可以使用以下设置：\n@Bean public VectorStore vectorStore(JdbcTemplate jdbcTemplate, EmbeddingModel embeddingModel) { return PgVectorStore.builder(jdbcTemplate, embeddingModel) .dimensions(1536) // Optional: defaults to model dimensions or 1536 .distanceType(COSINE_DISTANCE) // Optional: defaults to COSINE_DISTANCE .indexType(HNSW) // Optional: defaults to HNSW .initializeSchema(true) // Optional: defaults to false .schemaName(\u0026#34;public\u0026#34;) // Optional: defaults to \u0026#34;public\u0026#34; .vectorTableName(\u0026#34;vector_store\u0026#34;) // Optional: defaults to \u0026#34;vector_store\u0026#34; .maxDocumentBatchSize(10000) // Optional: defaults to 10000 .build(); } 在本地运行Postgres和PGVector DB # 您可以这样连接到此服务器：\n访问本机客户端 # PGVector Store实现通过getNativeClient（）方法提供对底层本机JDBC客户端（JdbcTemplate）的访问：\nPgVectorStore vectorStore = context.getBean(PgVectorStore.class); Optional\u0026lt;JdbcTemplate\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { JdbcTemplate jdbc = nativeClient.get(); // Use the native client for PostgreSQL-specific operations } 本机客户端为您提供对PostgreSQL特定功能和操作的访问，这些功能和操作可能不会通过VectorStore界面公开。\n"},{"id":81,"href":"/docs/%E5%87%BA%E8%B5%84%E6%8C%87%E5%8D%97/","title":"出资指南","section":"Docs","content":" 出资指南 # 代码格式和Javadoc # 在提交请购单之前，请运行以下命令以确保正确的格式和Javadoc处理\n./mvnw spring-javaformat:apply javadoc:javadoc -Pjavadoc -Pjavadoc是一个概要文件，它支持Javadoc处理，以避免开发时的长构建时间。\n贡献新的人工智能模型实现 # 本节概述了贡献新AI模型实现的步骤。 要创建新模型，请遵循以下步骤： 通过遵循这些准则，我们可以大大扩展框架的受支持模型范围\n"},{"id":82,"href":"/docs/%E5%8D%87%E7%BA%A7%E8%AF%B4%E6%98%8E/","title":"升级说明","section":"Docs","content":" 升级说明 # 升级到1.0.0-SNAPSHOT # 概述 # 1.0.0-SNAPSHOT版本包括对工件ID、包名称和模块结构的重大更改。本节提供了特定于使用SNAPSHOT版本的指南。\n添加快照存储库 # 要使用1.0.0-SNAPSHOT版本，需要将快照存储库添加到构建文件中。\n更新依赖项管理 # 在构建配置中将Spring AI BOM版本更新为1.0.0-SNAPSHOT。\n项目ID、包和模块更改 # 1.0.0-SNAPSHOT包括对工件ID、包名称和模块结构的更改。 有关详细信息，请参阅：\n升级到1.0.0-RC1 # 聊天客户和顾问 # 从ChatClient输入构建Prompt时，从systemText（）构建的SystemMessage现在放在消息列表的第一个位置。以前，它放在最后，导致几个模型提供程序出错。 在AbstractChatMemoryAdvisor中，doNextWithProtectFromBlockingBefore（）保护的方法已从接受旧的AdvisedRequest更改为接受新的ChatClientRequest。这是一个突破性的变化，因为替代品不是M8的一部分。 MessageAggregator具有一个新的方法来聚合来自ChatClientRequest的消息。以前聚合来自旧AdvisedRequest的消息的方法已被删除，因为它已在M8中标记为不推荐使用。 在SimpleLoggerAdvisor中，需要更新requestToString输入参数以使用ChatClientRequest。这是一个突破性的变化，因为替代方案还不是M8的一部分。构造函数也是一样的。 Advisors中的独立模板 # 执行即时增强的内置顾问已更新为使用自包含的模板。目标是让每个顾问都能够执行模板操作，而不影响或不受其他顾问中的模板和即时决策的影响。\nQuestionAnswerAdvisor需要具有以下占位符的模板（请参阅更多详细信息）： 用于接收用户问题的查询占位符。 questionanswercontext占位符以接收检索的上下文。 PromptChatMemoryAdvisor需要具有以下占位符的模板（请参阅更多详细信息）： 用于接收原始系统消息的指令占位符。 存储器占位符，用于接收检索到的会话存储器。 VectorStoreChatMemoryAdvisor需要具有以下占位符的模板（请参阅更多详细信息）： 用于接收原始系统消息的指令占位符。 long _termmemory占位符，用于接收检索到的会话内存。 正在中断更改 # Watson AI模型被删除，因为它基于旧的文本生成，而该文本生成被认为过时，因为有一个新的聊天生成模型可用。\n升级到1.0.0-M8 # 您可以使用OpenRewrite配方将升级过程自动化到1.0.0-M8。\n正在中断更改 # 从Spring AI 1.0 M7升级到1.0 M8时，以前注册了工具回调的用户遇到了破坏性更改，导致工具调用功能无提示失败。这特别影响了使用弃用的tools（）方法的代码。\n示例 # 下面是在M7中工作但在M8中不再正常工作的代码示例：\n// Old code in M7 - no longer works correctly in M8 chatClient.prompt(\u0026#34;What day is tomorrow?\u0026#34;) .tools(toolCallback) .call() .content(); 如何调整代码 # 要在升级到M8时解决此问题，您需要更新代码以使用新的工具Callback（）方法：\n// Updated code for M8 chatClient.prompt(\u0026#34;What day is tomorrow?\u0026#34;) .toolCallbacks(toolCallback) .call() .content(); 为什么进行此更改 # SpringAI团队重命名了重载的tools（）方法，以提高方法调度的清晰度并防止歧义。当Java编译器需要基于参数类型在多个重载方法之间进行选择时，以前的API设计导致了混淆。\n从M7到M8的方法映射 # 下面是旧方法如何映射到新方法的方法：\n改进的错误处理 # 在这个PR现在合并（spring项目/spring ai#2964）中，工具（对象…​ toolObjects）方法现在将在提供的对象上找不到@Tool方法时引发异常，而不是自动失败。这有助于开发人员立即确定迁移问题。\n迁移摘要 # 如果您要从M7升级到M8： 这些更改将确保您的工具调用功能在升级到Spring AI 1.0 M8后继续正常工作。\n聊天客户端 # ChatClient得到了增强，可以解决一些不一致或不需要的行为，只要用户和系统提示在顾问中使用之前没有呈现。新的行为确保在执行顾问链之前始终呈现用户和系统提示。作为此增强的一部分，AdvisedRequest和AdvisedResponse API已被弃用，由ChatClientRequest和ChatClientResponse。Advisors现在作用于ChatClientRequest中包含的完全构建的Prompt对象，而不是AdvisedRequest中使用的破坏格式，从而确保一致性和完整性。 例如，如果您有一个自定义顾问，该顾问在before方法中修改了请求提示符，则应按如下方式对其进行重构： // --- Before (using AdvisedRequest) --- @Override public AdvisedRequest before(AdvisedRequest advisedRequest) { // Access original user text and parameters directly from AdvisedRequest String originalUserText = new PromptTemplate(advisedRequest.userText(), advisedRequest.userParams()).render(); // ... retrieve documents, create augmented prompt text ... List\u0026lt;Document\u0026gt; retrievedDocuments = ...; String augmentedPromptText = ...; // create augmented text from originalUserText and retrievedDocuments // Copy existing context and add advisor-specific data Map\u0026lt;String, Object\u0026gt; context = new HashMap\u0026lt;\u0026gt;(advisedRequest.adviseContext()); context.put(\u0026#34;retrievedDocuments\u0026#34;, retrievedDocuments); // Example key // Use the AdvisedRequest builder pattern to return the modified request return AdvisedRequest.from(advisedRequest) .userText(augmentedPromptText) // Set the augmented user text .adviseContext(context) // Set the updated context .build(); } // --- After (using ChatClientRequest) --- @Override public ChatClientRequest before(ChatClientRequest chatClientRequest, AdvisorChain chain) { String originalUserText = chatClientRequest.prompt().getUserMessage().getText(); // Access prompt directly // ... retrieve documents ... List\u0026lt;Document\u0026gt; retrievedDocuments = ...; String augmentedQueryText = ...; // create augmented text // Initialize context with existing data and add advisor-specific data Map\u0026lt;String, Object\u0026gt; context = new HashMap\u0026lt;\u0026gt;(chatClientRequest.context()); (1) context.put(\u0026#34;retrievedDocuments\u0026#34;, retrievedDocuments); // Example key context.put(\u0026#34;originalUserQuery\u0026#34;, originalUserText); // Example key // Use immutable operations return chatClientRequest.mutate() .prompt(chatClientRequest.prompt() .augmentUserMessage(augmentedQueryText) (2) ) .context(context) (3) .build(); } 除了用增强的UserMessage（String）直接替换用户消息文本外，您还可以提供一个函数来更精细地修改现有的UserMessage:\nPrompt originalPrompt = new Prompt(new UserMessage(\u0026#34;Tell me about Large Language Models.\u0026#34;)); // Example: Append context or modify properties using a Function Prompt augmentedPrompt = originalPrompt.augmentUserMessage(userMessage -\u0026gt; userMessage.mutate() .text(userMessage.getText() + \u0026#34;\\n\\nFocus on their applications in software development.\u0026#34;) // .media(...) // Potentially add/modify media // .metadata(...) // Potentially add/modify metadata .build() ); // \u0026#39;augmentedPrompt\u0026#39; now contains the modified UserMessage 当您需要有条件地更改UserMessage的部分或使用其媒体和元数据时，这种方法提供了更多的控制，而不仅仅是替换文本内容。\nChatClient提示生成器API中的重载工具方法已被重命名，以保持清晰，并避免基于参数类型的方法调度中的模糊性。 聊天客户端。PromptRequestSpec#工具（字符串…​ toolNames）已重命名为ChatClient。PromptRequestSpec#toolNames（字符串…​ 工具名称）。使用此方法指定允许模型调用的工具函数的名称（在别处注册，例如，通过@Description的@Bean定义）。 聊天客户端。PromptRequestSpec#工具（ToolCallback…​ toolCallbacks）已重命名为ChatClient。PromptRequestSpec#toolCallbacks（ToolCallback…​ 工具回调）。使用此方法提供内联ToolCallback实例，其中包括函数实现、名称、描述和输入类型定义。 此更改解决了Java编译器可能不会基于提供的参数选择预期重载的潜在混淆。 即时模板和顾问 # 一些与提示创建和顾问定制相关的类和方法已经被弃用，取而代之的是使用构建器模式和TemplateRenderer接口的更灵活的方法。\nPromptTemplate折旧 # PromptTemplate类已弃用几个与旧的templateFormat枚举和直接变量注入相关的构造函数和方法：\n构造函数：不推荐使用PromptTemplate（String template，Map\u0026lt;String，Object\u0026gt;variables）和Prompt模板（Resource Resource，Map\u0026lt;字符串，Object\u0026gt;变量）。 字段：template和templateFormat已弃用。 方法：getTemplateFormat（）、getInputVariables（）和validate（Map\u0026lt;String，Object\u0026gt;model）已弃用。 迁移：使用PromptTemplate.builder（）模式创建实例。通过.template（）提供模板字符串，并可以通过.renderer（）配置自定义TemplateRenderer。使用.variables（）传递变量。 // Before (Deprecated) PromptTemplate oldTemplate = new PromptTemplate(\u0026#34;Hello {name}\u0026#34;, Map.of(\u0026#34;name\u0026#34;, \u0026#34;World\u0026#34;)); String oldRendered = oldTemplate.render(); // Variables passed at construction // After (Using Builder) PromptTemplate newTemplate = PromptTemplate.builder() .template(\u0026#34;Hello {name}\u0026#34;) .variables(Map.of(\u0026#34;name\u0026#34;, \u0026#34;World\u0026#34;)) // Variables passed during builder configuration .build(); Prompt prompt = newTemplate.create(); // Create prompt using baked-in variables String newRendered = prompt.getContents(); // Or use newTemplate.render() 问题解答顾问折旧 # QuestionAnswerAdvisor具有依赖于简单userTextAdvise字符串的弃用构造函数和构建器方法：\n采用userTextAdvise String参数的构造函数已弃用。 生成器方法：userTextAdvise（String userTextAdvis）已弃用。 迁移：使用.promptTemplate（promptTemplate promptTempte）构建器方法提供完全配置的Prompt模板对象，用于自定义合并检索的上下文的方式。 // Before (Deprecated) QuestionAnswerAdvisor oldAdvisor = QuestionAnswerAdvisor.builder(vectorStore) .userTextAdvise(\u0026#34;Context: {question_answer_context} Question: {question}\u0026#34;) // Simple string .build(); // After (Using PromptTemplate) PromptTemplate customTemplate = PromptTemplate.builder() .template(\u0026#34;Context: {question_answer_context} Question: {question}\u0026#34;) .build(); QuestionAnswerAdvisor newAdvisor = QuestionAnswerAdvisor.builder(vectorStore) .promptTemplate(customTemplate) // Provide PromptTemplate object .build(); 聊天室存储器 # 无论何时使用SpringAIModel启动器之一，ChatMemoryBean都会自动为您配置。默认情况下，它使用MessageWindowChatMemory实现，并将对话历史记录存储在内存中。 ChatMemory API得到了增强，以支持更灵活和可扩展的对话历史管理方法。存储机制已与ChatMemory接口解耦，现在由新的ChatMemoriyRepository接口处理。ChatMemory API现在可以用于实现不同的内存策略，而无需绑定到特定的存储机制。默认情况下，Spring AI提供MessageWindowChatMemory实现，该实现维护高达指定最大大小的消息窗口。 ChatMemory中的get（String conversationId，int lastN）方法已被弃用，取而代之的是在需要将消息保存在内存中不超过某个限制时使用MessageWindowChatMemorium。get（StringconversationId）方法现在是从内存中检索消息的首选方法，而ChatMemory的特定实现可以决定过滤、处理和返回消息的策略。 JdbcChatMemory已被弃用，取而代之的是将JdbcChatMemoryRepository与ChatMemority实现（如MessageWindowChatMemorary）一起使用。如果您依赖于自动配置的JdbcChatMemoryBean，则可以通过自动连接自动配置为在内部使用JdbcChatMemoryRepository来存储消息的ChatMem忆oryBean来替换它，只要相关依赖项在类路径中。 spring.ai.chat.memory.jdbc.initialize-schema属性已被弃用，取而代之的是spring.ai.chat.memory.repository.jdbc.initialize-schema。 有关新API以及如何使用它的更多详细信息，请参阅新的Chat Memory文档。 MessageWindowChatMemory.get（String conversationId，int lastN）方法已弃用。窗口大小现在基于实例化期间提供的配置在内部管理，因此只应使用get（String conversationId）。 提示模板化 # PromptTemplate API已经过重新设计，以支持更灵活和可扩展的模板提示方式，并依赖于新的TemplateRenderer API。作为此更改的一部分，getInputVariables（）和validate（）方法已被弃用，如果调用，将抛出UnsupportedOperationException。任何特定于模板引擎的逻辑都应通过TemplateRenderer API提供。 类包重构 # 为了更好地组织，已将几个类移到不同的模块和包中：\n移动的评估类： org.springframework.ai.e.评估。FactCheckingEvaluator在spring-ai客户端聊天中移动到org.springframework.ai.chat.evaluation包。 org.springframework.ai.e.评估。相关性Evaluator在spring ai客户端聊天中移动到org.springframework.ai.chat.evaluation包。 org.springframework.ai.e.评估。EvaluationRequest、evaluation Response和Evaluator从spring-ai客户端聊天移动到org.springframework.ai.evaluation包下的spring-ai-commons。 已移动的输出转换器类： org.springframework.ai.converter中的类（例如，BeanOutputConverter、ListOutputConverter、MapOutputInverter、StructuredOutputTransverter等）从spring-ai客户端聊天移动到spring-ai模型。 移动的变压器类别： org.springframework.ai.chat.transformer网站。关键字MetadataEnricher已移至org.springframework.ai.model.transformer。spring ai模型中的关键字MetadataEnricher。 org.springframework.ai.chat.transformer网站。SummaryMetadataEnricher已移至org.springframework.ai.model.transformer。spring ai模型中的SummaryMetadataEnricher。 移动的实用程序类： org.springframework.ai.util.网站。PromptAssert从spring-ai客户端聊天移至org.springframework.ai.rag.util。PromptAssert in spring ai rag.在春季发出提示。 请相应地更新您的导入。 可观察性 # spring.ai.客户观察的变化： spring.ai.chat.client.tool.function.names和spring.ai.chat.client.tool.function.callbacks属性已被弃用，替换为新的spring.ai.chat.客户端工具.names属性，该属性包括传递给ChatClient的所有工具的名称，而不管用于定义它们的底层机制如何。 spring.ai.chat.client.advisor.params属性已被弃用，将不会有替代属性。原因是存在暴露敏感信息或破坏检测的风险，因为advisor上下文中的条目用于在advisor之间传递任意Java对象，并且不一定是可序列化的。以前导出到这里的对话ID现在可以通过专用的spring.ai.chat.client.conversation.ID属性获得。如果需要将advisor上下文中的一些其他参数导出到可观测性系统，可以通过定义观测过滤器并明确决定要导出哪些参数来完成。有关灵感，可以参考ChatClientPromptContentObservation过滤器。 通过ChatClient API指定的提示内容可选地包括在spring.ai.客户端观察中，细分为几个属性：spring.ai.chat.client.user.text、spring.ai.chat.client.user.params、sprink.ai.chat.client.system.text、spring.ai.chat.client.system.params。所有这些属性现在都已弃用，由包含提示中所有消息的单个gen_ai.prompt属性替换，解决了影响弃用属性的问题，其中部分提示未包含在观察中，并与ChatModel API中使用的观察一致。这个新属性可以通过spring.ai.chat.observations.include-prompt配置属性启用，而以前的spring.ai.chat.obervations.include-input配置属性已弃用。 spring.ai.advisor观察的变化： spring.ai.advisor.type属性已被弃用。在以前的版本中，Advisor API是根据Advisor的类型（之前、之后、周围）进行分类的。这种区别不再适用，这意味着所有顾问现在都是相同的类型（周围）。 检索增强生成 # 引入了DocumentPostProcessor API以在模块化RAG体系结构中实现后期检索组件，取代了DocumentCompressor、DocumentRanker、DocumentSelector API，这些API现在已被弃用。 升级到1.0.0-M7 # 变更概述 # Spring AI 1.0.0-M7是RC1和GA发布之前的最后一个里程碑版本。它引入了对工件ID、包名称和模块结构的几个重要更改，这些更改将在最终版本中维护。\n项目ID、包和模块更改 # 1.0.0-M7包括与1.0.0-SNAPSHOT相同的结构变化。 有关详细信息，请参阅：\nMCP Java SDK升级至0.9.0 # Spring AI 1.0.0-M7现在使用MCP Java SDK 0.9.0版，其中包括与以前版本相比的重大变化。如果在应用程序中使用MCP，则需要更新代码以适应这些更改。 主要变化包括：\n接口重命名 # 客户端McpTransport→ Mcp客户端传输 服务器McpTransport→ Mcp服务器传输 默认McpSession→ McpClientSession或McpServerSession 所有*注册类别→ *规范等级 服务器创建更改 # 使用McpServerTransportProvider而不是ServerMcpTransport // Before ServerMcpTransport transport = new WebFluxSseServerTransport(objectMapper, \u0026#34;/mcp/message\u0026#34;); var server = McpServer.sync(transport) .serverInfo(\u0026#34;my-server\u0026#34;, \u0026#34;1.0.0\u0026#34;) .build(); // After McpServerTransportProvider transportProvider = new WebFluxSseServerTransportProvider(objectMapper, \u0026#34;/mcp/message\u0026#34;); var server = McpServer.sync(transportProvider) .serverInfo(\u0026#34;my-server\u0026#34;, \u0026#34;1.0.0\u0026#34;) .build(); 处理程序签名更改 # 现在，所有处理程序都会接收一个交换参数作为其第一个参数：\n// Before .tool(calculatorTool, args -\u0026gt; new CallToolResult(\u0026#34;Result: \u0026#34; + calculate(args))) // After .tool(calculatorTool, (exchange, args) -\u0026gt; new CallToolResult(\u0026#34;Result: \u0026#34; + calculate(args))) 通过Exchange进行客户端交互 # 服务器上以前可用的方法现在可以通过exchange对象访问：\n// Before ClientCapabilities capabilities = server.getClientCapabilities(); CreateMessageResult result = server.createMessage(new CreateMessageRequest(...)); // After ClientCapabilities capabilities = exchange.getClientCapabilities(); CreateMessageResult result = exchange.createMessage(new CreateMessageRequest(...)); 根更改处理程序 # // Before .rootsChangeConsumers(List.of( roots -\u0026gt; System.out.println(\u0026#34;Roots changed: \u0026#34; + roots) )) // After .rootsChangeHandlers(List.of( (exchange, roots) -\u0026gt; System.out.println(\u0026#34;Roots changed: \u0026#34; + roots) )) 有关迁移MCP代码的完整指南，请参阅 MCP迁移指南。\n启用/禁用模型自动配置 # 以前用于启用/禁用模型自动配置的配置属性已删除：\nspring.ai.\u0026lt;提供商\u0026gt;.chat.enabled spring.ai.\u0026lt;提供商\u0026gt;.embedding.enabled spring.ai.\u0026lt;提供商\u0026gt;.image.enabled spring.ai..mardiation.enabled（弹簧.ai.\u0026lt;提供商\u0026gt;.mardation.enable） 默认情况下，如果在类路径上找到模型提供程序（例如，OpenAI、Ollama），则会启用其相应的相关模型类型（聊天、嵌入等）的自动配置。如果存在相同型号的多个提供程序（例如，spring-ai-openai-spring-boot-starter和spring-ai-ollama spring-boot-starter），则可以使用以下属性来选择应激活的提供程序的自动配置，从而有效地禁用该特定型号的其他提供程序。 要完全禁用特定模型类型的自动配置，即使只有一个提供程序，请将相应的属性设置为与类路径上的任何提供程序都不匹配的值（例如，无或已禁用）。 您可以参考SpringAIModels枚举来获得已知提供程序值的列表。 spring.ai.model.audio.speech=\u0026lt;模型提供程序|无\u0026gt; spring.ai.model.audio.transcription=\u0026lt;模型提供程序|无\u0026gt; spring.ai.model.chat=\u0026lt;模型提供程序|无\u0026gt; spring.ai.model.embedding=\u0026lt;模型提供程序|无\u0026gt; spring.ai.model.embedding.multimal=\u0026lt;模型提供程序|无\u0026gt; spring.ai.model.embedding.text=\u0026lt;模型提供程序|无\u0026gt; spring.ai.model.image=\u0026lt;模型提供程序|无\u0026gt; spring.ai.model.moderation=\u0026lt;模型提供程序|无\u0026gt; 使用人工智能自动升级 # 您可以使用Claude Code CLI工具自动将升级过程升级到1.0.0-M7，并提供提示：\n跨版本的常见更改 # 项目ID更改 # Spring AI初学者工件的命名模式已更改。\n型号启动器：spring ai-{model}-spring-boot-starter → spring-ai-starter模型-{model} Vector Store启动器：spring ai-{store}-store-spring-boot-starter → spring ai起始向量存储-{store} MCP起动器：弹簧ai MCP-{type}-spring-boot-starter → spring-ai启动器mcp-{type} 示例 # 对Spring AI自动配置工件的更改 # Spring AI自动配置已经从单个单片工件更改为每个模型、向量存储和其他组件的单个自动配置工件。 原始的单片工件不再可用：\n\u0026lt;!-- NO LONGER AVAILABLE --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-spring-boot-autoconfigure\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${project.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 相反，每个组件现在都有自己的自动配置工件，遵循以下模式：\n模型自动配置：spring ai自动配置模型-{Model} 向量存储自动配置：spring ai自动配置向量存储-{Store} MCP自动配置：spring ai自动配置MCP-{type} 新自动配置工件的示例 # 程序包名称更改 # 您的IDE应该有助于重构到新的包位置。\n关键字MetadataEnricher和SummaryMetadataEnricher已从org.springframework.ai.transformer移动到org.springeframework.ai.chat.transformer。 Content、MediaContent和Media已从org.springframework.ai.model迁移到org.springeframework_ai.Content。 模块结构 # 该项目的模块和工件结构发生了重大变化。以前，spring-ai核心包含所有中央接口，但现在已将其划分为专门的域模块，以减少应用程序中不必要的依赖性。 春季人工智能公地 # 基本模块不依赖于其他Spring AI模块。包含：\n弹簧人工智能模型 # 提供人工智能功能抽象：\n春季人工智能矢量存储 # 统一的向量数据库抽象：\nspring ai客户端聊天 # 高级对话AI API：\nspring ai advisors向量存储 # Bridges与RAG的矢量存储聊天：\nspring-ai模型聊天记忆卡桑德拉 # ChatMemory的Apache Cassandra持久性： 聊天对话的Neo4j图形数据库持久性。\n春季ai抹布 # 检索增强生成的综合框架：\n依赖关系结构 # 依赖关系层次结构可以总结为：\nspring ai公共资源（基础） spring-ai模型（取决于通用） spring-ai向量存储和spring-ai客户端聊天（都取决于模型） springai顾问向量存储和springairag（取决于客户端聊天和向量存储） spring-ai模型聊天存储器-*模块（取决于客户端聊天） 工具上下文更改 # ToolContext类已被增强，以支持显式和隐式工具解析。工具现在可以是： 从1.0.0-M7开始，只有在提示符中显式请求或在调用中显式包含工具时，才会在对模型的调用中包含这些工具。 此外，ToolContext类现在已标记为final，无法再扩展。它从未被认为是子类。您可以以Map\u0026lt;String，Object\u0026gt;的形式添加实例化ToolContext时所需的所有上下文数据。有关更多信息，请查看[documentation]（ docs.spring.io/spring-ai/reference/api/tools.html#_tool_context）。\n升级到1.0.0-M6 # 对使用接口和DefaultUsage实现的更改 # Usage接口及其默认实现DefaultUsage经历了以下更改：\n所需的操作 # 将对getGenerationTokens（）的所有调用替换为getCompletionTokens.（） 更新DefaultUsage构造函数调用： JSON Ser/Deser更改 # 虽然M6为generationTokens字段的JSON反序列化保持向后兼容性，但该字段将在M7中删除。任何使用旧字段名的持久化JSON文档都应更新为使用completionTokens。 新JSON格式的示例：\n{ \u0026#34;promptTokens\u0026#34;: 100, \u0026#34;completionTokens\u0026#34;: 50, \u0026#34;totalTokens\u0026#34;: 150 } 更改用于工具调用的FunctionCallingOptions的用法 # 每个ChatModel实例在构造时接受可选的ChatOptions或FunctionCallingOptions实例 1.0.0-M6之前：\n通过默认FunctionCallingOptions实例的functions（）方法传递的任何工具都包含在中 通过默认FunctionCallingOptions实例的functionCallback（）方法传递的任何工具都只是 启动1.0.0-M6： 通过默认FunctionCallingOptions的functions（）方法或functionCallback（）传递的任何工具 如果希望使工具可用于运行时动态解析，并仅将其包含在对模型的聊天请求中 删除不推荐的Amazon Bedrock聊天模型 # 从1.0.0-M6开始，Spring AI过渡到使用Amazon Bedrock的Converse API来实现Spring AI。\n使用Spring Boot 3.4.2进行依赖关系管理的更改 # Spring AI更新为使用Spring Boot 3.4.2进行依赖项管理。您可以 在这里参考Spring Boot 3.4.2管理的依赖项\n所需的操作 # 如果您要升级到Spring Boot 3.4.2，请确保参考本文档以了解配置REST客户端所需的更改。值得注意的是，如果类路径上没有HTTP客户端库，这可能会导致使用JdkClientHttpRequestFactory，其中SimpleClientHttp请求工厂以前可能已经使用过。要切换到使用SimpleClientHttpRequestFactory，需要设置spring.http.client.factory=simple。 如果您正在使用不同版本的SpringBoot（比方说SpringBoot3.3.x），并且需要依赖项的特定版本，则可以在构建配置中覆盖它。 向量存储API更改 # 在版本1.0.0-M6中，VectorStore接口中的delete方法已被修改为void操作，而不是返回可选的。\n1.0.0-M6之前： # Optional\u0026lt;Boolean\u0026gt; result = vectorStore.delete(ids); if (result.isPresent() \u0026amp;\u0026amp; result.get()) { // handle successful deletion } 在1.0.0-M6及以后版本中： # vectorStore.delete(ids); // deletion successful if no exception is thrown 升级到1.0.0.M5 # 向量生成器已被重构以保持一致性。 当前的VectorStore实现构造函数已弃用，请使用生成器模式。 VectorStore实现包已移动到唯一的包名称中，避免了工件之间的冲突。例如，org.springframework.ai.vectorstore到org.spring框架.ai.pgvector.vectorstore。 升级到1.0.0.RC3 # 便携式聊天选项的类型（frequencyPenalty、presencePenalty，temperature，topP）已从Float更改为Double。 升级到1.0.0.M2 # Chroma Vector Store的配置前缀已从spring.ai.vectorstore.Chroma.Store更改为spring.ai.vectortore.Chroma，以与其他向量存储的命名约定保持一致。 现在，能够初始化架构的向量存储上的initialize schema属性的默认值设置为false。 在Bedrock Jurassic 2中，聊天选项countPenalty、frequencyPenalty和presencePenalty 在Azure OpenAI中，聊天选项frequencyPenalty和presencePenalty的类型 升级到1.0.0.M1 # 在发布1.0.0 M1的过程中，我们进行了几个突破性的更改。抱歉，这是最好的！\nChatClient更改 # 进行了一项重大更改，采用了“旧”ChatClient，并将功能移动到ChatModel中。“新”ChatClient现在采用ChatModel的实例。这样做是为了支持一个流畅的API，以类似于Spring生态系统中其他客户机类（如RestClient、WebClient和JdbcClient）的风格创建和执行提示。有关Fluent api的更多信息，请参阅[JavaDoc]（ docs.spring.io/spring-ai/docs/api），适当的参考文档即将发布。 我们将“旧”```ModelClient`重命名为Model，并重命名实现类，例如`ImageClient`被重命名为ImageModel``。模型实现表示在Spring AI API和底层AI模型API之间转换的可移植性层。\n适应变化 # 方法1 # 现在，您将获得ChatModel实例，而不是自动配置的ChatClient实例。重命名后的调用方法签名保持不变。\n@RestController public class OldSimpleAiController { private final ChatClient chatClient; public OldSimpleAiController(ChatClient chatClient) { this.chatClient = chatClient; } @GetMapping(\u0026#34;/ai/simple\u0026#34;) Map\u0026lt;String, String\u0026gt; completion(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, this.chatClient.call(message)); } } 现在在更改后，这将是\n@RestController public class SimpleAiController { private final ChatModel chatModel; public SimpleAiController(ChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/simple\u0026#34;) Map\u0026lt;String, String\u0026gt; completion(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, this.chatModel.call(message)); } } 方法2 # 在这种方法中，您将使用“新”ChatClient上提供的新fluent API 下面是更改之前的现有代码的示例\n@RestController class OldSimpleAiController { ChatClient chatClient; OldSimpleAiController(ChatClient chatClient) { this.chatClient = chatClient; } @GetMapping(\u0026#34;/ai/simple\u0026#34;) Map\u0026lt;String, String\u0026gt; completion(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of( \u0026#34;generation\u0026#34;, this.chatClient.call(message) ); } } 现在在更改后，这将是\n@RestController class SimpleAiController { private final ChatClient chatClient; SimpleAiController(ChatClient.Builder builder) { this.chatClient = builder.build(); } @GetMapping(\u0026#34;/ai/simple\u0026#34;) Map\u0026lt;String, String\u0026gt; completion(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of( \u0026#34;generation\u0026#34;, this.chatClient.prompt().user(message).call().content() ); } } 方法3 # GitHub存储库中有一个名为[v1.0.0-SNAPSHOT-before-chatchlient-changes]的标记（GitHub.com/spring projects/spring ai/tree/v1.0.0-NSAPSHOT-before-chatclient-cchanges），您可以签出该标记并进行本地构建，以避免在准备迁移代码库之前更新任何代码。\ngit checkout tags/v1.0.0-SNAPSHOT-before-chatclient-changes ./mvnw clean install -DskipTests 项目名称更改 # 重命名的POM项目名称：\n升级到0.8.1 # 以前的spring-ai-vertex-ai已重命名为spring-ai-vertex-ai-palm2，spring-ai-vertex-ai-spring-boot-starter已重命名为spring-ai-vertex-ai-palm2-spring-boot-starter。 因此，您需要将依赖关系从\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-vertex-ai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 到\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-vertex-ai-palm2\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; Palm2型号的相关Boot starter已从\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-vertex-ai-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 到\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-vertex-ai-palm2-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 更名类别（01.03.2024） 顶点AiApi→ 顶点AiPalm2Api 顶点AiClientChat→ VertexAiPalm2Chat客户端 顶点A嵌入客户端→ VertexAiPalm2嵌入客户端 顶点AiChatOptions→ 顶点AiPalm2ChatOptions 升级到0.8.0 # 2024年1月24日更新 # 将提示、消息和元数据包移动到org.springframework.ai.chat的子包 新功能是文本到图像客户端。类是OpenAiImageModel和StabilityAiImageModel。有关用法，请参阅集成测试，文档即将发布。 一种新的包模型，包含接口和基类，支持为任何输入/输出数据类型组合创建AI模型客户端。目前，聊天和图像模型包实现了这一点。我们将很快更新此新模型的嵌入包。 一种新的“便携选项”设计模式。我们希望在ModelCall中跨不同的基于聊天的AI模型提供尽可能多的可移植性。有一组常见的生成选项，然后是特定于模型提供程序的选项。使用了一种“duck typeing”方法。模型包中的ModelOptions是一个标记接口，指示此类的实现将为模型提供选项。请参阅ImageOptions，这是一个子接口，定义所有文本的可移植选项→image ImageModel实现。然后StabilityAiImageOptions和OpenAiImageOptions提供特定于每个模型提供程序的选项。所有选项类都是通过流畅的API构建器创建的，所有选项都可以传递到可移植的ImageModel API中。这些选项数据类型用于ImageModel实现的自动配置/配置属性。 2024年1月13日更新 # 以下OpenAi自动配置聊天属性已更改\n从spring.ai.openai.model到spring.ai.openai.chat.options.model。 从spring.ai.openai.temperature到spring.ai.openai.chat.options.temperature。 查找有关OpenAi属性的更新文档：docs.spring.io/spring-ai/reference/api/chat/OpenAi-chat.html 2023年12月27日更新 # 将SimplePersistentVectorStore和InMemoryVectorStore合并到SimpleVectorStore\n2023年12月20日更新 # 重构Ollama客户端和相关的类和包名称\n替换org.springframework.ai.ollama.client。ollama客户：org.springframework.ai.ollama。OllamaModelCall。 OllamaChatClient方法签名已更改。 将org.springframework.ai.autoconfigure.ollama重命名为。OllamaProperties转换为org.springframework.ai.model.ollama.autoconfigure。OllamaChatProperties并将后缀更改为：spring.ai.ollama.chat。一些属性也发生了变化。 2023年12月19日更新 # AiClient及相关类和包名称的重命名\n将AiClient重命名为ChatClient 将AiResponse重命名为ChatResponse 将AiStreamClient重命名为StreamingChatClient 将包org.sf.ai.client重命名为org.sf.ai.chat 重命名的项目ID 嵌入到spring ai变压器的变压器 将Maven模块从顶级目录和嵌入客户机子目录移动到单个模型目录下。 2023年12月1日 # 我们正在转换项目的组ID:\n发件人：org.springframework.experimental.ai 收件人：org.springframework.ai 工件仍将托管在快照存储库中，如下所示。 主分支将移动到0.8.0-SNAPSHOT版本。 您可以像以前一样访问0.7.1-SNAPSHOT工件，但仍然可以访问0.7.1.SNAPSHOT-Documentation。 0.7.1-SNAPSHOT依赖性 # Azure OpenAI软件 开放人工智能 "},{"id":83,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/%E5%9B%B0%E6%83%91%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/","title":"困惑聊天","section":"聊天模型API","content":" 困惑聊天 # 困惑人工智能提供了一种独特的人工智能服务，将其语言模型与实时搜索功能集成在一起。它提供了各种模型，并支持对话人工智能的流式响应。 Spring AI通过重用现有的OpenAI客户端与Perplexity AI集成。要开始，您需要获得一个Perplexity API密钥，配置基本URL，并选择一个受支持的模型。 检查PerplexityWithOpenAiChatModelIT.java测试，以获取将Perplexity与Spring AI一起使用的示例。\n前提条件 # 创建API密钥： 设置困惑库URL: 选择困惑模型： 设置聊天完成路径： 环境变量配置示例： export SPRING_AI_OPENAI_API_KEY=\u0026lt;INSERT PERPLEXITY API KEY HERE\u0026gt; export SPRING_AI_OPENAI_BASE_URL=https://api.perplexity.ai export SPRING_AI_OPENAI_CHAT_MODEL=llama-3.1-sonar-small-128k-online 添加存储库和BOM表 # Spring AI工件发布在Maven Central和Spring Snapshot 存储库中。 为了帮助进行依赖关系管理，Spring AI提供了BOM（物料清单），以确保在整个项目中使用一致版本的Spring AI。请参阅依赖项管理部分，将Spring AI BOM添加到构建系统中。\n自动配置 # Spring AI为OpenAI聊天客户端提供Spring Boot自动配置。\n聊天室属性 # 重试属性 # 前缀spring.ai.retry用作属性前缀，允许您为OpenAI聊天模型配置重试机制。\n连接属性 # 前缀spring.ai.openai用作允许连接到openai的属性前缀。\n配置属性 # 前缀spring.ai.openai.chat是属性前缀，允许您配置openai的聊天模型实现。\n运行时选项 # OpenAiChatOptions.java提供模型配置，例如要使用的模型、温度、频率惩罚等。 启动时，可以使用OpenAiChatModel（api，options）构造函数或spring.ai.openai.chat.options.*属性配置默认选项。 在运行时，可以通过向Prompt调用添加新的特定于请求的选项来覆盖默认选项。\nChatResponse response = chatModel.call( new Prompt( \u0026#34;Generate the names of 5 famous pirates.\u0026#34;, OpenAiChatOptions.builder() .model(\u0026#34;llama-3.1-sonar-large-128k-online\u0026#34;) .temperature(0.4) .build() )); 函数调用 # 多模态 # 样本控制器 # 创建一个新的SpringBoot项目，并将Springaistarter模型openai添加到pom（或gradle）依赖项中。 在src/main/resources目录下添加application.properties文件，以启用和配置OpenAi聊天模型：\nspring.ai.openai.api-key=\u0026lt;PERPLEXITY_API_KEY\u0026gt; spring.ai.openai.base-url=https://api.perplexity.ai spring.ai.openai.chat.completions-path=/chat/completions spring.ai.openai.chat.options.model=llama-3.1-sonar-small-128k-online spring.ai.openai.chat.options.temperature=0.7 # The Perplexity API doesn\u0026#39;t support embeddings, so we need to disable it. spring.ai.openai.embedding.enabled=false 这将创建一个OpenAiChatModel实现，您可以将其注入到类中。\n@RestController public class ChatController { private final OpenAiChatModel chatModel; @Autowired public ChatController(OpenAiChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, this.chatModel.call(message)); } @GetMapping(\u0026#34;/ai/generateStream\u0026#34;) public Flux\u0026lt;ChatResponse\u0026gt; generateStream(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { Prompt prompt = new Prompt(new UserMessage(message)); return this.chatModel.stream(prompt); } } 支持的型号 # Perplexity支持几个为搜索增强型对话人工智能优化的模型。有关详细信息，请参阅支持的模型。\n参考文献 # 文档主页 API参考 入门 利率限制 "},{"id":84,"href":"/docs/%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/%E6%9D%BE%E6%9E%9C%E4%BD%93/","title":"松果体","section":"向量数据库","content":" 松果体 # 本节将指导您设置Pinecone VectorStore，以存储文档嵌入并执行相似性搜索。 Pinecone是一种流行的基于云的向量数据库，它允许您高效地存储和搜索向量。\n前提条件 # 要设置PineconeVectorStore，请从Pinecone帐户收集以下详细信息：\nPinecone API密钥 松果体索引名称 Pinecone命名空间 自动配置 # Spring AI为Pinecone Vector Store提供Spring Boot自动配置。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-pinecone\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-vector-store-pinecone\u0026#39; } 此外，您还需要一个已配置的EmbeddingModelbean。有关更多信息，请参阅EmbeddingModel部分。 下面是所需bean的示例：\n@Bean public EmbeddingModel embeddingModel() { // Can be any other EmbeddingModel implementation. return new OpenAiEmbeddingModel(new OpenAiApi(System.getenv(\u0026#34;SPRING_AI_OPENAI_API_KEY\u0026#34;))); } 要连接到Pinecone，您需要提供实例的访问详细信息。\nspring.ai.vectorstore.pinecone.apiKey=\u0026lt;your api key\u0026gt; spring.ai.vectorstore.pinecone.index-name=\u0026lt;your index name\u0026gt; # API key if needed, e.g. OpenAI spring.ai.openai.api.key=\u0026lt;api-key\u0026gt; 请查看矢量存储的 配置参数列表，以了解默认值和配置选项。 现在，您可以在应用程序中自动连接Pinecone Vector Store并使用它\n@Autowired VectorStore vectorStore; // ... List \u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); // Add the documents vectorStore.add(documents); // Retrieve documents similar to a query List\u0026lt;Document\u0026gt; results = this.vectorStore.similaritySearch(SearchRequest.builder().query(\u0026#34;Spring\u0026#34;).topK(5).build()); 配置属性 # 您可以在SpringBoot配置中使用以下属性来定制Pinecone向量存储。\n元数据筛选 # 您可以在Pinecone存储中利用通用的、可移植的元数据过滤器。 例如，可以使用文本表达式语言：\nvectorStore.similaritySearch( SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(\u0026#34;author in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; article_type == \u0026#39;blog\u0026#39;\u0026#34;).build()); 或以编程方式使用筛选器。表达式DSL:\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(b.and( b.in(\u0026#34;author\u0026#34;,\u0026#34;john\u0026#34;, \u0026#34;jill\u0026#34;), b.eq(\u0026#34;article_type\u0026#34;, \u0026#34;blog\u0026#34;)).build()).build()); 手动配置 # 如果您喜欢手动配置PineconeVectorStore，则可以使用Pinecone VectorStore#Builder进行配置。 将这些依赖项添加到项目中：\nOpenAI：计算嵌入时需要。 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 松果体 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-pinecone-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 示例代码 # 要在应用程序中配置Pinecone，可以使用以下设置：\n@Bean public VectorStore pineconeVectorStore(EmbeddingModel embeddingModel) { return PineconeVectorStore.builder(embeddingModel) .apiKey(PINECONE_API_KEY) .indexName(PINECONE_INDEX_NAME) .namespace(PINECONE_NAMESPACE) // the free tier doesn\u0026#39;t support namespaces. .contentFieldName(CUSTOM_CONTENT_FIELD_NAME) // optional field to store the original content. Defaults to `document_content` .build(); } 在主代码中，创建一些文档：\nList\u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); 将文档添加到Pinecone：\nvectorStore.add(documents); 最后，检索类似于查询的文档：\nList\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch(SearchRequest.query(\u0026#34;Spring\u0026#34;).topK(5).build()); 如果一切顺利，您应该检索包含文本“Spring AI rocks！！”的文档。\n访问本机客户端 # Pinecone Vector Store实现通过getNativeClient（）方法提供对底层本机Pinecone.客户端（PineconeConnection）的访问：\nPineconeVectorStore vectorStore = context.getBean(PineconeVectorStore.class); Optional\u0026lt;PineconeConnection\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { PineconeConnection client = nativeClient.get(); // Use the native client for Pinecone-specific operations } 本机客户端允许您访问Pinecone特定的功能和操作，这些功能和操作可能不会通过VectorStore界面公开。\n"},{"id":85,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/%E5%BC%80%E6%94%BE%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/","title":"OpenAI聊天","section":"聊天模型API","content":" OpenAI聊天 # Spring AI支持OpenAI的各种人工智能语言模型，OpenAI是ChatGPT背后的公司，该公司创建了行业领先的文本生成模型和嵌入，有助于激发人们对人工智能驱动文本生成的兴趣。\n前提条件 # 您需要使用OpenAI创建一个API来访问ChatGPT模型。\nexport SPRING_AI_OPENAI_API_KEY=\u0026lt;INSERT KEY HERE\u0026gt; 添加存储库和BOM表 # Spring AI工件发布在Maven Central和Spring Snapshot 存储库中。 为了帮助进行依赖关系管理，Spring AI提供了BOM（物料清单），以确保在整个项目中使用一致版本的Spring AI。请参阅依赖项管理部分，将Spring AI BOM添加到构建系统中。\n自动配置 # Spring AI为OpenAI聊天客户端提供Spring Boot自动配置。\n聊天室属性 # 重试属性 # 前缀spring.ai.retry用作属性前缀，允许您为OpenAI聊天模型配置重试机制。\n连接属性 # 前缀spring.ai.openai用作允许连接到openai的属性前缀。\n配置属性 # 前缀spring.ai.openai.chat是属性前缀，允许您配置openai的聊天模型实现。\n运行时选项 # OpenAiChatOptions.java类提供模型配置，如要使用的模型、温度、频率惩罚等。 启动时，可以使用OpenAiChatModel（api，options）构造函数或spring.ai.openai.chat.options.*属性配置默认选项。 在运行时，可以通过向Prompt调用添加新的特定于请求的选项来覆盖默认选项。\nChatResponse response = chatModel.call( new Prompt( \u0026#34;Generate the names of 5 famous pirates.\u0026#34;, OpenAiChatOptions.builder() .model(\u0026#34;gpt-4o\u0026#34;) .temperature(0.4) .build() )); 函数调用 # 您可以使用OpenAiChatModel注册自定义Java函数，并让OpenAI模型智能地选择输出包含参数的JSON对象，以调用一个或多个已注册的函数。\n多模态 # 多模态是指模型同时理解和处理来自各种来源的信息的能力，包括文本、图像、音频和其他数据格式。\n愿景 # 提供视觉多模态支持的OpenAI模型包括gpt-4、gpt-4o和gpt-4o-mini。 OpenAI[用户 消息API]( https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages)可以将base64编码图像或图像URL的列表与 消息合并。 下面是摘自 OpenAiChatModelIT.java的代码示例，说明了使用gpt-4o模型将用户文本与图像融合。\nvar imageResource = new ClassPathResource(\u0026#34;/multimodal.test.png\u0026#34;); var userMessage = new UserMessage(\u0026#34;Explain what do you see on this picture?\u0026#34;, new Media(MimeTypeUtils.IMAGE_PNG, this.imageResource)); ChatResponse response = chatModel.call(new Prompt(this.userMessage, OpenAiChatOptions.builder().model(OpenAiApi.ChatModel.GPT_4_O.getValue()).build())); 或使用gpt-4o模型的等效图像URL：\nvar userMessage = new UserMessage(\u0026#34;Explain what do you see on this picture?\u0026#34;, new Media(MimeTypeUtils.IMAGE_PNG, URI.create(\u0026#34;https://docs.spring.io/spring-ai/reference/_images/multimodal.test.png\u0026#34;))); ChatResponse response = chatModel.call(new Prompt(this.userMessage, OpenAiChatOptions.builder().model(OpenAiApi.ChatModel.GPT_4_O.getValue()).build())); 该示例显示了一个将multimal.test.png图像作为输入的模型： 以及文本消息“解释您在这张图片上看到了什么？”，并生成如下响应：\n音频 # 提供输入 音频多模式支持的OpenAI型号包括gpt-4o-audio-preview。 OpenAI[用户 消息API]( https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages)可以将base64编码音频文件列表与 消息合并。 下面是摘自 OpenAiChatModelIT.java的代码示例，说明了使用gpt-4o-audio-preview模型将用户文本与音频文件融合。\nvar audioResource = new ClassPathResource(\u0026#34;speech1.mp3\u0026#34;); var userMessage = new UserMessage(\u0026#34;What is this recording about?\u0026#34;, List.of(new Media(MimeTypeUtils.parseMimeType(\u0026#34;audio/mp3\u0026#34;), audioResource))); ChatResponse response = chatModel.call(new Prompt(List.of(userMessage), OpenAiChatOptions.builder().model(OpenAiApi.ChatModel.GPT_4_O_AUDIO_PREVIEW).build())); 输出音频 # 提供输入 音频多模式支持的OpenAI型号包括gpt-4o-audio-preview。 OpenAI Assystant Message API可以包含 消息的base64编码音频文件列表。 下面是一个代码示例，说明使用gpt-4o-audio-preview模型的用户文本和音频字节数组的响应：\nvar userMessage = new UserMessage(\u0026#34;Tell me joke about Spring Framework\u0026#34;); ChatResponse response = chatModel.call(new Prompt(List.of(userMessage), OpenAiChatOptions.builder() .model(OpenAiApi.ChatModel.GPT_4_O_AUDIO_PREVIEW) .outputModalities(List.of(\u0026#34;text\u0026#34;, \u0026#34;audio\u0026#34;)) .outputAudio(new AudioParameters(Voice.ALLOY, AudioResponseFormat.WAV)) .build())); String text = response.getResult().getOutput().getContent(); // audio transcript byte[] waveAudio = response.getResult().getOutput().getMedia().get(0).getDataAsByteArray(); // audio data 您必须在OpenAiChatOptions中指定音频模态才能生成音频输出。\n结构化输出 # OpenAI提供定制的 结构化输出API，确保模型生成严格符合所提供的JSON模式的响应。\n配置 # Spring AI允许您使用OpenAiChatOptions构建器以编程方式或通过应用程序属性配置响应格式。\n使用聊天选项生成器 # 您可以使用OpenAiChatOptions构建器以编程方式设置响应格式，如下所示：\nString jsonSchema = \u0026#34;\u0026#34;\u0026#34; { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;steps\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, \u0026#34;items\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;explanation\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;output\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } }, \u0026#34;required\u0026#34;: [\u0026#34;explanation\u0026#34;, \u0026#34;output\u0026#34;], \u0026#34;additionalProperties\u0026#34;: false } }, \u0026#34;final_answer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } }, \u0026#34;required\u0026#34;: [\u0026#34;steps\u0026#34;, \u0026#34;final_answer\u0026#34;], \u0026#34;additionalProperties\u0026#34;: false } \u0026#34;\u0026#34;\u0026#34;; Prompt prompt = new Prompt(\u0026#34;how can I solve 8x + 7 = -23\u0026#34;, OpenAiChatOptions.builder() .model(ChatModel.GPT_4_O_MINI) .responseFormat(new ResponseFormat(ResponseFormat.Type.JSON_SCHEMA, this.jsonSchema)) .build()); ChatResponse response = this.openAiChatModel.call(this.prompt); 与BeanOutputConverter实用程序集成 # 您可以利用现有的BeanOutputConverter实用程序从域对象自动生成JSON架构，然后将结构化响应转换为特定于域的实例：\n通过应用程序属性配置 # 或者，当使用OpenAI自动配置时，您可以通过以下应用程序属性配置所需的响应格式：\nspring.ai.openai.api-key=YOUR_API_KEY spring.ai.openai.chat.options.model=gpt-4o-mini spring.ai.openai.chat.options.response-format.type=JSON_SCHEMA spring.ai.openai.chat.options.response-format.name=MySchemaName spring.ai.openai.chat.options.response-format.schema={\u0026#34;type\u0026#34;:\u0026#34;object\u0026#34;,\u0026#34;properties\u0026#34;:{\u0026#34;steps\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;array\u0026#34;,\u0026#34;items\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;object\u0026#34;,\u0026#34;properties\u0026#34;:{\u0026#34;explanation\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;},\u0026#34;output\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;}},\u0026#34;required\u0026#34;:[\u0026#34;explanation\u0026#34;,\u0026#34;output\u0026#34;],\u0026#34;additionalProperties\u0026#34;:false}},\u0026#34;final_answer\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;}},\u0026#34;required\u0026#34;:[\u0026#34;steps\u0026#34;,\u0026#34;final_answer\u0026#34;],\u0026#34;additionalProperties\u0026#34;:false} spring.ai.openai.chat.options.response-format.strict=true 样本控制器 # 创建一个新的SpringBoot项目，并将Springaistarter模型openai添加到pom（或gradle）依赖项中。 在src/main/resources目录下添加application.properties文件，以启用和配置OpenAi聊天模型：\nspring.ai.openai.api-key=YOUR_API_KEY spring.ai.openai.chat.options.model=gpt-4o spring.ai.openai.chat.options.temperature=0.7 这将创建一个OpenAiChatModel实现，您可以将其注入到类中。\n@RestController public class ChatController { private final OpenAiChatModel chatModel; @Autowired public ChatController(OpenAiChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map\u0026lt;String,String\u0026gt; generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, this.chatModel.call(message)); } @GetMapping(\u0026#34;/ai/generateStream\u0026#34;) public Flux\u0026lt;ChatResponse\u0026gt; generateStream(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { Prompt prompt = new Prompt(new UserMessage(message)); return this.chatModel.stream(prompt); } } 手动配置 # OpenAiChatModel实现ChatModels和StreamingChatModel.并使用 低级OpenAiApi客户端连接到OpenAI服务。 将spring ai openai依赖项添加到项目的Maven pom.xml文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-openai\u0026#39; } 接下来，创建OpenAiChatModel并将其用于文本生成：\nvar openAiApi = OpenAiApi.builder() .apiKey(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;)) .build(); var openAiChatOptions = OpenAiChatOptions.builder() .model(\u0026#34;gpt-3.5-turbo\u0026#34;) .temperature(0.4) .maxTokens(200) .build(); var chatModel = new OpenAiChatModel(this.openAiApi, this.openAiChatOptions); ChatResponse response = this.chatModel.call( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); // Or with streaming responses Flux\u0026lt;ChatResponse\u0026gt; response = this.chatModel.stream( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); OpenAiChatOptions提供聊天请求的配置信息。\n低级OpenAiApi客户端 # OpenAiApi为 OpenAI聊天API OpenAI Chat API提供了轻量级Java客户端。 下面的类图说明了OpenAiApi聊天接口和构建块： 下面是一个简单的片段，演示如何以编程方式使用API：\nOpenAiApi openAiApi = OpenAiApi.builder() .apiKey(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;)) .build(); ChatCompletionMessage chatCompletionMessage = new ChatCompletionMessage(\u0026#34;Hello world\u0026#34;, Role.USER); // Sync request ResponseEntity\u0026lt;ChatCompletion\u0026gt; response = this.openAiApi.chatCompletionEntity( new ChatCompletionRequest(List.of(this.chatCompletionMessage), \u0026#34;gpt-3.5-turbo\u0026#34;, 0.8, false)); // Streaming request Flux\u0026lt;ChatCompletionChunk\u0026gt; streamResponse = this.openAiApi.chatCompletionStream( new ChatCompletionRequest(List.of(this.chatCompletionMessage), \u0026#34;gpt-3.5-turbo\u0026#34;, 0.8, true)); 有关更多信息，请遵循OpenAiApi.java的JavaDoc。\n低级API示例 # java测试提供了一些如何使用轻量级库的一般示例。 OpenAiApiToolFunctionCallIT.java测试展示了如何使用低级API调用工具函数。 API密钥管理 # Spring AI通过ApiKey接口及其实现提供灵活的API密钥管理。默认实现SimpleApiKey适用于大多数用例，但您也可以为更复杂的场景创建自定义实现。\n默认配置 # 默认情况下，Spring Boot自动配置将使用Spring.ai.openai.API-key属性创建API密钥bean：\nspring.ai.openai.api-key=your-api-key-here 自定义API密钥配置 # 您可以使用构建器模式使用自己的ApiKey实现创建OpenAiApi的自定义实例：\nApiKey customApiKey = new ApiKey() { @Override public String getValue() { // Custom logic to retrieve API key return \u0026#34;your-api-key-here\u0026#34;; } }; OpenAiApi openAiApi = OpenAiApi.builder() .apiKey(customApiKey) .build(); // Create a chat client with the custom OpenAiApi instance OpenAiChatClient chatClient = new OpenAiChatClient(openAiApi); 当您需要：\n从安全密钥存储中检索API密钥 动态旋转API键 实现自定义API键选择逻辑 "},{"id":86,"href":"/docs/%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/qdrant%E8%B4%A8%E9%87%8F/","title":"Qdrant（质量）","section":"向量数据库","content":" Qdrant（质量） # 本节将指导您设置Qdrant VectorStore以存储文档嵌入并执行相似性搜索。 Qdrant是一个开源、高性能的向量搜索引擎/数据库。它使用HNSW（Hierarchical Navigable Small World）算法进行有效的k-NN搜索操作，并为基于元数据的查询提供高级过滤功能。\n前提条件 # Qdrant实例：按照Qdrant-文档中的安装说明设置Qdrant.实例。 如果需要，EmbeddingModel的API键，用于生成QdrantVectorStore存储的嵌入。 自动配置 # Spring AI为Qdrant Vector Store提供Spring Boot自动配置。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-qdrant\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-vector-store-qdrant\u0026#39; } 请查看矢量存储的 配置参数列表，以了解默认值和配置选项。 向量存储实现可以为您初始化所需的模式，但您必须通过在生成器中指定initializeSchema布尔值或通过设置…​在application.properties文件中初始化schema=true。 此外，您还需要一个已配置的EmbeddingModelbean。有关更多信息，请参阅EmbeddingModel部分。 现在，您可以在应用程序中自动将QdrantVectorStore连接为向量存储。\n@Autowired VectorStore vectorStore; // ... List\u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); // Add the documents to Qdrant vectorStore.add(documents); // Retrieve documents similar to a query List\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch(SearchRequest.builder().query(\u0026#34;Spring\u0026#34;).topK(5).build()); 配置属性 # 要连接到Qdrant并使用QdrantVectorStore，您需要提供实例的访问详细信息。\nspring: ai: vectorstore: qdrant: host: \u0026lt;qdrant host\u0026gt; port: \u0026lt;qdrant grpc port\u0026gt; api-key: \u0026lt;qdrant api key\u0026gt; collection-name: \u0026lt;collection name\u0026gt; use-tls: false initialize-schema: true 以spring.ai.vectorstore.qdrant.*开头的属性用于配置QdrantVectorStore:\n手动配置 # 您可以手动配置Qdrant向量存储，而不是使用SpringBoot自动配置。为此，您需要将spring ai qdrant存储添加到项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-qdrant-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-qdrant-store\u0026#39; } 创建Qdrant客户端bean：\n@Bean public QdrantClient qdrantClient() { QdrantGrpcClient.Builder grpcClientBuilder = QdrantGrpcClient.newBuilder( \u0026#34;\u0026lt;QDRANT_HOSTNAME\u0026gt;\u0026#34;, \u0026lt;QDRANT_GRPC_PORT\u0026gt;, \u0026lt;IS_TLS\u0026gt;); grpcClientBuilder.withApiKey(\u0026#34;\u0026lt;QDRANT_API_KEY\u0026gt;\u0026#34;); return new QdrantClient(grpcClientBuilder.build()); } 然后使用构建器模式创建QdrantVectorStore bean：\n@Bean public VectorStore vectorStore(QdrantClient qdrantClient, EmbeddingModel embeddingModel) { return QdrantVectorStore.builder(qdrantClient, embeddingModel) .collectionName(\u0026#34;custom-collection\u0026#34;) // Optional: defaults to \u0026#34;vector_store\u0026#34; .initializeSchema(true) // Optional: defaults to false .batchingStrategy(new TokenCountBatchingStrategy()) // Optional: defaults to TokenCountBatchingStrategy .build(); } // This can be any EmbeddingModel implementation @Bean public EmbeddingModel embeddingModel() { return new OpenAiEmbeddingModel(new OpenAiApi(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;))); } 元数据筛选 # 您也可以将通用的、可移植的元数据过滤器与Qdrant存储一起使用。 例如，可以使用文本表达式语言：\nvectorStore.similaritySearch( SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(\u0026#34;author in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; article_type == \u0026#39;blog\u0026#39;\u0026#34;).build()); 或以编程方式使用筛选器。表达式DSL:\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(b.and( b.in(\u0026#34;author\u0026#34;, \u0026#34;john\u0026#34;, \u0026#34;jill\u0026#34;), b.eq(\u0026#34;article_type\u0026#34;, \u0026#34;blog\u0026#34;)).build()).build()); 访问本机客户端 # Qdrant Vector Store实现通过getNativeClient（）方法提供对底层本机Qdrant客户端（QdrantClient）的访问：\nQdrantVectorStore vectorStore = context.getBean(QdrantVectorStore.class); Optional\u0026lt;QdrantClient\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { QdrantClient client = nativeClient.get(); // Use the native client for Qdrant-specific operations } 本机客户端允许您访问可能无法通过VectorStore界面公开的Qdrant特定功能和操作。\n"},{"id":87,"href":"/docs/%E6%8F%90%E7%A4%BA/","title":"提示","section":"Docs","content":" 提示 # 提示是引导人工智能模型生成特定输出的输入。 在与Spring AI中的AI模型的最低交互级别上，处理Spring人工智能中的提示有点类似于管理Spring MVC中的“视图”。 随着Spring AI的发展，它将引入更高级别的抽象来与AI模型交互。 提示的结构在人工智能领域中随着时间的推移而发展。\nAPI概述 # 提示 # 通常使用ChatModel的call（）方法，该方法接受Prompt实例并返回ChatResponse。 Prompt类充当一系列有组织的消息对象和请求ChatOptions的容器。 下面是Prompt类的截断版本，为了简洁起见，省略了构造函数和实用程序方法：\npublic class Prompt implements ModelRequest\u0026lt;List\u0026lt;Message\u0026gt;\u0026gt; { private final List\u0026lt;Message\u0026gt; messages; private ChatOptions chatOptions; } 消息 # 消息接口封装了提示文本内容、元数据属性的集合和名为MessageType的分类。 接口定义如下：\npublic interface Content { String getContent(); Map\u0026lt;String, Object\u0026gt; getMetadata(); } public interface Message extends Content { MessageType getMessageType(); } 多模式消息类型还实现了提供媒体内容对象列表的``MediaContent接口。\npublic interface MediaContent extends Content { Collection\u0026lt;Media\u0026gt; getMedia(); } 消息接口的各种实现对应于人工智能模型可以处理的不同类别的消息。 这些角色由MessageType有效地映射，如下所述。\n角色 # 每条消息都被分配了特定的角色。 主要作用是：\n系统角色：指导人工智能的行为和响应风格，设置人工智能如何解释和响应输入的参数或规则。这类似于在开始对话之前向人工智能提供指令。 用户角色：表示用户的输入–他们对人工智能的问题、命令或陈述。此角色是基本的，因为它构成了人工智能响应的基础。 助理角色：人工智能对用户输入的响应。 工具/功能角色：工具/功能Role专注于返回附加信息，以响应工具调用助手消息。 角色在Spring AI中表示为枚举，如下所示 public enum MessageType { USER(\u0026#34;user\u0026#34;), ASSISTANT(\u0026#34;assistant\u0026#34;), SYSTEM(\u0026#34;system\u0026#34;), TOOL(\u0026#34;tool\u0026#34;); ... } 提示模板 # Spring AI中提示模板化的一个关键组件是PromptTemplate类，旨在促进结构化提示的创建，然后将结构化提示发送到AI模型进行处理\npublic class PromptTemplate implements PromptTemplateActions, PromptTemplateMessageActions { // Other methods to be discussed later } 此类使用TemplateRenderer API来呈现模板。默认情况下，Spring AI使用StTemplateRenderer实现，该实现基于TerenceParr开发的开源StringTemplate引擎。模板变量由{}语法标识，但您可以配置分隔符以使用其他语法。\npublic interface TemplateRenderer extends BiFunction\u0026lt;String, Map\u0026lt;String, Object\u0026gt;, String\u0026gt; { @Override String apply(String template, Map\u0026lt;String, Object\u0026gt; variables); } Spring AI使用TemplateRenderer接口来处理变量到模板字符串中的实际替换。\nPromptTemplate promptTemplate = PromptTemplate.builder() .renderer(StTemplateRenderer.builder().startDelimiterToken(\u0026#39;\u0026lt;\u0026#39;).endDelimiterToken(\u0026#39;\u0026gt;\u0026#39;).build()) .template(\u0026#34;\u0026#34;\u0026#34; Tell me the names of 5 movies whose soundtrack was composed by \u0026lt;composer\u0026gt;. \u0026#34;\u0026#34;\u0026#34;) .build(); String prompt = promptTemplate.render(Map.of(\u0026#34;composer\u0026#34;, \u0026#34;John Williams\u0026#34;)); 此类实现的接口支持提示创建的不同方面： PromptTemplateStringActions专注于创建和呈现提示字符串，表示提示生成的最基本形式。 PromptTemplateMessageActions是为通过生成和操作消息对象来创建提示而定制的。 ``PromptTemplateActions旨在返回Prompt对象，该对象可以传递给ChatModel以生成响应。 虽然这些接口可能不会在许多项目中广泛使用，但它们显示了不同的提示创建方法。 实现的接口包括\npublic interface PromptTemplateStringActions { String render(); String render(Map\u0026lt;String, Object\u0026gt; model); } 方法String render（）：将提示模板呈现为最终的字符串格式，而无需外部输入，适用于没有占位符或动态内容的模板。 String render方法（Map\u0026lt;String，Object\u0026gt;model）：增强渲染功能以包括动态内容。它使用Map\u0026lt;String，Object\u0026gt;，其中映射键是提示模板中的占位符名称，值是要插入的动态内容。\npublic interface PromptTemplateMessageActions { Message createMessage(); Message createMessage(List\u0026lt;Media\u0026gt; mediaList); Message createMessage(Map\u0026lt;String, Object\u0026gt; model); } 方法Message createMessage（）：创建没有附加数据的Message对象，用于静态或预定义的消息内容。 Message createMessage（ListmediaList）方法：创建具有静态文本和媒体内容的Message对象。 方法Message createMessage（Map\u0026lt;String，Object\u0026gt;model）：扩展消息创建以集成动态内容，接受Map\u0026lt;String.Object\u0026gt;，其中每个条目表示消息模板中的占位符及其相应的动态值。\npublic interface PromptTemplateActions extends PromptTemplateStringActions { Prompt create(); Prompt create(ChatOptions modelOptions); Prompt create(Map\u0026lt;String, Object\u0026gt; model); Prompt create(Map\u0026lt;String, Object\u0026gt; model, ChatOptions modelOptions); } 方法Promptcreate（）：生成没有外部数据输入的Prompt对象，非常适合静态或预定义提示。 方法Promptcreate（ChatOptions modelOptions）：生成没有外部数据输入的Prompt对象，并具有聊天请求的特定选项。 方法Prompt create（Map\u0026lt;String，Object\u0026gt;model）：扩展提示创建功能以包括动态内容，采用Map\u0026lt;字符串，Object\u0026gt;，其中每个映射条目都是提示模板及其关联的动态值中的占位符。 方法Prompt create（Map\u0026lt;String，Object\u0026gt;model，ChatOptions modelOptions）：扩展提示创建功能以包括动态内容，采用Map\u0026lt;String.Object\u0026gt;，其中每个映射条目都是提示模板中的占位符及其关联的动态值，以及聊天请求的特定选项。\n示例用法 # 下面显示了一个来自PromptTemplates人工智能研讨会的简单示例。\nPromptTemplate promptTemplate = new PromptTemplate(\u0026#34;Tell me a {adjective} joke about {topic}\u0026#34;); Prompt prompt = promptTemplate.create(Map.of(\u0026#34;adjective\u0026#34;, adjective, \u0026#34;topic\u0026#34;, topic)); return chatModel.call(prompt).getResult(); 另一个来自 人工智能角色研讨会的例子如下所示。\nString userText = \u0026#34;\u0026#34;\u0026#34; Tell me about three famous pirates from the Golden Age of Piracy and why they did. Write at least a sentence for each pirate. \u0026#34;\u0026#34;\u0026#34;; Message userMessage = new UserMessage(userText); String systemText = \u0026#34;\u0026#34;\u0026#34; You are a helpful AI assistant that helps people find information. Your name is {name} You should reply to the user\u0026#39;s request with your name and also in the style of a {voice}. \u0026#34;\u0026#34;\u0026#34;; SystemPromptTemplate systemPromptTemplate = new SystemPromptTemplate(systemText); Message systemMessage = systemPromptTemplate.createMessage(Map.of(\u0026#34;name\u0026#34;, name, \u0026#34;voice\u0026#34;, voice)); Prompt prompt = new Prompt(List.of(userMessage, systemMessage)); List\u0026lt;Generation\u0026gt; response = chatModel.call(prompt).getResults(); 这显示了如何通过使用SystemPromptTemplate创建一个消息，系统角色传递占位符值来构建Prompt实例。\n使用自定义模板渲染器 # 通过实现TemplateRenderer接口并将其传递给PromptTemplate构造函数，可以使用自定义模板渲染器。您也可以继续使用默认的StTemplateRenderer，但需要自定义配置。 默认情况下，模板变量由{}语法标识。如果您计划在提示符中包含JSON，您可能希望使用不同的语法来避免与JSON语法冲突。例如，可以使用\u0026lt;和\u0026gt;分隔符。\nPromptTemplate promptTemplate = PromptTemplate.builder() .renderer(StTemplateRenderer.builder().startDelimiterToken(\u0026#39;\u0026lt;\u0026#39;).endDelimiterToken(\u0026#39;\u0026gt;\u0026#39;).build()) .template(\u0026#34;\u0026#34;\u0026#34; Tell me the names of 5 movies whose soundtrack was composed by \u0026lt;composer\u0026gt;. \u0026#34;\u0026#34;\u0026#34;) .build(); String prompt = promptTemplate.render(Map.of(\u0026#34;composer\u0026#34;, \u0026#34;John Williams\u0026#34;)); 使用资源而不是原始字符串 # Spring AI支持org.springframework.core.io.Resource``抽象，因此您可以将提示数据放在可以直接在PromptTemplate中使用的文件中。\n@Value(\u0026#34;classpath:/prompts/system-message.st\u0026#34;) private Resource systemResource; 然后将该资源直接传递给SystemPromptTemplate。\nSystemPromptTemplate systemPromptTemplate = new SystemPromptTemplate(systemResource); 即时工程（Prompt Engineering） # 在生成人工智能中，提示的创建是开发人员的一项关键任务。 分享和讨论提示是人工智能社区的常见做法。 这一领域的研究通常涉及分析和比较不同的提示，以评估它们在各种情况下的有效性。 掌握最有效地使用提示，特别是随着人工智能技术的快速发展，是一项持续的挑战。\n创建有效的提示 # 在开发提示时，重要的是集成几个关键组件，以确保清晰和有效：\n说明：向人工智能提供明确和直接的指示，类似于你如何与人沟通。这种清晰性对于帮助人工智能“理解”预期是至关重要的。 外部背景：必要时，包括人工智能响应的相关背景信息或具体指导。这种“外部上下文”框架了提示，并帮助人工智能掌握整个场景。 用户输入：这是简单的部分-用户的直接请求或问题构成提示的核心。 输出指示器：这方面可能很棘手。它涉及为AI的响应指定所需的格式，如JSON。然而，请注意，人工智能可能并不总是严格遵守这种格式。例如，它可能会在实际的JSON数据之前添加“here is your JSON”这样的短语，或者有时生成不准确的类似JSON的结构。 在制作提示时，为人工智能提供预期问答格式的示例可能非常有益。 以下是供进一步调查的资源列表。 简单的技术 # 文本摘要： 问题解答： 文本分类： 对话： 代码生成： 先进的技术 # 零射击，少射击学习： 思维链： 重新行动（原因+行动）： Microsoft指南 # 快速创建和优化框架： 代币 # 代币对于人工智能模型处理文本的方式至关重要，充当一座桥梁，将单词（如我们理解的那样）转换为人工智能模型可以处理的格式。 标记化是将文本分解为标记的过程，是人工智能模型理解和处理语言的基础。 为了更好地理解标记，请将它们视为单词的一部分。通常，标记表示大约四分之三的单词。例如，莎士比亚的全部作品，总计约90万字，将转化为约120万个代币。 尝试OpenAI标记器UI，看看单词如何转换为标记。 代币在人工智能处理中的技术作用之外还有实际意义，特别是在计费和模型功能方面：\n计费：AI模型服务通常基于令牌使用计费。输入（提示）和输出（响应）都有助于令牌总数，使较短的提示更具成本效益。 模型限制：不同的人工智能模型具有不同的令牌限制，定义了它们的“上下文窗口”——一次可以处理的最大信息量。例如，GPT-3的限制是4K代币，而其他模型（如Claude 2和Meta Llama 2）的限制是100K代币。一些研究模型可以处理多达100万个代币。 上下文窗口：模型的标记限制决定其上下文窗口。超过此限制的输入不由模型处理。只发送用于处理的最小有效信息集是至关重要的。例如，当询问“哈姆雷特”时，没有必要包括莎士比亚所有其他作品中的象征。 响应元数据：来自AI模型的响应的元数据包括所使用的令牌的数量，这是管理使用和成本的重要信息。 "},{"id":88,"href":"/docs/%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/redisredis/","title":"Redis（Redis）","section":"向量数据库","content":" Redis（Redis） # 本节将指导您设置RedisVectorStore以存储文档嵌入并执行相似性搜索。 Redis是一个开源（BSD许可）的内存中数据结构存储，用作数据库、缓存、消息代理和流引擎。Redis提供数据结构，如字符串、哈希、列表、集、带范围查询的排序集、位图、超日志、地理空间索引和流。 Redis Search and Query扩展了Redis OSS的核心功能，允许您将Redis用作向量数据库：\n前提条件 # 自动配置 # Spring AI为Redis Vector Store提供Spring Boot自动配置。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-redis\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-vector-store-redis\u0026#39; } 向量存储实现可以为您初始化必要的模式，但您必须通过在适当的构造函数中指定initializeSchema布尔值或通过设置…​在application.properties文件中初始化schema=true。 请查看矢量存储的 配置参数列表，以了解默认值和配置选项。 此外，您还需要一个已配置的EmbeddingModelbean。有关更多信息，请参阅EmbeddingModel部分。 现在，您可以将RedisVectorStore自动连接为应用程序中的向量存储。\n@Autowired VectorStore vectorStore; // ... List \u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); // Add the documents to Redis vectorStore.add(documents); // Retrieve documents similar to a query List\u0026lt;Document\u0026gt; results = this.vectorStore.similaritySearch(SearchRequest.builder().query(\u0026#34;Spring\u0026#34;).topK(5).build()); 配置属性 # 要连接到Redis并使用RedisVectorStore，您需要提供实例的访问详细信息。\nspring: data: redis: url: \u0026lt;redis instance url\u0026gt; ai: vectorstore: redis: initialize-schema: true index-name: custom-index prefix: custom-prefix 对于redis连接配置，或者，可以通过Spring Boot的application.properties提供简单的配置。\nspring.data.redis.host=localhost spring.data.redis.port=6379 spring.data.redis.username=default spring.data.redis.password= 以spring.ai.vectorstore.redis.*开头的属性用于配置RedisVectorStore:\n元数据筛选 # 您也可以在Redis中利用通用的、可移植的元数据过滤器。 例如，可以使用文本表达式语言：\nvectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(\u0026#34;country in [\u0026#39;UK\u0026#39;, \u0026#39;NL\u0026#39;] \u0026amp;\u0026amp; year \u0026gt;= 2020\u0026#34;).build()); 或以编程方式使用筛选器。表达式DSL:\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(b.and( b.in(\u0026#34;country\u0026#34;, \u0026#34;UK\u0026#34;, \u0026#34;NL\u0026#34;), b.gte(\u0026#34;year\u0026#34;, 2020)).build()).build()); 例如，此可移植筛选器表达式：\ncountry in [\u0026#39;UK\u0026#39;, \u0026#39;NL\u0026#39;] \u0026amp;\u0026amp; year \u0026gt;= 2020 转换为专有的Redis筛选器格式：\n@country:{UK | NL} @year:[2020 inf] 手动配置 # 您可以手动配置Redis向量存储，而不是使用Spring Boot自动配置。为此，您需要将spring ai redis商店添加到项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-redis-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-redis-store\u0026#39; } 创建JedisPooled bean：\n@Bean public JedisPooled jedisPooled() { return new JedisPooled(\u0026#34;\u0026lt;host\u0026gt;\u0026#34;, 6379); } 然后使用构建器模式创建RedisVectorStore bean：\n@Bean public VectorStore vectorStore(JedisPooled jedisPooled, EmbeddingModel embeddingModel) { return RedisVectorStore.builder(jedisPooled, embeddingModel) .indexName(\u0026#34;custom-index\u0026#34;) // Optional: defaults to \u0026#34;spring-ai-index\u0026#34; .prefix(\u0026#34;custom-prefix\u0026#34;) // Optional: defaults to \u0026#34;embedding:\u0026#34; .metadataFields( // Optional: define metadata fields for filtering MetadataField.tag(\u0026#34;country\u0026#34;), MetadataField.numeric(\u0026#34;year\u0026#34;)) .initializeSchema(true) // Optional: defaults to false .batchingStrategy(new TokenCountBatchingStrategy()) // Optional: defaults to TokenCountBatchingStrategy .build(); } // This can be any EmbeddingModel implementation @Bean public EmbeddingModel embeddingModel() { return new OpenAiEmbeddingModel(new OpenAiApi(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;))); } 访问本机客户端 # Redis Vector Store实现通过getNativeClient（）方法提供对底层本机Redis客户端（JedisPooled）的访问：\nRedisVectorStore vectorStore = context.getBean(RedisVectorStore.class); Optional\u0026lt;JedisPooled\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { JedisPooled jedis = nativeClient.get(); // Use the native client for Redis-specific operations } 本机客户端允许您访问Redis特定的功能和操作，这些功能和操作可能不会通过VectorStore界面公开。\n"},{"id":89,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/%E5%8D%83%E5%B8%86/","title":"千帆聊天","section":"聊天模型API","content":" 千帆聊天 # 此功能已移至Spring AI社区存储库。 有关最新版本，请访问github.com/spring-ai-community/chanfan。\n"},{"id":90,"href":"/docs/%E5%8D%B3%E6%97%B6%E5%B7%A5%E7%A8%8B%E6%A8%A1%E5%BC%8F/","title":"即时工程模式","section":"Docs","content":" 即时工程模式 # 基于全面的PromptEngineering指南的Prompt Engineering技术的实际实现。\n1.配置 # 配置部分概述了如何使用SpringAI设置和调优大型语言模型（LLM）。\nLLM提供程序选择 # 对于即时工程，您将从选择模型开始。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-anthropic\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 可以这样指定LLM模型名称：\n.options(ChatOptions.builder() .model(\u0026#34;claude-3-7-sonnet-latest\u0026#34;) // Use Anthropic\u0026#39;s Claude model .build()) 在 参考文档中查找有关启用每个模型的详细信息。\nLLM输出配置 # 在我们深入研究即时工程技术之前，有必要了解如何配置LLM的输出行为。Spring AI提供了几个配置选项，允许您通过ChatOptions构建器控制生成的各个方面。 所有配置都可以以编程方式应用，如下面的示例所示，或者在启动时通过Spring应用程序属性应用。\n温度 # 温度控制模型响应的随机性或“创造性”。\n较低的值（0.0-0.3）：更确定、更集中的响应。更适合于一致性至关重要的事实问题、分类或任务。 中等值（0.4-0.7）：在决定论和创造性之间平衡。适用于一般用例。 更高的值（0.8-1.0）：更具创造性、多样性和潜在的令人惊讶的响应。更适合创意写作、头脑风暴或产生多样的选择。 .options(ChatOptions.builder() .temperature(0.1) // Very deterministic output .build()) 理解温度对于快速工程至关重要，因为不同的技术受益于不同的温度设置。\n输出长度（MaxTokens） # maxTokens参数限制模型在其响应中可以生成的标记（单词片段）数量。\n低值（5-25）：用于单个单词、短短语或分类标签。 中等值（50-500）：用于段落或简短解释。 高值（1000+）：用于长格式内容、故事或复杂解释。 .options(ChatOptions.builder() .maxTokens(250) // Medium-length response .build()) 设置适当的输出长度对于确保获得完整的响应而没有不必要的冗长非常重要。\n采样控制（Top-K和Top-P） # 这些参数使您能够在生成期间对令牌选择过程进行细粒度控制。\nTop-K：将标记选择限制为K个最可能的下一个标记。较高的值（例如，40-50）引入更多的多样性。 Top-P（核采样）：从累积概率超过P的最小令牌集中动态选择。0.8-0.95这样的值很常见。 .options(ChatOptions.builder() .topK(40) // Consider only the top 40 tokens .topP(0.8) // Sample from tokens that cover 80% of probability mass .build()) 这些采样控制与温度-形状响应特性一起工作。\n结构化响应格式 # 除了纯文本响应（使用.content（））之外，SpringAI还可以使用.entity（）方法轻松地将LLM响应直接映射到Java对象。\nenum Sentiment { POSITIVE, NEUTRAL, NEGATIVE } Sentiment result = chatClient.prompt(\u0026#34;...\u0026#34;) .call() .entity(Sentiment.class); 当与指示模型返回结构化数据的系统提示相结合时，该功能特别强大。\n型号特定选项 # 虽然便携式ChatOptions在不同的LLM提供商之间提供了一致的接口，但SpringAI还提供了模型特定的选项类，这些类公开了提供商特定的功能和配置。这些特定于模型的选项允许您利用每个LLM提供程序的独特功能。\n// Using OpenAI-specific options OpenAiChatOptions openAiOptions = OpenAiChatOptions.builder() .model(\u0026#34;gpt-4o\u0026#34;) .temperature(0.2) .frequencyPenalty(0.5) // OpenAI-specific parameter .presencePenalty(0.3) // OpenAI-specific parameter .responseFormat(new ResponseFormat(\u0026#34;json_object\u0026#34;)) // OpenAI-specific JSON mode .seed(42) // OpenAI-specific deterministic generation .build(); String result = chatClient.prompt(\u0026#34;...\u0026#34;) .options(openAiOptions) .call() .content(); // Using Anthropic-specific options AnthropicChatOptions anthropicOptions = AnthropicChatOptions.builder() .model(\u0026#34;claude-3-7-sonnet-latest\u0026#34;) .temperature(0.2) .topK(40) // Anthropic-specific parameter .thinking(AnthropicApi.ThinkingType.ENABLED, 1000) // Anthropic-specific thinking configuration .build(); String result = chatClient.prompt(\u0026#34;...\u0026#34;) .options(anthropicOptions) .call() .content(); 每个模型提供程序都有自己的聊天选项实现（例如，OpenAiChatOptions、AnthropicChatOptions、MistralAiChatOption），这些选项公开特定于提供程序的参数，同时仍然实现通用接口。当您需要访问特定提供程序的独特功能时，此方法为您提供了使用可移植选项实现跨提供程序兼容性或特定于模型的选项的灵活性。 请注意，当使用特定于模型的选项时，代码将绑定到该特定的提供程序，从而降低可移植性。这是在访问特定于提供商的高级功能与在应用程序中维护提供商独立性之间的权衡。\n2.即时工程技术 # 下面的每个部分都实现了指南中的特定提示工程技术。\n2.1零炮提示 # 零触发提示涉及要求人工智能在不提供任何示例的情况下执行任务。这种方法测试模型从头开始理解和执行指令的能力。大型语言模型在大量文本语料库上训练，使它们能够在没有明确演示的情况下理解“翻译”、“总结”或“分类”等任务所需的内容。 Zero shot非常适合于简单的任务，其中模型可能在训练期间看到类似的示例，并且您希望最小化提示长度。然而，性能可能会因任务复杂性和指令的制定情况而异。\n// Implementation of Section 2.1: General prompting / zero shot (page 15) public void pt_zero_shot(ChatClient chatClient) { enum Sentiment { POSITIVE, NEUTRAL, NEGATIVE } Sentiment reviewSentiment = chatClient.prompt(\u0026#34;\u0026#34;\u0026#34; Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE. Review: \u0026#34;Her\u0026#34; is a disturbing study revealing the direction humanity is headed if AI is allowed to keep evolving, unchecked. I wish there were more movies like this masterpiece. Sentiment: \u0026#34;\u0026#34;\u0026#34;) .options(ChatOptions.builder() .model(\u0026#34;claude-3-7-sonnet-latest\u0026#34;) .temperature(0.1) .maxTokens(5) .build()) .call() .entity(Sentiment.class); System.out.println(\u0026#34;Output: \u0026#34; + reviewSentiment); } 此示例演示如何在不提供示例的情况下对电影评论情绪进行分类。请注意，低温（0.1）用于更具确定性的结果，以及直接.实体（Sentiment.class）映射到Java枚举。 参考文献：Brown，T.B.等人（2020）。“语言模型是很少的临时学习者。”arXiv:2005.14165。 https://arxiv.org/abs/2005.14165\n2.2单发和少发提示 # 少镜头提示为模型提供了一个或多个示例来帮助指导其响应，对于需要特定输出格式的任务特别有用。通过显示所需输入-输出对的模型示例，它可以学习模式，并将其应用于新的输入，而无需显式的参数更新。 一杆提供单个示例，当示例成本高昂或模式相对简单时，该示例非常有用。少数镜头使用多个示例（通常为3-5）来帮助模型更好地理解更复杂任务中的模式，或说明正确输出的不同变体。\n// Implementation of Section 2.2: One-shot \u0026amp; few-shot (page 16) public void pt_one_shot_few_shots(ChatClient chatClient) { String pizzaOrder = chatClient.prompt(\u0026#34;\u0026#34;\u0026#34; Parse a customer\u0026#39;s pizza order into valid JSON EXAMPLE 1: I want a small pizza with cheese, tomato sauce, and pepperoni. JSON Response: ``` { \u0026#34;size\u0026#34;: \u0026#34;small\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;normal\u0026#34;, \u0026#34;ingredients\u0026#34;: [\u0026#34;cheese\u0026#34;, \u0026#34;tomato sauce\u0026#34;, \u0026#34;pepperoni\u0026#34;] } ``` EXAMPLE 2: Can I get a large pizza with tomato sauce, basil and mozzarella. JSON Response: ``` { \u0026#34;size\u0026#34;: \u0026#34;large\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;normal\u0026#34;, \u0026#34;ingredients\u0026#34;: [\u0026#34;tomato sauce\u0026#34;, \u0026#34;basil\u0026#34;, \u0026#34;mozzarella\u0026#34;] } ``` Now, I would like a large pizza, with the first half cheese and mozzarella. And the other tomato sauce, ham and pineapple. \u0026#34;\u0026#34;\u0026#34;) .options(ChatOptions.builder() .model(\u0026#34;claude-3-7-sonnet-latest\u0026#34;) .temperature(0.1) .maxTokens(250) .build()) .call() .content(); } 对于需要特定格式、处理边界情况的任务，或者在没有示例的情况下任务定义可能不明确的情况下，很少触发提示特别有效。示例的质量和多样性显著影响性能。 参考文献：Brown，T.B.等人（2020）。“语言模型是很少的临时学习者。”arXiv:2005.14165。 https://arxiv.org/abs/2005.14165\n2.3系统、上下文和角色提示 # 系统提示 # 系统提示设置语言模型的总体上下文和目的，定义模型应该做什么的“大局”。它为模型的响应建立行为框架、约束和高级目标，与特定的用户查询分离。 系统提示在整个对话中充当持久的“任务陈述”，允许您设置全局参数，如输出格式、音调、道德边界或角色定义。与专注于特定任务的用户提示不同，系统提示确定了应如何解释所有用户提示。\n// Implementation of Section 2.3.1: System prompting public void pt_system_prompting_1(ChatClient chatClient) { String movieReview = chatClient .prompt() .system(\u0026#34;Classify movie reviews as positive, neutral or negative. Only return the label in uppercase.\u0026#34;) .user(\u0026#34;\u0026#34;\u0026#34; Review: \u0026#34;Her\u0026#34; is a disturbing study revealing the direction humanity is headed if AI is allowed to keep evolving, unchecked. It\u0026#39;s so disturbing I couldn\u0026#39;t watch it. Sentiment: \u0026#34;\u0026#34;\u0026#34;) .options(ChatOptions.builder() .model(\u0026#34;claude-3-7-sonnet-latest\u0026#34;) .temperature(1.0) .topK(40) .topP(0.8) .maxTokens(5) .build()) .call() .content(); } 当与Spring AI的实体映射功能相结合时，系统提示特别强大：\n// Implementation of Section 2.3.1: System prompting with JSON output record MovieReviews(Movie[] movie_reviews) { enum Sentiment { POSITIVE, NEUTRAL, NEGATIVE } record Movie(Sentiment sentiment, String name) { } } MovieReviews movieReviews = chatClient .prompt() .system(\u0026#34;\u0026#34;\u0026#34; Classify movie reviews as positive, neutral or negative. Return valid JSON. \u0026#34;\u0026#34;\u0026#34;) .user(\u0026#34;\u0026#34;\u0026#34; Review: \u0026#34;Her\u0026#34; is a disturbing study revealing the direction humanity is headed if AI is allowed to keep evolving, unchecked. It\u0026#39;s so disturbing I couldn\u0026#39;t watch it. JSON Response: \u0026#34;\u0026#34;\u0026#34;) .call() .entity(MovieReviews.class); 系统提示对于多回合对话、确保多个查询之间的一致行为以及建立应应用于所有响应的格式约束（如JSON输出）特别有用。 参考：OpenAI。（2022年）。“系统消息。” https://platform.openai.com/docs/guides/chat/introduction\n角色提示 # 角色提示指示模型采用特定的角色或角色，这会影响其生成内容的方式。通过为模型分配特定的标识、专业知识或视角，可以影响其响应的样式、音调、深度和框架。 角色提示利用模型模拟不同专业领域和通信风格的能力。常见角色包括专家（例如，“你是一名经验丰富的数据科学家”）、专业人员（例如，”充当导游“）或文体角色（例如，\u0026lsquo;像莎士比亚一样解释\u0026rsquo;）。\n// Implementation of Section 2.3.2: Role prompting public void pt_role_prompting_1(ChatClient chatClient) { String travelSuggestions = chatClient .prompt() .system(\u0026#34;\u0026#34;\u0026#34; I want you to act as a travel guide. I will write to you about my location and you will suggest 3 places to visit near me. In some cases, I will also give you the type of places I will visit. \u0026#34;\u0026#34;\u0026#34;) .user(\u0026#34;\u0026#34;\u0026#34; My suggestion: \u0026#34;I am in Amsterdam and I want to visit only museums.\u0026#34; Travel Suggestions: \u0026#34;\u0026#34;\u0026#34;) .call() .content(); } 可以使用样式说明来增强角色提示：\n// Implementation of Section 2.3.2: Role prompting with style instructions public void pt_role_prompting_2(ChatClient chatClient) { String humorousTravelSuggestions = chatClient .prompt() .system(\u0026#34;\u0026#34;\u0026#34; I want you to act as a travel guide. I will write to you about my location and you will suggest 3 places to visit near me in a humorous style. \u0026#34;\u0026#34;\u0026#34;) .user(\u0026#34;\u0026#34;\u0026#34; My suggestion: \u0026#34;I am in Amsterdam and I want to visit only museums.\u0026#34; Travel Suggestions: \u0026#34;\u0026#34;\u0026#34;) .call() .content(); } 该技术对于专门的领域知识特别有效，在响应中实现一致的基调，并与用户创建更具吸引力的个性化交互。 参考文献：Shanahan，M.等人（2023）。“大型语言模型的角色扮演。”arXiv:2305.16367。 https://arxiv.org/abs/2305.16367\n上下文提示 # 上下文提示通过传递上下文参数为模型提供额外的背景信息。该技术丰富了模型对特定情况的理解，在不干扰主指令的情况下实现更相关和定制的响应。 通过提供上下文信息，可以帮助模型理解与当前查询相关的特定域、受众、约束或背景事实。这导致更准确、相关和适当的框架响应。\n// Implementation of Section 2.3.3: Contextual prompting public void pt_contextual_prompting(ChatClient chatClient) { String articleSuggestions = chatClient .prompt() .user(u -\u0026gt; u.text(\u0026#34;\u0026#34;\u0026#34; Suggest 3 topics to write an article about with a few lines of description of what this article should contain. Context: {context} \u0026#34;\u0026#34;\u0026#34;) .param(\u0026#34;context\u0026#34;, \u0026#34;You are writing for a blog about retro 80\u0026#39;s arcade video games.\u0026#34;)) .call() .content(); } Spring AI通过param（）方法注入上下文变量来清除上下文提示。当模型需要特定的领域知识时，当将响应适应特定的受众或场景时，以及当确保响应与特定的约束或要求相一致时，该技术特别有价值。 参考文献：Liu，P.等人（2021）。“什么是GPT-3的良好上下文示例？”arXiv:2101.06804。 https://arxiv.org/abs/2101.06804\n2.4后退提示 # 后退提示通过首先获取背景知识将复杂的请求分解为简单的步骤。该技术鼓励模型首先从直接问题“后退”，以在处理特定查询之前考虑与问题相关的更广泛的上下文、基本原则或一般知识。 通过将复杂问题分解为更易于管理的组件，并首先建立基础知识，该模型可以为困难的问题提供更准确的响应。\n// Implementation of Section 2.4: Step-back prompting public void pt_step_back_prompting(ChatClient.Builder chatClientBuilder) { // Set common options for the chat client var chatClient = chatClientBuilder .defaultOptions(ChatOptions.builder() .model(\u0026#34;claude-3-7-sonnet-latest\u0026#34;) .temperature(1.0) .topK(40) .topP(0.8) .maxTokens(1024) .build()) .build(); // First get high-level concepts String stepBack = chatClient .prompt(\u0026#34;\u0026#34;\u0026#34; Based on popular first-person shooter action games, what are 5 fictional key settings that contribute to a challenging and engaging level storyline in a first-person shooter video game? \u0026#34;\u0026#34;\u0026#34;) .call() .content(); // Then use those concepts in the main task String story = chatClient .prompt() .user(u -\u0026gt; u.text(\u0026#34;\u0026#34;\u0026#34; Write a one paragraph storyline for a new level of a first- person shooter video game that is challenging and engaging. Context: {step-back} \u0026#34;\u0026#34;\u0026#34;) .param(\u0026#34;step-back\u0026#34;, stepBack)) .call() .content(); } 后退提示对于复杂的推理任务、需要专门领域知识的问题，以及当您想要更全面和周到的响应而不是直接的答案时，特别有效。 参考文献：Zheng，Z.等人（2023）。“后退一步：在大型语言模型中通过抽象激发推理”。arXiv:2310.06117。 https://arxiv.org/abs/2310.06117\n2.5思想链（CoT） # 思维链提示鼓励模型在问题中逐步推理，这提高了复杂推理任务的准确性。通过明确要求模型显示其工作或以逻辑步骤思考问题，可以显著提高需要多步骤推理的任务的性能。 CoT的工作原理是鼓励模型在产生最终答案之前生成中间推理步骤，类似于人类解决复杂问题的方式。这使得模型的思维过程明确，并有助于它得出更准确的结论。\n// Implementation of Section 2.5: Chain of Thought (CoT) - Zero-shot approach public void pt_chain_of_thought_zero_shot(ChatClient chatClient) { String output = chatClient .prompt(\u0026#34;\u0026#34;\u0026#34; When I was 3 years old, my partner was 3 times my age. Now, I am 20 years old. How old is my partner? Let\u0026#39;s think step by step. \u0026#34;\u0026#34;\u0026#34;) .call() .content(); } // Implementation of Section 2.5: Chain of Thought (CoT) - Few-shot approach public void pt_chain_of_thought_singleshot_fewshots(ChatClient chatClient) { String output = chatClient .prompt(\u0026#34;\u0026#34;\u0026#34; Q: When my brother was 2 years old, I was double his age. Now I am 40 years old. How old is my brother? Let\u0026#39;s think step by step. A: When my brother was 2 years, I was 2 * 2 = 4 years old. That\u0026#39;s an age difference of 2 years and I am older. Now I am 40 years old, so my brother is 40 - 2 = 38 years old. The answer is 38. Q: When I was 3 years old, my partner was 3 times my age. Now, I am 20 years old. How old is my partner? Let\u0026#39;s think step by step. A: \u0026#34;\u0026#34;\u0026#34;) .call() .content(); } 关键短语“让我们一步一步地思考”触发了模型，以显示其推理过程。CoT对于数学问题、逻辑推理任务和任何需要多步骤推理的问题特别有价值。它通过明确中间推理来帮助减少错误。 参考文献：Wei，J.等人（2022）。“思维链提示在大型语言模型中引发推理”，arXiv:2201.11903。 https://arxiv.org/abs/2201.11903\n2.6自一致性 # 自我一致性涉及多次运行模型，并聚合结果以获得更可靠的答案。该技术通过对同一问题的不同推理路径进行采样，并通过多数投票选择最一致的答案，来解决LLM输出的可变性。 通过生成具有不同温度或采样设置的多条推理路径，然后聚合最终答案，自一致性提高了复杂推理任务的准确性。它本质上是LLM输出的集成方法。\n// Implementation of Section 2.6: Self-consistency public void pt_self_consistency(ChatClient chatClient) { String email = \u0026#34;\u0026#34;\u0026#34; Hi, I have seen you use Wordpress for your website. A great open source content management system. I have used it in the past too. It comes with lots of great user plugins. And it\u0026#39;s pretty easy to set up. I did notice a bug in the contact form, which happens when you select the name field. See the attached screenshot of me entering text in the name field. Notice the JavaScript alert box that I inv0k3d. But for the rest it\u0026#39;s a great website. I enjoy reading it. Feel free to leave the bug in the website, because it gives me more interesting things to read. Cheers, Harry the Hacker. \u0026#34;\u0026#34;\u0026#34;; record EmailClassification(Classification classification, String reasoning) { enum Classification { IMPORTANT, NOT_IMPORTANT } } int importantCount = 0; int notImportantCount = 0; // Run the model 5 times with the same input for (int i = 0; i \u0026lt; 5; i++) { EmailClassification output = chatClient .prompt() .user(u -\u0026gt; u.text(\u0026#34;\u0026#34;\u0026#34; Email: {email} Classify the above email as IMPORTANT or NOT IMPORTANT. Let\u0026#39;s think step by step and explain why. \u0026#34;\u0026#34;\u0026#34;) .param(\u0026#34;email\u0026#34;, email)) .options(ChatOptions.builder() .temperature(1.0) // Higher temperature for more variation .build()) .call() .entity(EmailClassification.class); // Count results if (output.classification() == EmailClassification.Classification.IMPORTANT) { importantCount++; } else { notImportantCount++; } } // Determine the final classification by majority vote String finalClassification = importantCount \u0026gt; notImportantCount ? \u0026#34;IMPORTANT\u0026#34; : \u0026#34;NOT IMPORTANT\u0026#34;; } 自我一致性对于高风险决策、复杂的推理任务，以及当您需要比单个响应所能提供的更自信的答案时，特别有价值。代价是由于多个API调用而增加了计算成本和延迟。 参考文献：Wang，X.，et al.（2022）。“自我一致性改善了语言模型中的思维链推理”，arXiv:2203.11171。 https://arxiv.org/abs/2203.11171\n2.7思想树（ToT） # 思想树（ToT）是一种高级推理框架，它通过同时探索多个推理路径来扩展思想链。它将问题解决视为一个搜索过程，其中模型生成不同的中间步骤，评估其承诺，并探索最有希望的路径。 对于具有多个可能方法的复杂问题，或者当解决方案需要在找到最佳路径之前探索各种备选方案时，该技术特别强大。 游戏解决ToT示例：\n// Implementation of Section 2.7: Tree of Thoughts (ToT) - Game solving example public void pt_tree_of_thoughts_game(ChatClient chatClient) { // Step 1: Generate multiple initial moves String initialMoves = chatClient .prompt(\u0026#34;\u0026#34;\u0026#34; You are playing a game of chess. The board is in the starting position. Generate 3 different possible opening moves. For each move: 1. Describe the move in algebraic notation 2. Explain the strategic thinking behind this move 3. Rate the move\u0026#39;s strength from 1-10 \u0026#34;\u0026#34;\u0026#34;) .options(ChatOptions.builder() .temperature(0.7) .build()) .call() .content(); // Step 2: Evaluate and select the most promising move String bestMove = chatClient .prompt() .user(u -\u0026gt; u.text(\u0026#34;\u0026#34;\u0026#34; Analyze these opening moves and select the strongest one: {moves} Explain your reasoning step by step, considering: 1. Position control 2. Development potential 3. Long-term strategic advantage Then select the single best move. \u0026#34;\u0026#34;\u0026#34;).param(\u0026#34;moves\u0026#34;, initialMoves)) .call() .content(); // Step 3: Explore future game states from the best move String gameProjection = chatClient .prompt() .user(u -\u0026gt; u.text(\u0026#34;\u0026#34;\u0026#34; Based on this selected opening move: {best_move} Project the next 3 moves for both players. For each potential branch: 1. Describe the move and counter-move 2. Evaluate the resulting position 3. Identify the most promising continuation Finally, determine the most advantageous sequence of moves. \u0026#34;\u0026#34;\u0026#34;).param(\u0026#34;best_move\u0026#34;, bestMove)) .call() .content(); } 参考文献：Yao，S.等人（2023）。思想树：用大型语言模型精心解决问题。arXiv:2305.10601。 https://arxiv.org/abs/2305.10601\n2.8自动提示工程 # 自动提示工程（Automatic Prompt Engineering）使用人工智能来生成和评估备选提示。这种元技术利用语言模型本身来创建、改进和基准测试不同的提示变体，以找到特定任务的最佳公式。 通过系统地生成和评估提示变量，APE可以找到比手动工程更有效的提示，特别是对于复杂任务。这是一种使用人工智能来提高自身性能的方法。\n// Implementation of Section 2.8: Automatic Prompt Engineering public void pt_automatic_prompt_engineering(ChatClient chatClient) { // Generate variants of the same request String orderVariants = chatClient .prompt(\u0026#34;\u0026#34;\u0026#34; We have a band merchandise t-shirt webshop, and to train a chatbot we need various ways to order: \u0026#34;One Metallica t-shirt size S\u0026#34;. Generate 10 variants, with the same semantics but keep the same meaning. \u0026#34;\u0026#34;\u0026#34;) .options(ChatOptions.builder() .temperature(1.0) // High temperature for creativity .build()) .call() .content(); // Evaluate and select the best variant String output = chatClient .prompt() .user(u -\u0026gt; u.text(\u0026#34;\u0026#34;\u0026#34; Please perform BLEU (Bilingual Evaluation Understudy) evaluation on the following variants: ---- {variants} ---- Select the instruction candidate with the highest evaluation score. \u0026#34;\u0026#34;\u0026#34;).param(\u0026#34;variants\u0026#34;, orderVariants)) .call() .content(); } APE对于优化生产系统的提示、解决手动提示工程已达到其极限的具有挑战性的任务以及在规模上系统地提高提示质量特别有价值。 参考文献：Zhou，Y.等人（2022）。“大型语言模型是人类级别的即时工程师。”arXiv:2211.01910。 https://arxiv.org/abs/2211.01910\n2.9代码提示 # 代码提示是指用于代码相关任务的专门技术。这些技术利用LLM理解和生成编程语言的能力，使它们能够编写新代码、解释现有代码、调试问题和在语言之间进行转换。 有效的代码提示通常涉及明确的规范、适当的上下文（库、框架、样式指南），有时还包括类似代码的示例。对于更确定的输出，温度设置通常较低（0.1-0.3）。\n// Implementation of Section 2.9.1: Prompts for writing code public void pt_code_prompting_writing_code(ChatClient chatClient) { String bashScript = chatClient .prompt(\u0026#34;\u0026#34;\u0026#34; Write a code snippet in Bash, which asks for a folder name. Then it takes the contents of the folder and renames all the files inside by prepending the name draft to the file name. \u0026#34;\u0026#34;\u0026#34;) .options(ChatOptions.builder() .temperature(0.1) // Low temperature for deterministic code .build()) .call() .content(); } // Implementation of Section 2.9.2: Prompts for explaining code public void pt_code_prompting_explaining_code(ChatClient chatClient) { String code = \u0026#34;\u0026#34;\u0026#34; #!/bin/bash echo \u0026#34;Enter the folder name: \u0026#34; read folder_name if [ ! -d \u0026#34;$folder_name\u0026#34; ]; then echo \u0026#34;Folder does not exist.\u0026#34; exit 1 fi files=( \u0026#34;$folder_name\u0026#34;/* ) for file in \u0026#34;${files[@]}\u0026#34;; do new_file_name=\u0026#34;draft_$(basename \u0026#34;$file\u0026#34;)\u0026#34; mv \u0026#34;$file\u0026#34; \u0026#34;$new_file_name\u0026#34; done echo \u0026#34;Files renamed successfully.\u0026#34; \u0026#34;\u0026#34;\u0026#34;; String explanation = chatClient .prompt() .user(u -\u0026gt; u.text(\u0026#34;\u0026#34;\u0026#34; Explain to me the below Bash code: ``` {code} ``` \u0026#34;\u0026#34;\u0026#34;).param(\u0026#34;code\u0026#34;, code)) .call() .content(); } // Implementation of Section 2.9.3: Prompts for translating code public void pt_code_prompting_translating_code(ChatClient chatClient) { String bashCode = \u0026#34;\u0026#34;\u0026#34; #!/bin/bash echo \u0026#34;Enter the folder name: \u0026#34; read folder_name if [ ! -d \u0026#34;$folder_name\u0026#34; ]; then echo \u0026#34;Folder does not exist.\u0026#34; exit 1 fi files=( \u0026#34;$folder_name\u0026#34;/* ) for file in \u0026#34;${files[@]}\u0026#34;; do new_file_name=\u0026#34;draft_$(basename \u0026#34;$file\u0026#34;)\u0026#34; mv \u0026#34;$file\u0026#34; \u0026#34;$new_file_name\u0026#34; done echo \u0026#34;Files renamed successfully.\u0026#34; \u0026#34;\u0026#34;\u0026#34;; String pythonCode = chatClient .prompt() .user(u -\u0026gt; u.text(\u0026#34;\u0026#34;\u0026#34; Translate the below Bash code to a Python snippet: {code} \u0026#34;\u0026#34;\u0026#34;).param(\u0026#34;code\u0026#34;, bashCode)) .call() .content(); } 代码提示对于自动代码文档、原型制作、学习编程概念和在编程语言之间进行翻译特别有价值。通过将其与少量提示或思维链等技术相结合，可以进一步提高效率。 参考文献：Chen，M.等人（2021）。“评估在代码上训练的大型语言模型。”arXiv:2107.03374。 https://arxiv.org/abs/2107.03374\n结论 # SpringAI提供了一个优雅的JavaAPI，用于实现所有主要的提示工程技术。通过将这些技术与Spring强大的实体映射和流畅的API相结合，开发人员可以用干净、可维护的代码构建复杂的人工智能应用程序。 最有效的方法通常涉及结合多种技术——例如，使用系统提示和少量示例，或思维链和角色提示。Spring AI的灵活API使这些组合易于实现。 对于生产应用程序，请记住： 有了这些技术和Spring AI的强大抽象，您可以创建强大的人工智能驱动应用程序，提供一致、高质量的结果。\n参考文献 # "},{"id":91,"href":"/docs/docker%E4%BD%9C%E6%9B%B2/","title":"Docker作曲","section":"Docs","content":" Docker作曲 # Spring AI提供Spring Boot自动配置，用于建立与模型服务的连接 或到Gradle build.Gradle构建文件。\n服务连接 # spring ai spring boot docker compose模块中提供了以下服务连接工厂：\n"},{"id":92,"href":"/docs/%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/sap%E9%9F%A9%E4%BA%9A/","title":"SAP HANA云","section":"向量数据库","content":" SAP HANA云 # 前提条件 # 您需要SAP HANA云向量引擎帐户-参考SAP HANA Cloud向量引擎-提供试用帐户指南以创建试用帐户。 如果需要，EmbeddingModel的API键，用于生成向量存储区存储的嵌入。 自动配置 # Spring AI不为SAP Hana矢量存储提供专用模块。 请查看矢量存储的 配置参数列表，以了解默认值和配置选项。 此外，您还需要一个已配置的EmbeddingModelbean。有关更多信息，请参阅EmbeddingModel部分。\nHanaCloudVectorStore属性 # 您可以在SpringBoot配置中使用以下属性来定制SAPHana向量存储。\n构建示例RAG应用程序 # 演示如何设置使用SAP Hana Cloud作为向量数据库并利用OpenAI实现RAG模式的项目\n在SAP Hana DB中创建表CRICETE_WORLD_CUP： 在pom.xml中添加以下依赖项 您可以将属性spring ai版本设置为1.0.0-SNAPSHOT\u0026lt;/spring ai-version\u0026gt;： \u0026lt;dependencyManagement\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-bom\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${spring-ai-version}\u0026lt;/version\u0026gt; \u0026lt;type\u0026gt;pom\u0026lt;/type\u0026gt; \u0026lt;scope\u0026gt;import\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/dependencyManagement\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-pdf-document-reader\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-hana\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lombok\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.18.30\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; 在application.properties文件中添加以下属性： 创建名为CricketWorldCup的实体类，该类从HanaVectorEntity扩展： # package com.interviewpedia.spring.ai.hana; import jakarta.persistence.Column; import jakarta.persistence.Entity; import jakarta.persistence.Table; import lombok.Data; import lombok.NoArgsConstructor; import lombok.extern.jackson.Jacksonized; import org.springframework.ai.vectorstore.hanadb.HanaVectorEntity; @Entity @Table(name = \u0026#34;CRICKET_WORLD_CUP\u0026#34;) @Data @Jacksonized @NoArgsConstructor public class CricketWorldCup extends HanaVectorEntity { @Column(name = \u0026#34;content\u0026#34;) private String content; } 创建名为CricketWorldCupRepository的存储库，该存储库实现HanaVectorRepository: package com.interviewpedia.spring.ai.hana; import jakarta.persistence.EntityManager; import jakarta.persistence.PersistenceContext; import jakarta.transaction.Transactional; import org.springframework.ai.vectorstore.hanadb.HanaVectorRepository; import org.springframework.stereotype.Repository; import java.util.List; @Repository public class CricketWorldCupRepository implements HanaVectorRepository\u0026lt;CricketWorldCup\u0026gt; { @PersistenceContext private EntityManager entityManager; @Override @Transactional public void save(String tableName, String id, String embedding, String content) { String sql = String.format(\u0026#34;\u0026#34;\u0026#34; INSERT INTO %s (_ID, EMBEDDING, CONTENT) VALUES(:_id, TO_REAL_VECTOR(:embedding), :content) \u0026#34;\u0026#34;\u0026#34;, tableName); this.entityManager.createNativeQuery(sql) .setParameter(\u0026#34;_id\u0026#34;, id) .setParameter(\u0026#34;embedding\u0026#34;, embedding) .setParameter(\u0026#34;content\u0026#34;, content) .executeUpdate(); } @Override @Transactional public int deleteEmbeddingsById(String tableName, List\u0026lt;String\u0026gt; idList) { String sql = String.format(\u0026#34;\u0026#34;\u0026#34; DELETE FROM %s WHERE _ID IN (:ids) \u0026#34;\u0026#34;\u0026#34;, tableName); return this.entityManager.createNativeQuery(sql) .setParameter(\u0026#34;ids\u0026#34;, idList) .executeUpdate(); } @Override @Transactional public int deleteAllEmbeddings(String tableName) { String sql = String.format(\u0026#34;\u0026#34;\u0026#34; DELETE FROM %s \u0026#34;\u0026#34;\u0026#34;, tableName); return this.entityManager.createNativeQuery(sql).executeUpdate(); } @Override public List\u0026lt;CricketWorldCup\u0026gt; cosineSimilaritySearch(String tableName, int topK, String queryEmbedding) { String sql = String.format(\u0026#34;\u0026#34;\u0026#34; SELECT TOP :topK * FROM %s ORDER BY COSINE_SIMILARITY(EMBEDDING, TO_REAL_VECTOR(:queryEmbedding)) DESC \u0026#34;\u0026#34;\u0026#34;, tableName); return this.entityManager.createNativeQuery(sql, CricketWorldCup.class) .setParameter(\u0026#34;topK\u0026#34;, topK) .setParameter(\u0026#34;queryEmbedding\u0026#34;, queryEmbedding) .getResultList(); } } 现在，创建一个REST控制器类CricketWorldCupHanaController，并将autowire ChatModel和VectorStore作为依赖项 /ai/hana向量存储/板球世界杯/清除嵌入-从向量存储中清除所有嵌入 /ai/hana矢量存储/板球世界杯/upload-上传cricket_world_cup.pdf，以便其数据作为嵌入式存储在SAP hana Cloud vector DB中 /ai/hana矢量存储/板球世界杯-在SAP hana DB中使用Cosine_Similarity实现RAG package com.interviewpedia.spring.ai.hana; import lombok.extern.slf4j.Slf4j; import org.springframework.ai.chat.model.ChatModel; import org.springframework.ai.chat.messages.UserMessage; import org.springframework.ai.chat.prompt.Prompt; import org.springframework.ai.chat.prompt.SystemPromptTemplate; import org.springframework.ai.document.Document; import org.springframework.ai.reader.pdf.PagePdfDocumentReader; import org.springframework.ai.transformer.splitter.TokenTextSplitter; import org.springframework.ai.vectorstore.hanadb.HanaCloudVectorStore; import org.springframework.ai.vectorstore.VectorStore; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.core.io.Resource; import org.springframework.http.ResponseEntity; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.PostMapping; import org.springframework.web.bind.annotation.RequestParam; import org.springframework.web.bind.annotation.RestController; import org.springframework.web.multipart.MultipartFile; import java.io.IOException; import java.util.List; import java.util.Map; import java.util.function.Function; import java.util.function.Supplier; import java.util.stream.Collectors; @RestController @Slf4j public class CricketWorldCupHanaController { private final VectorStore hanaCloudVectorStore; private final ChatModel chatModel; @Autowired public CricketWorldCupHanaController(ChatModel chatModel, VectorStore hanaCloudVectorStore) { this.chatModel = chatModel; this.hanaCloudVectorStore = hanaCloudVectorStore; } @PostMapping(\u0026#34;/ai/hana-vector-store/cricket-world-cup/purge-embeddings\u0026#34;) public ResponseEntity\u0026lt;String\u0026gt; purgeEmbeddings() { int deleteCount = ((HanaCloudVectorStore) this.hanaCloudVectorStore).purgeEmbeddings(); log.info(\u0026#34;{} embeddings purged from CRICKET_WORLD_CUP table in Hana DB\u0026#34;, deleteCount); return ResponseEntity.ok().body(String.format(\u0026#34;%d embeddings purged from CRICKET_WORLD_CUP table in Hana DB\u0026#34;, deleteCount)); } @PostMapping(\u0026#34;/ai/hana-vector-store/cricket-world-cup/upload\u0026#34;) public ResponseEntity\u0026lt;String\u0026gt; handleFileUpload(@RequestParam(\u0026#34;pdf\u0026#34;) MultipartFile file) throws IOException { Resource pdf = file.getResource(); Supplier\u0026lt;List\u0026lt;Document\u0026gt;\u0026gt; reader = new PagePdfDocumentReader(pdf); Function\u0026lt;List\u0026lt;Document\u0026gt;, List\u0026lt;Document\u0026gt;\u0026gt; splitter = new TokenTextSplitter(); List\u0026lt;Document\u0026gt; documents = splitter.apply(reader.get()); log.info(\u0026#34;{} documents created from pdf file: {}\u0026#34;, documents.size(), pdf.getFilename()); this.hanaCloudVectorStore.accept(documents); return ResponseEntity.ok().body(String.format(\u0026#34;%d documents created from pdf file: %s\u0026#34;, documents.size(), pdf.getFilename())); } @GetMapping(\u0026#34;/ai/hana-vector-store/cricket-world-cup\u0026#34;) public Map\u0026lt;String, String\u0026gt; hanaVectorStoreSearch(@RequestParam(value = \u0026#34;message\u0026#34;) String message) { var documents = this.hanaCloudVectorStore.similaritySearch(message); var inlined = documents.stream().map(Document::getText).collect(Collectors.joining(System.lineSeparator())); var similarDocsMessage = new SystemPromptTemplate(\u0026#34;Based on the following: {documents}\u0026#34;) .createMessage(Map.of(\u0026#34;documents\u0026#34;, inlined)); var userMessage = new UserMessage(message); Prompt prompt = new Prompt(List.of(similarDocsMessage, userMessage)); String generation = this.chatModel.call(prompt).getResult().getOutput().getContent(); log.info(\u0026#34;Generation: {}\u0026#34;, generation); return Map.of(\u0026#34;generation\u0026#34;, generation); } } 由于HanaDB向量存储支持不提供自动配置模块，因此您还需要在应用程序中提供向量存储bean，如下所示，作为示例。\n@Bean public VectorStore hanaCloudVectorStore(CricketWorldCupRepository cricketWorldCupRepository, EmbeddingModel embeddingModel) { return HanaCloudVectorStore.builder(cricketWorldCupRepository, embeddingModel) .tableName(\u0026#34;CRICKET_WORLD_CUP\u0026#34;) .topK(1) .build(); } 使用维基百科中的上下文pdf文件 转到 维基百科，以PDF文件的形式下载板球世界杯页面。 使用我们在上一步中创建的文件上传REST端点上传该PDF文件。 "},{"id":93,"href":"/docs/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B/%E6%99%BA%E6%B5%A6ai/","title":"智浦AI聊天","section":"聊天模型API","content":" 智浦AI聊天 # Spring AI支持智浦AI的各种人工智能语言模型。您可以与智浦人工智能语言模块交互，并基于智浦人工模型创建多语言对话助手。\n前提条件 # 您需要使用ZhiPuAI创建API来访问ZhiPu AI语言模型。 在智浦AI注册页面创建账户，并在API Keys页面上生成代币。\nexport SPRING_AI_ZHIPU_AI_API_KEY=\u0026lt;INSERT KEY HERE\u0026gt; 添加存储库和BOM表 # Spring AI工件发布在Maven Central和Spring Snapshot 存储库中。 为了帮助进行依赖关系管理，Spring AI提供了BOM（物料清单），以确保在整个项目中使用一致版本的Spring AI。请参阅依赖项管理部分，将Spring AI BOM添加到构建系统中。\n自动配置 # Spring AI为智普AI聊天客户端提供Spring Boot自动配置。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-zhipuai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-zhipuai\u0026#39; } 聊天室属性 # 重试属性 # 前缀spring.ai.retry用作属性前缀，允许您为ZhiPu ai聊天模型配置重试机制。\n连接属性 # 前缀spring.ai.zhiPu用作允许连接到ZhiPuAI的属性前缀。\n配置属性 # 前缀spring.ai.zhipuai.chat是属性前缀，用于配置zhipuai的聊天模型实现。\n运行时选项 # ZhiPuAiChatOptions.java提供模型配置，如要使用的模型、温度、频率惩罚等。 启动时，可以使用ZhiPuAiChatModel（api，options）构造函数或spring.ai.zhipuai.chat.options.*属性配置默认选项。 在运行时，可以通过向Prompt调用添加新的特定于请求的选项来覆盖默认选项。\nChatResponse response = chatModel.call( new Prompt( \u0026#34;Generate the names of 5 famous pirates.\u0026#34;, ZhiPuAiChatOptions.builder() .model(ZhiPuAiApi.ChatModel.GLM_3_Turbo.getValue()) .temperature(0.5) .build() )); 样本控制器 # 创建一个新的SpringBoot项目，并将Spring-ai-starter模型Zipuai添加到pom（或gradle）依赖项中。 在src/main/resources目录下添加application.properties文件，以启用和配置ZhiPuAi聊天模型：\nspring.ai.zhipuai.api-key=YOUR_API_KEY spring.ai.zhipuai.chat.options.model=glm-4-air spring.ai.zhipuai.chat.options.temperature=0.7 这将创建一个ZhiPuAiChatModel实现，您可以将其注入到类中。\n@RestController public class ChatController { private final ZhiPuAiChatModel chatModel; @Autowired public ChatController(ZhiPuAiChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, this.chatModel.call(message)); } @GetMapping(\u0026#34;/ai/generateStream\u0026#34;) public Flux\u0026lt;ChatResponse\u0026gt; generateStream(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { var prompt = new Prompt(new UserMessage(message)); return this.chatModel.stream(prompt); } } 手动配置 # ZhiPuAiChatModel实现了ChatModels和StreamingChatModel.并使用 低级ZhiPuAiApi客户端连接到ZhiPuAI服务。 将spring ai zhipuai依赖项添加到项目的Maven pom.xml文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-zhipuai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-zhipuai\u0026#39; } 接下来，创建ZhiPuAiChatModel并将其用于文本生成：\nvar zhiPuAiApi = new ZhiPuAiApi(System.getenv(\u0026#34;ZHIPU_AI_API_KEY\u0026#34;)); var chatModel = new ZhiPuAiChatModel(this.zhiPuAiApi, ZhiPuAiChatOptions.builder() .model(ZhiPuAiApi.ChatModel.GLM_3_Turbo.getValue()) .temperature(0.4) .maxTokens(200) .build()); ChatResponse response = this.chatModel.call( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); // Or with streaming responses Flux\u0026lt;ChatResponse\u0026gt; streamResponse = this.chatModel.stream( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); ZhiPuAiChatOptions为聊天请求提供配置信息。\n低级ZhiPuAiApi客户端 # ZhiPuAiApi为ZhiPu AI API提供的是轻量级Java客户端。 下面是如何以编程方式使用api的简单片段：\nZhiPuAiApi zhiPuAiApi = new ZhiPuAiApi(System.getenv(\u0026#34;ZHIPU_AI_API_KEY\u0026#34;)); ChatCompletionMessage chatCompletionMessage = new ChatCompletionMessage(\u0026#34;Hello world\u0026#34;, Role.USER); // Sync request ResponseEntity\u0026lt;ChatCompletion\u0026gt; response = this.zhiPuAiApi.chatCompletionEntity( new ChatCompletionRequest(List.of(this.chatCompletionMessage), ZhiPuAiApi.ChatModel.GLM_3_Turbo.getValue(), 0.7, false)); // Streaming request Flux\u0026lt;ChatCompletionChunk\u0026gt; streamResponse = this.zhiPuAiApi.chatCompletionStream( new ChatCompletionRequest(List.of(this.chatCompletionMessage), ZhiPuAiApi.ChatModel.GLM_3_Turbo.getValue(), 0.7, true)); 有关更多信息，请遵循ZhiPuAiApi.java的JavaDoc。\nZhiPuAiApi样品 # java测试提供了一些如何使用轻量级库的通用示例。 "},{"id":94,"href":"/docs/%E6%B5%8B%E8%AF%95%E5%AE%B9%E5%99%A8/","title":"测试容器","section":"Docs","content":" 测试容器 # Spring AI提供Spring Boot自动配置，用于建立与模型服务的连接 或到Gradle build.Gradle构建文件。\n服务连接 # spring ai spring boot testcontainers模块中提供了以下服务连接工厂：\n"},{"id":95,"href":"/docs/%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/%E7%B1%BB%E5%9E%8B%E6%84%9Ftypesense/","title":"类型感（Typesense）","section":"向量数据库","content":" 类型感（Typesense） # 本节将指导您设置TypesenseVectorStore以存储文档嵌入并执行相似性搜索。 Typesense是一个开源的容忍打字错误的搜索引擎，为即时低于50毫秒的搜索进行了优化，同时提供了直观的开发人员体验。它提供向量搜索功能，允许您在常规搜索数据的同时存储和查询高维向量。\n前提条件 # 正在运行的Typesense实例。以下选项可用： Typesense云（推荐） Docker image typesense/typesense:最新 如果需要，EmbeddingModel的API键，用于生成TypesenseVectorStore存储的嵌入。 自动配置 # Spring AI为Typesense Vector Store提供Spring Boot自动配置。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-typesense\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-vector-store-typesense\u0026#39; } 请查看矢量存储的 配置参数列表，以了解默认值和配置选项。 向量存储实现可以为您初始化必要的架构，但您必须通过设置…​在application.properties文件中初始化schema=true。 此外，您还需要一个已配置的EmbeddingModelbean。有关更多信息，请参阅EmbeddingModel部分。 现在，您可以在应用程序中自动将TypesenseVectorStore连接为向量存储：\n@Autowired VectorStore vectorStore; // ... List\u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); // Add the documents to Typesense vectorStore.add(documents); // Retrieve documents similar to a query List\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch(SearchRequest.builder().query(\u0026#34;Spring\u0026#34;).topK(5).build()); 配置属性 # 要连接到Typesense并使用TypesenseVectorStore，您需要提供实例的访问详细信息。\nspring: ai: vectorstore: typesense: initialize-schema: true collection-name: vector_store embedding-dimension: 1536 client: protocol: http host: localhost port: 8108 api-key: xyz 以spring.ai.vectorstore.typesense.*开头的属性用于配置TypesenseVectorStore:\n手动配置 # 您可以手动配置Typesense向量存储，而不是使用Spring Boot自动配置。为此，您需要将spring ai typesense存储添加到项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-typesense-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-typesense-store\u0026#39; } 创建Typesense客户端bean：\n@Bean public Client typesenseClient() { List\u0026lt;Node\u0026gt; nodes = new ArrayList\u0026lt;\u0026gt;(); nodes.add(new Node(\u0026#34;http\u0026#34;, \u0026#34;localhost\u0026#34;, \u0026#34;8108\u0026#34;)); Configuration configuration = new Configuration(nodes, Duration.ofSeconds(5), \u0026#34;xyz\u0026#34;); return new Client(configuration); } 然后使用构建器模式创建TypesenseVectorStore bean：\n@Bean public VectorStore vectorStore(Client client, EmbeddingModel embeddingModel) { return TypesenseVectorStore.builder(client, embeddingModel) .collectionName(\u0026#34;custom_vectors\u0026#34;) // Optional: defaults to \u0026#34;vector_store\u0026#34; .embeddingDimension(1536) // Optional: defaults to 1536 .initializeSchema(true) // Optional: defaults to false .batchingStrategy(new TokenCountBatchingStrategy()) // Optional: defaults to TokenCountBatchingStrategy .build(); } // This can be any EmbeddingModel implementation @Bean public EmbeddingModel embeddingModel() { return new OpenAiEmbeddingModel(new OpenAiApi(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;))); } 元数据筛选 # 您也可以将通用可移植元数据过滤器与Typesense存储一起使用。 例如，可以使用文本表达式语言：\nvectorStore.similaritySearch( SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(\u0026#34;country in [\u0026#39;UK\u0026#39;, \u0026#39;NL\u0026#39;] \u0026amp;\u0026amp; year \u0026gt;= 2020\u0026#34;).build()); 或以编程方式使用筛选器。表达式DSL:\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(b.and( b.in(\u0026#34;country\u0026#34;, \u0026#34;UK\u0026#34;, \u0026#34;NL\u0026#34;), b.gte(\u0026#34;year\u0026#34;, 2020)).build()).build()); 例如，此可移植筛选器表达式：\ncountry in [\u0026#39;UK\u0026#39;, \u0026#39;NL\u0026#39;] \u0026amp;\u0026amp; year \u0026gt;= 2020 转换为专有的Typesense筛选器格式：\ncountry: [\u0026#39;UK\u0026#39;, \u0026#39;NL\u0026#39;] \u0026amp;\u0026amp; year: \u0026gt;=2020 访问本机客户端 # Typesense Vector Store实现通过getNativeClient（）方法提供对底层本机Typesense客户端（客户端）的访问：\nTypesenseVectorStore vectorStore = context.getBean(TypesenseVectorStore.class); Optional\u0026lt;Client\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { Client client = nativeClient.get(); // Use the native client for Typesense-specific operations } 本机客户端允许您访问可能无法通过VectorStore界面公开的Typesense特定功能和操作。\n"},{"id":96,"href":"/docs/%E4%BA%91%E7%BB%91%E5%AE%9Acloud-bindings/","title":"云绑定（Cloud Bindings）","section":"Docs","content":" 云绑定（Cloud Bindings） # Spring AI基于Spring云绑定中的基础为云绑定提供支持。 例如，使用OpenAi时，绑定类型为OpenAi。 要启用云绑定支持，请在应用程序中包含以下依赖项。 或到Gradle build.Gradle构建文件。\n可用的云绑定 # 以下是spring ai spring clou绑定模块中当前提供云绑定支持的组件：\n"},{"id":97,"href":"/docs/%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%BC%B1%E5%8C%96weaviate/","title":"弱化（Weaviate）","section":"向量数据库","content":" 弱化（Weaviate） # 本节将指导您设置Weaviate VectorStore以存储文档嵌入并执行相似性搜索。 Weaviate是一个开源向量数据库，允许您存储来自您喜爱的ML模型的数据对象和向量嵌入，并无缝扩展到数十亿个数据对象。\n前提条件 # 正在运行的Weaviate实例。以下选项可用： 弱化云服务（需要帐户创建和API密钥） Docker容器 如果需要，EmbeddingModel的API键，用于生成WeaviateVectorStore存储的嵌入。 依赖关系 # 将Weaviate Vector Store依赖项添加到项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-weaviate-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-weaviate-store\u0026#39; } 配置 # 要连接到Weaviate并使用WeaviateVectorStore，您需要提供实例的访问详细信息。\nspring.ai.vectorstore.weaviate.host=\u0026lt;host_of_your_weaviate_instance\u0026gt; spring.ai.vectorstore.weaviate.scheme=\u0026lt;http_or_https\u0026gt; spring.ai.vectorstore.weaviate.api-key=\u0026lt;your_api_key\u0026gt; # API key if needed, e.g. OpenAI spring.ai.openai.api-key=\u0026lt;api-key\u0026gt; 环境变量，\nexport SPRING_AI_VECTORSTORE_WEAVIATE_HOST=\u0026lt;host_of_your_weaviate_instance\u0026gt; export SPRING_AI_VECTORSTORE_WEAVIATE_SCHEME=\u0026lt;http_or_https\u0026gt; export SPRING_AI_VECTORSTORE_WEAVIATE_API_KEY=\u0026lt;your_api_key\u0026gt; # API key if needed, e.g. OpenAI export SPRING_AI_OPENAI_API_KEY=\u0026lt;api-key\u0026gt; 或者可以是这些的混合。\n自动配置 # Spring AI为Weaviate Vector Store提供Spring Boot自动配置。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-weaviate\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-vector-store-weaviate\u0026#39; } 请查看矢量存储的 配置参数列表，以了解默认值和配置选项。 此外，您还需要一个已配置的EmbeddingModelbean。有关更多信息，请参阅EmbeddingModel部分。 下面是所需bean的示例：\n@Bean public EmbeddingModel embeddingModel() { // Can be any other Embeddingmodel implementation. return new OpenAiEmbeddingModel(new OpenAiApi(System.getenv(\u0026#34;SPRING_AI_OPENAI_API_KEY\u0026#34;))); } 现在，您可以在应用程序中自动关联WeaviateVectorStore作为向量存储。\n手动配置 # 您可以使用构建器模式手动配置WeaviateVectorStore，而不是使用Spring Boot自动配置：\n@Bean public WeaviateClient weaviateClient() { return new WeaviateClient(new Config(\u0026#34;http\u0026#34;, \u0026#34;localhost:8080\u0026#34;)); } @Bean public VectorStore vectorStore(WeaviateClient weaviateClient, EmbeddingModel embeddingModel) { return WeaviateVectorStore.builder(weaviateClient, embeddingModel) .objectClass(\u0026#34;CustomClass\u0026#34;) // Optional: defaults to \u0026#34;SpringAiWeaviate\u0026#34; .consistencyLevel(ConsistentLevel.QUORUM) // Optional: defaults to ConsistentLevel.ONE .filterMetadataFields(List.of( // Optional: fields that can be used in filters MetadataField.text(\u0026#34;country\u0026#34;), MetadataField.number(\u0026#34;year\u0026#34;))) .build(); } 元数据筛选 # 您也可以将通用的、可移植的元数据过滤器与Weaviate存储一起使用。 例如，可以使用文本表达式语言：\nvectorStore.similaritySearch( SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(\u0026#34;country in [\u0026#39;UK\u0026#39;, \u0026#39;NL\u0026#39;] \u0026amp;\u0026amp; year \u0026gt;= 2020\u0026#34;).build()); 或以编程方式使用筛选器。表达式DSL:\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(b.and( b.in(\u0026#34;country\u0026#34;, \u0026#34;UK\u0026#34;, \u0026#34;NL\u0026#34;), b.gte(\u0026#34;year\u0026#34;, 2020)).build()).build()); 例如，此可移植筛选器表达式：\ncountry in [\u0026#39;UK\u0026#39;, \u0026#39;NL\u0026#39;] \u0026amp;\u0026amp; year \u0026gt;= 2020 转换为专有的Weaviate GraphQL过滤器格式：\noperator: And operands: [{ operator: Or operands: [{ path: [\u0026#34;meta_country\u0026#34;] operator: Equal valueText: \u0026#34;UK\u0026#34; }, { path: [\u0026#34;meta_country\u0026#34;] operator: Equal valueText: \u0026#34;NL\u0026#34; }] }, { path: [\u0026#34;meta_year\u0026#34;] operator: GreaterThanEqual valueNumber: 2020 }] 在Docker中运行Weaviate # 要快速开始使用本地Weaviate实例，可以在Docker中运行它：\ndocker run -it --rm --name weaviate \\ -e AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED=true \\ -e PERSISTENCE_DATA_PATH=/var/lib/weaviate \\ -e QUERY_DEFAULTS_LIMIT=25 \\ -e DEFAULT_VECTORIZER_MODULE=none \\ -e CLUSTER_HOSTNAME=node1 \\ -p 8080:8080 \\ semitechnologies/weaviate:1.22.4 这将启动可在localhost:8080访问的Weaviate实例。\nWeaviateVectorStore属性 # 您可以在SpringBoot配置中使用以下属性来定制Weaviate向量存储。\n访问本机客户端 # Weaviate Vector Store实现通过getNativeClient（）方法提供对底层本机Weaviade客户端（WeaviateClient）的访问：\nWeaviateVectorStore vectorStore = context.getBean(WeaviateVectorStore.class); Optional\u0026lt;WeaviateClient\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { WeaviateClient client = nativeClient.get(); // Use the native client for Weaviate-specific operations } 本机客户端允许您访问可能无法通过VectorStore界面公开的Weaviate特定功能和操作。\n"},{"id":98,"href":"/docs/%E7%9B%B8%E5%B9%B2cohere/","title":"OCI GenAI Cohere聊天","section":"Docs","content":" OCI GenAI Cohere聊天 # OCI GenAI服务提供与按需模型或专用AI集群的生成AI聊天。 OCI聊天模型页面和OCI Generative AI Playground提供了有关在OCI上使用和托管聊天模型的详细信息。\n前提条件 # 您需要一个活动的 Oracle云基础架构（OCI）帐户才能使用OCI GenAI Cohere Chat客户端。客户端提供四种不同的连接方式，包括使用用户和私钥的简单身份验证、工作负载标识、实例主体或OCI配置文件身份验证。\n添加存储库和BOM表 # Spring AI工件发布在Maven Central和Spring Snapshot 存储库中。 为了帮助进行依赖关系管理，Spring AI提供了BOM（物料清单），以确保在整个项目中使用一致版本的Spring AI。请参阅依赖项管理部分，将Spring AI BOM添加到构建系统中。\n自动配置 # Spring AI为OCI GenAI Cohere Chat Client提供Spring Boot自动配置。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-oci-genai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-oci-genai\u0026#39; } 聊天室属性 # 连接属性 # 前缀spring.ai.oci.genai是配置与oci genai的连接的属性前缀。\n配置属性 # 前缀spring.ai.oci.genai.chat.cohere是为oci genai cohere chat配置ChatModel实现的属性前缀。\n运行时选项 # OCICohereChatOptions.java提供模型配置，例如要使用的模型、温度、频率惩罚等。 启动时，可以使用OCICohereChatModel（api，options）构造函数或spring.ai.oci.genai.chat.cohere.options.*属性配置默认选项。 在运行时，可以通过向Prompt调用添加新的特定于请求的选项来覆盖默认选项。\nChatResponse response = chatModel.call( new Prompt( \u0026#34;Generate the names of 5 famous pirates.\u0026#34;, OCICohereChatOptions.builder() .model(\u0026#34;my-model-ocid\u0026#34;) .compartment(\u0026#34;my-compartment-ocid\u0026#34;) .temperature(0.5) .build() )); 样本控制器 # 创建一个新的SpringBoot项目，并将Spring-aistarter模型oci-genai添加到pom（或gradle）依赖项中。 在src/main/resources目录下添加application.properties文件，以启用和配置OCI GenAI Cohere聊天模型：\nspring.ai.oci.genai.authenticationType=file spring.ai.oci.genai.file=/path/to/oci/config/file spring.ai.oci.genai.cohere.chat.options.compartment=my-compartment-ocid spring.ai.oci.genai.cohere.chat.options.servingMode=on-demand spring.ai.oci.genai.cohere.chat.options.model=my-chat-model-ocid 这将创建一个OCICohereChatModel实现，您可以将其注入到类中。\n@RestController public class ChatController { private final OCICohereChatModel chatModel; @Autowired public ChatController(OCICohereChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, chatModel.call(message)); } @GetMapping(\u0026#34;/ai/generateStream\u0026#34;) public Flux\u0026lt;ChatResponse\u0026gt; generateStream(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { var prompt = new Prompt(new UserMessage(message)); return chatModel.stream(prompt); } } 手动配置 # OCICohereChatModel实现ChatModel.并使用OCI Java SDK连接到OCI GenAI服务。 将spring ai oci genai依赖项添加到项目的Maven pom.xml文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-oci-genai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-oci-genai\u0026#39; } 接下来，创建OCICohereChatModel并将其用于文本生成：\nvar CONFIG_FILE = Paths.get(System.getProperty(\u0026#34;user.home\u0026#34;), \u0026#34;.oci\u0026#34;, \u0026#34;config\u0026#34;).toString(); var COMPARTMENT_ID = System.getenv(\u0026#34;OCI_COMPARTMENT_ID\u0026#34;); var MODEL_ID = System.getenv(\u0026#34;OCI_CHAT_MODEL_ID\u0026#34;); ConfigFileAuthenticationDetailsProvider authProvider = new ConfigFileAuthenticationDetailsProvider( CONFIG_FILE, \u0026#34;DEFAULT\u0026#34; ); var genAi = GenerativeAiInferenceClient.builder() .region(Region.valueOf(\u0026#34;us-chicago-1\u0026#34;)) .build(authProvider); var chatModel = new OCICohereChatModel(genAi, OCICohereChatOptions.builder() .model(MODEL_ID) .compartment(COMPARTMENT_ID) .servingMode(\u0026#34;on-demand\u0026#34;) .build()); ChatResponse response = chatModel.call( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); OCICohereChatOptions提供聊天请求的配置信息。\n"},{"id":99,"href":"/docs/%E6%96%87%E6%9C%AC%E5%B5%8C%E5%85%A5/","title":"谷歌VertexAI文本嵌入","section":"Docs","content":" 谷歌VertexAI文本嵌入 # Vertex AI支持两种类型的嵌入模型，文本和多模态。 Vertex AI文本嵌入API使用密集向量表示。\n前提条件 # 安装适用于您的操作系统的gcloud CLI。 通过运行以下命令进行身份验证。 gcloud config set project \u0026lt;PROJECT_ID\u0026gt; \u0026amp;\u0026amp; gcloud auth application-default login \u0026lt;ACCOUNT\u0026gt; 添加存储库和BOM表 # Spring AI工件发布在Maven Central和Spring Snapshot 存储库中。 为了帮助进行依赖关系管理，Spring AI提供了BOM（物料清单），以确保在整个项目中使用一致版本的Spring AI。请参阅依赖项管理部分，将Spring AI BOM添加到构建系统中。\n自动配置 # Spring AI为VertexAI嵌入模型提供Spring Boot自动配置。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-vertex-ai-embedding\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-vertex-ai-embedding\u0026#39; } 嵌入属性 # 前缀spring.ai.vertex.ai.embedding用作允许连接到VertexAI嵌入API的属性前缀。 前缀spring.ai.vertex.ai.embedding.text是属性前缀，用于为VertexAI文本嵌入配置嵌入模型实现。\n样本控制器 # 创建一个新的SpringBoot项目，并将Spring-ai-starter模型顶点ai嵌入到pom（或gradle）依赖项中。 在src/main/resources目录下添加application.properties文件，以启用和配置VertexAi聊天模型：\nspring.ai.vertex.ai.embedding.project-id=\u0026lt;YOUR_PROJECT_ID\u0026gt; spring.ai.vertex.ai.embedding.location=\u0026lt;YOUR_PROJECT_LOCATION\u0026gt; spring.ai.vertex.ai.embedding.text.options.model=text-embedding-004 这将创建一个VertexAiTextEmbeddingModel实现，您可以将其注入到类中。\n@RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(\u0026#34;/ai/embedding\u0026#34;) public Map embed(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { EmbeddingResponse embeddingResponse = this.embeddingModel.embedForResponse(List.of(message)); return Map.of(\u0026#34;embedding\u0026#34;, embeddingResponse); } } 手动配置 # VertexAiTextEmbeddingModel实现了嵌入模型。 将spring ai顶点ai嵌入依赖项添加到项目的Maven pom.xml文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-vertex-ai-embedding\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-vertex-ai-embedding\u0026#39; } 接下来，创建VertexAiTextEmbeddingModel并将其用于文本生成：\nVertexAiEmbeddingConnectionDetails connectionDetails = VertexAiEmbeddingConnectionDetails.builder() .projectId(System.getenv(\u0026lt;VERTEX_AI_GEMINI_PROJECT_ID\u0026gt;)) .location(System.getenv(\u0026lt;VERTEX_AI_GEMINI_LOCATION\u0026gt;)) .build(); VertexAiTextEmbeddingOptions options = VertexAiTextEmbeddingOptions.builder() .model(VertexAiTextEmbeddingOptions.DEFAULT_MODEL_NAME) .build(); var embeddingModel = new VertexAiTextEmbeddingModel(this.connectionDetails, this.options); EmbeddingResponse embeddingResponse = this.embeddingModel .embedForResponse(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;)); 从Google服务帐户加载凭据 # 要从服务帐户json文件以编程方式加载GoogleCredentials，可以使用以下命令：\nGoogleCredentials credentials = GoogleCredentials.fromStream(\u0026lt;INPUT_STREAM_TO_CREDENTIALS_JSON\u0026gt;) .createScoped(\u0026#34;https://www.googleapis.com/auth/cloud-platform\u0026#34;); credentials.refreshIfExpired(); VertexAiEmbeddingConnectionDetails connectionDetails = VertexAiEmbeddingConnectionDetails.builder() .projectId(System.getenv(\u0026lt;VERTEX_AI_GEMINI_PROJECT_ID\u0026gt;)) .location(System.getenv(\u0026lt;VERTEX_AI_GEMINI_LOCATION\u0026gt;)) .apiEndpoint(endpoint) .predictionServiceSettings( PredictionServiceSettings.newBuilder() .setEndpoint(endpoint) .setCredentialsProvider(FixedCredentialsProvider.create(credentials)) .build()); "},{"id":100,"href":"/docs/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%B5%8C%E5%85%A5/","title":"谷歌VertexAI多模态嵌入","section":"Docs","content":" 谷歌VertexAI多模态嵌入 # Vertex AI支持两种类型的嵌入模型，文本和多模态。 多模式嵌入模型基于您提供的输入生成1408个维度向量，其中可以包括图像、文本和视频数据的组合。 图像嵌入向量和文本嵌入向量在相同的语义空间中，具有相同的维数。\n前提条件 # 安装适用于您的操作系统的gcloud CLI。 通过运行以下命令进行身份验证。 gcloud config set project \u0026lt;PROJECT_ID\u0026gt; \u0026amp;\u0026amp; gcloud auth application-default login \u0026lt;ACCOUNT\u0026gt; 添加存储库和BOM表 # Spring AI工件发布在Maven Central和Spring Snapshot 存储库中。 为了帮助进行依赖关系管理，Spring AI提供了BOM（物料清单），以确保在整个项目中使用一致版本的Spring AI。请参阅依赖项管理部分，将Spring AI BOM添加到构建系统中。\n自动配置 # Spring AI为VertexAI嵌入模型提供Spring Boot自动配置。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-vertex-ai-embedding\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-vertex-ai-embedding\u0026#39; } 嵌入属性 # 前缀spring.ai.vertex.ai.embedding用作允许连接到VertexAI嵌入API的属性前缀。 前缀spring.ai.vertex.ai.embedding.multimodal是属性前缀，允许您为VertexAI multimodal embedding配置嵌入模型实现。\n手动配置 # VertexAiMultimodalEmbeddingModel实现文档嵌入模型。 将spring ai顶点ai嵌入依赖项添加到项目的Maven pom.xml文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-vertex-ai-embedding\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或到Gradle build.Gradle构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-vertex-ai-embedding\u0026#39; } 接下来，创建VertexAiMultimodalEmbeddingModel，并将其用于嵌入生成：\nVertexAiEmbeddingConnectionDetails connectionDetails = VertexAiEmbeddingConnectionDetails.builder() .projectId(System.getenv(\u0026lt;VERTEX_AI_GEMINI_PROJECT_ID\u0026gt;)) .location(System.getenv(\u0026lt;VERTEX_AI_GEMINI_LOCATION\u0026gt;)) .build(); VertexAiMultimodalEmbeddingOptions options = VertexAiMultimodalEmbeddingOptions.builder() .model(VertexAiMultimodalEmbeddingOptions.DEFAULT_MODEL_NAME) .build(); var embeddingModel = new VertexAiMultimodalEmbeddingModel(this.connectionDetails, this.options); Media imageMedial = new Media(MimeTypeUtils.IMAGE_PNG, new ClassPathResource(\u0026#34;/test.image.png\u0026#34;)); Media videoMedial = new Media(new MimeType(\u0026#34;video\u0026#34;, \u0026#34;mp4\u0026#34;), new ClassPathResource(\u0026#34;/test.video.mp4\u0026#34;)); var document = new Document(\u0026#34;Explain what do you see on this video?\u0026#34;, List.of(this.imageMedial, this.videoMedial), Map.of()); EmbeddingResponse embeddingResponse = this.embeddingModel .embedForResponse(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;)); DocumentEmbeddingRequest embeddingRequest = new DocumentEmbeddingRequest(List.of(this.document), EmbeddingOptions.EMPTY); EmbeddingResponse embeddingResponse = multiModelEmbeddingModel.call(this.embeddingRequest); assertThat(embeddingResponse.getResults()).hasSize(3); "}]