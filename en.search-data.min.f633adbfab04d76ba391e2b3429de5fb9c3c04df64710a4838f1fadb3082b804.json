[{"id":0,"href":"/docs/%E5%8F%82%E8%80%83/%E7%9F%A2%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/azure-ai-%E6%9C%8D%E5%8A%A1/","title":"Azure AI 服务","section":"矢量数据库","content":" Azure AI 服务 # 本节将引导您设置 AzureVectorStore 来存储文档嵌入并使用 Azure AI 搜索服务执行相似性搜索。\n[ Azure AI Search]( https://azure.microsoft.com/en-us/products/ai-services/ai-search/) 是一个多功能的云托管云信息检索系统，是微软更强大的 AI 平台的一部分。除其他功能外，它还允许用户使用基于向量的存储和检索来查询信息。\n先决条件 # 配置 # 在启动时，如果您选择通过在构造函数中将相关的 initialize-schema boolean 属性设置为 true 来加入，或者如果使用 Spring Boot，则在 application.properties 文件中设置 …​initialize-schema=true ，则 AzureVectorStore 可以尝试在您的 AI 搜索服务实例中创建一个新索引。\n或者，您可以手动创建索引。\n要设置 AzureVectorStore，您将需要从上面的先决条件中检索到的设置以及索引名称：\nAzure AI 搜索终结点 Azure AI Search Endpoint Azure AI 搜索键 （可选）Azure OpenAI API 端点 （可选）Azure OpenAI API 密钥 您可以将这些值提供为操作系统环境变量。\nexport AZURE_AI_SEARCH_API_KEY=\u0026lt;My AI Search API Key\u0026gt; export AZURE_AI_SEARCH_ENDPOINT=\u0026lt;My AI Search Index\u0026gt; export OPENAI_API_KEY=\u0026lt;My Azure AI API Key\u0026gt; (Optional) 依赖项 # 将这些依赖项添加到您的项目：\n1. 选择 Embeddings 接口实现。您可以选择： # 2. Azure（AI 搜索）矢量存储 # \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-azure-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 配置属性 # 您可以在 Spring Boot 配置中使用以下属性来自定义 Azure 矢量存储。\n示例代码 # 要在应用程序中配置 Azure SearchIndexClient ，可以使用以下代码：\n@Bean public SearchIndexClient searchIndexClient() { return new SearchIndexClientBuilder().endpoint(System.getenv(\u0026#34;AZURE_AI_SEARCH_ENDPOINT\u0026#34;)) .credential(new AzureKeyCredential(System.getenv(\u0026#34;AZURE_AI_SEARCH_API_KEY\u0026#34;))) .buildClient(); } 要创建向量存储，您可以使用以下代码，通过注入上述示例中创建的 SearchIndexClient bean 以及实现所需 Embeddings 接口的 Spring AI 库提供的 EmbeddingModel 。\n@Bean public VectorStore vectorStore(SearchIndexClient searchIndexClient, EmbeddingModel embeddingModel) { return AzureVectorStore.builder(searchIndexClient, embeddingModel) .initializeSchema(true) // Define the metadata fields to be used // in the similarity search filters. .filterMetadataFields(List.of(MetadataField.text(\u0026#34;country\u0026#34;), MetadataField.int64(\u0026#34;year\u0026#34;), MetadataField.date(\u0026#34;activationDate\u0026#34;))) .defaultTopK(5) .defaultSimilarityThreshold(0.7) .indexName(\u0026#34;spring-ai-document-index\u0026#34;) .build(); } 在您的主代码中，创建一些文档：\nList\u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;country\u0026#34;, \u0026#34;BG\u0026#34;, \u0026#34;year\u0026#34;, 2020)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;country\u0026#34;, \u0026#34;NL\u0026#34;, \u0026#34;year\u0026#34;, 2023))); 将文档添加到您的矢量存储中：\nvectorStore.add(documents); 最后，检索与查询类似的文档：\nList\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch( SearchRequest.builder() .query(\u0026#34;Spring\u0026#34;) .topK(5).build()); 如果一切顺利，您应该检索包含文本“Spring AI rocks!!”的文档。\n元数据过滤 # 您还可以利用 AzureVectorStore 的通用、可移植元[ 数据过滤器]( https://docs.spring.io/spring-ai/reference/api/vectordbs.html#_metadata_filters) 。\n例如，您可以使用文本表达语言：\nvectorStore.similaritySearch( SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(\u0026#34;country in [\u0026#39;UK\u0026#39;, \u0026#39;NL\u0026#39;] \u0026amp;\u0026amp; year \u0026gt;= 2020\u0026#34;).build()); 或者以编程方式使用表达式 DSL：\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch( SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(b.and( b.in(\u0026#34;country\u0026#34;, \u0026#34;UK\u0026#34;, \u0026#34;NL\u0026#34;), b.gte(\u0026#34;year\u0026#34;, 2020)).build()).build()); 可移植筛选器表达式会自动转换为专有 Azure Search [ OData 筛选器]( https://learn.microsoft.com/en-us/azure/search/search-query-odata-filter) 。例如，以下可移植筛选器表达式：\ncountry in [\u0026#39;UK\u0026#39;, \u0026#39;NL\u0026#39;] \u0026amp;\u0026amp; year \u0026gt;= 2020 转换为以下 Azure OData [ 筛选表达式]( https://learn.microsoft.com/en-us/azure/search/search-query-odata-filter) ：\n$filter search.in(meta_country, \u0026#39;UK,NL\u0026#39;, \u0026#39;,\u0026#39;) and meta_year ge 2020 访问 Native Client # Azure 矢量存储实现通过 getNativeClient() 方法提供对底层本机 Azure 搜索客户端 ( SearchClient ) 的访问：\nAzureVectorStore vectorStore = context.getBean(AzureVectorStore.class); Optional\u0026lt;SearchClient\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { SearchClient client = nativeClient.get(); // Use the native client for Azure Search-specific operations } 本机客户端使您可以访问可能无法通过 VectorStore 界面公开的 Azure 搜索特定功能和操作。\n"},{"id":1,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E5%9B%BE%E5%83%8F%E6%A8%A1%E5%9E%8B-api/azure-openai-%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/","title":"Azure OpenAI 图像生成","section":"图像模型 API","content":" Azure OpenAI 图像生成 # Spring AI 支持 Azure OpenAI 的图像生成模型 DALL-E。\n先决条件 # 从 [ Azure 门户]( https://portal.azure.com)上的 Azure OpenAI 服务部分获取 Azure OpenAI endpoint 和 api-key 。\nSpring AI 定义了两个配置属性：\n您可以在 application.properties 文件中设置这些配置属性：\nspring.ai.azure.openai.api-key=\u0026lt;your-azure-openai-api-key\u0026gt; spring.ai.azure.openai.endpoint=\u0026lt;your-azure-openai-endpoint\u0026gt; 为了在处理 API 密钥等敏感信息时增强安全性，您可以使用 Spring 表达式语言 (SpEL) 来引用自定义环境变量：\n# In application.yml spring: ai: azure: openai: api-key: ${AZURE_OPENAI_API_KEY} endpoint: ${AZURE_OPENAI_ENDPOINT} # In your environment or .env file export AZURE_OPENAI_API_KEY=\u0026lt;your-azure-openai-api-key\u0026gt; export AZURE_OPENAI_ENDPOINT=\u0026lt;your-azure-openai-endpoint\u0026gt; 您还可以在应用程序代码中以编程方式设置这些配置：\n// Retrieve API key and endpoint from secure sources or environment variables String apiKey = System.getenv(\u0026#34;AZURE_OPENAI_API_KEY\u0026#34;); String endpoint = System.getenv(\u0026#34;AZURE_OPENAI_ENDPOINT\u0026#34;); 部署名称 # 要运行 Azure AI 应用程序，请通过 Azure AI 门户 创建 Azure AI 部署。\n在 Azure 中，每个客户端必须指定一个 Deployment Name 才能连接到 Azure OpenAI 服务。\n必须了解的是， Deployment Name 与您选择部署的模型不同\n例如，名为“MyImgAiDeployment”的部署可以配置为使用 Dalle3 模型或 Dalle2 模型。\n现在，为了简单起见，您可以使用以下设置创建部署：\n部署名称： MyImgAiDeployment 型号名称： Dalle3\n此 Azure 配置将与 Spring Boot Azure AI Starter 及其自动配置功能的默认配置保持一致。\n如果您使用不同的部署名称，请相应地更新配置属性：\nspring.ai.azure.openai.image.options.deployment-name=\u0026lt;my deployment name\u0026gt; Azure OpenAI 和 OpenAI 的部署结构不同，导致 Azure OpenAI 客户端库中出现了一个名为 deploymentOrModelName 的属性。这是因为 OpenAI 中没有 Deployment Name ，只有 Model Name 。\n添加存储库和 BOM # Spring AI 的构件已发布在 Maven Central 和 Spring Snapshot 仓库中。请参阅 [ “构件仓库”](../../getting-started.html#artifact-repositories) 部分，将这些仓库添加到您的构建系统中。\n为了帮助管理依赖项，Spring AI 提供了 BOM（物料清单），以确保在整个项目中使用一致版本的 Spring AI。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # Spring AI 为 Azure OpenAI 聊天客户端提供 Spring Boot 自动配置。Spring AI offers Spring Boot auto-configuration for the Azure OpenAI Chat Client. 要启用此功能，请将以下依赖项添加到项目的 Maven pom.xml 文件：To enable it, add the following dependency to your project\u0026rsquo;s Maven pom.xml file:\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-azure-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-azure-openai\u0026#39; } 图像生成属性 # 前缀 spring.ai.openai.image 是属性前缀，可让您为 OpenAI 配置 ImageModel 实现。\n连接属性 # 前缀 spring.ai.openai 用作允许您连接到 Azure OpenAI 的属性前缀。\n运行时选项 # [ OpenAiImageOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/[OpenAiImageOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/OpenAiImageOptions.java)) 提供模型配置，例如要使用的模型、质量、大小等。\n启动时，可以使用 AzureOpenAiImageModel(OpenAiImageApi openAiImageApi) 构造函数和 withDefaultOptions(OpenAiImageOptions defaultOptions) 方法配置默认选项。或者，也可以使用前面描述的 spring.ai.azure.openai.image.options.* 属性。\n在运行时，您可以通过向 ImagePrompt 调用添加新的、特定于请求的选项来覆盖默认选项。例如，要覆盖 OpenAI 特定的选项（例如质量和要创建的图像数量），请使用以下代码示例：\nImageResponse response = azureOpenaiImageModel.call( new ImagePrompt(\u0026#34;A light cream colored mini golden doodle\u0026#34;, OpenAiImageOptions.builder() .quality(\u0026#34;hd\u0026#34;) .N(4) .height(1024) .width(1024).build()) ); "},{"id":2,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E9%9F%B3%E9%A2%91%E6%A8%A1%E5%9E%8B/%E8%BD%AC%E5%BD%95-api/azure-openai-%E8%BD%AC%E5%BD%95/","title":"Azure OpenAI 转录","section":"转录 API","content":" Azure OpenAI 转录 # Spring AI 支持 [ Azure Whisper 模型]( https://learn.microsoft.com/en-us/azure/ai-services/openai/whisper-quickstart?tabs=command-line%2Cpython-new\u0026pivots=rest-api) 。\n先决条件 # 从 [ Azure 门户]( https://portal.azure.com)上的“Azure OpenAI 服务”部分获取 Azure OpenAI endpoint 和 api-key 。Spring AI 定义了一个名为 spring.ai.azure.openai.api-key 的配置属性，您应将其设置为从 Azure 获取的 API Key 的值。还有一个名为 spring.ai.azure.openai.endpoint 的配置属性，您应将其设置为在 Azure 中预配模型时获取的端点 URL。导出环境变量是设置该配置属性的一种方法：\n自动配置 # Spring AI 为 Azure OpenAI 转录生成客户端提供 Spring Boot 自动配置。 Spring AI offers Spring Boot auto-configuration for the Azure OpenAI Transcription Generation Client. 若要启用此功能，请将以下依赖项添加到项目的 Maven pom.xml 文件：To enable it, add the following dependency to your project\u0026rsquo;s Maven pom.xml file:\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-azure-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-azure-openai\u0026#39; } 转录特性 # 前缀 spring.ai.openai.audio.transcription 用作属性前缀，可让您配置 OpenAI 图像模型的重试机制。\n运行时选项 # AzureOpenAiAudioTranscriptionOptions 类提供了转录时要使用的选项。启动时，将使用 spring.ai.azure.openai.audio.transcription 指定的选项，但您可以在运行时覆盖这些选项。\n例如：\nAzureOpenAiAudioTranscriptionOptions.TranscriptResponseFormat responseFormat = AzureOpenAiAudioTranscriptionOptions.TranscriptResponseFormat.VTT; AzureOpenAiAudioTranscriptionOptions transcriptionOptions = AzureOpenAiAudioTranscriptionOptions.builder() .language(\u0026#34;en\u0026#34;) .prompt(\u0026#34;Ask not this, but ask that\u0026#34;) .temperature(0f) .responseFormat(this.responseFormat) .build(); AudioTranscriptionPrompt transcriptionRequest = new AudioTranscriptionPrompt(audioFile, this.transcriptionOptions); AudioTranscriptionResponse response = azureOpenAiTranscriptionModel.call(this.transcriptionRequest); 手动配置 # 将 spring-ai-openai 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-azure-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-azure-openai\u0026#39; } 接下来，创建一个 AzureOpenAiAudioTranscriptionModel\nvar openAIClient = new OpenAIClientBuilder() .credential(new AzureKeyCredential(System.getenv(\u0026#34;AZURE_OPENAI_API_KEY\u0026#34;))) .endpoint(System.getenv(\u0026#34;AZURE_OPENAI_ENDPOINT\u0026#34;)) .buildClient(); var azureOpenAiAudioTranscriptionModel = new AzureOpenAiAudioTranscriptionModel(this.openAIClient, null); var transcriptionOptions = AzureOpenAiAudioTranscriptionOptions.builder() .responseFormat(TranscriptResponseFormat.TEXT) .temperature(0f) .build(); var audioFile = new FileSystemResource(\u0026#34;/path/to/your/resource/speech/jfk.flac\u0026#34;); AudioTranscriptionPrompt transcriptionRequest = new AudioTranscriptionPrompt(this.audioFile, this.transcriptionOptions); AudioTranscriptionResponse response = this.azureOpenAiAudioTranscriptionModel.call(this.transcriptionRequest); "},{"id":3,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90/etl-%E7%AE%A1%E9%81%93/","title":"ETL 管道","section":"检索增强生成","content":" ETL 管道 # 提取、转换和加载 (ETL) 框架是检索增强生成 (RAG) 用例中数据处理的支柱。\nETL 管道协调从原始数据源到结构化向量存储的流程，确保数据采用 AI 模型检索的最佳格式。\nRAG 用例是文本，通过从数据主体中检索相关信息来增强生成模型的功能，从而提高生成输出的质量和相关性。\nAPI 概述 # ETL 管道创建、转换和存储 Document 实例。\nDocument 类包含文本、元数据和可选的附加媒体类型，如图像、音频和视频。\nETL 管道有三个主要组件，\n实现 Supplier\u0026lt;List\u0026gt; DocumentReader 实现 Function\u0026lt;List, List\u0026gt; 的 DocumentTransformer 实现 Consumer\u0026lt;List\u0026gt; 的 DocumentWriter Document 类内容是在 DocumentReader 的帮助下从 PDF、文本文件和其他文档类型创建的。\n要构建一个简单的 ETL 管道，您可以将每种类型的实例链接在一起。\n假设我们有以下这三种 ETL 类型的实例\nPagePdfDocumentReader 是 DocumentReader 的一个实现 TokenTextSplitter 是 DocumentTransformer 的一个实现 VectorStore 是 DocumentWriter 的一个实现 要执行将数据基本加载到矢量数据库中以供检索增强生成模式使用的操作，请使用以下 Java 函数样式语法代码。\nvectorStore.accept(tokenTextSplitter.apply(pdfReader.get())); 或者，您可以使用更自然地表达域的方法名称\nvectorStore.write(tokenTextSplitter.split(pdfReader.read())); ETL 接口 # ETL 管道由以下接口和实现组成。详细的 [ ETL 类图](#etl-class-diagram)在 [ ETL 类图](#etl-class-diagram)部分中展示。\n文档阅读器 # 提供来自不同来源的文档来源。\npublic interface DocumentReader extends Supplier\u0026lt;List\u0026lt;Document\u0026gt;\u0026gt; { default List\u0026lt;Document\u0026gt; read() { return get(); } } 文档转换器 # 作为处理工作流程的一部分，转换一批文档。\npublic interface DocumentTransformer extends Function\u0026lt;List\u0026lt;Document\u0026gt;, List\u0026lt;Document\u0026gt;\u0026gt; { default List\u0026lt;Document\u0026gt; transform(List\u0026lt;Document\u0026gt; transform) { return apply(transform); } } 文档编写器 # 管理 ETL 过程的最后阶段，准备要存储的文档。\npublic interface DocumentWriter extends Consumer\u0026lt;List\u0026lt;Document\u0026gt;\u0026gt; { default void write(List\u0026lt;Document\u0026gt; documents) { accept(documents); } } ETL 类图 # 下面的类图说明了 ETL 接口和实现。\n文档阅读器 # JsonReader 处理 JSON 文档，将其转换为 Document 对象列表。\n例子 # @Component class MyJsonReader { private final Resource resource; MyJsonReader(@Value(\u0026#34;classpath:bikes.json\u0026#34;) Resource resource) { this.resource = resource; } List\u0026lt;Document\u0026gt; loadJsonAsDocuments() { JsonReader jsonReader = new JsonReader(this.resource, \u0026#34;description\u0026#34;, \u0026#34;content\u0026#34;); return jsonReader.get(); } } 构造函数选项 # JsonReader 提供了几个构造函数选项：\n参数 # resource ：指向 JSON 文件的 Spring Resource 对象。 jsonKeysToUse ：JSON 中的键数组，应用于生成的 Document 对象中的文本内容。 jsonMetadataGenerator ：可选的 JsonMetadataGenerator ，用于为每个 Document 创建元数据。 行为 # JsonReader 对 JSON 内容的处理如下：\n它可以处理 JSON 数组和单个 JSON 对象。 对于每个 JSON 对象（无论是数组还是单个对象）： 它根据指定的 jsonKeysToUse 提取内容。 如果没有指定键，则使用整个 JSON 对象作为内容。 它使用提供的 JsonMetadataGenerator （如果未提供，则为空）生成元数据。 它使用提取的内容和元数据创建一个 Document 对象。 它根据指定的 jsonKeysToUse 提取内容。 如果没有指定键，则使用整个 JSON 对象作为内容。 它使用提供的 JsonMetadataGenerator （如果未提供，则为空）生成元数据。 它使用提取的内容和元数据创建一个 Document 对象。 使用 JSON 指针 # JsonReader 现在支持使用 JSON 指针检索 JSON 文档的特定部分。此功能让您可以轻松地从复杂的 JSON 结构中提取嵌套数据。\nget(String pointer) 方法 # public List\u0026lt;Document\u0026gt; get(String pointer) 此方法允许您使用 JSON 指针来检索 JSON 文档的特定部分。\n参数 # pointer ：JSON 指针字符串（如 RFC 6901 中所定义），用于在 JSON 结构中定位所需元素。 返回值 # 返回一个 List 其中包含从指针定位的 JSON 元素解析的文档。 行为 # 该方法使用提供的 JSON 指针导航到 JSON 结构中的特定位置。 如果指针有效并且指向现有元素： 对于 JSON 对象：它返回一个包含单个文档的列表。 对于 JSON 数组：它返回一个文档列表，数组中的每个元素对应一个文档。 对于 JSON 对象：它返回一个包含单个文档的列表。 对于 JSON 数组：它返回一个文档列表，数组中的每个元素对应一个文档。 如果指针无效或指向不存在的元素，则会抛出 IllegalArgumentException 。 例子 # JsonReader jsonReader = new JsonReader(resource, \u0026#34;description\u0026#34;); List\u0026lt;Document\u0026gt; documents = this.jsonReader.get(\u0026#34;/store/books/0\u0026#34;); JSON 结构示例 # [ { \u0026#34;id\u0026#34;: 1, \u0026#34;brand\u0026#34;: \u0026#34;Trek\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;A high-performance mountain bike for trail riding.\u0026#34; }, { \u0026#34;id\u0026#34;: 2, \u0026#34;brand\u0026#34;: \u0026#34;Cannondale\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;An aerodynamic road bike for racing enthusiasts.\u0026#34; } ] 在这个例子中，如果 JsonReader 配置了 \u0026quot;description\u0026quot; 作为 jsonKeysToUse ，它将创建 Document 对象，其内容是数组中每辆自行车的“description”字段的值。\n笔记 # JsonReader 使用 Jackson 进行 JSON 解析。 它可以通过使用数组流来有效地处理大型 JSON 文件。 如果在 jsonKeysToUse 中指定了多个键，则内容将是这些键的值的串联。 该读取器非常灵活，可以通过自定义 jsonKeysToUse 和 JsonMetadataGenerator 来适应各种 JSON 结构。 文本 # TextReader 处理纯文本文档，将其转换为 Document 对象列表。\n例子 # @Component class MyTextReader { private final Resource resource; MyTextReader(@Value(\u0026#34;classpath:text-source.txt\u0026#34;) Resource resource) { this.resource = resource; } List\u0026lt;Document\u0026gt; loadText() { TextReader textReader = new TextReader(this.resource); textReader.getCustomMetadata().put(\u0026#34;filename\u0026#34;, \u0026#34;text-source.txt\u0026#34;); return textReader.read(); } } 构造函数选项 # TextReader 提供了两个构造函数选项：\n参数 # resourceUrl ：表示要读取的资源的 URL 的字符串。 resource ：指向文本文件的 Spring Resource 对象。 配置 # setCharset(Charset charset) ：设置用于读取文本文件的字符集。默认为 UTF-8。 getCustomMetadata() ：返回一个可变映射，您可以在其中为文档添加自定义元数据。 行为 # TextReader 处理文本内容如下：\n它将文本文件的全部内容读入单个 Document 对象。 文件的内容成为 Document 的内容。 元数据会自动添加到 Document 中： charset ：用于读取文件的字符集（默认值：“UTF-8”）。 source ：源文本文件的文件名。 charset ：用于读取文件的字符集（默认值：“UTF-8”）。 source ：源文本文件的文件名。 通过 getCustomMetadata() 添加的任何自定义元数据都包含在 Document 中。 笔记 # TextReader 将整个文件内容读入内存，因此它可能不适合非常大的文件。 如果需要将文本拆分成更小的块，则可以在阅读文档后使用像 TokenTextSplitter 这样的文本拆分器： List\u0026lt;Document\u0026gt; documents = textReader.get(); List\u0026lt;Document\u0026gt; splitDocuments = new TokenTextSplitter().apply(this.documents); 读取器使用 Spring 的 Resource 抽象，允许它从各种来源（类路径、文件系统、URL 等）读取。 可以使用 getCustomMetadata() 方法将自定义元数据添加到阅读器创建的所有文档中。 HTML（JSoup） # Jsoup``Document``Reader 处理 HTML 文档，使用 JSoup 库将其转换为 Document 对象列表。\n例子 # @Component class MyHtmlReader { private final Resource resource; MyHtmlReader(@Value(\u0026#34;classpath:/my-page.html\u0026#34;) Resource resource) { this.resource = resource; } List\u0026lt;Document\u0026gt; loadHtml() { JsoupDocumentReaderConfig config = JsoupDocumentReaderConfig.builder() .selector(\u0026#34;article p\u0026#34;) // Extract paragraphs within \u0026lt;article\u0026gt; tags .charset(\u0026#34;ISO-8859-1\u0026#34;) // Use ISO-8859-1 encoding .includeLinkUrls(true) // Include link URLs in metadata .metadataTags(List.of(\u0026#34;author\u0026#34;, \u0026#34;date\u0026#34;)) // Extract author and date meta tags .additionalMetadata(\u0026#34;source\u0026#34;, \u0026#34;my-page.html\u0026#34;) // Add custom metadata .build(); JsoupDocumentReader reader = new JsoupDocumentReader(this.resource, config); return reader.get(); } } ```JsoupDocumentReaderConfig` 允许您自定义 JsoupDocumentReader`` 的行为：\ncharset ：指定 HTML 文档的字符编码（默认为“UTF-8”）。 selector ：JSoup CSS 选择器，指定从哪些元素中提取文本（默认为“body”）。 separator ：用于连接多个选定元素的文本的字符串（默认为“\\n”）。 allElements ：如果为 true ，则从 元素中提取所有文本，忽略 selector （默认为 false ）。 groupByElement ：如果为 true ，则为 selector 匹配的每个元素创建一个单独的 Document （默认为 false ）。 includeLinkUrls ：如果为 true ，则提取绝对链接 URL 并将其添加到元数据中（默认为 false ）。 metadataTags ：要从中提取内容的 标签名称列表（默认为 [\u0026ldquo;description\u0026rdquo;, \u0026ldquo;keywords\u0026rdquo;] ）。 additionalMetadata ：允许您向所有创建的 Document 对象添加自定义元数据。 示例文档：my-page.html # \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;My Web Page\u0026lt;/title\u0026gt; \u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;A sample web page for Spring AI\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;keywords\u0026#34; content=\u0026#34;spring, ai, html, example\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;author\u0026#34; content=\u0026#34;John Doe\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;date\u0026#34; content=\u0026#34;2024-01-15\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;style.css\u0026#34;\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;header\u0026gt; \u0026lt;h1\u0026gt;Welcome to My Page\u0026lt;/h1\u0026gt; \u0026lt;/header\u0026gt; \u0026lt;nav\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;/\u0026#34;\u0026gt;Home\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;/about\u0026#34;\u0026gt;About\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/nav\u0026gt; \u0026lt;article\u0026gt; \u0026lt;h2\u0026gt;Main Content\u0026lt;/h2\u0026gt; \u0026lt;p\u0026gt;This is the main content of my web page.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;It contains multiple paragraphs.\u0026lt;/p\u0026gt; \u0026lt;a href=\u0026#34;https://www.example.com\u0026#34;\u0026gt;External Link\u0026lt;/a\u0026gt; \u0026lt;/article\u0026gt; \u0026lt;footer\u0026gt; \u0026lt;p\u0026gt;\u0026amp;copy; 2024 John Doe\u0026lt;/p\u0026gt; \u0026lt;/footer\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 行为：\nJsoup``Document``Reader 处理 HTML 内容并根据配置创建 Document 对象：\nselector 确定哪些元素用于文本提取。 如果 allElements 为 true ， 内的所有文本都将提取到单个 Document 中。 如果 groupByElement 为 true ，则与 selector 匹配的每个元素都会创建一个单独的 Document 。 如果 allElements 和 groupByElement 都不为 true ，则使用 separator 连接所有与 selector 匹配的元素的文本。 文档标题、指定 标签的内容以及（可选）链接 URL 被添加到 Document 元数据中。 用于解析相对链接的基本 URI 将从 URL 资源中提取。 阅读器保留所选元素的文本内容，但删除其中的任何 HTML 标签。\nMarkdown``Document``Reader 处理 Markdown 文档，将其转换为 Document 对象列表。\n例子 # @Component class MyMarkdownReader { private final Resource resource; MyMarkdownReader(@Value(\u0026#34;classpath:code.md\u0026#34;) Resource resource) { this.resource = resource; } List\u0026lt;Document\u0026gt; loadMarkdown() { MarkdownDocumentReaderConfig config = MarkdownDocumentReaderConfig.builder() .withHorizontalRuleCreateDocument(true) .withIncludeCodeBlock(false) .withIncludeBlockquote(false) .withAdditionalMetadata(\u0026#34;filename\u0026#34;, \u0026#34;code.md\u0026#34;) .build(); MarkdownDocumentReader reader = new MarkdownDocumentReader(this.resource, config); return reader.get(); } } MarkdownDocumentReaderConfig 允许您自定义 MarkdownDocumentReader 的行为：\nhorizontalRuleCreateDocument ：设置为 true 时，Markdown 中的水平规则将创建新的 Document 对象。 includeCodeBlock ：设置为 true 时，代码块将与周围文本包含在同一个 Document 中。设置为 false 时，代码块将创建单独的 Document 对象。 includeBlockquote ：设置为 true 时，区块引用将与周围文本包含在同一个 Document 中。设置为 false 时，区块引用将创建单独的 Document 对象。 additionalMetadata ：允许您向所有创建的 Document 对象添加自定义元数据。 示例文档：code.md # This is a Java sample application: ```java package com.example.demo; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; @SpringBootApplication public class DemoApplication { public static void main(String[] args) { SpringApplication.run(DemoApplication.class, args); } } Markdown also provides the possibility to use inline code formatting throughout the entire sentence.\nAnother possibility is to set block code without specific highlighting:\n./mvnw spring-javaformat:apply 行为：MarkdownDocumentReader 处理 Markdown 内容并根据配置创建 Document 对象： - 标题成为 Document 对象中的元数据。 - 段落成为 Document 对象的内容。 - 代码块可以分离到自己的 Document 对象中或包含在周围的文本中。 - 块引用可以分离到自己的 Document 对象中，也可以包含在周围的文本中。 - 水平规则可用于将内容分割成单独的 Document 对象。 阅读器保留 Document 对象内容中的内联代码、列表和文本样式等格式。 ## PDF 页面 ``PagePdfDocumentReader`` 使用 Apache PdfBox 库来解析 PDF 文档 使用 Maven 或 Gradle 将依赖项添加到您的项目。 ```xml \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-pdf-document-reader\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-pdf-document-reader\u0026#39; } 例子 # @Component public class MyPagePdfDocumentReader { List\u0026lt;Document\u0026gt; getDocsFromPdf() { PagePdfDocumentReader pdfReader = new PagePdfDocumentReader(\u0026#34;classpath:/sample1.pdf\u0026#34;, PdfDocumentReaderConfig.builder() .withPageTopMargin(0) .withPageExtractedTextFormatter(ExtractedTextFormatter.builder() .withNumberOfTopTextLinesToDelete(0) .build()) .withPagesPerDocument(1) .build()); return pdfReader.read(); } } PDF 段落 # ParagraphPdf``Document``Reader 使用 PDF 目录（例如目录）信息将输入的 PDF 拆分为文本段落，并按段落输出单个 Document 。注意：并非所有 PDF 文档都包含 PDF 目录。\n依赖项 # 使用 Maven 或 Gradle 将依赖项添加到您的项目。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-pdf-document-reader\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-pdf-document-reader\u0026#39; } 例子 # @Component public class MyPagePdfDocumentReader { List\u0026lt;Document\u0026gt; getDocsFromPdfWithCatalog() { ParagraphPdfDocumentReader pdfReader = new ParagraphPdfDocumentReader(\u0026#34;classpath:/sample1.pdf\u0026#34;, PdfDocumentReaderConfig.builder() .withPageTopMargin(0) .withPageExtractedTextFormatter(ExtractedTextFormatter.builder() .withNumberOfTopTextLinesToDelete(0) .build()) .withPagesPerDocument(1) .build()); return pdfReader.read(); } } 蒂卡（DOCX、PPTX、HTML……） # TikaDocumentReader 使用 Apache Tika 从各种文档格式（例如 PDF、DOC/DOCX、PPT/PPTX 和 HTML）中提取文本。有关支持格式的完整列表，请参阅 [ Tika 文档]( https://tika.apache.org/3.1.0/formats.html) 。\n依赖项 # \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-tika-document-reader\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-tika-document-reader\u0026#39; } 例子 # @Component class MyTikaDocumentReader { private final Resource resource; MyTikaDocumentReader(@Value(\u0026#34;classpath:/word-sample.docx\u0026#34;) Resource resource) { this.resource = resource; } List\u0026lt;Document\u0026gt; loadText() { TikaDocumentReader tikaDocumentReader = new TikaDocumentReader(this.resource); return tikaDocumentReader.read(); } } 变形金刚 # 文本分割器 # TextSplitter 是一个抽象基类，有助于划分文档以适应 AI 模型的上下文窗口。\n`TokenTextSplitter``` 是 TextSplitter`` 的一个实现，它使用 CL100K_BASE 编码根据标记计数将文本拆分成块。\n用法 # @Component class MyTokenTextSplitter { public List\u0026lt;Document\u0026gt; splitDocuments(List\u0026lt;Document\u0026gt; documents) { TokenTextSplitter splitter = new TokenTextSplitter(); return splitter.apply(documents); } public List\u0026lt;Document\u0026gt; splitCustomized(List\u0026lt;Document\u0026gt; documents) { TokenTextSplitter splitter = new TokenTextSplitter(1000, 400, 10, 5000, true); return splitter.apply(documents); } } 构造函数选项 # TokenTextSplitter 提供了两个构造函数选项：\n参数 # defaultChunkSize ：标记中每个文本块的目标大小（默认值：800）。 minChunkSizeChars ：每个文本块的最小字符大小（默认值：350）。 minChunkLengthToEmbed ：要包含的块的最小长度（默认值：5）。 maxNumChunks ：从文本生成的最大块数（默认值：10000）。 keepSeparator ：是否在块中保留分隔符（如换行符）（默认值：true）。 行为 # TokenTextSplitter 对文本内容进行如下处理：\n例子 # Document doc1 = new Document(\u0026#34;This is a long piece of text that needs to be split into smaller chunks for processing.\u0026#34;, Map.of(\u0026#34;source\u0026#34;, \u0026#34;example.txt\u0026#34;)); Document doc2 = new Document(\u0026#34;Another document with content that will be split based on token count.\u0026#34;, Map.of(\u0026#34;source\u0026#34;, \u0026#34;example2.txt\u0026#34;)); TokenTextSplitter splitter = new TokenTextSplitter(); List\u0026lt;Document\u0026gt; splitDocuments = this.splitter.apply(List.of(this.doc1, this.doc2)); for (Document doc : splitDocuments) { System.out.println(\u0026#34;Chunk: \u0026#34; + doc.getContent()); System.out.println(\u0026#34;Metadata: \u0026#34; + doc.getMetadata()); } 笔记 # TokenTextSplitter 使用 jtokkit 库中的 CL100K_BASE 编码，与较新的 OpenAI 模型兼容。 分割器尝试通过在可能的情况下断开句子边界来创建具有语义意义的块。 原始文档的元数据被保留并复制到从该文档派生的所有块。 如果 copyContentFormatter 设置为 true （默认行为），则原始文档中的内容格式化程序（如果设置）也会被复制到派生块中。 该分割器对于为具有标记限制的大型语言模型准备文本特别有用，可确保每个块都在模型的处理能力范围内。 内容格式转换器 # 确保所有文档的内容格式统一。\n关键字元数据丰富器 # KeywordMetadataEnricher 是一个 DocumentTransformer ，它使用生成式 AI 模型从文档内容中提取关键字并将其添加为元数据。\n用法 # @Component class MyKeywordEnricher { private final ChatModel chatModel; MyKeywordEnricher(ChatModel chatModel) { this.chatModel = chatModel; } List\u0026lt;Document\u0026gt; enrichDocuments(List\u0026lt;Document\u0026gt; documents) { KeywordMetadataEnricher enricher = new KeywordMetadataEnricher(this.chatModel, 5); return enricher.apply(documents); } } 构造函数 # KeywordMetadataEnricher 构造函数采用两个参数：\n行为 # KeywordMetadataEnricher 处理文档如下：\n定制 # 可以通过修改类中的 KEYWORDS_TEMPLATE 常量来自定义关键字提取提示。默认模板为：\n\\{context_str}. Give %s unique keywords for this document. Format as comma separated. Keywords: 其中 {context_str} 替换为文档内容， %s 替换为指定的关键字计数。\n例子 # ChatModel chatModel = // initialize your chat model KeywordMetadataEnricher enricher = new KeywordMetadataEnricher(chatModel, 5); Document doc = new Document(\u0026#34;This is a document about artificial intelligence and its applications in modern technology.\u0026#34;); List\u0026lt;Document\u0026gt; enrichedDocs = enricher.apply(List.of(this.doc)); Document enrichedDoc = this.enrichedDocs.get(0); String keywords = (String) this.enrichedDoc.getMetadata().get(\u0026#34;excerpt_keywords\u0026#34;); System.out.println(\u0026#34;Extracted keywords: \u0026#34; + keywords); 笔记 # KeywordMetadataEnricher 需要一个功能正常的 ChatModel 来生成关键字。 关键字数量必须为 1 或更大。 丰富器将“excerpt_keywords”元数据字段添加到每个处理的文档中。 生成的关键字以逗号分隔的字符串形式返回。 该丰富器对于提高文档可搜索性和为文档生成标签或类别特别有用。 摘要元数据丰富器 # SummaryMetadataEnricher 是一个 DocumentTransformer ，它使用生成式 AI 模型为文档创建摘要并将其添加为元数据。它可以为当前文档以及相邻文档（上一个和下一个）生成摘要。\n用法 # @Configuration class EnricherConfig { @Bean public SummaryMetadataEnricher summaryMetadata(OpenAiChatModel aiClient) { return new SummaryMetadataEnricher(aiClient, List.of(SummaryType.PREVIOUS, SummaryType.CURRENT, SummaryType.NEXT)); } } @Component class MySummaryEnricher { private final SummaryMetadataEnricher enricher; MySummaryEnricher(SummaryMetadataEnricher enricher) { this.enricher = enricher; } List\u0026lt;Document\u0026gt; enrichDocuments(List\u0026lt;Document\u0026gt; documents) { return this.enricher.apply(documents); } } 构造函数 # SummaryMetadataEnricher 提供了两个构造函数：\n参数 # chatModel ：用于生成摘要的 AI 模型。 summaryTypes ： SummaryType 枚举值列表，指示要生成哪些摘要（PREVIOUS、CURRENT、NEXT）。 summaryTemplate ：用于摘要生成的自定义模板（可选）。 metadataMode ：指定生成摘要时如何处理文档元数据（可选）。 行为 # SummaryMetadataEnricher 处理文档如下：\n定制 # 可以通过提供自定义 summaryTemplate 来定制摘要生成提示。默认模板为：\n\u0026#34;\u0026#34;\u0026#34; Here is the content of the section: {context_str} Summarize the key topics and entities of the section. Summary: \u0026#34;\u0026#34;\u0026#34; 例子 # ChatModel chatModel = // initialize your chat model SummaryMetadataEnricher enricher = new SummaryMetadataEnricher(chatModel, List.of(SummaryType.PREVIOUS, SummaryType.CURRENT, SummaryType.NEXT)); Document doc1 = new Document(\u0026#34;Content of document 1\u0026#34;); Document doc2 = new Document(\u0026#34;Content of document 2\u0026#34;); List\u0026lt;Document\u0026gt; enrichedDocs = enricher.apply(List.of(this.doc1, this.doc2)); // Check the metadata of the enriched documents for (Document doc : enrichedDocs) { System.out.println(\u0026#34;Current summary: \u0026#34; + doc.getMetadata().get(\u0026#34;section_summary\u0026#34;)); System.out.println(\u0026#34;Previous summary: \u0026#34; + doc.getMetadata().get(\u0026#34;prev_section_summary\u0026#34;)); System.out.println(\u0026#34;Next summary: \u0026#34; + doc.getMetadata().get(\u0026#34;next_section_summary\u0026#34;)); } 提供的示例演示了预期的行为：\n对于两个文档的列表，两个文档都会收到一个 section_summary 。 第一个文档收到 next_section_summary 但没有 prev_section_summary 。 第二个文档收到 prev_section_summary 但没有 next_section_summary 。 第一个文档的 section_summary 与第二个文档的 prev_section_summary 匹配。 第一个文档的 next_section_summary 与第二个文档的 section_summary 匹配。 笔记 # SummaryMetadataEnricher 需要一个功能正常的 ChatModel 来生成摘要。 丰富器可以处理任意大小的文档列表，并正确处理第一个和最后一个文档的边缘情况。 该丰富器对于创建上下文感知摘要特别有用，可以更好地理解序列中的文档关系。 MetadataMode 参数允许控制如何将现有元数据合并到摘要生成过程中。 作家 # 文件 # File```Document``Writer`` 是 ```Document``Writer 实现，它将 Document 对象列表的内容写入文件。\n用法 # @Component class MyDocumentWriter { public void writeDocuments(List\u0026lt;Document\u0026gt; documents) { FileDocumentWriter writer = new FileDocumentWriter(\u0026#34;output.txt\u0026#34;, true, MetadataMode.ALL, false); writer.accept(documents); } } 构造函数 # FileDocumentWriter 提供了三个构造函数：\n参数 # fileName ：要写入文档的文件的名称。 withDocumentMarkers ：是否在输出中包含文档标记（默认值：false）。 metadataMode ：指定要写入文件的文档内容（默认值：MetadataMode.NONE）。 append ：如果为 true，数据将被写入文件末尾而不是开头（默认值：false）。 行为 # FileDocumentWriter 按如下方式处理文档：\n文档标记 # 当 withDocumentMarkers 设置为 true 时，编写器将以以下格式为每个文档添加标记：\n### Doc: [index], pages:[start_page_number,end_page_number] 元数据处理 # 编写器使用两个特定的元数据键：\npage_number ：表示文档的起始页码。 end_page_number ：表示文档的结束页码。 这些用于编写文档标记。\n例子 # List\u0026lt;Document\u0026gt; documents = // initialize your documents FileDocumentWriter writer = new FileDocumentWriter(\u0026#34;output.txt\u0026#34;, true, MetadataMode.ALL, true); writer.accept(documents); 这会将所有文档写入“output.txt”，包括文档标记，使用所有可用的元数据，如果文件已存在则附加到该文件。\n笔记 # 写入器使用 FileWriter ，因此它使用操作系统的默认字符编码写入文本文件。 如果在写入过程中发生错误，则会抛出 RuntimeException ，并以原始异常作为其原因。 metadataMode 参数允许控制如何将现有元数据合并到书面内容中。 该编写器对于调试或创建文档集合的人类可读输出特别有用。 矢量存储 # 提供与各种向量存储的集成。完整列表请参阅 [ Vector DB 文档](vectordbs.html) 。\n"},{"id":4,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B-api/google-vertexai/google-vertexai-%E6%96%87%E6%9C%AC%E5%B5%8C%E5%85%A5/","title":"Google VertexAI 文本嵌入","section":"Google VertexAI","content":" Google VertexAI 文本嵌入 # Vertex AI 支持两种类型的嵌入模型：文本和多模态。本文档介绍如何使用 Vertex AI [ 文本嵌入 API]( https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/text-embeddings-api) 创建文本嵌入。\nVertex AI 文本嵌入 API 使用密集向量表示。与倾向于直接将单词映射到数字的稀疏向量不同，密集向量旨在更好地表示文本的含义。在生成式 AI 中使用密集向量嵌入的好处在于，您无需搜索直接的单词或语法匹配，而是可以更好地搜索与查询含义相符的段落，即使这些段落使用的语言不同。\n先决条件 # 安装适合您操作系统的 gcloud CLI。 通过运行以下命令进行身份验证。将 PROJECT_ID 替换为您的 Google Cloud 项目 ID，将 ACCOUNT 为您的 Google Cloud 用户名。 gcloud config set project \u0026lt;PROJECT_ID\u0026gt; \u0026amp;\u0026amp; gcloud auth application-default login \u0026lt;ACCOUNT\u0026gt; 添加存储库和 BOM # Spring AI 的构件已发布在 Maven Central 和 Spring Snapshot 仓库中。请参阅 [ “构件仓库”](../../getting-started.html#artifact-repositories) 部分，将这些仓库添加到您的构建系统中。\n为了帮助管理依赖项，Spring AI 提供了 BOM（物料清单），以确保在整个项目中使用一致版本的 Spring AI。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # Spring AI 为 VertexAI 嵌入模型提供了 Spring Boot 自动配置功能。要启用此功能，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-vertex-ai-embedding\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-vertex-ai-embedding\u0026#39; } 嵌入属性 # 前缀 spring.ai.vertex.ai.embedding 用作允许您连接到 VertexAI Embedding API 的属性前缀。\n前缀 spring.ai.vertex.ai.embedding.text 是属性前缀，可让您配置 VertexAI 文本嵌入的嵌入模型实现。\n样品控制器 # [ 创建]( https://start.spring.io/)一个新的 Spring Boot 项目并将 spring-ai-starter-model-vertex-ai-embedding 添加到您的 pom（或 gradle）依赖项中。\n在 src/main/resources 目录下添加一个 application.properties 文件，以启用和配置 VertexAi 聊天模型：\nspring.ai.vertex.ai.embedding.project-id=\u0026lt;YOUR_PROJECT_ID\u0026gt; spring.ai.vertex.ai.embedding.location=\u0026lt;YOUR_PROJECT_LOCATION\u0026gt; spring.ai.vertex.ai.embedding.text.options.model=text-embedding-004 这将创建一个 VertexAiTextEmbeddingModel 实现，您可以将其注入到您的类中。这是一个简单的 @Controller 类的示例，它使用嵌入模型进行嵌入生成。\n@RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(\u0026#34;/ai/embedding\u0026#34;) public Map embed(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { EmbeddingResponse embeddingResponse = this.embeddingModel.embedForResponse(List.of(message)); return Map.of(\u0026#34;embedding\u0026#34;, embeddingResponse); } } 手动配置 # VertexAiTextEmbeddingModel 实现了 EmbeddingModel 。\n将 spring-ai-vertex-ai-embedding 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-vertex-ai-embedding\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-vertex-ai-embedding\u0026#39; } 接下来，创建一个 VertexAiTextEmbeddingModel 并将其用于文本生成：\nVertexAiEmbeddingConnectionDetails connectionDetails = VertexAiEmbeddingConnectionDetails.builder() .projectId(System.getenv(\u0026lt;VERTEX_AI_GEMINI_PROJECT_ID\u0026gt;)) .location(System.getenv(\u0026lt;VERTEX_AI_GEMINI_LOCATION\u0026gt;)) .build(); VertexAiTextEmbeddingOptions options = VertexAiTextEmbeddingOptions.builder() .model(VertexAiTextEmbeddingOptions.DEFAULT_MODEL_NAME) .build(); var embeddingModel = new VertexAiTextEmbeddingModel(this.connectionDetails, this.options); EmbeddingResponse embeddingResponse = this.embeddingModel .embedForResponse(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;)); 从 Google 服务帐户加载凭据 # 要以编程方式从服务帐户 json 文件加载 GoogleCredentials，您可以使用以下命令：\nGoogleCredentials credentials = GoogleCredentials.fromStream(\u0026lt;INPUT_STREAM_TO_CREDENTIALS_JSON\u0026gt;) .createScoped(\u0026#34;https://www.googleapis.com/auth/cloud-platform\u0026#34;); credentials.refreshIfExpired(); VertexAiEmbeddingConnectionDetails connectionDetails = VertexAiEmbeddingConnectionDetails.builder() .projectId(System.getenv(\u0026lt;VERTEX_AI_GEMINI_PROJECT_ID\u0026gt;)) .location(System.getenv(\u0026lt;VERTEX_AI_GEMINI_LOCATION\u0026gt;)) .apiEndpoint(endpoint) .predictionServiceSettings( PredictionServiceSettings.newBuilder() .setEndpoint(endpoint) .setCredentialsProvider(FixedCredentialsProvider.create(credentials)) .build()); "},{"id":5,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B%E4%B8%8A%E4%B8%8B%E6%96%87%E5%8D%8F%E8%AE%AEmcp/mcp-%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%90%AF%E5%8A%A8%E5%99%A8/","title":"MCP 客户端启动器","section":"模型上下文协议（MCP）","content":" MCP 客户端启动器 # Spring AI MCP（模型上下文协议）客户端启动器为 Spring Boot 应用程序中的 MCP 客户端功能提供自动配置。它支持同步和异步客户端实现，并提供多种传输选项。\nMCP 客户端启动器提供：\n管理多个客户端实例 自动客户端初始化（如果启用） 支持多种命名传输 与 Spring AI 的工具执行框架集成 适当的生命周期管理，在应用程序上下文关闭时自动清理资源 通过定制器创建可定制的客户端 开胃菜 # 标准 MCP 客户端 # \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-mcp-client\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 标准启动器通过 STDIO （进程内）和/或 SSE （远程）传输同时连接到一个或多个 MCP 服务器。SSE 连接使用基于 HttpClient 的传输实现。每个与 MCP 服务器的连接都会创建一个新的 MCP 客户端实例。您可以选择 SYNC 或 ASYNC MCP 客户端（注意：不能混合使用同步和异步客户端）。对于生产部署，我们建议使用基于 WebFlux 的 SSE 连接和 spring-ai-starter-mcp-client-webflux 。\nWebFlux 客户端 # WebFlux 启动器提供与标准启动器类似的功能，但使用基于 WebFlux 的 SSE 传输实现。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-mcp-client-webflux\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 配置属性 # 通用属性 # 通用属性以 spring.ai.mcp.client 为前缀：\nStdio 传输属性 # 标准 I/O 传输的属性以 spring.ai.mcp.client.stdio 为前缀：\n示例配置：\nspring: ai: mcp: client: stdio: root-change-notification: true connections: server1: command: /path/to/server args: - --port=8080 - --mode=production env: API_KEY: your-api-key DEBUG: \u0026#34;true\u0026#34; 或者，您可以使用采用 [ Claude Desktop 格式的]( https://modelcontextprotocol.io/quickstart/user)外部 JSON 文件配置 stdio 连接：\nspring: ai: mcp: client: stdio: servers-configuration: classpath:mcp-servers.json Claude Desktop 格式如下所示：\n{ \u0026#34;mcpServers\u0026#34;: { \u0026#34;filesystem\u0026#34;: { \u0026#34;command\u0026#34;: \u0026#34;npx\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;-y\u0026#34;, \u0026#34;@modelcontextprotocol/server-filesystem\u0026#34;, \u0026#34;/Users/username/Desktop\u0026#34;, \u0026#34;/Users/username/Downloads\u0026#34; ] } } } 目前，Claude Desktop 格式仅支持 STDIO 连接类型。\n上交所运输属性 # 服务器发送事件（SSE）传输的属性以 spring.ai.mcp.client.sse 为前缀：\n示例配置：\nspring: ai: mcp: client: sse: connections: server1: url: http://localhost:8080 server2: url: http://otherserver:8081 sse-endpoint: /custom-sse 特征 # 同步/异步客户端类型 # 该启动器支持两种类型的客户端：\n同步 - 默认客户端类型，适用于具有阻塞操作的传统请求-响应模式 异步 - 适用于具有非阻塞操作的反应式应用程序，使用 spring.ai.mcp.client.type=ASYNC 配置 客户端定制 # 自动配置通过回调接口提供了广泛的客户端规范自定义功能。这些自定义器允许您配置 MCP 客户端行为的各个方面，从请求超时到事件处理和消息处理。\n定制类型 # 可以使用以下自定义选项：\n请求配置 - 设置自定义请求超时 自定义采样处理程序 - 服务器通过客户端向 LLM 请求 LLM 采样（ completions 或 generations ）的标准化方式。此流程允许客户端保持对模型访问、选择和权限的控制，同时使服务器能够利用 AI 功能——无需服务器 API 密钥。 文件系统（根目录）访问 - 客户端向服务器公开文件系统 roots 目录的标准化方式。根目录定义了服务器在文件系统中可以操作的边界，使服务器能够了解其可以访问哪些目录和文件。服务器可以向支持客户端请求根目录列表，并在列表更改时收到通知。 事件处理程序 ——当某个服务器事件发生时通知客户端的处理程序： 工具变更通知 - 当可用服务器工具列表发生变化时 资源变更通知 - 当可用服务器资源列表发生变化时。 提示更改通知 - 当可用服务器提示列表发生变化时。 工具变更通知 - 当可用服务器工具列表发生变化时 资源变更通知 - 当可用服务器资源列表发生变化时。 提示更改通知 - 当可用服务器提示列表发生变化时。 日志处理程序 - 服务器向客户端发送结构化日志消息的标准化方式。客户端可以通过设置最低日志级别来控制日志的详细程度。 您可以为同步客户端实现 McpSyncClientCustomizer ，或为异步客户端 McpAsyncClientCustomizer ，具体取决于您的应用程序的需求。\nserverConfigurationName 参数是应用定制器并为其创建 MCP 客户端的服务器配置的名称。\nMCP 客户端自动配置会自动检测并应用在应用程序上下文中找到的任何定制器。\n运输支持 # 自动配置支持多种传输类型：\n标准 I/O（Stdio）（由 spring-ai-starter-mcp-client 激活） SSE HTTP（由 spring-ai-starter-mcp-client 激活） SSE WebFlux（由 spring-ai-starter-mcp-client-webflux 激活） 与 Spring AI 集成 # 启动器可以配置与 Spring AI 工具执行框架集成的工具回调，从而允许将 MCP 工具用作 AI 交互的一部分。此集成默认启用，可通过设置 spring.ai.mcp.client.toolcallback.enabled=false 属性停用。\n使用示例 # 将适当的启动依赖项添加到您的项目并在 application.properties 或 application.yml 中配置客户端：\nspring: ai: mcp: client: enabled: true name: my-mcp-client version: 1.0.0 request-timeout: 30s type: SYNC # or ASYNC for reactive applications sse: connections: server1: url: http://localhost:8080 server2: url: http://otherserver:8081 stdio: root-change-notification: false connections: server1: command: /path/to/server args: - --port=8080 - --mode=production env: API_KEY: your-api-key DEBUG: \u0026#34;true\u0026#34; MCP 客户端 bean 将自动配置并可供注入：\n@Autowired private List\u0026lt;McpSyncClient\u0026gt; mcpSyncClients; // For sync client // OR @Autowired private List\u0026lt;McpAsyncClient\u0026gt; mcpAsyncClients; // For async client 当启用工具回调（默认行为）时，所有 MCP 客户端注册的 MCP 工具都将作为 ToolCallbackProvider 实例提供：\n@Autowired private SyncMcpToolCallbackProvider toolCallbackProvider; ToolCallback[] toolCallbacks = toolCallbackProvider.getToolCallbacks(); 示例应用程序 # Brave Web Search Chatbot - 使用模型上下文协议与网络搜索服务器交互的聊天机器人。 默认 MCP 客户端启动器 - 使用默认 spring-ai-starter-mcp-client MCP 客户端启动器的简单示例。 WebFlux MCP 客户端启动器 - 使用 spring-ai-starter-mcp-client-webflux MCP 客户端启动器的简单示例。 其他资源 # Spring AI 文档 模型上下文协议规范 Spring Boot 自动配置 "},{"id":6,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B-api/oci-%E7%94%9F%E6%88%90%E5%BC%8F-ai/oci-genai-%E5%8D%8F%E5%90%8C%E8%81%8A%E5%A4%A9/","title":"OCI GenAI 协同聊天","section":"OCI 生成式 AI","content":" OCI GenAI 协同聊天 # [ OCI GenAI 服务]( https://www.oracle.com/artificial-intelligence/generative-ai/generative-ai-service/)提供按需模型或专用 AI 集群的生成 AI 聊天。\n[ OCI 聊天模型页面]( https://docs.oracle.com/en-us/iaas/Content/generative-ai/chat-models.htm)和 [ OCI 生成 AI 游乐场]( https://docs.oracle.com/en-us/iaas/Content/generative-ai/use-playground-embed.htm)提供了有关在 OCI 上使用和托管聊天模型的详细信息。\n先决条件 # 您需要一个有效的 [ Oracle 云基础设施 (OCI)]( https://signup.oraclecloud.com/) 账户才能使用 OCI GenAI Cohere Chat 客户端。该客户端提供四种不同的连接方式，包括使用用户和私钥的简单身份验证、工作负载身份、实例主体以及 OCI 配置文件身份验证。\n添加存储库和 BOM # Spring AI 的构件已发布在 Maven Central 和 Spring Snapshot 仓库中。请参阅 [ “构件仓库”](../../../getting-started.html#artifact-repositories) 部分，将这些仓库添加到您的构建系统中。\n为了帮助管理依赖项，Spring AI 提供了 BOM（物料清单），以确保在整个项目中使用一致版本的 Spring AI。请参阅[ 依赖项管理](../../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # Spring AI 为 OCI GenAI Cohere Chat Client 提供了 Spring Boot 自动配置功能。要启用此功能，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-oci-genai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-oci-genai\u0026#39; } 聊天属性 # 连接属性 # 前缀 spring.ai.oci.genai 是配置与 OCI GenAI 的连接的属性前缀。\n配置属性 # 前缀 spring.ai.oci.genai.chat.cohere 是配置 OCI GenAI Cohere Chat 的 ChatModel 实现的属性前缀。\n运行时选项 # [ OCICohereChatOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-oci-genai/src/main/java/org/springframework/ai/oci/cohere/[OCICohereChatOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-oci-genai/src/main/java/org/springframework/ai/oci/cohere/OCICohereChatOptions.java)) 提供模型配置，例如要使用的模型、温度、频率惩罚等。\n启动时，可以使用 OCICohereChatModel(api, options) 构造函数或 spring.ai.oci.genai.chat.cohere.options.* 属性配置默认选项。\n在运行时，您可以通过向 Prompt 调用添加新的、特定于请求的选项来覆盖默认选项。例如，要覆盖特定请求的默认模型和温度：\nChatResponse response = chatModel.call( new Prompt( \u0026#34;Generate the names of 5 famous pirates.\u0026#34;, OCICohereChatOptions.builder() .model(\u0026#34;my-model-ocid\u0026#34;) .compartment(\u0026#34;my-compartment-ocid\u0026#34;) .temperature(0.5) .build() )); 样品控制器 # [ 创建]( https://start.spring.io/)一个新的 Spring Boot 项目并将 spring-ai-starter-model-oci-genai 添加到您的 pom（或 gradle）依赖项中。\n在 src/main/resources 目录下添加一个 application.properties 文件，以启用和配置 OCI GenAI Cohere 聊天模型：\nspring.ai.oci.genai.authenticationType=file spring.ai.oci.genai.file=/path/to/oci/config/file spring.ai.oci.genai.cohere.chat.options.compartment=my-compartment-ocid spring.ai.oci.genai.cohere.chat.options.servingMode=on-demand spring.ai.oci.genai.cohere.chat.options.model=my-chat-model-ocid 这将创建一个 OCICohereChatModel 实现，您可以将其注入到您的类中。这是一个使用聊天模型生成文本的简单 @Controller 类的示例。\n@RestController public class ChatController { private final OCICohereChatModel chatModel; @Autowired public ChatController(OCICohereChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, chatModel.call(message)); } @GetMapping(\u0026#34;/ai/generateStream\u0026#34;) public Flux\u0026lt;ChatResponse\u0026gt; generateStream(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { var prompt = new Prompt(new UserMessage(message)); return chatModel.stream(prompt); } } 手动配置 # OCICohereChatModel 实现了 ChatModel 并使用 OCI Java SDK 连接到 OCI GenAI 服务。\n将 spring-ai-oci-genai 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-oci-genai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-oci-genai\u0026#39; } 接下来，创建一个 OCICohereChatModel 并将其用于文本生成：\nvar CONFIG_FILE = Paths.get(System.getProperty(\u0026#34;user.home\u0026#34;), \u0026#34;.oci\u0026#34;, \u0026#34;config\u0026#34;).toString(); var COMPARTMENT_ID = System.getenv(\u0026#34;OCI_COMPARTMENT_ID\u0026#34;); var MODEL_ID = System.getenv(\u0026#34;OCI_CHAT_MODEL_ID\u0026#34;); ConfigFileAuthenticationDetailsProvider authProvider = new ConfigFileAuthenticationDetailsProvider( CONFIG_FILE, \u0026#34;DEFAULT\u0026#34; ); var genAi = GenerativeAiInferenceClient.builder() .region(Region.valueOf(\u0026#34;us-chicago-1\u0026#34;)) .build(authProvider); var chatModel = new OCICohereChatModel(genAi, OCICohereChatOptions.builder() .model(MODEL_ID) .compartment(COMPARTMENT_ID) .servingMode(\u0026#34;on-demand\u0026#34;) .build()); ChatResponse response = chatModel.call( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); OCICohereChatOptions 提供聊天请求的配置信息。OCICohereChatOptions.Builder 是一个流畅的选项构建 OCICohereChatOptions.Builder 。\n"},{"id":7,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E9%9F%B3%E9%A2%91%E6%A8%A1%E5%9E%8B/%E6%96%87%E6%9C%AC%E8%BD%AC%E8%AF%AD%E9%9F%B3-tts-api/openai-%E6%96%87%E6%9C%AC%E8%BD%AC%E8%AF%AD%E9%9F%B3-tts/","title":"OpenAI 文本转语音 (TTS)","section":"文本转语音 (TTS) API","content":" OpenAI 文本转语音 (TTS) # 介绍 # 音频 API 提供基于 OpenAI 的 TTS（文本转语音）模型的语音端点，使用户能够：\n叙述一篇书面博客文章。 制作多种语言的音频。 使用流媒体提供实时音频输出。 先决条件 # 自动配置 # Spring AI 为 OpenAI 文本转语音客户端提供了 Spring Boot 自动配置功能。要启用此功能，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件：\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-openai\u0026#39; } 语音属性 # 连接属性 # 前缀 spring.ai.openai 用作允许您连接到 OpenAI 的属性前缀。\n配置属性 # 前缀 spring.ai.openai.audio.speech 用作属性前缀，可让您配置 OpenAI 文本到语音客户端。\n运行时选项 # OpenAiAudioSpeechOptions 类提供了发出文本转语音请求时使用的选项。启动时，将使用 spring.ai.openai.audio.speech 指定的选项，但您可以在运行时覆盖这些选项。\n例如：\nOpenAiAudioSpeechOptions speechOptions = OpenAiAudioSpeechOptions.builder() .model(\u0026#34;tts-1\u0026#34;) .voice(OpenAiAudioApi.SpeechRequest.Voice.ALLOY) .responseFormat(OpenAiAudioApi.SpeechRequest.AudioResponseFormat.MP3) .speed(1.0f) .build(); SpeechPrompt speechPrompt = new SpeechPrompt(\u0026#34;Hello, this is a text-to-speech example.\u0026#34;, speechOptions); SpeechResponse response = openAiAudioSpeechModel.call(speechPrompt); 手动配置 # 将 spring-ai-openai 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件：\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-openai\u0026#39; } 接下来，创建一个 OpenAiAudioSpeechModel ：\nvar openAiAudioApi = new OpenAiAudioApi() .apiKey(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;)) .build(); var openAiAudioSpeechModel = new OpenAiAudioSpeechModel(openAiAudioApi); var speechOptions = OpenAiAudioSpeechOptions.builder() .responseFormat(OpenAiAudioApi.SpeechRequest.AudioResponseFormat.MP3) .speed(1.0f) .model(OpenAiAudioApi.TtsModel.TTS_1.value) .build(); var speechPrompt = new SpeechPrompt(\u0026#34;Hello, this is a text-to-speech example.\u0026#34;, speechOptions); SpeechResponse response = openAiAudioSpeechModel.call(speechPrompt); // Accessing metadata (rate limit info) OpenAiAudioSpeechResponseMetadata metadata = response.getMetadata(); byte[] responseAsBytes = response.getResult().getOutput(); 流式实时音频 # Speech API 支持使用块传输编码的实时音频流。这意味着音频可以在完整文件生成并可供访问之前播放。\nvar openAiAudioApi = new OpenAiAudioApi() .apiKey(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;)) .build(); var openAiAudioSpeechModel = new OpenAiAudioSpeechModel(openAiAudioApi); OpenAiAudioSpeechOptions speechOptions = OpenAiAudioSpeechOptions.builder() .voice(OpenAiAudioApi.SpeechRequest.Voice.ALLOY) .speed(1.0f) .responseFormat(OpenAiAudioApi.SpeechRequest.AudioResponseFormat.MP3) .model(OpenAiAudioApi.TtsModel.TTS_1.value) .build(); SpeechPrompt speechPrompt = new SpeechPrompt(\u0026#34;Today is a wonderful day to build something people love!\u0026#34;, speechOptions); Flux\u0026lt;SpeechResponse\u0026gt; responseStream = openAiAudioSpeechModel.stream(speechPrompt); 示例代码 # OpenAiSpeechModelIT.java 测试提供了一些如何使用该库的一般示例。 "},{"id":8,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B-api/google-vertexai/vertexai-gemini-%E8%81%8A%E5%A4%A9/","title":"VertexAI Gemini 聊天","section":"Google VertexAI","content":" VertexAI Gemini 聊天 # [ Vertex AI Gemini API]( https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/overview) 允许开发者使用 Gemini 模型构建生成式 AI 应用程序。[ Vertex AI Gemini API]( https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/overview) 支持多模态提示作为输入和输出文本或代码。多模态模型是一种能够处理多种模态信息（包括图像、视频和文本）的模型。例如，您可以向模型发送一张饼干的照片，并要求它提供这些饼干的食谱。\nGemini 是由 Google DeepMind 开发的一系列生成式 AI 模型，专为多模态用例而设计。Gemini API 允许您访问 [ Gemini 2.0 Flash]( https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-0-flash) 和 [ Gemini 2.0 Flash]( https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-0-flash)-Lite 。有关 Vertex AI Gemini API 模型的规格，请参阅[ 模型信息]( https://cloud.google.com/vertex-ai/generative-ai/docs/models#gemini-models) 。\nGemini API 参考\n先决条件 # 安装适合您操作系统的 gcloud CLI。 通过运行以下命令进行身份验证。将 PROJECT_ID 替换为您的 Google Cloud 项目 ID，将 ACCOUNT 为您的 Google Cloud 用户名。 gcloud config set project \u0026lt;PROJECT_ID\u0026gt; \u0026amp;\u0026amp; gcloud auth application-default login \u0026lt;ACCOUNT\u0026gt; 自动配置 # Spring AI 为 VertexAI Gemini 聊天客户端提供 Spring Boot 自动配置。要启用此功能，请将以下依赖项添加到项目的 Maven pom.xml 或 Gradle build.gradle 构建文件中：\n聊天属性 # 前缀 spring.ai.vertex.ai.gemini 用作允许您连接到 VertexAI 的属性前缀。\n前缀 spring.ai.vertex.ai.gemini.chat 是属性前缀，可让您配置 VertexAI Gemini Chat 的聊天模型实现。\n运行时选项 # [ VertexAiGeminiChatOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-vertex-ai-gemini/src/main/java/org/springframework/ai/vertexai/gemini/[VertexAiGeminiChatOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-vertex-ai-gemini/src/main/java/org/springframework/ai/vertexai/gemini/VertexAiGeminiChatOptions.java)) 提供模型配置，例如温度、topK 等。\n启动时，可以使用 VertexAiGeminiChatModel(api, options) 构造函数或 spring.ai.vertex.ai.chat.options.* 属性配置默认选项。\n在运行时，您可以通过向 Prompt 调用添加新的、特定于请求的选项来覆盖默认选项。例如，要覆盖特定请求的默认温度：\nChatResponse response = chatModel.call( new Prompt( \u0026#34;Generate the names of 5 famous pirates.\u0026#34;, VertexAiGeminiChatOptions.builder() .temperature(0.4) .build() )); 工具调用 # Vertex AI Gemini 模型支持工具调用（在 Google Gemini 中称为 function calling ）功能，允许模型在对话过程中使用工具。以下是如何定义和使用基于 @Tool 的工具的示例：\npublic class WeatherService { @Tool(description = \u0026#34;Get the weather in location\u0026#34;) public String weatherByLocation(@ToolParam(description= \u0026#34;City or state name\u0026#34;) String location) { ... } } String response = ChatClient.create(this.chatModel) .prompt(\u0026#34;What\u0026#39;s the weather like in Boston?\u0026#34;) .tools(new WeatherService()) .call() .content(); 您也可以使用 java.util.function beans 作为工具：\n@Bean @Description(\u0026#34;Get the weather in location. Return temperature in 36°F or 36°C format.\u0026#34;) public Function\u0026lt;Request, Response\u0026gt; weatherFunction() { return new MockWeatherService(); } String response = ChatClient.create(this.chatModel) .prompt(\u0026#34;What\u0026#39;s the weather like in Boston?\u0026#34;) .tools(\u0026#34;weatherFunction\u0026#34;) .inputType(Request.class) .call() .content(); 在[ 工具](../tools.html)文档中查找更多信息。\n多式联运 # 多模态是指模型同时理解和处理来自各种（输入）源的信息的能力，包括 text 、 pdf 、 images 、 audio 和其他数据格式。\n图像、音频、视频 # Google 的 Gemini AI 模型通过理解和整合文本、代码、音频、图像和视频来支持此功能。更多详情，请参阅博客文章 [ “Gemini 简介”]( https://blog.google/technology/ai/google-gemini-ai/#introducing-gemini) 。\nSpring AI 的 Message 接口通过引入 Media 类型来支持多模态 AI 模型。此类型包含消息中媒体附件的数据和信息，使用 Spring 的 org.springframework.util.MimeType 和 java.lang.Object 作为原始媒体数据。\n下面是从 [ VertexAiGeminiChatModelIT#multiModalityTest()]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-vertex-ai-gemini/src/test/java/org/springframework/ai/vertexai/gemini/VertexAiGeminiChatModelIT.java) 中提取的简单代码示例，演示了用户文本与图像的组合。\nbyte[] data = new ClassPathResource(\u0026#34;/vertex-test.png\u0026#34;).getContentAsByteArray(); var userMessage = new UserMessage(\u0026#34;Explain what do you see on this picture?\u0026#34;, List.of(new Media(MimeTypeUtils.IMAGE_PNG, this.data))); ChatResponse response = chatModel.call(new Prompt(List.of(this.userMessage))); 最新的 Vertex Gemini 提供了对 PDF 输入类型的支持。使用 application/pdf 媒体类型将 PDF 文件附加到消息：\nvar pdfData = new ClassPathResource(\u0026#34;/spring-ai-reference-overview.pdf\u0026#34;); var userMessage = new UserMessage( \u0026#34;You are a very professional document summarization specialist. Please summarize the given document.\u0026#34;, List.of(new Media(new MimeType(\u0026#34;application\u0026#34;, \u0026#34;pdf\u0026#34;), pdfData))); var response = this.chatModel.call(new Prompt(List.of(userMessage))); 样品控制器 # [ 创建]( https://start.spring.io/)一个新的 Spring Boot 项目并将 spring-ai-starter-model-vertex-ai-gemini 添加到您的 pom（或 gradle）依赖项中。\n在 src/main/resources 目录下添加一个 application.properties 文件，以启用和配置 VertexAi 聊天模型：\nspring.ai.vertex.ai.gemini.project-id=PROJECT_ID spring.ai.vertex.ai.gemini.location=LOCATION spring.ai.vertex.ai.gemini.chat.options.model=gemini-2.0-flash spring.ai.vertex.ai.gemini.chat.options.temperature=0.5 这将创建一个 VertexAiGeminiChatModel 实现，您可以将其注入到您的类中。这是一个使用聊天模型生成文本的简单 @Controller 类的示例。\n@RestController public class ChatController { private final VertexAiGeminiChatModel chatModel; @Autowired public ChatController(VertexAiGeminiChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, this.chatModel.call(message)); } @GetMapping(\u0026#34;/ai/generateStream\u0026#34;) public Flux\u0026lt;ChatResponse\u0026gt; generateStream(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { Prompt prompt = new Prompt(new UserMessage(message)); return this.chatModel.stream(prompt); } } 手动配置 # VertexAiGeminiChatModel 实现了 ChatModel 并使用 VertexAI 连接到 Vertex AI Gemini 服务。\n将 spring-ai-vertex-ai-gemini 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-vertex-ai-gemini\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-vertex-ai-gemini\u0026#39; } 接下来，创建一个 VertexAiGeminiChatModel 并将其用于文本生成：\nVertexAI vertexApi = new VertexAI(projectId, location); var chatModel = new VertexAiGeminiChatModel(this.vertexApi, VertexAiGeminiChatOptions.builder() .model(ChatModel.GEMINI_2_0_FLASH) .temperature(0.4) .build()); ChatResponse response = this.chatModel.call( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); VertexAiGeminiChatOptions 提供聊天请求的配置信息。 VertexAiGeminiChatOptions.Builder 是流畅的选项构建器。\n低级 Java 客户端 # 以下类图说明了 Vertex AI Gemini 原生 Java API：\n"},{"id":9,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B-api/%E4%BA%9A%E9%A9%AC%E9%80%8A%E5%9F%BA%E5%B2%A9/","title":"亚马逊基岩","section":"嵌入模型 API","content":" 亚马逊基岩 # [ Amazon Bedrock]( https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html) 是一种托管服务，可通过统一的 API 提供来自各种 AI 提供商的基础模型。\nSpring AI 通过实现 Spring EmbeddingModel 接口支持通过 Amazon Bedrock 提供的 [ Embedding AI 模型]( https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids-arns.html) 。\n此外，Spring AI 为所有客户端提供 Spring 自动配置和 Boot Starters，从而可以轻松地引导和配置 Bedrock 模型。\n入门 # 有几个步骤可以开始\n将 Bedrock 的 Spring Boot 启动器添加到您的项目中。 获取 AWS 凭证：如果您尚未配置 AWS 账户和 AWS CLI，本视频指南可以帮助您进行配置： 不到 4 分钟即可完成 AWS CLI 和 SDK 设置！ 您应该能够获取访问密钥和安全密钥。 启用要使用的模型：转到 Amazon Bedrock ，从左侧的模型访问菜单中配置您要使用的模型的访问权限。 项目依赖关系 # 然后将 Spring Boot Starter 依赖项添加到项目的 Maven pom.xml 构建文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-bedrock\u0026lt;/artifactId\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-bedrock\u0026#39; } 连接到 AWS Bedrock # 使用 BedrockAwsConnectionProperties 配置 AWS 凭证和区域：\nspring.ai.bedrock.aws.region=us-east-1 spring.ai.bedrock.aws.access-key=YOUR_ACCESS_KEY spring.ai.bedrock.aws.secret-key=YOUR_SECRET_KEY spring.ai.bedrock.aws.timeout=10m region 属性是强制性的。\nAWS 凭证按以下顺序解析：\nAWS 区域按以下顺序解析：\n除了标准的 Spring-AI Bedrock 凭证和区域属性配置之外，Spring-AI 还提供对自定义 AwsCredentialsProvider 和 AwsRegionProvider bean 的支持。\n启用选定的 Bedrock 模型 # 以下是支持的\u0026lt;model\u0026gt; s:\n例如，要启用 Bedrock Cohere 嵌入模型，您需要设置 spring.ai.bedrock.cohere.embedding.enabled=true 。\n接下来，您可以使用 spring.ai.bedrock.\u0026lt;model\u0026gt;.embedding.* 属性来配置所提供的每个模型。\n有关更多信息，请参阅下面针对每个受支持型号的文档。\nSpring AI Bedrock Cohere 嵌入 ： spring.ai.bedrock.cohere.embedding.enabled=true Spring AI Bedrock Titan 嵌入 ： spring.ai.bedrock.titan.embedding.enabled=true "},{"id":10,"href":"/docs/%E4%BB%8B%E7%BB%8D/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A6%82%E5%BF%B5/","title":"人工智能概念","section":"介绍","content":" 人工智能概念 # 本节介绍 Spring AI 使用的核心概念。建议您仔细阅读，以了解 Spring AI 实现背后的思想。\n模型 # 人工智能模型是用于处理和生成信息的算法，通常模仿人类的认知功能。通过从大型数据集中学习模式和洞察，这些模型可以进行预测、文本、图像或其他输出，从而增强各行各业的各种应用。\nAI 模型种类繁多，每种模型都适用于特定的用例。虽然 ChatGPT 及其生成式 AI 功能通过文本输入和输出吸引了众多用户，但许多模型和公司也提供多样化的输入和输出。在 ChatGPT 出现之前，许多人对文本转图像生成模型（例如 Midjourney 和 Stable Diffusion）着迷。\n下表根据输入和输出类型对几种模型进行了分类：\nSpring AI 目前支持将输入和输出处理为语言、图像和音频的模型。上表中的最后一行接受文本作为输入并输出数字，这通常被称为嵌入文本，代表 AI 模型中使用的内部数据结构。Spring AI 支持嵌入，以实现更高级的用例。\nGPT 等模型的独特之处在于其预训练特性，正如 GPT 中的“P”（聊天生成预训练 Transformer）所示。这项预训练功能将 AI 转变为通用的开发工具，无需丰富的机器学习或模型训练背景。\n提示 # 提示是基于语言的输入的基础，引导 AI 模型生成特定的输出。对于熟悉 ChatGPT 的人来说，提示可能看起来仅仅是在对话框中输入并发送到 API 的文本。然而，它包含的内容远不止于此。在许多 AI 模型中，提示的文本不仅仅是一个简单的字符串。\nChatGPT 的 API 在一个提示中包含多个文本输入，每个文本输入都被分配一个角色。例如，系统角色用于指示模型如何操作并设置交互的上下文。此外，还有用户角色，通常是来自用户的输入。\n制作有效的提示既是一门艺术，也是一门科学。ChatGPT 是为人类对话而设计的。这与使用 SQL 之类的语言“提问”截然不同。人们必须像与人交谈一样与 AI 模型进行交流。\n这种交互方式如此重要，以至于“提示工程”一词已发展成为一门独立的学科。目前，有越来越多的技术可以提高提示的有效性。投入时间精心设计提示可以显著提升最终效果。\n分享提示已成为一种公共实践，学术界也正在积极开展这方面的研究。为了说明创建有效提示（例如，与 SQL 对比）是多么违反直觉， [ 最近的一篇研究论文]( https://arxiv.org/abs/2205.11916)发现，最有效的提示之一以“深呼吸，一步一步来”这句话开头。这应该能让你明白语言为何如此重要。我们尚未完全了解如何最有效地利用这项技术的早期版本，例如 ChatGPT 3.5，更不用说正在开发的新版本了。\n提示模板 # 创建有效的提示涉及建立请求的上下文以及用特定于用户输入的值替换请求的各部分。\n此过程使用传统的基于文本的模板引擎来创建和管理提示。Spring AI 为此使用了 OSS 库 [ StringTemplate]( https://www.stringtemplate.org/) 。\n例如，考虑简单的提示模板：\nTell me a {adjective} joke about {content}. 在 Spring AI 中，提示模板可以类比为 Spring MVC 架构中的“视图”。它提供了一个模型对象（通常是 java.util.Map ），用于填充模板中的占位符。“渲染”后的字符串将成为提供给 AI 模型的提示内容。\n发送给模型的提示的具体数据格式存在相当大的差异。提示最初只是一些简单的字符串，后来演变为包含多条消息，每条消息中的每个字符串都代表着模型的不同角色。\n嵌入 # 嵌入是文本、图像或视频的数字表示，用于捕捉输入之间的关系。\n嵌入的工作原理是将文本、图像和视频转换为浮点数数组（称为向量）。这些向量旨在捕捉文本、图像和视频的含义。嵌入数组的长度称为向量的维数。\n通过计算两段文本的向量表示之间的数值距离，应用程序可以确定用于生成嵌入向量的对象之间的相似性。\n作为探索 AI 的 Java 开发者，无需理解这些向量表示背后的复杂数学理论或具体实现。只需了解它们在 AI 系统中的作用和功能即可，尤其是在将 AI 功能集成到应用程序中时。\n嵌入在实际应用中尤为重要，例如检索增强生成 (RAG) 模式。它们能够将数据表示为语义空间中的点，语义空间类似于欧氏几何的二维空间，但维度更高。这意味着，就像欧氏几何中平面上的点根据其坐标可以远近一样，在语义空间中，点的接近程度反映了含义的相似性。在这个多维空间中，关于相似主题的句子位置更近，就像图上彼此靠近的点一样。这种接近性有助于文本分类、语义搜索甚至产品推荐等任务，因为它允许 AI 根据相关概念在这个扩展的语义景观中的“位置”来识别和分组它们。\n你可以把这个语义空间想象成一个向量。\n代币 # 标记是 AI 模型运作的基石。输入时，模型将单词转换为标记。输出时，模型将标记转换回单词。\n在英语中，一个标记大致对应一个单词的 75%。作为参考，莎士比亚全集总计约 90 万字，翻译过来大约有 120 万个标记。\n或许更重要的是，代币=金钱。在托管 AI 模型的背景下，你的费用取决于使用的代币数量。输入和输出都会计入代币总量。\n此外，模型还受到令牌限制的影响，这会限制单个 API 调用中处理的文本量。此阈值通常称为“上下文窗口”。模型不会处理任何超出此限制的文本。\n例如，ChatGPT3 的代币限制为 4K，而 GPT4 则提供 8K、16K 和 32K 等不同选项。Anthropic 的 Claude AI 模型的代币限制为 10 万，而 Meta 的最新研究则提出了 100 万代币限制模型。\n要使用 GPT4 总结莎士比亚全集，您需要设计软件工程策略来切分数据，并在模型的上下文窗口限制内呈现数据。Spring AI 项目可以帮助您完成这项任务。\n结构化输出 # 即使你要求回复为 JSON 格式，AI 模型的输出通常也会以 java.lang.String 的形式出现。它可能是正确的 JSON，但它并非 JSON 数据结构。它只是一个字符串。此外，在提示中要求“需要 JSON”并非 100% 准确。\n这种复杂性导致了一个专门领域的出现，该领域涉及创建提示以产生预期的输出，然后将生成的简单字符串转换为可用于应用程序集成的数据结构。\n[ 结构化输出转换](api/structured-output-converter.html#_structuredoutputconverter)采用精心设计的提示，通常需要与模型进行多次交互才能实现所需的格式。\n将您的数据和 API 引入 AI 模型 # 如何为人工智能模型配备尚未训练过的信息？\n请注意，GPT 3.5/4.0 数据集仅持续到 2021 年 9 月。因此，该模型表示，它不知道需要该日期之后知识的问题的答案。有趣的是，该数据集的大小约为 650GB。\n有三种技术可以定制 AI 模型来整合您的数据：\n微调 ：这项传统的机器学习技术涉及调整模型并更改其内部权重。然而，对于机器学习专家来说，这是一个极具挑战性的过程，而且由于 GPT 等模型的规模庞大，会极其耗费资源。此外，某些模型可能不提供此选项。 提示填充 ：一种更实用的替代方案是将数据嵌入到提供给模型的提示中。考虑到模型的令牌限制，需要采用一些技术在模型的上下文窗口中呈现相关数据。这种方法通俗地称为“填充提示”。Spring AI 库可以帮助您基于“填充提示”技术（也称为检索增强生成 (RAG)） 实现解决方案。 工具调用 ：此技术允许注册工具（用户定义的服务），将大型语言模型连接到外部系统的 API。Spring AI 大大简化了支持工具调用所需的代码。 检索增强生成 # 一种称为检索增强生成 (RAG) 的技术已经出现，用于解决将相关数据纳入准确的 AI 模型响应提示的挑战。\n该方法涉及批处理式编程模型，其中作业从文档中读取非结构化数据，进行转换，然后将其写入矢量数据库。从高层次上讲，这是一个 ETL（提取、转换和加载）流程。矢量数据库用于 RAG 技术的检索部分。\n将非结构化数据加载到矢量数据库的过程中，最重要的转换之一是将原始文档拆分成更小的块。将原始文档拆分成更小块的过程包含两个重要步骤：\nRAG 的下一个阶段是处理用户输入。当 AI 模型需要回答用户的问题时，该问题和所有“相似”的文档片段都会被放入发送给 AI 模型的提示中。这就是使用矢量数据库的原因。它非常擅长查找相似内容。\nETL 管道提供了有关协调从数据源提取数据并将其存储在结构化向量存储中的流程的更多信息，确保数据在传递给 AI 模型时具有最佳的检索格式。 ChatClient - RAG 解释了如何使用 QuestionAnswerAdvisor 在您的应用程序中启用 RAG 功能。 工具调用 # 大型语言模型 (LLM) 在训练后被冻结，导致知识陈旧，并且无法访问或修改外部数据。\n[ 工具调用](api/tools.html)机制解决了这些缺陷。它允许您将自己的服务注册为工具，将大型语言模型连接到外部系统的 API。这些系统可以为 LLM 提供实时数据，并代表 LLM 执行数据处理操作。\nSpring AI 极大地简化了您编写支持工具调用所需的代码。它为您处理工具调用对话。您可以将工具作为 @Tool 注解的方法提供，并在提示选项中提供，以便模型可以使用它。此外，您还可以在单​​个提示中定义和引用多个工具。\n请遵循[ 工具调用](api/tools.html)文档以获取有关如何在不同的 AI 模型中使用此功能的更多信息。\n评估人工智能的反应 # 有效地评估人工智能系统响应用户请求的输出，对于确保最终应用的准确性和实用性至关重要。一些新兴技术使得预训练模型本身能够实现这一目标。\n此评估过程涉及分析生成的响应是否符合用户意图和查询上下文。相关性、连贯性和事实正确性等指标用于衡量 AI 生成的响应的质量。\n一种方法是同时呈现用户的请求和 AI 模型的响应，查询响应是否与提供的数据一致。\n此外，利用矢量数据库中存储的信息作为补充数据可以增强评估过程，有助于确定响应相关性。\nSpring AI 项目提供了一个 Evaluator API，目前支持使用一些基本策略来评估模型响应。更多信息，请参阅[ 评估测试](api/testing.html)文档。\n"},{"id":11,"href":"/docs/%E4%BB%8B%E7%BB%8D/","title":"介绍","section":"Docs","content":" 介绍 # Spring AI 项目旨在简化包含人工智能功能的应用程序的开发，而不会产生不必要的复杂性。\n该项目的灵感源自一些著名的 Python 项目，例如 LangChain 和 LlamaIndex，但 Spring AI 并非这些项目的直接移植。该项目的创立基于这样一种信念：下一波生成式 AI 应用将不仅面向 Python 开发人员，还将遍及多种编程语言。\nSpring AI 提供了一些抽象，作为开发 AI 应用程序的基础。这些抽象具有多种实现，只需极少的代码更改即可轻松实现组件的替换。\nSpring AI 提供以下功能：\n跨 AI 提供商的可移植 API 支持聊天、文本转图像和嵌入模型。支持同步和流式 API 选项。此外，还提供特定于模型的功能访问。 支持所有主流 AI 模型提供商， 例如 Anthropic、OpenAI、Microsoft、Amazon、Google 和 Ollama。支持的模型类型包括： 聊天完成 嵌入 文本转图像 音频转录 文本转语音 适度 聊天完成 嵌入 文本转图像 音频转录 文本转语音 适度 结构化输出 - AI 模型输出到 POJO 的映射。 支持所有主要的矢量数据库提供商 ，例如 Apache Cassandra、Azure Cosmos DB、Azure Vector Search、Chroma、Elasticsearch、GemFire、MariaDB、Milvus、MongoDB Atlas、Neo4j、OpenSearch、Oracle、PostgreSQL/PGVector、PineCone、Qdrant、Redis、SAP Hana、Typesense 和 Weaviate。 跨 Vector Store 提供商的可移植 API，包括新颖的类似 SQL 的元数据过滤器 API。 工具/功能调用 ——允许模型请求执行客户端工具和功能，从而根据需要访问必要的实时信息并采取行动。 可观察性 ——提供对 AI 相关操作的洞察。 用于数据工程的文档提取 ETL 框架 。 AI 模型评估 - 帮助评估生成的内容并防止幻觉反应的实用程序。 Spring Boot 自动配置和 AI 模型和矢量存储的启动器。 ChatClient API - 用于与 AI 聊天模型通信的流畅 API，惯用语类似于 WebClient 和 RestClient API。 顾问 API - 封装重复的生成式 AI 模式，转换发送到和来自语言模型 (LLM) 的数据，并提供跨各种模型和用例的可移植性。 支持聊天对话记忆和检索增强生成（RAG） 。 此功能集可让您实现常见的用例，例如“通过文档进行问答”或“通过文档进行聊天”。\n[ 概念部分](concepts.html)提供了 AI 概念及其在 Spring AI 中的表示的高级概述。\n[ “入门”](getting-started.html) 部分将向您展示如何创建您的第一个 AI 应用程序。后续章节将以代码为中心的方式深入探讨每个组件和常见用例。\n"},{"id":12,"href":"/docs/%E5%8D%87%E7%BA%A7%E8%AF%B4%E6%98%8E/%E4%BB%8E-functioncallback-%E8%BF%81%E7%A7%BB%E5%88%B0-toolcallback-api/","title":"从 FunctionCallback 迁移到 ToolCallback API","section":"升级说明","content":" 从 FunctionCallback 迁移到 ToolCallback API # 本指南将帮助您从已弃用的 FunctionCallback API 迁移到 Spring AI 中全新的 ToolCallback API。如需了解更多关于新 API 的信息，请参阅[ 工具调用](tools.html)文档。\n变更概述 # 这些变化是 Spring AI 改进和扩展工具调用功能的一部分。此外，新 API 的术语从“函数”改为“工具”，以更好地符合行业惯例。这涉及多项 API 更改，同时通过弃用的方法保持向后兼容性。\n关键变化 # 迁移示例 # 1. 基本函数回调 # 前：\nFunctionCallback.builder() .function(\u0026#34;getCurrentWeather\u0026#34;, new MockWeatherService()) .description(\u0026#34;Get the weather in location\u0026#34;) .inputType(MockWeatherService.Request.class) .build() 后：\nFunctionToolCallback.builder(\u0026#34;getCurrentWeather\u0026#34;, new MockWeatherService()) .description(\u0026#34;Get the weather in location\u0026#34;) .inputType(MockWeatherService.Request.class) .build() 2. ChatClient 使用 # 前：\nString response = ChatClient.create(chatModel) .prompt() .user(\u0026#34;What\u0026#39;s the weather like in San Francisco?\u0026#34;) .functions(FunctionCallback.builder() .function(\u0026#34;getCurrentWeather\u0026#34;, new MockWeatherService()) .description(\u0026#34;Get the weather in location\u0026#34;) .inputType(MockWeatherService.Request.class) .build()) .call() .content(); 后：\nString response = ChatClient.create(chatModel) .prompt() .user(\u0026#34;What\u0026#39;s the weather like in San Francisco?\u0026#34;) .tools(FunctionToolCallback.builder(\u0026#34;getCurrentWeather\u0026#34;, new MockWeatherService()) .description(\u0026#34;Get the weather in location\u0026#34;) .inputType(MockWeatherService.Request.class) .build()) .call() .content(); 3.基于方法的函数回调 # 前：\nFunctionCallback.builder() .method(\u0026#34;getWeatherInLocation\u0026#34;, String.class, Unit.class) .description(\u0026#34;Get the weather in location\u0026#34;) .targetClass(TestFunctionClass.class) .build() 后：\nvar toolMethod = ReflectionUtils.findMethod(TestFunctionClass.class, \u0026#34;getWeatherInLocation\u0026#34;); MethodToolCallback.builder() .toolDefinition(ToolDefinition.builder(toolMethod) .description(\u0026#34;Get the weather in location\u0026#34;) .build()) .toolMethod(toolMethod) .build() 或者采用声明式方法：\nclass WeatherTools { @Tool(description = \u0026#34;Get the weather in location\u0026#34;) public void getWeatherInLocation(String location, Unit unit) { // ... } } 您可以使用相同的 ChatClient#tools() API 来注册基于方法的工具回调：\nString response = ChatClient.create(chatModel) .prompt() .user(\u0026#34;What\u0026#39;s the weather like in San Francisco?\u0026#34;) .tools(MethodToolCallback.builder() .toolDefinition(ToolDefinition.builder(toolMethod) .description(\u0026#34;Get the weather in location\u0026#34;) .build()) .toolMethod(toolMethod) .build()) .call() .content(); 或者采用声明式方法：\nString response = ChatClient.create(chatModel) .prompt() .user(\u0026#34;What\u0026#39;s the weather like in San Francisco?\u0026#34;) .tools(new WeatherTools()) .call() .content(); 4. 选项配置 # 前：\nFunctionCallingOptions.builder() .model(modelName) .function(\u0026#34;weatherFunction\u0026#34;) .build() 后：\nToolCallingChatOptions.builder() .model(modelName) .toolNames(\u0026#34;weatherFunction\u0026#34;) .build() 5. ChatClient Builder 中的默认函数 # 前：\nChatClient.builder(chatModel) .defaultFunctions(FunctionCallback.builder() .function(\u0026#34;getCurrentWeather\u0026#34;, new MockWeatherService()) .description(\u0026#34;Get the weather in location\u0026#34;) .inputType(MockWeatherService.Request.class) .build()) .build() 后：\nChatClient.builder(chatModel) .defaultTools(FunctionToolCallback.builder(\u0026#34;getCurrentWeather\u0026#34;, new MockWeatherService()) .description(\u0026#34;Get the weather in location\u0026#34;) .inputType(MockWeatherService.Request.class) .build()) .build() 6. Spring Bean 配置 # 前：\n@Bean public FunctionCallback weatherFunctionInfo() { return FunctionCallback.builder() .function(\u0026#34;WeatherInfo\u0026#34;, new MockWeatherService()) .description(\u0026#34;Get the current weather\u0026#34;) .inputType(MockWeatherService.Request.class) .build(); } 后：\n@Bean public ToolCallback weatherFunctionInfo() { return FunctionToolCallback.builder(\u0026#34;WeatherInfo\u0026#34;, new MockWeatherService()) .description(\u0026#34;Get the current weather\u0026#34;) .inputType(MockWeatherService.Request.class) .build(); } 重大变化 # 已弃用的方法 # 以下方法已被弃用并将在未来的版本中删除：\n改用其对应的 tools 。\n使用@Tool 进行声明式规范 # 现在您可以使用方法级注释（ @Tool ）向 Spring AI 注册工具：\nclass Home { @Tool(description = \u0026#34;Turn light On or Off in a room.\u0026#34;) void turnLight(String roomName, boolean on) { // ... logger.info(\u0026#34;Turn light in room: {} to: {}\u0026#34;, roomName, on); } } String response = ChatClient.create(this.chatModel).prompt() .user(\u0026#34;Turn the light in the living room On.\u0026#34;) .tools(new Home()) .call() .content(); 附加说明 # 时间线 # 已弃用的方法将在当前里程碑版本中保留，以实现向后兼容性，但将在下一个里程碑版本中移除。建议尽快迁移到新的 API。\n"},{"id":13,"href":"/docs/%E5%8F%82%E8%80%83/%E6%B5%8B%E8%AF%95/%E6%B5%8B%E8%AF%95%E5%AE%B9%E5%99%A8/","title":"测试容器","section":"测试","content":" 测试容器 # Spring AI 提供了 Spring Boot 自动配置，用于建立与通过 Testcontainers 运行的模型服务或向量存储的连接。要启用此功能，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-spring-boot-testcontainers\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-spring-boot-testcontainers\u0026#39; } 服务连接 # spring-ai-spring-boot-testcontainers 模块中提供了以下服务连接工厂：\n"},{"id":14,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B-api/%E4%BA%9A%E9%A9%AC%E9%80%8A%E5%9F%BA%E5%B2%A9/%E7%9B%B8%E5%B9%B2%E5%B5%8C%E5%85%A5/","title":"相干嵌入","section":"亚马逊基岩","content":" 相干嵌入 # 提供 Bedrock Cohere 嵌入模型。将生成式 AI 功能集成到关键应用和工作流程中，从而提升业务成果。\n[ AWS Bedrock Cohere 模型页面]( https://aws.amazon.com/bedrock/cohere-command-embed/)和 [ Amazon Bedrock 用户指南]( https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html)包含有关如何使用 AWS 托管模型的详细信息。\n先决条件 # 请参阅 [ Amazon Bedrock 上的 Spring AI 文档](../bedrock.html)以了解如何设置 API 访问。\n添加存储库和 BOM # Spring AI 的构件已发布在 Maven Central 和 Spring Snapshot 仓库中。请参阅 [ “构件仓库”](../../getting-started.html#artifact-repositories) 部分，将这些仓库添加到您的构建系统中。\n为了帮助管理依赖项，Spring AI 提供了 BOM（物料清单），以确保在整个项目中使用一致版本的 Spring AI。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # 将 spring-ai-starter-model-bedrock 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-bedrock\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-bedrock\u0026#39; } 启用 Cohell 嵌入支持 # 默认情况下，Cohere 嵌入模型是禁用的。要启用它，请在应用程序配置中将 spring.ai.model.embedding 属性设置为 bedrock-cohere ：\nspring.ai.model.embedding=bedrock-cohere 或者，您可以使用 Spring 表达式语言 (SpEL) 来引用环境变量：\n# In application.yml spring: ai: model: embedding: ${AI_MODEL_EMBEDDING} # In your environment or .env file export AI_MODEL_EMBEDDING=bedrock-cohere 您还可以在启动应用程序时使用 Java 系统属性设置此属性：\njava -Dspring.ai.model.embedding=bedrock-cohere -jar your-application.jar 嵌入属性 # 前缀 spring.ai.bedrock.aws 是配置与 AWS Bedrock 的连接的属性前缀。\n前缀 spring.ai.bedrock.cohere.embedding （在 BedrockCohereEmbeddingProperties 中定义）是配置 Cohere 嵌入模型实现的属性前缀。\n查看 [ CohereEmbeddingModel]( https://github.com/spring-projects/spring-ai/blob/056b95a00efa5b014a1f488329fbd07a46c02378/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/cohere/api/CohereEmbeddingBedrockApi.java#L150) 以获取其他模型 ID。支持的值为： cohere.embed-multilingual-v3 和 cohere.embed-english-v3 。模型 ID 值也可以在 [ AWS Bedrock 文档的基本模型 ID]( https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids-arns.html) 中找到。\n运行时选项 # [ BedrockCohereEmbeddingOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/cohere/[BedrockCohereEmbeddingOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/cohere/BedrockCohereEmbeddingOptions.java)) 提供模型配置，例如 input-type 或 truncate 。\n启动时，可以使用 BedrockCohereEmbeddingModel(api, options) 构造函数或 spring.ai.bedrock.cohere.embedding.options.* 属性配置默认选项。\n在运行时，您可以通过向 EmbeddingRequest 调用添加新的、特定于请求的选项来覆盖默认选项。例如，要覆盖特定请求的默认输入类型：\nEmbeddingResponse embeddingResponse = embeddingModel.call( new EmbeddingRequest(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;), BedrockCohereEmbeddingOptions.builder() .withInputType(InputType.SEARCH_DOCUMENT) .build())); 样品控制器 # [ 创建]( https://start.spring.io/)一个新的 Spring Boot 项目并将 spring-ai-starter-model-bedrock 添加到您的 pom（或 gradle）依赖项中。\n在 src/main/resources 目录下添加一个 application.properties 文件，以启用和配置 Cohere Embedding 模型：\nspring.ai.bedrock.aws.region=eu-central-1 spring.ai.bedrock.aws.access-key=${AWS_ACCESS_KEY_ID} spring.ai.bedrock.aws.secret-key=${AWS_SECRET_ACCESS_KEY} spring.ai.model.embedding=bedrock-cohere spring.ai.bedrock.cohere.embedding.options.input-type=search-document 这将创建一个 BedrockCohereEmbeddingModel 实现，您可以将其注入到您的类中。这是一个使用聊天模型进行文本生成的简单 @Controller 类的示例。\n@RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(\u0026#34;/ai/embedding\u0026#34;) public Map embed(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { EmbeddingResponse embeddingResponse = this.embeddingModel.embedForResponse(List.of(message)); return Map.of(\u0026#34;embedding\u0026#34;, embeddingResponse); } } 手动配置 # BedrockCohereEmbeddingModel 实现了 EmbeddingModel 并使用 [ Low-level CohereEmbeddingBedrockApi Client](#low-level-api) 连接到 Bedrock Cohere 服务。\n将 spring-ai-bedrock 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-bedrock\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-bedrock\u0026#39; } 接下来，创建一个 [ BedrockCohereEmbeddingModel]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/cohere/[BedrockCohereEmbeddingModel](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/cohere/BedrockCohereEmbeddingModel.java).java) 并将其用于文本嵌入：\nvar cohereEmbeddingApi =new CohereEmbeddingBedrockApi( CohereEmbeddingModel.COHERE_EMBED_MULTILINGUAL_V1.id(), EnvironmentVariableCredentialsProvider.create(), Region.US_EAST_1.id(), new ObjectMapper()); var embeddingModel = new BedrockCohereEmbeddingModel(this.cohereEmbeddingApi); EmbeddingResponse embeddingResponse = this.embeddingModel .embedForResponse(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;)); 低级 CohereEmbeddingBedrockApi 客户端 # [ CohereEmbeddingBedrockApi]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/cohere/api/[CohereEmbeddingBedrockApi](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/cohere/api/CohereEmbeddingBedrockApi.java).java) 在 AWS Bedrock [ Cohere Command 模型]( https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-cohere-command.html)之上提供了轻量级 Java 客户端。\n以下类图说明了 CohereEmbeddingBedrockApi 接口和构建块：\nCohereEmbeddingBedrockApi 支持 cohere.embed-english-v3 和 cohere.embed-multilingual-v3 模型，用于单次和批量嵌入计算。\n以下是以编程方式使用 API 的简单代码片段：\nCohereEmbeddingBedrockApi api = new CohereEmbeddingBedrockApi( CohereEmbeddingModel.COHERE_EMBED_MULTILINGUAL_V1.id(), EnvironmentVariableCredentialsProvider.create(), Region.US_EAST_1.id(), new ObjectMapper()); CohereEmbeddingRequest request = new CohereEmbeddingRequest( List.of(\u0026#34;I like to eat apples\u0026#34;, \u0026#34;I like to eat oranges\u0026#34;), CohereEmbeddingRequest.InputType.search_document, CohereEmbeddingRequest.Truncate.NONE); CohereEmbeddingResponse response = this.api.embedding(this.request); "},{"id":15,"href":"/docs/%E5%8F%82%E8%80%83/%E8%81%8A%E5%A4%A9%E5%AE%A2%E6%88%B7%E7%AB%AF-api/","title":"聊天客户端 API","section":"参考","content":" 聊天客户端 API # ChatClient 提供了流畅的 API 用于与 AI 模型进行通信。它支持同步和流式编程模型。\nFluent API 提供了一些方法，用于构建 [[Prompt](prompt.html#_prompt)](prompt.html#_prompt) 的组成部分，并将其作为输入传递给 AI 模型。[[Prompt](prompt.html#_prompt)](prompt.html#_prompt) 包含指导 AI 模型输出和行为的说明性文本。从 API 的角度来看， [[Prompt](prompt.html#_prompt)](prompt.html#_prompt) 由一系列消息组成。\nAI 模型处理两种主要类型的消息：用户消息（来自用户的直接输入）和系统消息（由系统生成以引导对话）。\n这些消息通常包含占位符，这些占位符会在运行时根据用户输入进行替换，以定制 AI 模型对用户输入的响应。\n还可以指定提示选项，例如要使用的 AI 模型的名称以及控制生成输出的随机性或创造性的温度设置。\n创建 ChatClient # ChatClient 是使用 ChatClient.Builder 对象创建的。您可以为任何 [ ChatModel](chatmodel.html) Spring Boot 自动配置获取一个自动配置的 ChatClient.Builder 实例，也可以通过编程方式创建一个。\n使用自动配置的 ChatClient.Builder # 在最简单的用例中，Spring AI 提供了 Spring Boot 自动配置功能，它会创建一个原型 ChatClient.Builder bean 供您注入到您的类中。以下是一个简单的示例，用于检索简单用户请求的 String 响应。\n@RestController class MyController { private final ChatClient chatClient; public MyController(ChatClient.Builder chatClientBuilder) { this.chatClient = chatClientBuilder.build(); } @GetMapping(\u0026#34;/ai\u0026#34;) String generation(String userInput) { return this.chatClient.prompt() .user(userInput) .call() .content(); } } 在这个简单的示例中，用户输入设置了用户消息的内容。call call() 方法向 AI 模型发送请求， content() 方法以 String 形式返回 AI 模型的响应。\n使用多种聊天模型 # 在以下几种情况下，您可能需要在单个应用程序中使用多个聊天模型：\n对不同类型的任务使用不同的模型（例如，对于复杂的推理使用强大的模型，对于简单的任务使用更快、更便宜的模型） 当一个模型服务不可用时实施回退机制 对不同的模型或配置进行 A/B 测试 根据用户的喜好为他们提供模型选择 结合专门的模型（一个用于代码生成，另一个用于创意内容等） 默认情况下，Spring AI 会自动配置一个 ChatClient.Builder bean。但是，您可能需要在应用程序中使用多个聊天模型。以下是处理这种情况的方法：\n在所有情况下，您都需要通过设置属性 spring.ai.chat.client.enabled=false 来禁用 ChatClient.Builder 自动配置。\n这使您可以手动创建多个 ChatClient 实例。\n具有单一模型类型的多个 ChatClients # 本节介绍一个常见的用例，您需要创建多个 ChatClient 实例，这些实例都使用相同的底层模型类型，但具有不同的配置。\n// Create ChatClient instances programmatically ChatModel myChatModel = ... // already autoconfigured by Spring Boot ChatClient chatClient = ChatClient.create(myChatModel); // Or use the builder for more control ChatClient.Builder builder = ChatClient.builder(myChatModel); ChatClient customChatClient = builder .defaultSystemPrompt(\u0026#34;You are a helpful assistant.\u0026#34;) .build(); 不同模型类型的 ChatClients # 当使用多个 AI 模型时，您可以为每个模型定义单独的 ChatClient bean：\nimport org.springframework.ai.chat.ChatClient; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; @Configuration public class ChatClientConfig { @Bean public ChatClient openAiChatClient(OpenAiChatModel chatModel) { return ChatClient.create(chatModel); } @Bean public ChatClient anthropicChatClient(AnthropicChatModel chatModel) { return ChatClient.create(chatModel); } } 然后，您可以使用 @Qualifier 注释将这些 bean 注入到应用程序组件中：\n@Configuration public class ChatClientExample { @Bean CommandLineRunner cli( @Qualifier(\u0026#34;openAiChatClient\u0026#34;) ChatClient openAiChatClient, @Qualifier(\u0026#34;anthropicChatClient\u0026#34;) ChatClient anthropicChatClient) { return args -\u0026gt; { var scanner = new Scanner(System.in); ChatClient chat; // Model selection System.out.println(\u0026#34;\\nSelect your AI model:\u0026#34;); System.out.println(\u0026#34;1. OpenAI\u0026#34;); System.out.println(\u0026#34;2. Anthropic\u0026#34;); System.out.print(\u0026#34;Enter your choice (1 or 2): \u0026#34;); String choice = scanner.nextLine().trim(); if (choice.equals(\u0026#34;1\u0026#34;)) { chat = openAiChatClient; System.out.println(\u0026#34;Using OpenAI model\u0026#34;); } else { chat = anthropicChatClient; System.out.println(\u0026#34;Using Anthropic model\u0026#34;); } // Use the selected chat client System.out.print(\u0026#34;\\nEnter your question: \u0026#34;); String input = scanner.nextLine(); String response = chat.prompt(input).call().content(); System.out.println(\u0026#34;ASSISTANT: \u0026#34; + response); scanner.close(); }; } } 多个与 OpenAI 兼容的 API 端点 # OpenAiApi 和 OpenAiChatModel 类提供了一个 mutate() 方法，允许您创建具有不同属性的现有实例的变体。当您需要使用多个与 OpenAI 兼容的 API 时，此功能尤其有用。\n@Service public class MultiModelService { private static final Logger logger = LoggerFactory.getLogger(MultiModelService.class); @Autowired private OpenAiChatModel baseChatModel; @Autowired private OpenAiApi baseOpenAiApi; public void multiClientFlow() { try { // Derive a new OpenAiApi for Groq (Llama3) OpenAiApi groqApi = baseOpenAiApi.mutate() .baseUrl(\u0026#34;https://api.groq.com/openai\u0026#34;) .apiKey(System.getenv(\u0026#34;GROQ_API_KEY\u0026#34;)) .build(); // Derive a new OpenAiApi for OpenAI GPT-4 OpenAiApi gpt4Api = baseOpenAiApi.mutate() .baseUrl(\u0026#34;https://api.openai.com\u0026#34;) .apiKey(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;)) .build(); // Derive a new OpenAiChatModel for Groq OpenAiChatModel groqModel = baseChatModel.mutate() .openAiApi(groqApi) .defaultOptions(OpenAiChatOptions.builder().model(\u0026#34;llama3-70b-8192\u0026#34;).temperature(0.5).build()) .build(); // Derive a new OpenAiChatModel for GPT-4 OpenAiChatModel gpt4Model = baseChatModel.mutate() .openAiApi(gpt4Api) .defaultOptions(OpenAiChatOptions.builder().model(\u0026#34;gpt-4\u0026#34;).temperature(0.7).build()) .build(); // Simple prompt for both models String prompt = \u0026#34;What is the capital of France?\u0026#34;; String groqResponse = ChatClient.builder(groqModel).build().prompt(prompt).call().content(); String gpt4Response = ChatClient.builder(gpt4Model).build().prompt(prompt).call().content(); logger.info(\u0026#34;Groq (Llama3) response: {}\u0026#34;, groqResponse); logger.info(\u0026#34;OpenAI GPT-4 response: {}\u0026#34;, gpt4Response); } catch (Exception e) { logger.error(\u0026#34;Error in multi-client flow\u0026#34;, e); } } } ChatClient Fluent API 允许您使用重载的 prompt 方法以三种不同的方式创建提示来启动 Fluent API：\nprompt() ：这个没有参数的方法允许您开始使用流畅的 API，允许您构建用户、系统和提示的其他部分。 prompt(Prompt prompt) ：此方法接受 Prompt 参数，让您传入使用 Prompt 的非流畅 API 创建的 Prompt 实例。 prompt(String content) ：这是一个类似于上一个重载的便捷方法。它接受用户的文本内容。 ChatClient 响应 # ChatClient API 提供了多种使用流畅 API 格式化 AI 模型响应的方法。\n返回 ChatResponse # AI 模型的响应是一个由 [[ChatResponse](chatmodel.html#ChatResponse)](chatmodel.html#[ChatResponse](chatmodel.html#ChatResponse)) 类型定义的丰富结构。它包含有关响应生成方式的元数据，还可以包含多个响应（称为 [ Generation](chatmodel.html# Generation) ），每个响应都有各自的元数据。元数据包含用于创建响应的令牌数量（每个令牌大约占一个单词的 3/4）。此信息非常重要，因为托管的 AI 模型会根据每个请求使用的令牌数量收费。\n下面显示了通过在 call() 方法后调用 chatResponse() 来返回包含元数据的 ChatResponse 对象的示例。\nChatResponse chatResponse = chatClient.prompt() .user(\u0026#34;Tell me a joke\u0026#34;) .call() .chatResponse(); 返回实体 # 您经常希望返回一个从返回的 String 映射而来的实体类。 entity() 方法提供了此功能。\n例如，给定 Java 记录：\nrecord ActorFilms(String actor, List\u0026lt;String\u0026gt; movies) {} 您可以使用 entity() 方法轻松地将 AI 模型的输出映射到此记录，如下所示：\nActorFilms actorFilms = chatClient.prompt() .user(\u0026#34;Generate the filmography for a random actor.\u0026#34;) .call() .entity(ActorFilms.class); 还有一种带有签名 entity(ParameterizedTypeReference type) 的重载 entity 方法，可让您指定诸如通用列表之类的类型：\nList\u0026lt;ActorFilms\u0026gt; actorFilms = chatClient.prompt() .user(\u0026#34;Generate the filmography of 5 movies for Tom Hanks and Bill Murray.\u0026#34;) .call() .entity(new ParameterizedTypeReference\u0026lt;List\u0026lt;ActorFilms\u0026gt;\u0026gt;() {}); 流式响应 # stream() 方法可让您获得异步响应，如下所示：\nFlux\u0026lt;String\u0026gt; output = chatClient.prompt() .user(\u0026#34;Tell me a joke\u0026#34;) .stream() .content(); 您还可以使用方法 Flux\u0026lt;ChatResponse\u0026gt; chatResponse() 来流式传输 ChatResponse 。\n未来，我们将提供一种便捷方法，让您使用响应式 stream() 方法返回 Java 实体。同时，您应该使用[ 结构化输出转换器](structured-output-converter.html#StructuredOutputConverter)显式转换聚合响应，如下所示。这也演示了 Fluent API 中参数的用法，我们将在文档的后续部分中更详细地讨论。\nvar converter = new BeanOutputConverter\u0026lt;\u0026gt;(new ParameterizedTypeReference\u0026lt;List\u0026lt;ActorsFilms\u0026gt;\u0026gt;() {}); Flux\u0026lt;String\u0026gt; flux = this.chatClient.prompt() .user(u -\u0026gt; u.text(\u0026#34;\u0026#34;\u0026#34; Generate the filmography for a random actor. {format} \u0026#34;\u0026#34;\u0026#34;) .param(\u0026#34;format\u0026#34;, this.converter.getFormat())) .stream() .content(); String content = this.flux.collectList().block().stream().collect(Collectors.joining()); List\u0026lt;ActorFilms\u0026gt; actorFilms = this.converter.convert(this.content); 提示模板 # ChatClient 流畅 API 允许您提供用户和系统文本作为模板，并在运行时替换变量。\nString answer = ChatClient.create(chatModel).prompt() .user(u -\u0026gt; u .text(\u0026#34;Tell me the names of 5 movies whose soundtrack was composed by {composer}\u0026#34;) .param(\u0026#34;composer\u0026#34;, \u0026#34;John Williams\u0026#34;)) .call() .content(); 在内部，ChatClient 使用 PromptTemplate 类来处理用户和系统文本，并根据给定的 TemplateRenderer 实现将变量替换为运行时提供的值。默认情况下，Spring AI 使用 StTemplateRenderer 实现，该实现基于 Terence Parr 开发的开源 [ StringTemplate]( https://www.stringtemplate.org/) 引擎。\n对于不需要模板处理的情况，Spring AI 还提供了 NoOpTemplateRenderer 。\nSpring AI 还提供了 NoOpTemplateRenderer 。\n如果您希望使用其他模板引擎，可以直接向 ChatClient 提供 TemplateRenderer 接口的自定义实现。您也可以继续使用默认的 StTemplateRenderer ，但需要自定义配置。\n例如，默认情况下，模板变量由 {} 语法标识。如果您计划在提示符中包含 JSON，则可能需要使用其他语法来避免与 JSON 语法冲突。例如，您可以使用 \u0026lt; 和 \u0026gt; 分隔符。\nString answer = ChatClient.create(chatModel).prompt() .user(u -\u0026gt; u .text(\u0026#34;Tell me the names of 5 movies whose soundtrack was composed by \u0026lt;composer\u0026gt;\u0026#34;) .param(\u0026#34;composer\u0026#34;, \u0026#34;John Williams\u0026#34;)) .templateRenderer(StTemplateRenderer.builder().startDelimiterToken(\u0026#39;\u0026lt;\u0026#39;).endDelimiterToken(\u0026#39;\u0026gt;\u0026#39;).build()) .call() .content(); call() 返回值 # 在 ChatClient 上指定 call() 方法后，响应类型有几种不同的选项。\nString content() ：返回响应的字符串内容 ChatResponse chatResponse() ：返回包含多个代以及有关响应的元数据的 ChatResponse 对象，例如，使用了多少个令牌来创建响应。 ChatClientResponse chatClientResponse() ：返回一个 ChatClientResponse 对象，该对象包含 ChatResponse 对象和 ChatClient 执行上下文，使您能够访问顾问执行期间使用的其他数据（例如，在 RAG 流中检索到的相关文档）。 entity() 返回 Java 类型 entity(ParameterizedTypeReference type) ：用于返回实体类型的 Collection 。 entity(Class type) ：用于返回特定的实体类型。 entity(StructuredOutputConverter structuredOutputConverter) ：用于指定 StructuredOutputConverter 的实例，以将 String 转换为实体类型。 entity(ParameterizedTypeReference type) ：用于返回实体类型的 Collection 。 entity(Class type) ：用于返回特定的实体类型。 entity(StructuredOutputConverter structuredOutputConverter) ：用于指定 StructuredOutputConverter 的实例，以将 String 转换为实体类型。 您还可以调用 stream() 方法而不是 call() 。\nstream() 返回值 # 在 ChatClient 上指定 stream() 方法后，响应类型有以下几种选项：\nFlux content() ：返回由 AI 模型生成的字符串的 Flux 。 Flux chatResponse() ：返回 ChatResponse 对象的 Flux ，其中包含有关响应的其他元数据。 Flux chatClientResponse() ：返回包含 ChatResponse 对象和 ChatClient 执行上下文的 ChatClientResponse 对象的 Flux ，使您能够访问顾问执行期间使用的其他数据（例如，在 RAG 流中检索到的相关文档）。 使用默认值 # 在 @Configuration 类中创建带有默认系统文本的 ChatClient 可以简化运行时代码。通过设置默认值，您只需在调用 ChatClient 时指定用户文本，无需在运行时代码路径中为每个请求设置系统文本。\n默认系统文本 # 在以下示例中，我们将配置系统文本，使其始终以海盗的声音回复。为了避免在运行时代码中重复系统文本，我们将在 @Configuration 类中创建一个 ChatClient 实例。\n@Configuration class Config { @Bean ChatClient chatClient(ChatClient.Builder builder) { return builder.defaultSystem(\u0026#34;You are a friendly chat bot that answers question in the voice of a Pirate\u0026#34;) .build(); } } 并使用 @RestController 来调用它：\n@RestController class AIController { private final ChatClient chatClient; AIController(ChatClient chatClient) { this.chatClient = chatClient; } @GetMapping(\u0026#34;/ai/simple\u0026#34;) public Map\u0026lt;String, String\u0026gt; completion(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;completion\u0026#34;, this.chatClient.prompt().user(message).call().content()); } } 通过 curl 调用应用程序端点时，结果为：\n❯ curl localhost:8080/ai/simple {\u0026#34;completion\u0026#34;:\u0026#34;Why did the pirate go to the comedy club? To hear some arrr-rated jokes! Arrr, matey!\u0026#34;} 带参数的默认系统文本 # 在下面的例子中，我们将使用系统文本中的占位符来指定在运行时而不是设计时完成的声音。\n@Configuration class Config { @Bean ChatClient chatClient(ChatClient.Builder builder) { return builder.defaultSystem(\u0026#34;You are a friendly chat bot that answers question in the voice of a {voice}\u0026#34;) .build(); } } @RestController class AIController { private final ChatClient chatClient; AIController(ChatClient chatClient) { this.chatClient = chatClient; } @GetMapping(\u0026#34;/ai\u0026#34;) Map\u0026lt;String, String\u0026gt; completion(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message, String voice) { return Map.of(\u0026#34;completion\u0026#34;, this.chatClient.prompt() .system(sp -\u0026gt; sp.param(\u0026#34;voice\u0026#34;, voice)) .user(message) .call() .content()); } } 通过 httpie 调用应用程序端点时，结果为：\nhttp localhost:8080/ai voice==\u0026#39;Robert DeNiro\u0026#39; { \u0026#34;completion\u0026#34;: \u0026#34;You talkin\u0026#39; to me? Okay, here\u0026#39;s a joke for ya: Why couldn\u0026#39;t the bicycle stand up by itself? Because it was two tired! Classic, right?\u0026#34; } 其他默认设置 # 在 ChatClient.Builder 级别，您可以指定默认提示配置。\ndefaultOptions(ChatOptions chatOptions) ：传入 ChatOptions 类中定义的可移植选项或特定于模型的选项（例如 OpenAiChatOptions 中的选项）。有关特定于模型的 ChatOptions 实现的更多信息，请参阅 JavaDocs。 defaultFunction(String name, String description, java.util.function.Function\u0026lt;I, O\u0026gt; function) ： name 用于在用户文本中引用该函数。 description 解释了该函数的用途，并帮助 AI 模型选择正确的函数以获得准确的响应。 function 参数是一个 Java 函数实例，模型会在必要时执行该实例。 defaultFunctions(String…​ functionNames) ：应用程序上下文中定义的“java.util.Function”的 bean 名称。 defaultUser(String text) 、 defaultUser(Resource text) 、 defaultUser(Consumer userSpecConsumer) ：这些方法允许您定义用户文本。Consumer Consumer 允许您使用 lambda 表达式指定用户文本和任何默认参数。 defaultAdvisors(Advisor…​ advisor) ：Advisor 允许修改用于创建 Prompt 数据。QuestionAnswerAdvisor 实现通过在 QuestionAnswerAdvisor 中附加与用户文本相关的上下文信息来启用 Retrieval Augmented Generation 模式。 defaultAdvisors(Consumer advisorSpecConsumer) ：此方法允许您定义一个 Consumer ，以便使用 AdvisorSpec 配置多个顾问。顾问可以修改用于创建最终 Prompt 数据。Consumer Consumer 允许您指定一个 lambda 表达式来添加顾问，例如 QuestionAnswerAdvisor ，它通过根据用户文本在提示中附加相关的上下文信息来支持 Retrieval Augmented Generation 。 您可以在运行时使用不带 default 前缀的相应方法覆盖这些默认值。\nuser(String text) ， user(Resource text) ， user(Consumer userSpecConsumer) 顾问 # [ Advisors API](advisors.html) 提供了一种灵活而强大的方法来拦截、修改和增强 Spring 应用程序中的 AI 驱动的交互。\n使用用户文本调用 AI 模型时的一个常见模式是使用上下文数据附加或扩充提示。\n这些上下文数据可以属于不同类型。常见类型包括：\n您自己的数据 ：这是 AI 模型尚未训练过的数据。即使模型已经见过类似的数据，附加的上下文数据在生成响应时也会优先使用。 对话历史记录 ：聊天模型的 API 是无状态的。如果您告诉 AI 模型您的姓名，它不会在后续交互中记住它。每次请求都必须发送对话历史记录，以确保在生成响应时考虑到之前的交互。 ChatClient 中的顾问配置 # ChatClient Fluent API 提供了一个 AdvisorSpec 接口，用于配置顾问。该接口提供了一些方法，用于添加参数、一次性设置多个参数以及将一个或多个顾问添加到链中。\ninterface AdvisorSpec { AdvisorSpec param(String k, Object v); AdvisorSpec params(Map\u0026lt;String, Object\u0026gt; p); AdvisorSpec advisors(Advisor... advisors); AdvisorSpec advisors(List\u0026lt;Advisor\u0026gt; advisors); } ChatClient.builder(chatModel) .build() .prompt() .advisors( MessageChatMemoryAdvisor.builder(chatMemory).build(), QuestionAnswerAdvisor.builder(vectorStore).build() ) .user(userText) .call() .content(); 在此配置中， MessageChatMemoryAdvisor 将首先执行，并将对话历史记录添加到提示中。然后， QuestionAnswerAdvisor 将根据用户的问题和添加的对话历史记录执行搜索，从而可能提供更相关的结果。\n了解问答顾问\n检索增强生成 # 请参阅[ 检索增强生成](retrieval-augmented-generation.html)指南。\n日志记录 # SimpleLoggerAdvisor 是一个用于记录 ChatClient request 和 response 数据的顾问程序。这对于调试和监控 AI 交互非常有用。\n要启用日志记录，请在创建 ChatClient 时将 SimpleLoggerAdvisor 添加到顾问链中。建议将其添加到链的末尾：\nChatResponse response = ChatClient.create(chatModel).prompt() .advisors(new SimpleLoggerAdvisor()) .user(\u0026#34;Tell me a joke?\u0026#34;) .call() .chatResponse(); 要查看日志，请将顾问包的日志记录级别设置为 DEBUG ：\n将其添加到您的 application.properties 或 application.yaml 文件中。\n您可以使用以下构造函数自定义记录来自 AdvisedRequest 和 ChatResponse 的数据：\nSimpleLoggerAdvisor( Function\u0026lt;AdvisedRequest, String\u0026gt; requestToString, Function\u0026lt;ChatResponse, String\u0026gt; responseToString ) 使用示例：\nSimpleLoggerAdvisor customLogger = new SimpleLoggerAdvisor( request -\u0026gt; \u0026#34;Custom request: \u0026#34; + request.userText, response -\u0026gt; \u0026#34;Custom response: \u0026#34; + response.getResult() ); 这允许您根据您的特定需求定制记录的信息。\n聊天记忆 # ChatMemory 接口表示聊天对话内存的存储。它提供了向对话添加消息、从对话中检索消息以及清除对话历史记录的方法。\n目前有一个内置实现： MessageWindowChatMemory 。\nMessageWindowChatMemory 是一个聊天内存实现，它维护一个消息窗口，窗口大小不超过指定的最大限制（默认值：20 条消息）。当消息数量超过此限制时，较旧的消息将被清除，但系统消息将被保留。如果添加了新的系统消息，所有先前的系统消息都将从内存中移除。这确保了对话始终可以使用最新的上下文，同时限制了内存占用。\nMessageWindowChatMemory 由 ChatMemoryRepository 抽象支持，该抽象提供了聊天对话内存的存储实现。目前有多种实现可用，包括 InMemoryChatMemoryRepository 、 JdbcChatMemoryRepository 、 CassandraChatMemoryRepository 和 Neo4jChatMemoryRepository 。\n有关更多详细信息和使用示例，请参阅[ 聊天记忆](chat-memory.html)文档。\n实施说明 # ChatClient 结合了命令式和响应式编程模型，这是该 API 的一个独特之处。通常，一个应用程序要么是响应式的，要么是命令式的，但不会同时采用这两种模式。\n当定制模型实现的 HTTP 客户端交互时，必须同时配置 RestClient 和 WebClient。\n流式传输仅通过 Reactive 技术栈支持。因此，命令式应用程序必须包含 Reactive 技术栈（例如 spring-boot-starter-webflux）。\n非流式传输仅通过 Servlet 技术栈支持。因此，响应式应用程序必须包含 Servlet 技术栈（例如 spring-boot-starter-web），并且预期某些调用会被阻塞。\n工具调用是必需的，这会导致工作流程阻塞。这还会导致 Micrometer 观测结果不完整/中断（例如，ChatClient 跨度和工具调用跨度未连接，因此导致第一个跨度不完整）。\n内置的 advisor 对标准调用执行阻塞操作，对流式调用执行非阻塞操作。 advisor 流式调用所使用的 Reactor Scheduler 可以通过每个 Advisor 类上的 Builder 进行配置。\n"},{"id":16,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B-api/","title":"聊天模型 API","section":"模型","content":" 聊天模型 API # 聊天模型 API 使开发者能够将 AI 驱动的聊天补全功能集成到他们的应用程序中。它利用预训练的语言模型（例如 GPT，生成式预训练 Transformer），对用户的自然语言输入生成类似人类的响应。\n该 API 通常通过向 AI 模型发送提示或部分对话来工作，然后模型会根据其训练数据和对自然语言模式的理解，生成完整或延续的对话。完整的响应随后返回给应用程序，应用程序可以将其呈现给用户或用于进一步处理。\nSpring AI Chat Model API 旨在提供一个简单易用的接口，用于与各种 [ AI 模型](../concepts.html#_models)进行交互，使开发人员能够以最少的代码更改在不同模型之间切换。此设计符合 Spring 的模块化和可互换性理念。\n此外，借助 Prompt 用于输入封装）和 ChatResponse （用于输出处理）等辅助类，聊天模型 API 统一了与 AI 模型的通信。它管理了请求准备和响应解析的复杂性，提供了直接且简化的 API 交互。\n您可以在 [ “可用实现”](#_available_implementations) 部分找到有关可用实现的更多信息，并在 [ “聊天模型比较”](chat/comparison.html) 部分找到详细的比较。\nAPI 概述 # 本节提供 Spring AI Chat Model API 接口和相关类的指南。\n聊天模型 # 以下是 [ ChatModel]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-client-chat/src/main/java/org/springframework/ai/chat//model/[ChatModel](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-client-chat/src/main/java/org/springframework/ai/chat//model/ChatModel.java).java) 接口定义：\npublic interface ChatModel extends Model\u0026lt;Prompt, ChatResponse\u0026gt; { default String call(String message) {...} @Override ChatResponse call(Prompt prompt); } 带有 String 参数的 call() 方法简化了初始使用，避免了更复杂的 Prompt 和 ChatResponse 类的复杂性。在实际应用中，更常见的是使用 call() 方法，该方法接受 Prompt 实例并返回 ChatResponse 。\n流式聊天模型 # 以下是 [ StreamingChatModel]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/model/[StreamingChatModel](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/model/StreamingChatModel.java).java) 接口定义：\npublic interface StreamingChatModel extends StreamingModel\u0026lt;Prompt, ChatResponse\u0026gt; { default Flux\u0026lt;String\u0026gt; stream(String message) {...} @Override Flux\u0026lt;ChatResponse\u0026gt; stream(Prompt prompt); } stream() 方法采用类似于 ChatModel String 或 Prompt 参数，但它使用反应式 Flux API 来流式传输响应。\n迅速的 # [[Prompt](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-client-chat/src/main/java/org/springframework/ai/chat/prompt/Prompt.java)](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-client-chat/src/main/java/org/springframework/ai/chat/prompt/[Prompt](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-client-chat/src/main/java/org/springframework/ai/chat/prompt/Prompt.java).java) 是一个 ModelRequest ，它封装了 [ Message]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/messages/[Message](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/messages/Message.java).java) 对象列表和可选的模型请求选项。以下列表展示了 [[Prompt](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-client-chat/src/main/java/org/springframework/ai/chat/prompt/Prompt.java)](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-client-chat/src/main/java/org/springframework/ai/chat/prompt/[Prompt](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-client-chat/src/main/java/org/springframework/ai/chat/prompt/Prompt.java).java) 类的精简版本，不包括构造函数和其他实用方法：\npublic class Prompt implements ModelRequest\u0026lt;List\u0026lt;Message\u0026gt;\u0026gt; { private final List\u0026lt;Message\u0026gt; messages; private ChatOptions modelOptions; @Override public ChatOptions getOptions() {...} @Override public List\u0026lt;Message\u0026gt; getInstructions() {...} // constructors and utility methods omitted } 信息 # Message 接口封装了 Prompt 文本、元数据属性集合以及称为 MessageType 的分类。\n该接口定义如下：\npublic interface Content { String getText(); Map\u0026lt;String, Object\u0026gt; getMetadata(); } public interface Message extends Content { MessageType getMessageType(); } 多模式消息类型还实现了 ```MediaContent` 接口，提供了 Media`` 内容对象的列表。\npublic interface MediaContent extends Content { Collection\u0026lt;Media\u0026gt; getMedia(); } Message 接口有各种实现，分别对应于 AI 模型可以处理的消息类别：\n聊天完成端点，根据对话角色区分消息类别，通过 MessageType 有效映射。\n例如，OpenAI 可以识别不同对话角色（如 system 、 user 、 function 或 assistant 的消息类别。\n虽然术语 MessageType 可能暗示特定的消息格式，但在这种情况下，它实际上指定了消息在对话中所扮演的角色。\n对于不使用特定角色的 AI 模型， `UserMessage``` 实现充当标准类别，通常表示用户生成的查询或指令。要了解 Prompt和Message`` 实际应用以及它们之间的关系，尤其是在这些角色或消息类别的背景下，请参阅 [ “提示”](prompt.html) 部分中的详细解释。\n聊天选项 # 表示可传递给 AI 模型的选项。ChatOptions 类是 ChatOptions ModelOptions 子类，用于定义一些可传递给 AI 模型的可移植选项。ChatOptions ChatOptions 的定义如下：\npublic interface ChatOptions extends ModelOptions { String getModel(); Float getFrequencyPenalty(); Integer getMaxTokens(); Float getPresencePenalty(); List\u0026lt;String\u0026gt; getStopSequences(); Float getTemperature(); Integer getTopK(); Float getTopP(); ChatOptions copy(); } 此外，每个特定于模型的 ChatModel/StreamingChatModel 实现都可以拥有自己的选项，这些选项可以传递给 AI 模型。例如，OpenAI Chat Completion 模型就有自己的选项，例如 logitBias 、 seed 和 user 。\n这是一个强大的功能，允许开发人员在启动应用程序时使用特定于模型的选项，然后在运行时使用 Prompt 请求覆盖它们。\nSpring AI 提供了一个完善的聊天模型配置和使用系统。它允许在启动时设置默认配置，同时还提供了根据每个请求灵活覆盖这些设置的灵活性。这种方法使开发者能够轻松地使用不同的 AI 模型并根据需要调整参数，所有这些都在 Spring AI 框架提供的统一接口内完成。\n以下流程图说明了 Spring AI 如何处理聊天模型的配置和执行，结合启动和运行时选项：\n启动和运行时选项的分离允许进行全局配置和特定于请求的调整。\n聊天回复 # ChatResponse 类的结构如下：\npublic class ChatResponse implements ModelResponse\u0026lt;Generation\u0026gt; { private final ChatResponseMetadata chatResponseMetadata; private final List\u0026lt;Generation\u0026gt; generations; @Override public ChatResponseMetadata getMetadata() {...} @Override public List\u0026lt;Generation\u0026gt; getResults() {...} // other methods omitted } [ ChatResponse]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/model/[ChatResponse](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/model/ChatResponse.java).java) 类保存 AI 模型的输出，每个 Generation 实例包含由单个提示产生的多个潜在输出之一。\nChatResponse 类还携带有关 AI 模型响应的 ChatResponseMetadata 元数据。\n一代 # 最后， [ Generation]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/model/[Generation](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/model/Generation.java).java) 类从 ModelResult 扩展来表示模型输出（辅助消息）和相关元数据：\npublic class Generation implements ModelResult\u0026lt;AssistantMessage\u0026gt; { private final AssistantMessage assistantMessage; private ChatGenerationMetadata chatGenerationMetadata; @Override public AssistantMessage getOutput() {...} @Override public ChatGenerationMetadata getMetadata() {...} // other methods omitted } 可用的实现 # 该图说明了统一接口 ChatModel 和 StreamingChatModel ，用于与来自不同提供商的各种 AI 聊天模型进行交互，从而允许轻松集成和在不同的 AI 服务之间切换，同时为客户端应用程序维护一致的 API。\nOpenAI 聊天完成 （流式传输、多模态和函数调用支持） Microsoft Azure Open AI 聊天完成 （流媒体和函数调用支持） Ollama 聊天完成 （流式传输、多模式和函数调用支持） 拥抱脸部聊天完成 （不支持流媒体） Google Vertex AI Gemini 聊天完成 （流媒体、多模式和函数调用支持） 亚马逊基岩 Mistral AI 聊天完成 （流媒体和函数调用支持） 人类聊天完成 （流媒体和函数调用支持） 聊天模型 API # Spring AI 聊天模型 API 构建于 Spring AI Generic Model API 之上，提供特定于聊天功能的抽象和实现。这使得用户可以轻松地集成和切换不同的 AI 服务，同时为客户端应用程序维护一致的 API。以下类图展示了 Spring AI 聊天模型 API 的主要类和接口。\n"},{"id":17,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B-api/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B%E6%AF%94%E8%BE%83/","title":"聊天模型比较","section":"聊天模型 API","content":" 聊天模型比较 # 下表比较了 Spring AI 支持的各种聊天模型，详细说明了它们的功能：\n多模态 ：模型可以处理的输入类型（例如文本、图像、音频、视频）。 工具/功能调用 ：模型是否支持功能调用或工具使用。 流式：如果模型提供流式响应。 重试：支持重试机制。 可观察性 ：监控和调试的功能。 内置 JSON ：原生支持 JSON 输出。 本地部署：模型是否可以在本地运行。 OpenAI API 兼容性：模型是否与 OpenAI 的 API 兼容。 "},{"id":18,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E9%9F%B3%E9%A2%91%E6%A8%A1%E5%9E%8B/%E8%BD%AC%E5%BD%95-api/","title":"转录 API","section":"音频模型","content":" 转录 API # Spring AI 支持 OpenAI 的 Transcription API。当 Transcription 的其他提供程序实现后，将提取一个通用的 AudioTranscriptionModel 接口。\n"},{"id":19,"href":"/docs/%E5%8F%82%E8%80%83/%E8%81%8A%E5%A4%A9%E5%AE%A2%E6%88%B7%E7%AB%AF-api/%E9%A1%BE%E9%97%AE-api/","title":"顾问 API","section":"聊天客户端 API","content":" 顾问 API # Spring AI Advisors API 提供了一种灵活而强大的方法来拦截、修改和增强 Spring 应用程序中的 AI 驱动交互。通过利用 Advisors API，开发人员可以创建更复杂、可重用且更易于维护的 AI 组件。\n主要优势包括封装重复的生成式 AI 模式、转换发送到和来自大型语言模型 (LLM) 的数据，以及提供跨各种模型和用例的可移植性。\n您可以使用 [ ChatClient API](chatclient.html#_advisor_configuration_in_chatclient) 配置现有的顾问，如以下示例所示：\nvar chatClient = ChatClient.builder(chatModel) .defaultAdvisors( MessageChatMemoryAdvisor.builder(chatMemory).build(), // chat-memory advisor QuestionAnswerAdvisor.builder((vectorStore).builder() // RAG advisor ) .build(); var conversationId = \u0026#34;678\u0026#34;; String response = this.chatClient.prompt() // Set advisor parameters at runtime .advisors(advisor -\u0026gt; advisor.param(ChatMemory.CONVERSATION_ID, conversationId)) .user(userText) .call() .content(); 建议在构建时使用构建器的 defaultAdvisors() 方法注册顾问。\n顾问也参与可观察性堆栈，因此您可以查看与其执行相关的指标和跟踪。\n了解问答顾问 了解聊天记忆顾问 核心组件 # 该 API 包含用于非流式场景的 CallAroundAdvisor 和 CallAroundAdvisorChain ，以及用于流式场景的 StreamAroundAdvisor 和 StreamAroundAdvisorChain 。此外，它还包含用于表示未密封 Prompt 请求的 AdvisedRequest 和用于表示聊天完成响应的 AdvisedResponse 。两者都包含一个 advise-context 用于在顾问链之间共享状态。\nnextAroundCall() 和 nextAroundStream() 是关键的顾问方法，通常执行的操作包括检查未密封的提示数据、自定义和扩充提示数据、调用顾问链中的下一个实体、可选地阻止请求、检查聊天完成响应以及抛出异常以指示处理错误。\n此外， getOrder() 方法确定链中的顾问顺序，而 getName() 提供唯一的顾问名称。\nSpring AI 框架创建的 Advisor Chain 允许按 getOrder() 值的顺序调用多个 Advisor。值较低的 Advisor 优先执行。最后一个 Advisor 会自动添加，并将请求发送到 LLM。\n以下流程图说明了顾问链和聊天模型之间的交互：\n顾问订单 # 链中 advisor 的执行顺序由 getOrder() 方法决定。需要理解的关键点：\n具有较低顺序值的顾问将首先执行。 顾问链以堆栈的形式运行： 链中的第一位顾问是第一个处理请求的。 它也是最后处理响应的。 链中的第一位顾问是第一个处理请求的。 它也是最后处理响应的。 控制执行顺序： 将顺序设置为接近 Ordered.HIGHEST_PRECEDENCE 以确保顾问在链中首先执行（首先执行请求处理，最后执行响应处理）。 将顺序设置为接近 Ordered.LOWEST_PRECEDENCE 以确保顾问在链中最后执行（请求处理最后执行，响应处理首先执行）。 将顺序设置为接近 Ordered.HIGHEST_PRECEDENCE 以确保顾问在链中首先执行（首先执行请求处理，最后执行响应处理）。 将顺序设置为接近 Ordered.LOWEST_PRECEDENCE 以确保顾问在链中最后执行（请求处理最后执行，响应处理首先执行）。 值越高，优先级越低。 如果多个顾问具有相同的订单值，则无法保证它们的执行顺序。 提醒一下，以下是 Spring Ordered 接口的语义：\npublic interface Ordered { /** * Constant for the highest precedence value. * @see java.lang.Integer#MIN_VALUE */ int HIGHEST_PRECEDENCE = Integer.MIN_VALUE; /** * Constant for the lowest precedence value. * @see java.lang.Integer#MAX_VALUE */ int LOWEST_PRECEDENCE = Integer.MAX_VALUE; /** * Get the order value of this object. * \u0026lt;p\u0026gt;Higher values are interpreted as lower priority. As a consequence, * the object with the lowest value has the highest priority (somewhat * analogous to Servlet {@code load-on-startup} values). * \u0026lt;p\u0026gt;Same order values will result in arbitrary sort positions for the * affected objects. * @return the order value * @see #HIGHEST_PRECEDENCE * @see #LOWEST_PRECEDENCE */ int getOrder(); } API 概述 # 主要的 Advisor 接口位于包 org.springframework.ai.chat.client.advisor.api 中。以下是您在创建自己的 Advisor 时会遇到的关键接口：\npublic interface Advisor extends Ordered { String getName(); } 同步和反应式顾问的两个子接口是\npublic interface CallAroundAdvisor extends Advisor { /** * Around advice that wraps the ChatModel#call(Prompt) method. * @param advisedRequest the advised request * @param chain the advisor chain * @return the response */ AdvisedResponse aroundCall(AdvisedRequest advisedRequest, CallAroundAdvisorChain chain); } 和\npublic interface StreamAroundAdvisor extends Advisor { /** * Around advice that wraps the invocation of the advised request. * @param advisedRequest the advised request * @param chain the chain of advisors to execute * @return the result of the advised request */ Flux\u0026lt;AdvisedResponse\u0026gt; aroundStream(AdvisedRequest advisedRequest, StreamAroundAdvisorChain chain); } 要继续 Advice 链，请在 Advice 实现中使用 CallAroundAdvisorChain 和 StreamAroundAdvisorChain ：\n接口是\npublic interface CallAroundAdvisorChain { AdvisedResponse nextAroundCall(AdvisedRequest advisedRequest); } 和\npublic interface StreamAroundAdvisorChain { Flux\u0026lt;AdvisedResponse\u0026gt; nextAroundStream(AdvisedRequest advisedRequest); } 实现顾问 # 要创建 Advisor，请实现 CallAroundAdvisor 或 StreamAroundAdvisor （或两者兼有）。非流式 Advisor 的关键方法是实现 nextAroundCall() ，而流式 Advisor 的关键方法是 nextAroundStream() 。\n示例 # 我们将提供一些实际的例子来说明如何实现顾问来观察和扩充用例。\n日志顾问 # 我们可以实现一个简单的日志顾问，在调用链中的下一个顾问之前记录 AdvisedRequest ，并在调用之后 AdvisedResponse 。请注意，该顾问仅观察请求和响应，而不会对其进行修改。此实现支持非流式和流式场景。\npublic class SimpleLoggerAdvisor implements CallAroundAdvisor, StreamAroundAdvisor { private static final Logger logger = LoggerFactory.getLogger(SimpleLoggerAdvisor.class); @Override public String getName() { (1) return this.getClass().getSimpleName(); } @Override public int getOrder() { (2) return 0; } @Override public AdvisedResponse aroundCall(AdvisedRequest advisedRequest, CallAroundAdvisorChain chain) { logger.debug(\u0026#34;BEFORE: {}\u0026#34;, advisedRequest); AdvisedResponse advisedResponse = chain.nextAroundCall(advisedRequest); logger.debug(\u0026#34;AFTER: {}\u0026#34;, advisedResponse); return advisedResponse; } @Override public Flux\u0026lt;AdvisedResponse\u0026gt; aroundStream(AdvisedRequest advisedRequest, StreamAroundAdvisorChain chain) { logger.debug(\u0026#34;BEFORE: {}\u0026#34;, advisedRequest); Flux\u0026lt;AdvisedResponse\u0026gt; advisedResponses = chain.nextAroundStream(advisedRequest); return new MessageAggregator().aggregateAdvisedResponse(advisedResponses, advisedResponse -\u0026gt; logger.debug(\u0026#34;AFTER: {}\u0026#34;, advisedResponse)); (3) } } 重读（Re2）顾问 # “ [ 重读提升大型语言模型的推理能力]( https://arxiv.org/pdf/2309.06275) ”一文介绍了一种名为重读（Re2）的技术，它可以提升大型语言模型的推理能力。Re2 技术需要像这样扩充输入提示：\n实现将 Re2 技术应用于用户输入查询的顾问可以如下完成：\npublic class ReReadingAdvisor implements CallAroundAdvisor, StreamAroundAdvisor { private AdvisedRequest before(AdvisedRequest advisedRequest) { (1) Map\u0026lt;String, Object\u0026gt; advisedUserParams = new HashMap\u0026lt;\u0026gt;(advisedRequest.userParams()); advisedUserParams.put(\u0026#34;re2_input_query\u0026#34;, advisedRequest.userText()); return AdvisedRequest.from(advisedRequest) .userText(\u0026#34;\u0026#34;\u0026#34; {re2_input_query} Read the question again: {re2_input_query} \u0026#34;\u0026#34;\u0026#34;) .userParams(advisedUserParams) .build(); } @Override public AdvisedResponse aroundCall(AdvisedRequest advisedRequest, CallAroundAdvisorChain chain) { (2) return chain.nextAroundCall(this.before(advisedRequest)); } @Override public Flux\u0026lt;AdvisedResponse\u0026gt; aroundStream(AdvisedRequest advisedRequest, StreamAroundAdvisorChain chain) { (3) return chain.nextAroundStream(this.before(advisedRequest)); } @Override public int getOrder() { (4) return 0; } @Override public String getName() { (5) return this.getClass().getSimpleName(); } } Spring AI 内置顾问 # Spring AI 框架提供了多个内置顾问，以增强您的 AI 交互。以下是可用顾问的概述：\n聊天记忆顾问 # 这些顾问在聊天记忆库中管理对话历史记录：\n问答顾问 # 内容安全顾问 # 流式传输 vs. 非流式传输 # 非流式顾问处理完整的请求和响应。 流顾问使用反应式编程概念（例如，用于响应的 Flux）将请求和响应作为连续流进行处理。 @Override public Flux\u0026lt;AdvisedResponse\u0026gt; aroundStream(AdvisedRequest advisedRequest, StreamAroundAdvisorChain chain) { return Mono.just(advisedRequest) .publishOn(Schedulers.boundedElastic()) .map(request -\u0026gt; { // This can be executed by blocking and non-blocking Threads. // Advisor before next section }) .flatMapMany(request -\u0026gt; chain.nextAroundStream(request)) .map(response -\u0026gt; { // Advisor after next section }); } 最佳实践 # 向后兼容性 # 重大 API 变更 # Spring AI Advisor Chain 从 1.0 M2 版本到 1.0 M3 版本发生了较大变化，主要修改如下：\n顾问界面 # 在 1.0 M2 中，有单独的 RequestAdvisor 和 ResponseAdvisor 接口。 RequestAdvisor 在 ChatModel.call 和 ChatModel.stream 方法之前被调用。 在这些方法之后调用了 ResponseAdvisor 。 RequestAdvisor 在 ChatModel.call 和 ChatModel.stream 方法之前被调用。 在这些方法之后调用了 ResponseAdvisor 。 在 1.0 M3 中，这些接口已被替换为： StreamResponseMode ，以前是 ResponseAdvisor 的一部分，已被删除。 上下文映射处理 # 在 1.0 平方米中： 上下文图是一个单独的方法参数。 该地图是可变的，并沿着链条传递。 上下文图是一个单独的方法参数。 该地图是可变的，并沿着链条传递。 在 1.0 M3 中： 上下文映射现在是 AdvisedRequest 和 AdvisedResponse 记录的一部分。 该地图是不可变的。 要更新上下文，请使用 updateContext 方法，该方法使用更新的内容创建一个新的不可修改的映射。 上下文映射现在是 AdvisedRequest 和 AdvisedResponse 记录的一部分。 该地图是不可变的。 要更新上下文，请使用 updateContext 方法，该方法使用更新的内容创建一个新的不可修改的映射。 1.0 M3 中更新上下文的示例：\n@Override public AdvisedResponse aroundCall(AdvisedRequest advisedRequest, CallAroundAdvisorChain chain) { this.advisedRequest = advisedRequest.updateContext(context -\u0026gt; { context.put(\u0026#34;aroundCallBefore\u0026#34; + getName(), \u0026#34;AROUND_CALL_BEFORE \u0026#34; + getName()); // Add multiple key-value pairs context.put(\u0026#34;lastBefore\u0026#34;, getName()); // Add a single key-value pair return context; }); // Method implementation continues... } "},{"id":20,"href":"/docs/%E5%8F%82%E8%80%83/%E7%9F%A2%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/azure-cosmos-db/","title":"Azure Cosmos DB","section":"矢量数据库","content":" Azure Cosmos DB # 本节将引导您设置 CosmosDBVectorStore 来存储文档嵌入并执行相似性搜索。\n什么是 Azure Cosmos DB？ # [ Azure Cosmos DB]( https://azure.microsoft.com/en-us/services/cosmos-db/) 是微软专为关键任务应用程序设计的全球分布式云原生数据库服务。它提供高可用性、低延迟以及水平扩展能力，以满足现代应用程序的需求。它从零开始构建，以全球分布、细粒度多租户和水平扩展为核心。它是 Azure 的一项基础服务，被微软全球范围内的大多数关键任务应用程序所使用，包括 Teams、Skype、Xbox Live、Office 365、Bing、Azure Active Directory、Azure Portal、Microsoft Store 等。此外，它还被数千家外部客户所使用，包括 OpenAI 的 ChatGPT 和其他需要弹性扩展、交钥匙全球分布以及全球低延迟和高可用性的关键任务 AI 应用程序。\n什么是 DiskANN？ # DiskANN（基于磁盘的近似最近邻搜索）是 Azure Cosmos DB 中使用的一项创新技术，用于增强向量搜索的性能。它通过索引存储在 Cosmos DB 中的嵌入，实现跨高维数据的高效且可扩展的相似性搜索。\nDiskANN 具有以下优势：\n效率 ：通过利用基于磁盘的结构，与传统方法相比，DiskANN 显著减少了查找最近邻居所需的时间。 可扩展性 ：它可以处理超出内存容量的大型数据集，使其适用于各种应用，包括机器学习和人工智能驱动的解决方案。 低延迟 ：DiskANN 最大限度地减少搜索操作期间的延迟，确保应用程序即使数据量很大也能快速检索结果。 在 Spring AI for Azure Cosmos DB 的环境中，向量搜索将创建并利用 DiskANN 索引来确保相似性查询的最佳性能。\n使用自动配置设置 Azure Cosmos DB 矢量存储 # 以下代码演示了如何使用自动配置设置 CosmosDBVectorStore ：\npackage com.example.demo; import io.micrometer.observation.ObservationRegistry; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import org.springframework.ai.document.Document; import org.springframework.ai.vectorstore.SearchRequest; import org.springframework.ai.vectorstore.VectorStore; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.autoconfigure.EnableAutoConfiguration; import org.springframework.boot.CommandLineRunner; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Lazy; import java.util.List; import java.util.Map; import java.util.UUID; import static org.assertj.core.api.Assertions.assertThat; @SpringBootApplication @EnableAutoConfiguration public class DemoApplication implements CommandLineRunner { private static final Logger log = LoggerFactory.getLogger(DemoApplication.class); @Lazy @Autowired private VectorStore vectorStore; public static void main(String[] args) { SpringApplication.run(DemoApplication.class, args); } @Override public void run(String... args) throws Exception { Document document1 = new Document(UUID.randomUUID().toString(), \u0026#34;Sample content1\u0026#34;, Map.of(\u0026#34;key1\u0026#34;, \u0026#34;value1\u0026#34;)); Document document2 = new Document(UUID.randomUUID().toString(), \u0026#34;Sample content2\u0026#34;, Map.of(\u0026#34;key2\u0026#34;, \u0026#34;value2\u0026#34;)); this.vectorStore.add(List.of(document1, document2)); List\u0026lt;Document\u0026gt; results = this.vectorStore.similaritySearch(SearchRequest.builder().query(\u0026#34;Sample content\u0026#34;).topK(1).build()); log.info(\u0026#34;Search results: {}\u0026#34;, results); // Remove the documents from the vector store this.vectorStore.delete(List.of(document1.getId(), document2.getId())); } @Bean public ObservationRegistry observationRegistry() { return ObservationRegistry.create(); } } 自动配置 # 将以下依赖项添加到您的 Maven 项目：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-azure-cosmos-db\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 配置属性 # Cosmos DB 向量存储具有以下配置属性：\n使用过滤器进行复杂搜索 # 可以使用 Cosmos DB 向量存储中的筛选器执行更复杂的搜索。以下示例演示了如何在搜索查询中使用筛选器。\nMap\u0026lt;String, Object\u0026gt; metadata1 = new HashMap\u0026lt;\u0026gt;(); metadata1.put(\u0026#34;country\u0026#34;, \u0026#34;UK\u0026#34;); metadata1.put(\u0026#34;year\u0026#34;, 2021); metadata1.put(\u0026#34;city\u0026#34;, \u0026#34;London\u0026#34;); Map\u0026lt;String, Object\u0026gt; metadata2 = new HashMap\u0026lt;\u0026gt;(); metadata2.put(\u0026#34;country\u0026#34;, \u0026#34;NL\u0026#34;); metadata2.put(\u0026#34;year\u0026#34;, 2022); metadata2.put(\u0026#34;city\u0026#34;, \u0026#34;Amsterdam\u0026#34;); Document document1 = new Document(\u0026#34;1\u0026#34;, \u0026#34;A document about the UK\u0026#34;, this.metadata1); Document document2 = new Document(\u0026#34;2\u0026#34;, \u0026#34;A document about the Netherlands\u0026#34;, this.metadata2); vectorStore.add(List.of(document1, document2)); FilterExpressionBuilder builder = new FilterExpressionBuilder(); List\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch(SearchRequest.builder().query(\u0026#34;The World\u0026#34;) .topK(10) .filterExpression((this.builder.in(\u0026#34;country\u0026#34;, \u0026#34;UK\u0026#34;, \u0026#34;NL\u0026#34;)).build()).build()); 不使用自动配置设置 Azure Cosmos DB 矢量存储 # 以下代码演示了如何在不依赖自动配置的情况下设置 CosmosDBVectorStore 。建议使用 DefaultAzureCredential 进行 Azure Cosmos DB 身份验证。\n@Bean public VectorStore vectorStore(ObservationRegistry observationRegistry) { // Create the Cosmos DB client CosmosAsyncClient cosmosClient = new CosmosClientBuilder() .endpoint(System.getenv(\u0026#34;COSMOSDB_AI_ENDPOINT\u0026#34;)) .credential(new DefaultAzureCredentialBuilder().build()) .userAgentSuffix(\u0026#34;SpringAI-CDBNoSQL-VectorStore\u0026#34;) .gatewayMode() .buildAsyncClient(); // Create and configure the vector store return CosmosDBVectorStore.builder(cosmosClient, embeddingModel) .databaseName(\u0026#34;test-database\u0026#34;) .containerName(\u0026#34;test-container\u0026#34;) // Configure metadata fields for filtering .metadataFields(List.of(\u0026#34;country\u0026#34;, \u0026#34;year\u0026#34;, \u0026#34;city\u0026#34;)) // Set the partition key path (optional) .partitionKeyPath(\u0026#34;/id\u0026#34;) // Configure performance settings .vectorStoreThroughput(1000) .vectorDimensions(1536) // Match your embedding model\u0026#39;s dimensions // Add custom batching strategy (optional) .batchingStrategy(new TokenCountBatchingStrategy()) // Add observation registry for metrics .observationRegistry(observationRegistry) .build(); } @Bean public EmbeddingModel embeddingModel() { return new TransformersEmbeddingModel(); } 此配置显示所有可用的构建器选项：\ndatabaseName ：Cosmos DB 数据库的名称 containerName ：数据库中容器的名称 partitionKeyPath ：分区键的路径（例如“/id”） metadataFields ：用于过滤的元数据字段列表 vectorStoreThroughput 存储容器的吞吐量（RU/s） vectorDimensions ：矢量的维数（应与嵌入模型匹配） batchingStrategy ：批处理文档操作的策略（可选） 手动依赖设置 # 在您的 Maven 项目中添加以下依赖项：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-azure-cosmos-db-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 访问 Native Client # Azure Cosmos DB 向量存储实现通过 getNativeClient() 方法提供对底层本机 Azure Cosmos DB 客户端 ( CosmosClient ) 的访问：\nCosmosDBVectorStore vectorStore = context.getBean(CosmosDBVectorStore.class); Optional\u0026lt;CosmosClient\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { CosmosClient client = nativeClient.get(); // Use the native client for Azure Cosmos DB-specific operations } 本机客户端使您可以访问可能无法通过 VectorStore 接口公开的 Azure Cosmos DB 特定功能和操作。\n"},{"id":21,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B-api/azure-openai-%E5%B5%8C%E5%85%A5/","title":"Azure OpenAI 嵌入","section":"嵌入模型 API","content":" Azure OpenAI 嵌入 # Azure 的 OpenAI 扩展了 OpenAI 功能，为各种任务提供安全的文本生成和嵌入计算模型：\n相似性嵌入擅长捕捉两段或多段文本之间的语义相似性。 文本搜索嵌入有助于衡量长文档是否与短查询相关。 代码搜索嵌入对于嵌入代码片段和嵌入自然语言搜索查询很有用。 Azure OpenAI 嵌入依靠 cosine similarity 来计算文档和查询之间的相似度。\n先决条件 # Azure OpenAI 客户端提供三种连接选项：使用 Azure API 密钥或使用 OpenAI API 密钥，或使用 Microsoft Entra ID。\nAzure API 密钥和端点 # 从 [ Azure 门户]( https://portal.azure.com)上的 Azure OpenAI 服务部分获取 Azure OpenAI endpoint 和 api-key 。\nSpring AI 定义了两个配置属性：\n您可以在 application.properties 或 application.yml 文件中设置这些配置属性：\nspring.ai.azure.openai.api-key=\u0026lt;your-azure-api-key\u0026gt; spring.ai.azure.openai.endpoint=\u0026lt;your-azure-endpoint-url\u0026gt; 如果您希望使用环境变量来存储 API 密钥等敏感信息，则可以在配置中使用 Spring 表达式语言 (SpEL)：\n# In application.yml spring: ai: azure: openai: api-key: ${AZURE_OPENAI_API_KEY} endpoint: ${AZURE_OPENAI_ENDPOINT} # In your environment or .env file export AZURE_OPENAI_API_KEY=\u0026lt;your-azure-openai-api-key\u0026gt; export AZURE_OPENAI_ENDPOINT=\u0026lt;your-azure-endpoint-url\u0026gt; OpenAI 密钥 # 要使用 OpenAI 服务（而非 Azure）进行身份验证，请提供 OpenAI API 密钥。这将自动将端点设置为 [ api.openai.com/v1](https:// api.openai.com/v1) 。\n使用此方法时，将 spring.ai.azure.openai.chat.options.deployment-name 属性设置为您希望使用的 [ OpenAI 模型]( https://platform.openai.com/docs/models)的名称。\n在您的应用程序配置中：\nspring.ai.azure.openai.openai-api-key=\u0026lt;your-azure-openai-key\u0026gt; spring.ai.azure.openai.chat.options.deployment-name=\u0026lt;openai-model-name\u0026gt; 将环境变量与 SpEL 结合使用：\n# In application.yml spring: ai: azure: openai: openai-api-key: ${AZURE_OPENAI_API_KEY} chat: options: deployment-name: ${OPENAI_MODEL_NAME} # In your environment or .env file export AZURE_OPENAI_API_KEY=\u0026lt;your-openai-key\u0026gt; export OPENAI_MODEL_NAME=\u0026lt;openai-model-name\u0026gt; 对于使用 Microsoft Entra ID（以前称为 Azure Active Directory）进行无密钥身份验证，请仅设置 spring.ai.azure.openai.endpoint 配置属性，而不要设置上面提到的 api-key 属性。\n仅找到端点属性，您的应用程序将评估检索凭据的几种不同选项，并使用令牌凭据创建 OpenAIClient 实例。\n添加存储库和 BOM # Spring AI 的构件已发布在 Maven Central 和 Spring Snapshot 仓库中。请参阅 [ “构件仓库”](../../getting-started.html#artifact-repositories) 部分，将这些仓库添加到您的构建系统中。\n为了帮助管理依赖项，Spring AI 提供了 BOM（物料清单），以确保在整个项目中使用一致版本的 Spring AI。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # Spring AI 为 Azure OpenAI 嵌入模型提供 Spring Boot 自动配置。 Spring AI offers Spring Boot auto-configuration for the Azure OpenAI Embedding Model. 要启用它，请将以下依赖项添加到项目的 Maven pom.xml 文件：To enable it, add the following dependency to your project\u0026rsquo;s Maven pom.xml file:\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-azure-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-azure-openai\u0026#39; } 嵌入属性 # 前缀 spring.ai.azure.openai 是配置与 Azure OpenAI 的连接的属性前缀。\n前缀 spring.ai.azure.openai.embedding 是配置 Azure OpenAI 的 EmbeddingModel 实现的属性前缀\n运行时选项 # AzureOpenAiEmbeddingOptions 提供嵌入请求的配置信息。AzureOpenAiEmbeddingOptions 提供了一个 AzureOpenAiEmbeddingOptions 器来创建选项。\n启动时，使用 AzureOpenAiEmbeddingModel 构造函数设置所有嵌入请求的默认选项。运行时，您可以通过将 AzureOpenAiEmbeddingOptions 实例传递给 EmbeddingRequest 请求来覆盖默认选项。\n例如，要覆盖特定请求的默认模型名称：\nEmbeddingResponse embeddingResponse = embeddingModel.call( new EmbeddingRequest(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;), AzureOpenAiEmbeddingOptions.builder() .model(\u0026#34;Different-Embedding-Model-Deployment-Name\u0026#34;) .build())); 示例代码 # 这将创建一个 EmbeddingModel 实现，您可以将其注入到您的类中。这是一个使用 EmbeddingModel 实现的简单 @Controller 类的示例。\nspring.ai.azure.openai.api-key=YOUR_API_KEY spring.ai.azure.openai.endpoint=YOUR_ENDPOINT spring.ai.azure.openai.embedding.options.model=text-embedding-ada-002 @RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(\u0026#34;/ai/embedding\u0026#34;) public Map embed(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { EmbeddingResponse embeddingResponse = this.embeddingModel.embedForResponse(List.of(message)); return Map.of(\u0026#34;embedding\u0026#34;, embeddingResponse); } } 手动配置 # 如果您不想使用 Spring Boot 自动配置，可以在应用程序中手动配置 AzureOpenAiEmbeddingModel 。为此，请将 spring-ai-azure-openai 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-azure-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-azure-openai\u0026#39; } 接下来，创建一个 AzureOpenAiEmbeddingModel 实例并使用它来计算两个输入文本之间的相似度：\nvar openAIClient = OpenAIClientBuilder() .credential(new AzureKeyCredential(System.getenv(\u0026#34;AZURE_OPENAI_API_KEY\u0026#34;))) .endpoint(System.getenv(\u0026#34;AZURE_OPENAI_ENDPOINT\u0026#34;)) .buildClient(); var embeddingModel = new AzureOpenAiEmbeddingModel(this.openAIClient) .withDefaultOptions(AzureOpenAiEmbeddingOptions.builder() .model(\u0026#34;text-embedding-ada-002\u0026#34;) .user(\u0026#34;user-6\u0026#34;) .build()); EmbeddingResponse embeddingResponse = this.embeddingModel .embedForResponse(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;)); "},{"id":22,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B-api/google-vertexai/google-vertexai-%E5%A4%9A%E6%A8%A1%E6%80%81%E5%B5%8C%E5%85%A5/","title":"Google VertexAI 多模态嵌入","section":"Google VertexAI","content":" Google VertexAI 多模态嵌入 # Vertex AI 支持两种类型的嵌入模型：文本和多模态。本文档介绍如何使用 Vertex AI [ 多模态嵌入 API 创建多模态]( https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings)嵌入。\n多模态嵌入模型根据您提供的输入生成 1408 维向量，这些输入可以包含图像、文本和视频数据的组合。这些嵌入向量随后可用于后续任务，例如图像分类或视频内容审核。\n图像嵌入向量和文本嵌入向量位于相同的语义空间，且维度相同。因此，这些向量可以互换使用，例如通过文本搜索图像或通过图像搜索视频。\n先决条件 # 安装适合您操作系统的 gcloud CLI。 通过运行以下命令进行身份验证。将 PROJECT_ID 替换为您的 Google Cloud 项目 ID，将 ACCOUNT 为您的 Google Cloud 用户名。 gcloud config set project \u0026lt;PROJECT_ID\u0026gt; \u0026amp;\u0026amp; gcloud auth application-default login \u0026lt;ACCOUNT\u0026gt; 添加存储库和 BOM # Spring AI 的构件已发布在 Maven Central 和 Spring Snapshot 仓库中。请参阅 [ “构件仓库”](../../getting-started.html#artifact-repositories) 部分，将这些仓库添加到您的构建系统中。\n为了帮助管理依赖项，Spring AI 提供了 BOM（物料清单），以确保在整个项目中使用一致版本的 Spring AI。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # Spring AI 为 VertexAI 嵌入模型提供了 Spring Boot 自动配置功能。要启用此功能，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-vertex-ai-embedding\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-vertex-ai-embedding\u0026#39; } 嵌入属性 # 前缀 spring.ai.vertex.ai.embedding 用作允许您连接到 VertexAI Embedding API 的属性前缀。\n前缀 spring.ai.vertex.ai.embedding.multimodal 是属性前缀，可让您配置 VertexAI 多模态嵌入的嵌入模型实现。\n手动配置 # [ VertexAiMultimodalEmbeddingModel]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-vertex-ai-embedding/src/main/java/org/springframework/ai/vertexai/embedding/[VertexAiMultimodalEmbeddingModel](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-vertex-ai-embedding/src/main/java/org/springframework/ai/vertexai/embedding/VertexAiMultimodalEmbeddingModel.java).java) 实现了 DocumentEmbeddingModel 。\n将 spring-ai-vertex-ai-embedding 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-vertex-ai-embedding\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-vertex-ai-embedding\u0026#39; } 接下来，创建一个 VertexAiMultimodalEmbeddingModel 并将其用于嵌入生成：\nVertexAiEmbeddingConnectionDetails connectionDetails = VertexAiEmbeddingConnectionDetails.builder() .projectId(System.getenv(\u0026lt;VERTEX_AI_GEMINI_PROJECT_ID\u0026gt;)) .location(System.getenv(\u0026lt;VERTEX_AI_GEMINI_LOCATION\u0026gt;)) .build(); VertexAiMultimodalEmbeddingOptions options = VertexAiMultimodalEmbeddingOptions.builder() .model(VertexAiMultimodalEmbeddingOptions.DEFAULT_MODEL_NAME) .build(); var embeddingModel = new VertexAiMultimodalEmbeddingModel(this.connectionDetails, this.options); Media imageMedial = new Media(MimeTypeUtils.IMAGE_PNG, new ClassPathResource(\u0026#34;/test.image.png\u0026#34;)); Media videoMedial = new Media(new MimeType(\u0026#34;video\u0026#34;, \u0026#34;mp4\u0026#34;), new ClassPathResource(\u0026#34;/test.video.mp4\u0026#34;)); var document = new Document(\u0026#34;Explain what do you see on this video?\u0026#34;, List.of(this.imageMedial, this.videoMedial), Map.of()); EmbeddingResponse embeddingResponse = this.embeddingModel .embedForResponse(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;)); DocumentEmbeddingRequest embeddingRequest = new DocumentEmbeddingRequest(List.of(this.document), EmbeddingOptions.EMPTY); EmbeddingResponse embeddingResponse = multiModelEmbeddingModel.call(this.embeddingRequest); assertThat(embeddingResponse.getResults()).hasSize(3); "},{"id":23,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B%E4%B8%8A%E4%B8%8B%E6%96%87%E5%8D%8F%E8%AE%AEmcp/mcp-%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%90%AF%E5%8A%A8%E5%99%A8/","title":"MCP 服务器启动器","section":"模型上下文协议（MCP）","content":" MCP 服务器启动器 # Spring AI MCP（模型上下文协议）服务器启动启动器提供了在 Spring Boot 应用程序中设置 MCP 服务器的自动配置功能。它支持 MCP 服务器功能与 Spring Boot 的自动配置系统无缝集成。\nMCP 服务器启动器提供：\nMCP 服务器组件的自动配置 支持同步和异步操作模式 多种传输层选项 灵活的工具、资源和提示规范 更改通知功能 开胃菜 # 根据您的运输需求选择以下其中一种启动器：\n标准 MCP 服务器 # 完整的 MCP 服务器功能支持 STDIO 服务器传输。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-mcp-server-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 适用于命令行和桌面工具 无需额外的 Web 依赖项 启动器激活 McpServerAutoConfiguration 自动配置，负责：\n配置基本服务器组件 处理工具、资源和提示规范 管理服务器功能和变更通知 提供同步和异步服务器实现 WebMVC 服务器传输 # 完整的 MCP 服务器功能支持基于 Spring MVC 的 SSE （服务器发送事件）服务器传输和可选的 STDIO 传输。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-mcp-server-webmvc\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 启动器激活 McpWebMvcServerAutoConfiguration 和 McpServerAutoConfiguration 自动配置以提供：\n使用 Spring MVC ( WebMvcSseServerTransportProvider ) 的基于 HTTP 的传输 自动配置的 SSE 端点 可选的 STDIO 传输（通过设置 spring.ai.mcp.server.stdio=true 启用） 包含 spring-boot-starter-web 和 mcp-spring-webmvc 依赖项 WebFlux 服务器传输 # 完整的 MCP 服务器功能支持基于 Spring WebFlux 的 SSE （服务器发送事件）服务器传输和可选的 STDIO 传输。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-mcp-server-webflux\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 启动器激活 McpWebFluxServerAutoConfiguration 和 McpServerAutoConfiguration 自动配置以提供：\n使用 Spring WebFlux 进行反应式传输 ( WebFluxSseServerTransportProvider ) 自动配置的反应式 SSE 端点 可选的 STDIO 传输（通过设置 spring.ai.mcp.server.stdio=true 启用） 包含 spring-boot-starter-webflux 和 mcp-spring-webflux 依赖项 配置属性 # 所有属性都以 spring.ai.mcp.server 为前缀：\n同步/异步服务器类型 # 同步服务器 - 使用 McpSyncServer 实现的默认服务器类型。它专为应用程序中简单的请求-响应模式而设计。要启用此服务器类型，请在配置中设置 spring.ai.mcp.server.type=SYNC 。激活后，它会自动处理同步工具规范的配置。 异步服务器 - 异步服务器实现使用 McpAsyncServer ，并针对非阻塞操作进行了优化。要启用此服务器类型，请使用 spring.ai.mcp.server.type=ASYNC 配置您的应用程序。此服务器类型会自动设置异步工具规范，并内置 Project Reactor 支持。 服务器功能 # MCP 服务器支持四种主要功能类型，可以单独启用或禁用：\n工具 - 使用 spring.ai.mcp.server.capabilities.tool=true|false 启用/禁用工具功能 资源 - 使用 spring.ai.mcp.server.capabilities.resource=true|false 启用/禁用资源功能 提示 - 使用 spring.ai.mcp.server.capabilities.prompt=true|false 启用/禁用提示功能 完成 - 使用 spring.ai.mcp.server.capabilities.completion=true|false 启用/禁用完成功能 所有功能默认启用。禁用某项功能将阻止服务器注册并向客户端公开相应的功能。\n交通选择 # MCP 服务器支持三种传输机制，每种机制都有其专用的启动器：\n标准输入/输出（STDIO） - spring-ai-starter-mcp-server Spring MVC（服务器发送事件）- spring-ai-starter-mcp-server-webmvc Spring WebFlux（反应式 SSE）- spring-ai-starter-mcp-server-webflux 特性和功能 # MCP 服务器启动器允许服务器向客户端公开工具、资源和提示。它会自动将注册为 Spring Bean 的自定义功能处理程序根据服务器类型转换为同步/异步规范：\n工具 # 允许服务器公开可通过语言模型调用的工具。MCP 服务器启动启动器提供：\n变更通知支持 Spring AI Tools 根据服务器类型自动转换为同步/异步规范 通过 Spring beans 自动指定工具： @Bean public ToolCallbackProvider myTools(...) { List\u0026lt;ToolCallback\u0026gt; tools = ... return ToolCallbackProvider.from(tools); } 或者使用低级 API：\n@Bean public List\u0026lt;McpServerFeatures.SyncToolSpecification\u0026gt; myTools(...) { List\u0026lt;McpServerFeatures.SyncToolSpecification\u0026gt; tools = ... return tools; } 自动配置将自动检测并注册所有工具回调：* 单个 ToolCallback beans * ToolCallback beans 列表 * ToolCallbackProvider beans\n工具按名称进行重复数据删除，并使用每个工具名称的第一次出现。\n工具上下文支持 # 支持 [ ToolContext](../tools.html#_tool_context) ，允许将上下文信息传递给工具调用。它包含一个 McpSyncServerExchange 实例，该实例的 exchange 键为 McpToolUtils.getMcpExchange(toolContext) ，可通过 exchange.loggingNotification(…​) 访问。请参阅此[ 示例，]( https://github.com/spring-projects/spring-ai-examples/blob/3fab8483b8deddc241b1e16b8b049616604b7767/model-context-protocol/sampling/mcp-weather-webmvc-server/src/main/java/org/springframework/ai/mcp/sample/server/WeatherService.java#L59-L126) 其中演示了 exchange.loggingNotification(…​) 和 exchange.createMessage(…​) 。\n资源管理 # 为服务器向客户端公开资源提供标准化的方式。\n静态和动态资源规范 可选变更通知 支持资源模板 同步/异步资源规范之间的自动转换 通过 Spring bean 自动指定资源： @Bean public List\u0026lt;McpServerFeatures.SyncResourceSpecification\u0026gt; myResources(...) { var systemInfoResource = new McpSchema.Resource(...); var resourceSpecification = new McpServerFeatures.SyncResourceSpecification(systemInfoResource, (exchange, request) -\u0026gt; { try { var systemInfo = Map.of(...); String jsonContent = new ObjectMapper().writeValueAsString(systemInfo); return new McpSchema.ReadResourceResult( List.of(new McpSchema.TextResourceContents(request.uri(), \u0026#34;application/json\u0026#34;, jsonContent))); } catch (Exception e) { throw new RuntimeException(\u0026#34;Failed to generate system info\u0026#34;, e); } }); return List.of(resourceSpecification); } 及时管理 # 为服务器提供向客户端公开提示模板的标准化方式。\n变更通知支持 模板版本控制 同步/异步提示规范之间的自动转换 通过 Spring beans 自动提示规范： @Bean public List\u0026lt;McpServerFeatures.SyncPromptSpecification\u0026gt; myPrompts() { var prompt = new McpSchema.Prompt(\u0026#34;greeting\u0026#34;, \u0026#34;A friendly greeting prompt\u0026#34;, List.of(new McpSchema.PromptArgument(\u0026#34;name\u0026#34;, \u0026#34;The name to greet\u0026#34;, true))); var promptSpecification = new McpServerFeatures.SyncPromptSpecification(prompt, (exchange, getPromptRequest) -\u0026gt; { String nameArgument = (String) getPromptRequest.arguments().get(\u0026#34;name\u0026#34;); if (nameArgument == null) { nameArgument = \u0026#34;friend\u0026#34;; } var userMessage = new PromptMessage(Role.USER, new TextContent(\u0026#34;Hello \u0026#34; + nameArgument + \u0026#34;! How can I assist you today?\u0026#34;)); return new GetPromptResult(\u0026#34;A personalized greeting message\u0026#34;, List.of(userMessage)); }); return List.of(promptSpecification); } 完工管理 # 为服务器提供一种向客户端公开完成功能的标准化方法。\n支持同步和异步完成规范 通过 Spring beans 自动注册： @Bean public List\u0026lt;McpServerFeatures.SyncCompletionSpecification\u0026gt; myCompletions() { var completion = new McpServerFeatures.SyncCompletionSpecification( \u0026#34;code-completion\u0026#34;, \u0026#34;Provides code completion suggestions\u0026#34;, (exchange, request) -\u0026gt; { // Implementation that returns completion suggestions return new McpSchema.CompletionResult(List.of( new McpSchema.Completion(\u0026#34;suggestion1\u0026#34;, \u0026#34;First suggestion\u0026#34;), new McpSchema.Completion(\u0026#34;suggestion2\u0026#34;, \u0026#34;Second suggestion\u0026#34;) )); } ); return List.of(completion); } 根源改变消费者 # 当根发生改变时，支持 listChanged 的​​客户端会发送 Root Change 通知。\n支持监控根更改 自动转换为反应式应用程序的异步使用者 通过 Spring beans 进行可选注册 @Bean public BiConsumer\u0026lt;McpSyncServerExchange, List\u0026lt;McpSchema.Root\u0026gt;\u0026gt; rootsChangeHandler() { return (exchange, roots) -\u0026gt; { logger.info(\u0026#34;Registering root resources: {}\u0026#34;, roots); }; } 使用示例 # 标准 STDIO 服务器配置 # # Using spring-ai-starter-mcp-server spring: ai: mcp: server: name: stdio-mcp-server version: 1.0.0 type: SYNC WebMVC 服务器配置 # # Using spring-ai-starter-mcp-server-webmvc spring: ai: mcp: server: name: webmvc-mcp-server version: 1.0.0 type: SYNC instructions: \u0026#34;This server provides weather information tools and resources\u0026#34; sse-message-endpoint: /mcp/messages capabilities: tool: true resource: true prompt: true completion: true WebFlux 服务器配置 # # Using spring-ai-starter-mcp-server-webflux spring: ai: mcp: server: name: webflux-mcp-server version: 1.0.0 type: ASYNC # Recommended for reactive applications instructions: \u0026#34;This reactive server provides weather information tools and resources\u0026#34; sse-message-endpoint: /mcp/messages capabilities: tool: true resource: true prompt: true completion: true 使用 MCP Server 创建 Spring Boot 应用程序 # @Service public class WeatherService { @Tool(description = \u0026#34;Get weather information by city name\u0026#34;) public String getWeather(String cityName) { // Implementation } } @SpringBootApplication public class McpServerApplication { private static final Logger logger = LoggerFactory.getLogger(McpServerApplication.class); public static void main(String[] args) { SpringApplication.run(McpServerApplication.class, args); } @Bean public ToolCallbackProvider weatherTools(WeatherService weatherService) { return MethodToolCallbackProvider.builder().toolObjects(weatherService).build(); } } 自动配置会自动将工具回调注册为 MCP 工具。您可以有多个 bean 生成 ToolCallbacks。自动配置会将它们合并。\n示例应用程序 # 天气服务器（WebFlux） ——带有 WebFlux 传输的 Spring AI MCP 服务器启动器。 天气服务器（STDIO） - 带有 STDIO 传输的 Spring AI MCP 服务器启动器。 天气服务器手动配置 - Spring AI MCP 服务器启动器不使用自动配置，而是使用 Java SDK 手动配置服务器。 其他资源 # Spring AI 文档 模型上下文协议规范 Spring Boot 自动配置 "},{"id":24,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E5%9B%BE%E5%83%8F%E6%A8%A1%E5%9E%8B-api/openai-%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/","title":"OpenAI 图像生成","section":"图像模型 API","content":" OpenAI 图像生成 # Spring AI 支持 OpenAI 的图像生成模型 DALL-E。\n先决条件 # 您需要使用 OpenAI 创建 API 密钥才能访问 ChatGPT 模型。\n在 [ OpenAI 注册页面]( https://platform.openai.com/signup)创建一个帐户，并在 [ API 密钥页面]( https://platform.openai.com/account/api-keys)上生成令牌。\nSpring AI 项目定义了一个名为 spring.ai.openai.api-key 的配置属性，您应该将其设置为从 openai.com 获取的 API Key 的值。\n您可以在 application.properties 文件中设置此配置属性：\nspring.ai.openai.api-key=\u0026lt;your-openai-api-key\u0026gt; 为了在处理 API 密钥等敏感信息时增强安全性，您可以使用 Spring 表达式语言 (SpEL) 引用自定义环境变量：\n# In application.yml spring: ai: openai: api-key: ${OPENAI_API_KEY} # In your environment or .env file export OPENAI_API_KEY=\u0026lt;your-openai-api-key\u0026gt; 您还可以在应用程序代码中以编程方式设置此配置：\n// Retrieve API key from a secure source or environment variable String apiKey = System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;); 自动配置 # Spring AI 为 OpenAI 图像生成客户端提供了 Spring Boot 自动配置。要启用此功能，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-openai\u0026#39; } 图像生成属性 # 连接属性 # 前缀 spring.ai.openai 用作允许您连接到 OpenAI 的属性前缀。\n重试属性 # 前缀 spring.ai.retry 用作属性前缀，可让您配置 OpenAI Image 客户端的重试机制。\n配置属性 # 前缀 spring.ai.openai.image 是属性前缀，可让您为 OpenAI 配置 ImageModel 实现。\n运行时选项 # [ OpenAiImageOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/[OpenAiImageOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/OpenAiImageOptions.java)) 提供模型配置，例如要使用的模型、质量、大小等。\n启动时，可以使用 OpenAiImageModel(OpenAiImageApi openAiImageApi) 构造函数和 withDefaultOptions(OpenAiImageOptions defaultOptions) 方法配置默认选项。或者，也可以使用前面描述的 spring.ai.openai.image.options.* 属性。\n在运行时，您可以通过向 ImagePrompt 调用添加新的、特定于请求的选项来覆盖默认选项。例如，要覆盖 OpenAI 特定的选项（例如质量和要创建的图像数量），请使用以下代码示例：\nImageResponse response = openaiImageModel.call( new ImagePrompt(\u0026#34;A light cream colored mini golden doodle\u0026#34;, OpenAiImageOptions.builder() .quality(\u0026#34;hd\u0026#34;) .N(4) .height(1024) .width(1024).build()) ); "},{"id":25,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B-api/%E4%BA%9A%E9%A9%AC%E9%80%8A%E5%9F%BA%E5%B2%A9-converse/","title":"亚马逊基岩 Converse","section":"聊天模型 API","content":" 亚马逊基岩 Converse # [ Amazon Bedrock Converse API]( https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html) 为对话式 AI 模型提供了统一的界面，具有增强的功能，包括函数/工具调用、多模式输入和流式响应。\nBedrock Converse API 具有以下高级功能：\n工具/功能调用：支持对话过程中的功能定义和工具使用 多模式输入：能够在对话中处理文本和图像输入 流支持：模型响应的实时流 系统消息：支持系统级指令和上下文设置 先决条件 # 请参阅 [ Amazon Bedrock 入门指南]( https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html)以设置 API 访问\n获取 AWS 凭证：如果您尚未配置 AWS 账户和 AWS CLI，本视频指南可以帮助您进行配置： 不到 4 分钟即可完成 AWS CLI 和 SDK 设置！ 您应该能够获取访问密钥和安全密钥。 启用要使用的模型：转到 Amazon Bedrock ，从左侧的模型访问菜单中配置您要使用的模型的访问权限。 自动配置 # 将 spring-ai-starter-model-bedrock-converse 依赖项添加到项目的 Maven pom.xml 或 Gradle build.gradle 构建文件中：\n聊天属性 # 前缀 spring.ai.bedrock.aws 是配置与 AWS Bedrock 的连接的属性前缀。\n前缀 spring.ai.bedrock.converse.chat 是配置 Converse API 聊天模型实现的属性前缀。\n运行时选项 # 使用便携式 ChatOptions 或 ToolCallingChatOptions 便携式构建器创建模型配置，例如温度、maxToken、topP 等。\n启动时，可以使用 BedrockConverseProxyChatModel(api, options) 构造函数或 spring.ai.bedrock.converse.chat.options.* 属性配置默认选项。\n在运行时，您可以通过向 Prompt 调用添加新的、特定于请求的选项来覆盖默认选项：\nvar options = ToolCallingChatOptions.builder() .model(\u0026#34;anthropic.claude-3-5-sonnet-20240620-v1:0\u0026#34;) .temperature(0.6) .maxTokens(300) .toolCallbacks(List.of(FunctionToolCallback.builder(\u0026#34;getCurrentWeather\u0026#34;, new WeatherService()) .description(\u0026#34;Get the weather in location. Return temperature in 36°F or 36°C format. Use multi-turn if needed.\u0026#34;) .inputType(WeatherService.Request.class) .build())) .build(); String response = ChatClient.create(this.chatModel) .prompt(\u0026#34;What is current weather in Amsterdam?\u0026#34;) .options(options) .call() .content(); 工具调用 # Bedrock Converse API 支持工具调用功能，允许模型在对话过程中使用工具。以下是如何定义和使用基于 @Tool 的工具的示例：\npublic class WeatherService { @Tool(description = \u0026#34;Get the weather in location\u0026#34;) public String weatherByLocation(@ToolParam(description= \u0026#34;City or state name\u0026#34;) String location) { ... } } String response = ChatClient.create(this.chatModel) .prompt(\u0026#34;What\u0026#39;s the weather like in Boston?\u0026#34;) .tools(new WeatherService()) .call() .content(); 您也可以使用 java.util.function beans 作为工具：\n@Bean @Description(\u0026#34;Get the weather in location. Return temperature in 36°F or 36°C format.\u0026#34;) public Function\u0026lt;Request, Response\u0026gt; weatherFunction() { return new MockWeatherService(); } String response = ChatClient.create(this.chatModel) .prompt(\u0026#34;What\u0026#39;s the weather like in Boston?\u0026#34;) .tools(\u0026#34;weatherFunction\u0026#34;) .inputType(Request.class) .call() .content(); 在[ 工具](../tools.html)文档中查找更多信息。\n多式联运 # 多模态是指模型同时理解和处理来自各种来源的信息的能力，包括文本、图像、视频、pdf、doc、html、md 和更多数据格式。\nBedrock Converse API 支持多模式输入，包括文本和图像输入，并可以根据组合输入生成文本响应。\n您需要一个支持多模式输入的模型，例如 Anthropic Claude 或 Amazon Nova 模型。\n图片 # 对于支持视觉多模态的[ 模型]( https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference-supported-models-features.html) ，例如 Amazon Nova、Anthropic Claude 和 Llama 3.2，Amazon Bedrock Converse API 允许您在有效载荷中包含多幅图像。这些[ 模型]( https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference-supported-models-features.html)可以分析传入的图像并回答问题、对图像进行分类，以及根据提供的指令对图像进行汇总。\n目前，Bedrock Converse 支持 image/jpeg 、 image/png 、 image/gif 和 image/webp mime 类型的 base64 编码图像。\nSpring AI 的 Message 接口通过引入 Media 类型来支持多模态 AI 模型。该接口包含消息中媒体附件的数据和信息，并使用 Spring 的 org.springframework.util.MimeType 和 java.lang.Object 作为原始媒体数据。\n下面是一个简单的代码示例，演示了用户文本与图像的组合。\nString response = ChatClient.create(chatModel) .prompt() .user(u -\u0026gt; u.text(\u0026#34;Explain what do you see on this picture?\u0026#34;) .media(Media.Format.IMAGE_PNG, new ClassPathResource(\u0026#34;/test.png\u0026#34;))) .call() .content(); logger.info(response); 它将 test.png 图像作为输入：\n以及文本消息“解释一下你在图片上看到了什么？”，并生成类似这样的回应：\n视频 # [ Amazon Nova 模型]( https://docs.aws.amazon.com/nova/latest/userguide/modalities-video.html)允许您在有效负载中包含单个视频，该视频可以以 base64 格式提供，也可以通过 Amazon S3 URI 提供。\n目前，Bedrock Nova 支持 video/x-matros 、 video/quicktime 、 video/mp4 、 video/video/webm 、 video/x-flv 、 video/mpeg 、 video/x-ms-wmv 和 image/3gpp mime 类型的图像。\nSpring AI 的 Message 接口通过引入 Media``` 类型来支持多模态 AI 模型。该接口包含消息中媒体附件的数据和信息，并使用 Spring 的 org.springframework.util.MimeType和java.lang.Object`` 作为原始媒体数据。\n下面是一个简单的代码示例，演示了用户文本与视频的结合。\nString response = ChatClient.create(chatModel) .prompt() .user(u -\u0026gt; u.text(\u0026#34;Explain what do you see in this video?\u0026#34;) .media(Media.Format.VIDEO_MP4, new ClassPathResource(\u0026#34;/test.video.mp4\u0026#34;))) .call() .content(); logger.info(response); 它将 test.video.mp4 图像作为输入：\n以及文本消息“解释一下你在这段视频中看到了什么？”，并生成类似这样的回应：\n文件 # 对于某些模型，Bedrock 允许你通过 Converse API 文档支持在有效负载中包含文档，这些文档可以以字节为单位提供。文档支持有两种不同的变体，如下所述：\n文本文档类型 （txt、csv、html、md 等），重点在于文本理解。这些用例包括基于文档文本元素的解答。 媒体文档类型 （pdf、docx、xlsx），重点在于基于视觉的理解来解答问题。这些用例包括基于图表、图形等来解答问题。 目前，Anthropic [ PDF 支持（测试版）]( https://docs.anthropic.com/en/docs/build-with-claude/pdf-support) 和 Amazon Bedrock Nova 模型支持文档多模式。\n下面是一个简单的代码示例，演示了用户文本与媒体文档的组合。\nString response = ChatClient.create(chatModel) .prompt() .user(u -\u0026gt; u.text( \u0026#34;You are a very professional document summarization specialist. Please summarize the given document.\u0026#34;) .media(Media.Format.DOC_PDF, new ClassPathResource(\u0026#34;/spring-ai-reference-overview.pdf\u0026#34;))) .call() .content(); logger.info(response); 它将 spring-ai-reference-overview.pdf 文档作为输入：\n以及文本消息“您是一位非常专业的文档摘要专家。请总结给定的文档。”，并生成类似以下内容的回复：\n样品控制器 # 创建一个新的 Spring Boot 项目并将 spring-ai-starter-model-bedrock-converse 添加到您的依赖项中。\n在 src/main/resources 下添加 application.properties 文件：\nspring.ai.bedrock.aws.region=eu-central-1 spring.ai.bedrock.aws.timeout=10m spring.ai.bedrock.aws.access-key=${AWS_ACCESS_KEY_ID} spring.ai.bedrock.aws.secret-key=${AWS_SECRET_ACCESS_KEY} # session token is only required for temporary credentials spring.ai.bedrock.aws.session-token=${AWS_SESSION_TOKEN} spring.ai.bedrock.converse.chat.options.temperature=0.8 spring.ai.bedrock.converse.chat.options.top-k=15 以下是使用聊天模型的示例控制器：\n@RestController public class ChatController { private final ChatClient chatClient; @Autowired public ChatController(ChatClient.Builder builder) { this.chatClient = builder.build(); } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, this.chatClient.prompt(message).call().content()); } @GetMapping(\u0026#34;/ai/generateStream\u0026#34;) public Flux\u0026lt;ChatResponse\u0026gt; generateStream(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return this.chatClient.prompt(message).stream().content(); } } "},{"id":26,"href":"/docs/%E5%85%A5%E9%97%A8/","title":"入门","section":"Docs","content":" 入门 # 本节提供如何开始使用 Spring AI 的起点。\n您应该根据需要遵循以下每个部分中的步骤。\n前往 [ start.spring.io](https:// start.spring.io/) 并选择您想要在新应用程序中使用的 AI 模型和矢量存储。\n工件存储库 # 里程碑 - 使用 Maven Central # 从 1.0.0-M6 开始，Maven Central 中已提供发布版本。无需更改构建文件。\n快照 - 添加快照存储库 # 要使用快照（和 1.0.0-M6 里程碑之前的版本），您需要在构建文件中添加以下快照存储库。\n将以下存储库定义添加到您的 Maven 或 Gradle 构建文件中：\n注意： 将 Maven 与 Spring AI 快照结合使用时，请注意 Maven 镜像配置。如果您在 settings.xml 中配置了镜像，如下所示：\n\u0026lt;mirror\u0026gt; \u0026lt;id\u0026gt;my-mirror\u0026lt;/id\u0026gt; \u0026lt;mirrorOf\u0026gt;*\u0026lt;/mirrorOf\u0026gt; \u0026lt;url\u0026gt;https://my-company-repository.com/maven\u0026lt;/url\u0026gt; \u0026lt;/mirror\u0026gt; 通配符 * 会将所有仓库请求重定向到你的镜像仓库，从而阻止访问 Spring 快照仓库。要解决此问题，请修改 mirrorOf 配置以排除 Spring 仓库：\n\u0026lt;mirror\u0026gt; \u0026lt;id\u0026gt;my-mirror\u0026lt;/id\u0026gt; \u0026lt;mirrorOf\u0026gt;*,!spring-snapshots,!central-portal-snapshots\u0026lt;/mirrorOf\u0026gt; \u0026lt;url\u0026gt;https://my-company-repository.com/maven\u0026lt;/url\u0026gt; \u0026lt;/mirror\u0026gt; 此配置允许 Maven 直接访问 Spring 快照存储库，同时仍使用镜像来获取其他依赖项。\n依赖管理 # Spring AI 物料清单 (BOM) 声明了 Spring AI 特定版本所使用的所有依赖项的推荐版本。这是一个仅包含 BOM 的版本，它仅包含依赖项管理，不包含插件声明或对 Spring 或 Spring Boot 的直接引用。您可以使用 Spring Boot 父 POM，也可以使用 Spring Boot 中的 BOM ( spring-boot-dependencies ) 来管理 Spring Boot 版本。\n将 BOM 添加到您的项目：\n为特定组件添加依赖项 # 文档中的以下每个部分都显示了您需要添加到项目构建系统中的依赖项。\n聊天模型 嵌入模型 图像生成模型 转录模型 文本转语音 (TTS) 模型 矢量数据库 Spring AI 示例 # 请参阅[ 此页面]( https://github.com/danvega/awesome-spring-ai)以获取更多与 Spring AI 相关的资源和示例。\n"},{"id":27,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B-api/","title":"嵌入模型 API","section":"模型","content":" 嵌入模型 API # 嵌入是文本、图像或视频的数字表示，用于捕捉输入之间的关系。\n嵌入的工作原理是将文本、图像和视频转换为浮点数数组（称为向量）。这些向量旨在捕捉文本、图像和视频的含义。嵌入数组的长度称为向量的维数。\n通过计算两段文本的向量表示之间的数值距离，应用程序可以确定用于生成嵌入向量的对象之间的相似性。\nEmbeddingModel 接口旨在与 AI 和机器学习中的嵌入模型直接集成。其主要功能是将文本转换为数值向量，通常称为嵌入。这些嵌入对于语义分析和文本分类等各种任务至关重要。\nEmbeddingModel 接口的设计围绕两个主要目标：\n可移植性 ：此接口确保轻松适应各种嵌入模型。它允许开发人员以最少的代码更改在不同的嵌入技术或模型之间切换。此设计符合 Spring 的模块化和可互换性理念。 简洁性 ：EmbeddingModel 简化了将文本转换为嵌入的过程。通过提供诸如 embed(String text) 和 embed(Document document) 等简单易用的方法，它简化了处理原始文本数据和嵌入算法的复杂性。这种设计选择使开发人员（尤其是 AI 新手）能够更轻松地在其应用程序中使用嵌入，而无需深入研究底层机制。 API 概述 # 嵌入模型 API 构建于通用的 [ Spring AI 模型 API]( https://github.com/spring-projects/spring-ai/tree/main/spring-ai-model/src/main/java/org/springframework/ai/model) 之上，后者是 Spring AI 库的一部分。因此，EmbeddingModel 接口扩展了 Model 接口，后者提供了一组用于与 AI 模型交互 EmbeddingRequest 标准方法。EmbeddingRequest 和 EmbeddingResponse 类分别扩展自 ModelRequest 和 ModelResponse ，用于封装嵌入模型的输入和输出。\n反过来，嵌入 API 被更高级别的组件用来实现特定嵌入模型的嵌入模型，例如 OpenAI、Titan、Azure OpenAI、Ollie 等。\n下图说明了嵌入 API 及其与 Spring AI 模型 API 和嵌入模型的关系：\n嵌入模型 # 本节提供 EmbeddingModel 接口和相关类的指南。\npublic interface EmbeddingModel extends Model\u0026lt;EmbeddingRequest, EmbeddingResponse\u0026gt; { @Override EmbeddingResponse call(EmbeddingRequest request); /** * Embeds the given document\u0026#39;s content into a vector. * @param document the document to embed. * @return the embedded vector. */ float[] embed(Document document); /** * Embeds the given text into a vector. * @param text the text to embed. * @return the embedded vector. */ default float[] embed(String text) { Assert.notNull(text, \u0026#34;Text must not be null\u0026#34;); return this.embed(List.of(text)).iterator().next(); } /** * Embeds a batch of texts into vectors. * @param texts list of texts to embed. * @return list of list of embedded vectors. */ default List\u0026lt;float[]\u0026gt; embed(List\u0026lt;String\u0026gt; texts) { Assert.notNull(texts, \u0026#34;Texts must not be null\u0026#34;); return this.call(new EmbeddingRequest(texts, EmbeddingOptions.EMPTY)) .getResults() .stream() .map(Embedding::getOutput) .toList(); } /** * Embeds a batch of texts into vectors and returns the {@link EmbeddingResponse}. * @param texts list of texts to embed. * @return the embedding response. */ default EmbeddingResponse embedForResponse(List\u0026lt;String\u0026gt; texts) { Assert.notNull(texts, \u0026#34;Texts must not be null\u0026#34;); return this.call(new EmbeddingRequest(texts, EmbeddingOptions.EMPTY)); } /** * @return the number of dimensions of the embedded vectors. It is generative * specific. */ default int dimensions() { return embed(\u0026#34;Test String\u0026#34;).size(); } } 嵌入方法提供了将文本转换为嵌入、容纳单个字符串、结构化 Document 对象或批量文本的各种选项。\n嵌入文本提供了多种快捷方法，包括 embed(String text) 方法，该方法接受单个字符串并返回相应的嵌入向量。所有快捷方式均围绕 call 方法实现，该方法是调用嵌入模型的主要方法。\n通常，嵌入会返回一个浮点数列表，以数值向量格式表示嵌入。\nembedForResponse 方法提供了更全面的输出，可能包括有关嵌入的附加信息。\n维度方法是开发人员快速确定嵌入向量大小的便捷工具，这对于理解嵌入空间和后续处理步骤非常重要。\n嵌入请求 # EmbeddingRequest 是一个 ModelRequest ，它接受一个文本对象列表和可选的嵌入请求选项。以下列表展示了 EmbeddingRequest 类的精简版本，不包括构造函数和其他实用方法：\npublic class EmbeddingRequest implements ModelRequest\u0026lt;List\u0026lt;String\u0026gt;\u0026gt; { private final List\u0026lt;String\u0026gt; inputs; private final EmbeddingOptions options; // other methods omitted } 嵌入响应 # EmbeddingResponse 类的结构如下：\npublic class EmbeddingResponse implements ModelResponse\u0026lt;Embedding\u0026gt; { private List\u0026lt;Embedding\u0026gt; embeddings; private EmbeddingResponseMetadata metadata = new EmbeddingResponseMetadata(); // other methods omitted } ```EmbeddingResponse` 类保存 AI 模型的输出，每个 Embedding`` 实例包含来自单个文本输入的结果向量数据。\nEmbeddingResponse 类还携带有关 AI 模型响应的 EmbeddingResponseMetadata 元数据。\n嵌入 # Embedding 表示单个嵌入向量。\npublic class Embedding implements ModelResult\u0026lt;float[]\u0026gt; { private float[] embedding; private Integer index; private EmbeddingResultMetadata metadata; // other methods omitted } 可用的实现 # 各种 EmbeddingModel 实现在内部使用不同的低级库和 API 来执行嵌入任务。以下是一些可用的 EmbeddingModel 实现：\nSpring AI OpenAI 嵌入 Spring AI Azure OpenAI 嵌入 Spring AI Ollama 嵌入 Spring AI Transformers（ONNX）嵌入 Spring AI PostgresML 嵌入 Spring AI Bedrock Cohere 嵌入 Spring AI Bedrock Titan 嵌入 Spring AI VertexAI 嵌入 Spring AI Mistral AI 嵌入 Spring AI Oracle 云基础设施 GenAI 嵌入 "},{"id":28,"href":"/docs/%E5%8F%82%E8%80%83/%E6%8F%90%E7%A4%BA/","title":"提示","section":"参考","content":" 提示 # 提示是引导 AI 模型生成特定输出的输入。这些提示的设计和措辞会显著影响模型的响应。\n在 Spring AI 中与 AI 模型交互的最低级别，处理 Spring AI 中的提示有点类似于管理 Spring MVC 中的“视图”。这涉及创建包含动态内容占位符的大量文本。然后，这些占位符会根据用户请求或应用程序中的其他代码进行替换。另一个类比是包含特定表达式占位符的 SQL 语句。\n随着 Spring AI 的发展，它将引入更高级别的抽象，以便与 AI 模型进行交互。本节中描述的基础类在角色和功能方面可以类比为 JDBC。例如， ChatModel 类类似于 JDK 中的核心 ChatClient 库。ChatClient 类可以类比为 JdbcClient ，它构建于 ChatModel 之上，并通过 Advisor 提供更高级的构造。 考虑过去与模型的交互，用额外的上下文文档扩充提示，并引入代理行为。\n在人工智能领域，提示符的结构一直在不断发展。最初，提示符只是简单的字符串。随着时间的推移，它们逐渐包含特定输入的占位符，例如“USER:”，而人工智能模型可以识别这些占位符。OpenAI 通过在人工智能模型处理多个消息字符串之前，将它们分类到不同的角色中，为提示符引入了更丰富的结构。\nAPI 概述 # 迅速的 # 通常使用 ChatModel 的 call() 方法，该方法接受一个 Prompt 实例并返回一个 ChatResponse 。\nPrompt 类充当一系列有序的 Message 对象和一个 ChatOptions 请求的容器。每条 Message 在提示中都体现着独特的角色，其内容和意图各不相同。这些角色可以涵盖各种元素，从用户查询到 AI 生成的响应，再到相关的背景信息。这种安排使得与 AI 模型进行复杂而细致的交互成为可能，因为提示由多条消息构成，每条消息在对话中都被赋予了特定的角色。\n下面是 Prompt 类的截断版本，为了简洁起见省略了构造函数和实用方法：\npublic class Prompt implements ModelRequest\u0026lt;List\u0026lt;Message\u0026gt;\u0026gt; { private final List\u0026lt;Message\u0026gt; messages; private ChatOptions chatOptions; } 信息 # Message 接口封装了 Prompt 文本、元数据属性集合以及称为 MessageType 的分类。\n该接口定义如下：\npublic interface Content { String getContent(); Map\u0026lt;String, Object\u0026gt; getMetadata(); } public interface Message extends Content { MessageType getMessageType(); } 多模式消息类型还实现了 ```MediaContent` 接口，提供了 Media`` 内容对象的列表。\npublic interface MediaContent extends Content { Collection\u0026lt;Media\u0026gt; getMedia(); } Message 接口的各种实现对应着 AI 模型可以处理的不同类别的消息。模型会根据对话角色来区分不同的消息类别。\n这些角色由 MessageType 有效映射，如下所述。\n角色 # 每条消息都被赋予了特定的角色。这些角色对消息进行分类，为 AI 模型明确提示中每个部分的上下文和目的。这种结构化方法增强了与 AI 沟通的细微差别和有效性，因为提示的每个部分在交互中都扮演着独特而明确的角色。\n主要角色是：\n系统角色：引导 AI 的行为和响应方式，设置 AI 如何解释和回复输入的参数或规则。这类似于在发起对话之前向 AI 提供指令。 用户角色：代表用户的输入——他们向 AI 提出的问题、命令或语句。这个角色至关重要，因为它构成了 AI 响应的基础。 助手角色：AI 对用户输入的响应。它不仅仅是一个答案或反应，对于维持对话的流畅性至关重要。通过追踪 AI 之前的响应（其“助手角色”消息），系统可以确保交互的连贯性以及与上下文的相关性。助手消息也可能包含功能工具调用请求信息。它就像 AI 中的一项特殊功能，在需要执行特定功能（例如计算、获取数据或其他不仅仅是对话的任务）时使用。 工具/功能角色：工具/功能角色专注于响应工具调用助手消息返回附加信息。 角色在 Spring AI 中表示为枚举，如下所示\npublic enum MessageType { USER(\u0026#34;user\u0026#34;), ASSISTANT(\u0026#34;assistant\u0026#34;), SYSTEM(\u0026#34;system\u0026#34;), TOOL(\u0026#34;tool\u0026#34;); ... } 提示模板 # Spring AI 中提示模板的一个关键组件是 PromptTemplate 类，旨在促进结构化提示的创建，然后将其发送到 AI 模型进行处理\npublic class PromptTemplate implements PromptTemplateActions, PromptTemplateMessageActions { // Other methods to be discussed later } 此类使用 TemplateRenderer API 来渲染模板。默认情况下，Spring AI 使用 StTemplateRenderer 实现，该实现基于 Terence Parr 开发的开源 [ StringTemplate]( https://www.stringtemplate.org/) 引擎。模板变量由 {} 语法标识，但您也可以配置分隔符以使用其他语法。\npublic interface TemplateRenderer extends BiFunction\u0026lt;String, Map\u0026lt;String, Object\u0026gt;, String\u0026gt; { @Override String apply(String template, Map\u0026lt;String, Object\u0026gt; variables); } Spring AI 使用 TemplateRenderer 接口来处理将变量实际替换到模板字符串中。默认实现使用 [ [StringTemplate]](#StringTemplate) 。如果需要自定义逻辑，可以提供自己的 TemplateRenderer 实现。对于不需要模板渲染的场景（例如，模板字符串已经完成），可以使用提供的 NoOpTemplateRenderer 。\nPromptTemplate promptTemplate = PromptTemplate.builder() .renderer(StTemplateRenderer.builder().startDelimiterToken(\u0026#39;\u0026lt;\u0026#39;).endDelimiterToken(\u0026#39;\u0026gt;\u0026#39;).build()) .template(\u0026#34;\u0026#34;\u0026#34; Tell me the names of 5 movies whose soundtrack was composed by \u0026lt;composer\u0026gt;. \u0026#34;\u0026#34;\u0026#34;) .build(); String prompt = promptTemplate.render(Map.of(\u0026#34;composer\u0026#34;, \u0026#34;John Williams\u0026#34;)); 此类实现的接口支持提示创建的不同方面：\nPromptTemplateStringActions 专注于创建和渲染提示字符串，代表最基本的提示生成形式。\nPromptTemplate``Message``Actions 专门用于通过生成和操作 Message 对象来创建提示。\n```PromptTemplateActions` 旨在返回 Prompt对象，该对象可以传递给ChatModel`` 以生成响应。\n虽然这些界面可能在许多项目中没有被广泛使用，但它们展示了提示创建的不同方法。\n实现的接口是\npublic interface PromptTemplateStringActions { String render(); String render(Map\u0026lt;String, Object\u0026gt; model); } 方法 String render() ：将提示模板渲染为最终的字符串格式，无需外部输入，适用于没有占位符或动态内容的模板。\n方法 String render(``Map\u0026lt;String, Object\u0026gt;`` model) ：增强了渲染功能以包含动态内容。它使用 Map\u0026lt;String, Object\u0026gt; 其中映射键是提示模板中的占位符名称，值是要插入的动态内容。\npublic interface PromptTemplateMessageActions { Message createMessage(); Message createMessage(List\u0026lt;Media\u0026gt; mediaList); Message createMessage(Map\u0026lt;String, Object\u0026gt; model); } 方法 ```Message createMessage()` ：创建一个没有附加数据的 Message`` 对象，用于静态或预定义的消息内容。\n方法 ```Message createMessage(List\u0026lt;Media\u0026gt; mediaList)` ：创建具有静态文本和媒体内容的 Message`` 对象。\n方法 Message createMessage(``Map\u0026lt;String, Object\u0026gt;`` model) ：扩展消息创建以集成动态内容，接受 Map\u0026lt;String, Object\u0026gt; 其中每个条目代表消息模板中的占位符及其对应的动态值。\npublic interface PromptTemplateActions extends PromptTemplateStringActions { Prompt create(); Prompt create(ChatOptions modelOptions); Prompt create(Map\u0026lt;String, Object\u0026gt; model); Prompt create(Map\u0026lt;String, Object\u0026gt; model, ChatOptions modelOptions); } 方法 ```Promptcreate()` ：生成无需外部数据输入的Prompt`` 对象，非常适合静态或预定义提示。\n方法 ```Promptcreate(ChatOptions modelOptions)` ：生成一个无需外部数据输入但具有聊天请求特定选项的Prompt`` 对象。\n方法 Prompt create(``Map\u0026lt;String, Object\u0026gt;`` model) ：扩展提示创建功能以包含动态内容，采用 Map\u0026lt;String, Object\u0026gt; 其中每个映射条目都是提示模板及其关联动态值中的占位符。\n方法 Prompt create(``Map\u0026lt;String, Object\u0026gt;`` model, ChatOptions modelOptions) ：扩展提示创建功能以包含动态内容，采用 Map\u0026lt;String, Object\u0026gt; （其中每个映射条目都是提示模板中的占位符及其关联的动态值）以及聊天请求的特定选项。\n示例用法 # 下面是一个取自 [ PromptTemplates 人工智能研讨会的]( https://github.com/Azure-Samples/spring-ai-azure-workshop/blob/main/2-README-prompt-templating.md)简单示例。\nPromptTemplate promptTemplate = new PromptTemplate(\u0026#34;Tell me a {adjective} joke about {topic}\u0026#34;); Prompt prompt = promptTemplate.create(Map.of(\u0026#34;adjective\u0026#34;, adjective, \u0026#34;topic\u0026#34;, topic)); return chatModel.call(prompt).getResult(); 下面是从[ 角色人工智能研讨会中]( https://github.com/Azure-Samples/spring-ai-azure-workshop/blob/main/3-README-prompt-roles.md)摘取的另一个示例。\nString userText = \u0026#34;\u0026#34;\u0026#34; Tell me about three famous pirates from the Golden Age of Piracy and why they did. Write at least a sentence for each pirate. \u0026#34;\u0026#34;\u0026#34;; Message userMessage = new UserMessage(userText); String systemText = \u0026#34;\u0026#34;\u0026#34; You are a helpful AI assistant that helps people find information. Your name is {name} You should reply to the user\u0026#39;s request with your name and also in the style of a {voice}. \u0026#34;\u0026#34;\u0026#34;; SystemPromptTemplate systemPromptTemplate = new SystemPromptTemplate(systemText); Message systemMessage = systemPromptTemplate.createMessage(Map.of(\u0026#34;name\u0026#34;, name, \u0026#34;voice\u0026#34;, voice)); Prompt prompt = new Prompt(List.of(userMessage, systemMessage)); List\u0026lt;Generation\u0026gt; response = chatModel.call(prompt).getResults(); 这展示了如何使用 SystemPromptTemplate 创建一条 Message 来构建 Prompt 实例，该 Message 带有系统角色，并传入占位符值。然后将带有角色 user 的消息与角色 system 的消息组合起来形成提示。之后，该提示会被传递给 ChatModel 以获得生成的响应。\n使用自定义模板渲染器 # 您可以通过实现 TemplateRenderer 接口并将其传递给 PromptTemplate 构造函数来使用自定义模板渲染器。您也可以继续使用默认的 StTemplateRenderer ，但需要自定义配置。\n默认情况下，模板变量由 {} 语法标识。如果您计划在提示中包含 JSON，则可能需要使用其他语法以避免与 JSON 语法冲突。例如，您可以使用 \u0026lt; 和 \u0026gt; 分隔符。\nPromptTemplate promptTemplate = PromptTemplate.builder() .renderer(StTemplateRenderer.builder().startDelimiterToken(\u0026#39;\u0026lt;\u0026#39;).endDelimiterToken(\u0026#39;\u0026gt;\u0026#39;).build()) .template(\u0026#34;\u0026#34;\u0026#34; Tell me the names of 5 movies whose soundtrack was composed by \u0026lt;composer\u0026gt;. \u0026#34;\u0026#34;\u0026#34;) .build(); String prompt = promptTemplate.render(Map.of(\u0026#34;composer\u0026#34;, \u0026#34;John Williams\u0026#34;)); 使用资源而不是原始字符串 # Spring AI 支持 `org.springframework.core.io.Resource``` 抽象，因此您可以将提示数据放入可直接在 PromptTemplate中使用的文件中。例如，您可以在 Spring 托管组件中定义一个字段来检索Resource`` 。\n@Value(\u0026#34;classpath:/prompts/system-message.st\u0026#34;) private Resource systemResource; 然后将该资源直接传递给 SystemPromptTemplate 。\nSystemPromptTemplate systemPromptTemplate = new SystemPromptTemplate(systemResource); 快速工程 # 在生成式人工智能中，创建提示对于开发人员来说是一项至关重要的任务。这些提示的质量和结构会显著影响人工智能输出的有效性。投入时间和精力设计周到的提示可以显著提升人工智能的成果。\n分享和讨论提示是人工智能社区的常见做法。这种协作方式不仅营造了共享的学习环境，还能帮助人们识别和使用高效的提示。\n该领域的研究通常涉及分析和比较不同的提示，以评估它们在不同情况下的有效性。例如，一项重要的研究表明，以“深呼吸，一步一步解决这个问题”作为提示开头，可以显著提高解决问题的效率。这凸显了精心选择的语言对生成式人工智能系统性能的影响。\n掌握最有效的提示使用方法，尤其是在人工智能技术飞速发展的今天，是一项持续的挑战。您应该认识到提示工程的重要性，并考虑借鉴社区和研究的洞见来改进提示创建策略。\n创建有效的提示 # 在制定提示时，整合几个关键组件以确保清晰度和有效性非常重要：\n指示 ：向 AI 提供清晰直接的指令，类似于与人沟通的方式。这种清晰的指令对于帮助 AI“理解”预期至关重要。 外部背景 ：在必要时，包含相关的背景信息或 AI 响应的具体指导。这种“外部背景”构成了提示的框架，并帮助 AI 掌握整体场景。 用户输入 ：这是最直接的部分——用户的直接请求或问题构成提示的核心。 输出指示符 ：这方面可能比较棘手。它需要指定 AI 响应所需的格式，例如 JSON。但请注意，AI 可能并不总是严格遵循此格式。例如，它可能会将“这是你的 JSON”之类的短语添加到实际 JSON 数据之前，或者有时会生成不准确的类似 JSON 的结构。 在设计提示时，为 AI 提供预期问答格式的示例非常有益。这种做法有助于 AI“理解”查询的结构和意图，从而提供更精确、更相关的响应。虽然本文档并未深入探讨这些技术，但它们为进一步探索 AI 提示工程提供了一个起点。\n以下是需要进一步调查的资源列表。\n简单技巧 # 将大量文本精简为简洁的摘要，捕捉关键点和主要思想，同时省略不太重要的细节。 专注于根据用户提出的问题，从提供的文本中获取具体答案。它旨在精准定位并提取相关信息以响应查询。 系统地将文本分类到预定义的类别或组中，分析文本并根据其内容将其分配到最合适的类别。 创建交互式对话，让人工智能可以与用户进行来回交流，模拟自然的对话流程。 根据特定的用户要求或描述生成功能代码片段，将自然语言指令转换为可执行代码。 高级技术 # 使模型能够在特定问题类型的极少或没有先前示例的情况下做出准确的预测或响应，并使用学习到的概括来理解和执行新任务。 链接多个 AI 响应，创建连贯且符合语境的对话。它有助于 AI 保持讨论的线索，确保相关性和连续性。 在这种方法中，人工智能首先分析输入（原因），然后确定最合适的行动或响应方案。它将理解与决策结合在一起。 Microsoft 指导 # 微软提供了一种结构化的方法来开发和完善提示。该框架指导用户创建有效的提示，以便从 AI 模型中获取所需的响应，并优化交互以提高清晰度和效率。 代币 # 标记在 AI 模型处理文本的过程中至关重要，它充当着将我们理解的单词转换为 AI 模型能够处理的格式的桥梁。这种转换分为两个阶段：输入时将单词转换为标记，然后在输出时将这些标记转换回单词。\n标记化是将文本分解成标记的过程，它是 AI 模型理解和处理语言的基础。AI 模型运用这种标记化格式来理解并响应提示。\n为了更好地理解标记，可以将它们视为单词的一部分。通常，一个标记代表一个单词的四分之三左右。例如，莎士比亚全集共约 90 万字，翻译过来大约需要 120 万个标记。\n试验 [ OpenAI Tokenizer UI]( https://platform.openai.com/tokenizer) 来查看单词如何转换为标记。\n代币除了在人工智能处理中的技术作用外，还具有实际意义，特别是在计费和模型功能方面：\n计费：AI 模型服务通常根据令牌使用情况计费。输入（提示）和输出（响应）都会计入令牌总数，因此较短的提示更具成本效益。 模型限制：不同的 AI 模型具有不同的令牌限制，这定义了它们的“上下文窗口”——即它们一次可以处理的最大信息量。例如，GPT-3 的限制为 4000 个令牌，而 Claude 2 和 Meta Llama 2 等其他模型的限制为 10 万个令牌，一些研究模型最多可以处理 100 万个令牌。 上下文窗口：模型的令牌限制决定了其上下文窗口。超过此限制的输入不会被模型处理。务必仅发送最少的有效信息集进行处理。例如，在查询《哈姆雷特》时，无需包含莎士比亚所有其他作品的令牌。 响应元数据：来自 AI 模型的响应的元数据包括使用的令牌数量，这是管理使用情况和成本的重要信息。 "},{"id":29,"href":"/docs/%E6%8C%87%E5%8D%97/%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E6%A8%A1%E5%BC%8F/","title":"提示工程模式","section":"指南","content":" 提示工程模式 # 基于全面的[ 即时工程指南]( https://www.kaggle.com/whitepaper-prompt-engineering) ，我们将对即时工程技术进行实际应用。该指南涵盖了有效即时工程的理论、原则和模式，并演示了如何使用 Spring AI 流畅的 [ ChatClient API](../chatclient.html) 将这些概念转化为可运行的 Java 代码。本文中使用的演示源代码可在以下位置获取： [ 即时工程模式示例]( https://github.com/spring-projects/spring-ai-examples/tree/main/prompt-engineering/prompt-engineering-patterns) 。\n1.配置 # 配置部分概述了如何使用 Spring AI 设置和调整大型语言模型 (LLM)。它涵盖了如何根据用例选择合适的 LLM 提供程序，以及如何配置重要的生成参数，以控制模型输出的质量、样式和格式。\n法学硕士（LLM）提供商选择 # 为了快速进行工程设计，您需要先选择一个模型。Spring AI 支持[ 多个 LLM 提供商](comparison.html) （例如 OpenAI、Anthropic、Google Vertex AI、AWS Bedrock、Ollama 等），让您无需更改应用程序代码即可切换提供商 - 只需更新配置即可。只需添加所选的启动依赖项 spring-ai-starter-model-\u0026lt;MODEL-PROVIDER-NAME\u0026gt; 即可。例如，以下是如何启用 Anthropic Claude API：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-anthropic\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 您可以像这样指定 LLM 模型名称：\n.options(ChatOptions.builder() .model(\u0026#34;claude-3-7-sonnet-latest\u0026#34;) // Use Anthropic\u0026#39;s Claude model .build()) 在[ 参考文档](../chatmodel.html)中查找有关启用每个模型的详细信息。\nLLM 输出配置 # 在深入研究即时工程技术之前，我们有必要了解如何配置 LLM 的输出行为。Spring AI 提供了多个配置选项，可让您通过 [ ChatOptions](../chatmodel.html#_chat_options) 构建器控制生成的各个方面。\n所有配置都可以以编程方式应用，如下面的示例所示，或者在启动时通过 Spring 应用程序属性应用。\n温度 # 温度控制模型响应的随机性或“创造性”。\n较低值（0.0-0.3） ：响应更确定、更集中。更适合事实类问题、分类问题或一致性至关重要的任务。 中等值（0.4-0.7） ：在确定性和创造性之间取得平衡。适用于一般用例。 值越高 (0.8-1.0) ：回复更具创意、多样性，且可能带来惊喜。更适合创意写作、头脑风暴或生成多样化选项。 .options(ChatOptions.builder() .temperature(0.1) // Very deterministic output .build()) 了解温度对于快速工程至关重要，因为不同的技术受益于不同的温度设置。\n输出长度（MaxTokens） # maxTokens 参数限制了模型在响应中可以生成的标记（词片段）的数量。\n低值（5-25） ：适用于单个单词、短语或分类标签。 中等值（50-500） ：用于段落或简短解释。 高值（1000+） ：适用于长篇内容、故事或复杂的解释。 .options(ChatOptions.builder() .maxTokens(250) // Medium-length response .build()) 设置适当的输出长度非常重要，以确保您获得完整的响应而没有不必要的冗长。\n采样控制（Top-K 和 Top-P） # 这些参数使您可以对生成过程中的令牌选择过程进行细粒度的控制。\nTop-K ：将 token 的选择范围限制为 K 个最有可能的后续 token。值越高（例如 40-50），多样性越好。 Top-P（核采样） ：从最小的标记集中动态选择，其累积概率超过 P。0.8-0.95 之类的值很常见。 .options(ChatOptions.builder() .topK(40) // Consider only the top 40 tokens .topP(0.8) // Sample from tokens that cover 80% of probability mass .build()) 这些采样控制与温度协同作用来形成响应特性。\n结构化响应格式 # 除了纯文本响应（使用 .content() ）之外，Spring AI 还可以轻松地使用 .entity() 方法将 LLM 响应直接映射到 Java 对象。\nenum Sentiment { POSITIVE, NEUTRAL, NEGATIVE } Sentiment result = chatClient.prompt(\u0026#34;...\u0026#34;) .call() .entity(Sentiment.class); 当与指示模型返回结构化数据的系统提示相结合时，此功能特别强大。\n特定于模型的选项 # 可移植的 ChatOptions 不仅为不同的 LLM 提供程序提供了一致的接口，Spring AI 还提供了特定于模型的选项类，用于公开特定于提供程序的功能和配置。这些特定于模型的选项允许您利用每个 LLM 提供程序的独特功能。\n// Using OpenAI-specific options OpenAiChatOptions openAiOptions = OpenAiChatOptions.builder() .model(\u0026#34;gpt-4o\u0026#34;) .temperature(0.2) .frequencyPenalty(0.5) // OpenAI-specific parameter .presencePenalty(0.3) // OpenAI-specific parameter .responseFormat(new ResponseFormat(\u0026#34;json_object\u0026#34;)) // OpenAI-specific JSON mode .seed(42) // OpenAI-specific deterministic generation .build(); String result = chatClient.prompt(\u0026#34;...\u0026#34;) .options(openAiOptions) .call() .content(); // Using Anthropic-specific options AnthropicChatOptions anthropicOptions = AnthropicChatOptions.builder() .model(\u0026#34;claude-3-7-sonnet-latest\u0026#34;) .temperature(0.2) .topK(40) // Anthropic-specific parameter .thinking(AnthropicApi.ThinkingType.ENABLED, 1000) // Anthropic-specific thinking configuration .build(); String result = chatClient.prompt(\u0026#34;...\u0026#34;) .options(anthropicOptions) .call() .content(); 每个模型提供商都有其自己的聊天选项实现（例如 OpenAiChatOptions 、 AnthropicChatOptions 、 MistralAiChatOptions ），这些实现在公开提供商特定参数的同时，仍实现通用接口。这种方法让您可以灵活地使用可移植选项来实现跨提供商兼容性，或者在需要访问特定提供商的独特功能时使用模型特定的选项。\n请注意，使用特定于模型的选项时，您的代码将与该特定提供程序绑定，从而降低可移植性。您需要在访问特定于提供程序的高级功能与在应用程序中保持提供程序独立性之间进行权衡。\n2. 快捷的工程技术 # 以下每个部分都实现了指南中一项特定的快速工程技术。通过学习“快速工程”指南和这些实现，您不仅可以深入了解可用的快速工程技术，还能了解如何在生产 Java 应用程序中有效地实现它们。\n2.1 零次提示 # 零样本提示是指要求人工智能在不提供任何示例的情况下执行任务。这种方法测试模型从零开始理解和执行指令的能力。大型语言模型在海量文本语料库上进行训练，使其能够在没有明确演示的情况下理解“翻译”、“摘要”或“分类”等任务的含义。\n零样本训练非常适合一些简单的任务，因为模型在训练过程中很可能已经见过类似的样本，而且您希望尽量缩短提示长度。然而，其性能可能会因任务复杂度和指令的制定方式而有所不同。\n// Implementation of Section 2.1: General prompting / zero shot (page 15) public void pt_zero_shot(ChatClient chatClient) { enum Sentiment { POSITIVE, NEUTRAL, NEGATIVE } Sentiment reviewSentiment = chatClient.prompt(\u0026#34;\u0026#34;\u0026#34; Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE. Review: \u0026#34;Her\u0026#34; is a disturbing study revealing the direction humanity is headed if AI is allowed to keep evolving, unchecked. I wish there were more movies like this masterpiece. Sentiment: \u0026#34;\u0026#34;\u0026#34;) .options(ChatOptions.builder() .model(\u0026#34;claude-3-7-sonnet-latest\u0026#34;) .temperature(0.1) .maxTokens(5) .build()) .call() .entity(Sentiment.class); System.out.println(\u0026#34;Output: \u0026#34; + reviewSentiment); } 此示例展示了如何在不提供示例的情况下对电影评论情绪进行分类。请注意，为了获得更确定的结果，我们采用了较低的温度 (0.1)，并且将 .entity(Sentiment.class) 直接映射到 Java 枚举。\n参考文献： Brown, TB 等人 (2020)。“语言模型是少样本学习器。”arXiv:2005.14165。https [ ://arxiv.org/abs/2005.14165](https ://arxiv.org/abs/2005.14165)\n2.2 一次性提示和少量提示 # 少样本提示为模型提供了一个或多个示例，以帮助指导其响应，这对于需要特定输出格式的任务特别有用。通过向模型展示所需输入-输出对的示例，它可以学习该模式并将其应用于新的输入，而无需显式更新参数。\n单样本训练仅提供单个样本，当样本成本高昂或模式相对简单时非常有用。少样本训练则使用多个样本（通常为 3-5 个），以帮助模型更好地理解更复杂任务中的模式，或展示正确输出的不同变体。\n// Implementation of Section 2.2: One-shot \u0026amp; few-shot (page 16) public void pt_one_shot_few_shots(ChatClient chatClient) { String pizzaOrder = chatClient.prompt(\u0026#34;\u0026#34;\u0026#34; Parse a customer\u0026#39;s pizza order into valid JSON EXAMPLE 1: I want a small pizza with cheese, tomato sauce, and pepperoni. JSON Response: ``` { \u0026#34;size\u0026#34;: \u0026#34;small\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;normal\u0026#34;, \u0026#34;ingredients\u0026#34;: [\u0026#34;cheese\u0026#34;, \u0026#34;tomato sauce\u0026#34;, \u0026#34;pepperoni\u0026#34;] } ``` EXAMPLE 2: Can I get a large pizza with tomato sauce, basil and mozzarella. JSON Response: ``` { \u0026#34;size\u0026#34;: \u0026#34;large\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;normal\u0026#34;, \u0026#34;ingredients\u0026#34;: [\u0026#34;tomato sauce\u0026#34;, \u0026#34;basil\u0026#34;, \u0026#34;mozzarella\u0026#34;] } ``` Now, I would like a large pizza, with the first half cheese and mozzarella. And the other tomato sauce, ham and pineapple. \u0026#34;\u0026#34;\u0026#34;) .options(ChatOptions.builder() .model(\u0026#34;claude-3-7-sonnet-latest\u0026#34;) .temperature(0.1) .maxTokens(250) .build()) .call() .content(); } 对于需要特定格式、处理边缘情况，或在没有示例的情况下任务定义可能含糊不清的任务，小样本提示尤其有效。示例的质量和多样性会显著影响性能。\n参考文献： Brown, TB 等人 (2020)。“语言模型是少样本学习器。”arXiv:2005.14165。https [ ://arxiv.org/abs/2005.14165](https ://arxiv.org/abs/2005.14165)\n2.3 系统、情境和角色提示 # 系统提示 # 系统提示设定了语言模型的整体背景和目的，定义了模型应该做什么的“总体情况”。它为模型的响应建立了行为框架、约束条件和高级目标，并与具体的用户查询区分开来。\n系统提示在整个对话过程中充当着持续的“使命宣言”，允许您设置全局参数，例如输出格式、语气、道德界限或角色定义。与专注于特定任务的用户提示不同，系统提示框定了所有用户提示的解读方式。\n// Implementation of Section 2.3.1: System prompting public void pt_system_prompting_1(ChatClient chatClient) { String movieReview = chatClient .prompt() .system(\u0026#34;Classify movie reviews as positive, neutral or negative. Only return the label in uppercase.\u0026#34;) .user(\u0026#34;\u0026#34;\u0026#34; Review: \u0026#34;Her\u0026#34; is a disturbing study revealing the direction humanity is headed if AI is allowed to keep evolving, unchecked. It\u0026#39;s so disturbing I couldn\u0026#39;t watch it. Sentiment: \u0026#34;\u0026#34;\u0026#34;) .options(ChatOptions.builder() .model(\u0026#34;claude-3-7-sonnet-latest\u0026#34;) .temperature(1.0) .topK(40) .topP(0.8) .maxTokens(5) .build()) .call() .content(); } 系统提示与 Spring AI 的实体映射功能结合使用时尤其强大：\n// Implementation of Section 2.3.1: System prompting with JSON output record MovieReviews(Movie[] movie_reviews) { enum Sentiment { POSITIVE, NEUTRAL, NEGATIVE } record Movie(Sentiment sentiment, String name) { } } MovieReviews movieReviews = chatClient .prompt() .system(\u0026#34;\u0026#34;\u0026#34; Classify movie reviews as positive, neutral or negative. Return valid JSON. \u0026#34;\u0026#34;\u0026#34;) .user(\u0026#34;\u0026#34;\u0026#34; Review: \u0026#34;Her\u0026#34; is a disturbing study revealing the direction humanity is headed if AI is allowed to keep evolving, unchecked. It\u0026#39;s so disturbing I couldn\u0026#39;t watch it. JSON Response: \u0026#34;\u0026#34;\u0026#34;) .call() .entity(MovieReviews.class); 系统提示对于多轮对话尤其有价值，可确保跨多个查询的一致行为，并建立适用于所有响应的格式约束（如 JSON 输出）。\n参考文献： OpenAI。(2022)。“系统消息”。https [ ://platform.openai.com/docs/guides/chat/introduction](https ://platform.openai.com/docs/guides/chat/introduction)\n角色提示 # 角色提示会指示模型采用特定的角色或人物，这会影响其生成内容的方式。通过为模型分配特定的身份、专业知识或视角，您可以影响其响应的风格、语气、深度和框架。\n角色提示利用模型模拟不同专业领域和沟通风格的能力。常见角色包括专家（例如，“您是一位经验丰富的数据科学家”）、专业人士（例如，“充当导游”）或风格人物（例如，“像莎士比亚一样解释”）。\n// Implementation of Section 2.3.2: Role prompting public void pt_role_prompting_1(ChatClient chatClient) { String travelSuggestions = chatClient .prompt() .system(\u0026#34;\u0026#34;\u0026#34; I want you to act as a travel guide. I will write to you about my location and you will suggest 3 places to visit near me. In some cases, I will also give you the type of places I will visit. \u0026#34;\u0026#34;\u0026#34;) .user(\u0026#34;\u0026#34;\u0026#34; My suggestion: \u0026#34;I am in Amsterdam and I want to visit only museums.\u0026#34; Travel Suggestions: \u0026#34;\u0026#34;\u0026#34;) .call() .content(); } 可以通过样式说明来增强角色提示：\n// Implementation of Section 2.3.2: Role prompting with style instructions public void pt_role_prompting_2(ChatClient chatClient) { String humorousTravelSuggestions = chatClient .prompt() .system(\u0026#34;\u0026#34;\u0026#34; I want you to act as a travel guide. I will write to you about my location and you will suggest 3 places to visit near me in a humorous style. \u0026#34;\u0026#34;\u0026#34;) .user(\u0026#34;\u0026#34;\u0026#34; My suggestion: \u0026#34;I am in Amsterdam and I want to visit only museums.\u0026#34; Travel Suggestions: \u0026#34;\u0026#34;\u0026#34;) .call() .content(); } 这种技术对于专业领域知识特别有效，可以在响应中实现一致的语气，并与用户创建更具吸引力、个性化的互动。\n参考文献： Shanahan, M. 等人 (2023)。“使用大型语言模型进行角色扮演。”arXiv:2305.16367。https [ ://arxiv.org/abs/2305.16367](https ://arxiv.org/abs/2305.16367)\n情境提示 # 情境提示通过传递情境参数为模型提供额外的背景信息。这项技术丰富了模型对具体情境的理解，使其能够提供更相关、更有针对性的响应，而不会扰乱主要指令。\n通过提供上下文信息，您可以帮助模型理解与当前查询相关的特定领域、受众、约束或背景事实。这可以带来更准确、更相关、更恰当的响应。\n// Implementation of Section 2.3.3: Contextual prompting public void pt_contextual_prompting(ChatClient chatClient) { String articleSuggestions = chatClient .prompt() .user(u -\u0026gt; u.text(\u0026#34;\u0026#34;\u0026#34; Suggest 3 topics to write an article about with a few lines of description of what this article should contain. Context: {context} \u0026#34;\u0026#34;\u0026#34;) .param(\u0026#34;context\u0026#34;, \u0026#34;You are writing for a blog about retro 80\u0026#39;s arcade video games.\u0026#34;)) .call() .content(); } Spring AI 使用 param() 方法注入上下文变量，使上下文提示更加清晰。当模型需要特定领域知识、根据特定受众或场景调整响应，以及确保响应符合特定约束或要求时，此技术尤为有用。\n参考文献： Liu, P. 等 (2021)。“什么是 GPT-3 的良好上下文示例？”arXiv:2101.06804。https [ ://arxiv.org/abs/2101.06804](https ://arxiv.org/abs/2101.06804)\n2.4 后退提示 # 后退提示法通过先获取背景知识，将复杂的请求分解成更简单的步骤。这种技术鼓励模型先从当前问题“后退一步”，思考更广泛的背景、基本原理或与问题相关的常识，然后再处理具体的问题。\n通过将复杂问题分解为更易于管理的部分并首先建立基础知识，该模型可以对难题提供更准确的答案。\n// Implementation of Section 2.4: Step-back prompting public void pt_step_back_prompting(ChatClient.Builder chatClientBuilder) { // Set common options for the chat client var chatClient = chatClientBuilder .defaultOptions(ChatOptions.builder() .model(\u0026#34;claude-3-7-sonnet-latest\u0026#34;) .temperature(1.0) .topK(40) .topP(0.8) .maxTokens(1024) .build()) .build(); // First get high-level concepts String stepBack = chatClient .prompt(\u0026#34;\u0026#34;\u0026#34; Based on popular first-person shooter action games, what are 5 fictional key settings that contribute to a challenging and engaging level storyline in a first-person shooter video game? \u0026#34;\u0026#34;\u0026#34;) .call() .content(); // Then use those concepts in the main task String story = chatClient .prompt() .user(u -\u0026gt; u.text(\u0026#34;\u0026#34;\u0026#34; Write a one paragraph storyline for a new level of a first- person shooter video game that is challenging and engaging. Context: {step-back} \u0026#34;\u0026#34;\u0026#34;) .param(\u0026#34;step-back\u0026#34;, stepBack)) .call() .content(); } 退后提示对于复杂的推理任务、需要专业领域知识的问题以及当您想要更全面、更周到的回应而不是立即得到答案时特别有效。\n参考文献： Zheng, Z. 等 (2023)。“退一步思考：在大型语言模型中通过抽象引发推理。”arXiv:2310.06117。https [ ://arxiv.org/abs/2310.06117](https ://arxiv.org/abs/2310.06117)\n2.5 思路链（CoT） # 思路链提示鼓励模型逐步推理问题，从而提高复杂推理任务的准确性。通过明确要求模型展示其工作成果或以逻辑步骤思考问题，您可以显著提高需要多步骤推理的任务的性能。\nCoT 的工作原理是鼓励模型在得出最终答案之前生成中间推理步骤，类似于人类解决复杂问题的方式。这使得模型的思维过程更加清晰，并有助于其得出更准确的结论。\n// Implementation of Section 2.5: Chain of Thought (CoT) - Zero-shot approach public void pt_chain_of_thought_zero_shot(ChatClient chatClient) { String output = chatClient .prompt(\u0026#34;\u0026#34;\u0026#34; When I was 3 years old, my partner was 3 times my age. Now, I am 20 years old. How old is my partner? Let\u0026#39;s think step by step. \u0026#34;\u0026#34;\u0026#34;) .call() .content(); } // Implementation of Section 2.5: Chain of Thought (CoT) - Few-shot approach public void pt_chain_of_thought_singleshot_fewshots(ChatClient chatClient) { String output = chatClient .prompt(\u0026#34;\u0026#34;\u0026#34; Q: When my brother was 2 years old, I was double his age. Now I am 40 years old. How old is my brother? Let\u0026#39;s think step by step. A: When my brother was 2 years, I was 2 * 2 = 4 years old. That\u0026#39;s an age difference of 2 years and I am older. Now I am 40 years old, so my brother is 40 - 2 = 38 years old. The answer is 38. Q: When I was 3 years old, my partner was 3 times my age. Now, I am 20 years old. How old is my partner? Let\u0026#39;s think step by step. A: \u0026#34;\u0026#34;\u0026#34;) .call() .content(); } 关键词“让我们一步一步思考”会触发模型展示其推理过程。CoT 对于数学问题、逻辑推理任务以及任何需要多步推理的问题尤其有用。它通过明确中间推理来帮助减少错误。\n参考文献： Wei, J. 等人 (2022)。“思维链提示在大型语言模型中引发推理。”arXiv:2201.11903。https [ ://arxiv.org/abs/2201.11903](https ://arxiv.org/abs/2201.11903)\n2.6 自洽性 # 自洽性是指多次运行模型并汇总结果以获得更可靠的答案。该技术通过对同一问题进行不同的推理路径采样，并通过多数表决选出最一致的答案，解决了 LLM 输出结果的差异性问题。\n通过生成具有不同温度或采样设置的多条推理路径，然后聚合最终答案，自洽性可以提高复杂推理任务的准确性。它本质上是一种针对 LLM 输出的集成方法。\n// Implementation of Section 2.6: Self-consistency public void pt_self_consistency(ChatClient chatClient) { String email = \u0026#34;\u0026#34;\u0026#34; Hi, I have seen you use Wordpress for your website. A great open source content management system. I have used it in the past too. It comes with lots of great user plugins. And it\u0026#39;s pretty easy to set up. I did notice a bug in the contact form, which happens when you select the name field. See the attached screenshot of me entering text in the name field. Notice the JavaScript alert box that I inv0k3d. But for the rest it\u0026#39;s a great website. I enjoy reading it. Feel free to leave the bug in the website, because it gives me more interesting things to read. Cheers, Harry the Hacker. \u0026#34;\u0026#34;\u0026#34;; record EmailClassification(Classification classification, String reasoning) { enum Classification { IMPORTANT, NOT_IMPORTANT } } int importantCount = 0; int notImportantCount = 0; // Run the model 5 times with the same input for (int i = 0; i \u0026lt; 5; i++) { EmailClassification output = chatClient .prompt() .user(u -\u0026gt; u.text(\u0026#34;\u0026#34;\u0026#34; Email: {email} Classify the above email as IMPORTANT or NOT IMPORTANT. Let\u0026#39;s think step by step and explain why. \u0026#34;\u0026#34;\u0026#34;) .param(\u0026#34;email\u0026#34;, email)) .options(ChatOptions.builder() .temperature(1.0) // Higher temperature for more variation .build()) .call() .entity(EmailClassification.class); // Count results if (output.classification() == EmailClassification.Classification.IMPORTANT) { importantCount++; } else { notImportantCount++; } } // Determine the final classification by majority vote String finalClassification = importantCount \u0026gt; notImportantCount ? \u0026#34;IMPORTANT\u0026#34; : \u0026#34;NOT IMPORTANT\u0026#34;; } 对于高风险决策、复杂推理任务以及需要比单一响应更可靠答案的情况，自洽性尤为重要。但其弊端是由于多次 API 调用会增加计算成本和延迟。\n参考文献： Wang, X. 等人 (2022)。“自一致性提升语言模型的思路链推理能力。”arXiv:2203.11171。https [ ://arxiv.org/abs/2203.11171](https ://arxiv.org/abs/2203.11171)\n2.7 思想之树（ToT） # 思路树 (ToT) 是一种高级推理框架，它通过同时探索多条推理路径来扩展思路链。它将问题解决视为一个搜索过程，模型会生成不同的中间步骤，评估其可行性，并探索最具潜力的路径。\n这种技术对于具有多种可能方法的复杂问题或解决方案需要探索各种替代方案才能找到最佳路径的情况特别有效。\n游戏解决 ToT 示例：\n// Implementation of Section 2.7: Tree of Thoughts (ToT) - Game solving example public void pt_tree_of_thoughts_game(ChatClient chatClient) { // Step 1: Generate multiple initial moves String initialMoves = chatClient .prompt(\u0026#34;\u0026#34;\u0026#34; You are playing a game of chess. The board is in the starting position. Generate 3 different possible opening moves. For each move: 1. Describe the move in algebraic notation 2. Explain the strategic thinking behind this move 3. Rate the move\u0026#39;s strength from 1-10 \u0026#34;\u0026#34;\u0026#34;) .options(ChatOptions.builder() .temperature(0.7) .build()) .call() .content(); // Step 2: Evaluate and select the most promising move String bestMove = chatClient .prompt() .user(u -\u0026gt; u.text(\u0026#34;\u0026#34;\u0026#34; Analyze these opening moves and select the strongest one: {moves} Explain your reasoning step by step, considering: 1. Position control 2. Development potential 3. Long-term strategic advantage Then select the single best move. \u0026#34;\u0026#34;\u0026#34;).param(\u0026#34;moves\u0026#34;, initialMoves)) .call() .content(); // Step 3: Explore future game states from the best move String gameProjection = chatClient .prompt() .user(u -\u0026gt; u.text(\u0026#34;\u0026#34;\u0026#34; Based on this selected opening move: {best_move} Project the next 3 moves for both players. For each potential branch: 1. Describe the move and counter-move 2. Evaluate the resulting position 3. Identify the most promising continuation Finally, determine the most advantageous sequence of moves. \u0026#34;\u0026#34;\u0026#34;).param(\u0026#34;best_move\u0026#34;, bestMove)) .call() .content(); } 参考文献： Yao, S. 等 (2023)。“思维树：基于大型语言模型的深思熟虑的问题求解”。arXiv:2305.10601。https [ ://arxiv.org/abs/2305.10601](https ://arxiv.org/abs/2305.10601)\n2.8 自动提示工程 # 自动提示工程利用人工智能生成并评估备选提示。这项元技术利用语言模型本身来创建、改进和基准测试不同的提示变体，以找到特定任务的最佳方案。\n通过系统地生成和评估提示语的变化，APE 可以找到比人工设计更有效的提示语，尤其是在处理复杂任务时。这是利用 AI 提升自身性能的一种方式。\n// Implementation of Section 2.8: Automatic Prompt Engineering public void pt_automatic_prompt_engineering(ChatClient chatClient) { // Generate variants of the same request String orderVariants = chatClient .prompt(\u0026#34;\u0026#34;\u0026#34; We have a band merchandise t-shirt webshop, and to train a chatbot we need various ways to order: \u0026#34;One Metallica t-shirt size S\u0026#34;. Generate 10 variants, with the same semantics but keep the same meaning. \u0026#34;\u0026#34;\u0026#34;) .options(ChatOptions.builder() .temperature(1.0) // High temperature for creativity .build()) .call() .content(); // Evaluate and select the best variant String output = chatClient .prompt() .user(u -\u0026gt; u.text(\u0026#34;\u0026#34;\u0026#34; Please perform BLEU (Bilingual Evaluation Understudy) evaluation on the following variants: ---- {variants} ---- Select the instruction candidate with the highest evaluation score. \u0026#34;\u0026#34;\u0026#34;).param(\u0026#34;variants\u0026#34;, orderVariants)) .call() .content(); } APE 对于优化生产系统的提示、解决手动提示工程已达到极限的挑战性任务以及系统地大规模提高提示质量特别有价值。\n参考文献： Zhou, Y. 等 (2022)。“大型语言模型是人类级别的快速工程师。”arXiv:2211.01910。https [ ://arxiv.org/abs/2211.01910](https ://arxiv.org/abs/2211.01910)\n2.9 代码提示 # 代码提示是指针对代码相关任务的专门技术。这些技术利用法学硕士 (LLM) 理解和生成编程语言的能力，使他们能够编写新代码、解释现有代码、调试问题以及在语言之间进行转换。\n有效的代码提示通常包含清晰的规范、合适的上下文（库、框架、代码规范），有时还会包含类似代码的示例。为了获得更确定的输出，温度设置通常较低（0.1-0.3）。\n// Implementation of Section 2.9.1: Prompts for writing code public void pt_code_prompting_writing_code(ChatClient chatClient) { String bashScript = chatClient .prompt(\u0026#34;\u0026#34;\u0026#34; Write a code snippet in Bash, which asks for a folder name. Then it takes the contents of the folder and renames all the files inside by prepending the name draft to the file name. \u0026#34;\u0026#34;\u0026#34;) .options(ChatOptions.builder() .temperature(0.1) // Low temperature for deterministic code .build()) .call() .content(); } // Implementation of Section 2.9.2: Prompts for explaining code public void pt_code_prompting_explaining_code(ChatClient chatClient) { String code = \u0026#34;\u0026#34;\u0026#34; #!/bin/bash echo \u0026#34;Enter the folder name: \u0026#34; read folder_name if [ ! -d \u0026#34;$folder_name\u0026#34; ]; then echo \u0026#34;Folder does not exist.\u0026#34; exit 1 fi files=( \u0026#34;$folder_name\u0026#34;/* ) for file in \u0026#34;${files[@]}\u0026#34;; do new_file_name=\u0026#34;draft_$(basename \u0026#34;$file\u0026#34;)\u0026#34; mv \u0026#34;$file\u0026#34; \u0026#34;$new_file_name\u0026#34; done echo \u0026#34;Files renamed successfully.\u0026#34; \u0026#34;\u0026#34;\u0026#34;; String explanation = chatClient .prompt() .user(u -\u0026gt; u.text(\u0026#34;\u0026#34;\u0026#34; Explain to me the below Bash code: ``` {code} ``` \u0026#34;\u0026#34;\u0026#34;).param(\u0026#34;code\u0026#34;, code)) .call() .content(); } // Implementation of Section 2.9.3: Prompts for translating code public void pt_code_prompting_translating_code(ChatClient chatClient) { String bashCode = \u0026#34;\u0026#34;\u0026#34; #!/bin/bash echo \u0026#34;Enter the folder name: \u0026#34; read folder_name if [ ! -d \u0026#34;$folder_name\u0026#34; ]; then echo \u0026#34;Folder does not exist.\u0026#34; exit 1 fi files=( \u0026#34;$folder_name\u0026#34;/* ) for file in \u0026#34;${files[@]}\u0026#34;; do new_file_name=\u0026#34;draft_$(basename \u0026#34;$file\u0026#34;)\u0026#34; mv \u0026#34;$file\u0026#34; \u0026#34;$new_file_name\u0026#34; done echo \u0026#34;Files renamed successfully.\u0026#34; \u0026#34;\u0026#34;\u0026#34;; String pythonCode = chatClient .prompt() .user(u -\u0026gt; u.text(\u0026#34;\u0026#34;\u0026#34; Translate the below Bash code to a Python snippet: {code} \u0026#34;\u0026#34;\u0026#34;).param(\u0026#34;code\u0026#34;, bashCode)) .call() .content(); } 代码提示对于自动化代码文档、原型设计、学习编程概念以及编程语言间的转换尤其有用。将其与少样本提示或思路链等技术结合使用，可以进一步提升其有效性。\n参考文献： Chen, M. 等人 (2021)。“评估基于代码训练的大型语言模型。”arXiv:2107.03374。https [ ://arxiv.org/abs/2107.03374](https ://arxiv.org/abs/2107.03374)\n结论 # Spring AI 提供了优雅的 Java API，用于实现所有主要的即时工程技术。通过将这些技术与 Spring 强大的实体映射和流畅的 API 相结合，开发人员可以使用简洁、可维护的代码构建复杂的 AI 应用。\n最有效的方法通常需要结合多种技术——例如，将系统提示与少量样本示例结合使用，或将思路链与角色提示结合使用。Spring AI 灵活的 API 使这些组合易于实现。\n对于生产应用程序，请记住：\n借助这些技术和 Spring AI 强大的抽象，您可以创建强大的 AI 驱动应用程序，提供一致、高质量的结果。\n参考 # "},{"id":30,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E9%9F%B3%E9%A2%91%E6%A8%A1%E5%9E%8B/%E6%96%87%E6%9C%AC%E8%BD%AC%E8%AF%AD%E9%9F%B3-tts-api/","title":"文本转语音 (TTS) API","section":"音频模型","content":" 文本转语音 (TTS) API # Spring AI 支持 OpenAI 的 Speech API。当实现更多 Speech 提供程序时，将提取通用的 SpeechModel 和 StreamingSpeechModel 接口。\n"},{"id":31,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B-api/%E4%BA%9A%E9%A9%AC%E9%80%8A%E5%9F%BA%E5%B2%A9/%E6%B3%B0%E5%9D%A6%E5%B5%8C%E5%85%A5/","title":"泰坦嵌入","section":"亚马逊基岩","content":" 泰坦嵌入 # 提供 Bedrock Titan 嵌入模型。 [ Amazon Titan]( https://aws.amazon.com/bedrock/titan/) 基础模型 (FM) 通过完全托管的 API 为客户提供丰富的高性能图像、多模态嵌入和文本模型选择。[ Amazon Titan]( https://aws.amazon.com/bedrock/titan/) 模型由 AWS 创建，并在大型数据集上进行预训练，使其成为功能强大的通用模型，旨在支持各种用例，同时支持以负责任的方式使用 AI。您可以按原样使用它们，也可以使用您自己的数据进行个性化定制。\n[ AWS Bedrock Titan 模型页面]( https://aws.amazon.com/bedrock/titan/)和 [ Amazon Bedrock 用户指南]( https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html)包含有关如何使用 AWS 托管模型的详细信息。\n先决条件 # 请参阅 [ Amazon Bedrock 上的 Spring AI 文档](../bedrock.html)以了解如何设置 API 访问。\n添加存储库和 BOM # Spring AI 的构件已发布在 Maven Central 和 Spring Snapshot 仓库中。请参阅 [ “构件仓库”](../../getting-started.html#artifact-repositories) 部分，将这些仓库添加到您的构建系统中。\n为了帮助管理依赖项，Spring AI 提供了 BOM（物料清单），以确保在整个项目中使用一致版本的 Spring AI。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # 将 spring-ai-starter-model-bedrock 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-bedrock\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-bedrock\u0026#39; } 启用 Titan 嵌入支持 # 默认情况下，Titan 嵌入模型是禁用的。要启用它，请在应用程序配置中将 spring.ai.model.embedding 属性设置为 bedrock-titan ：\nspring.ai.model.embedding=bedrock-titan 或者，您可以使用 Spring 表达式语言 (SpEL) 来引用环境变量：\n# In application.yml spring: ai: model: embedding: ${AI_MODEL_EMBEDDING} # In your environment or .env file export AI_MODEL_EMBEDDING=bedrock-titan 您还可以在启动应用程序时使用 Java 系统属性设置此属性：\njava -Dspring.ai.model.embedding=bedrock-titan -jar your-application.jar 嵌入属性 # 前缀 spring.ai.bedrock.aws 是配置与 AWS Bedrock 的连接的属性前缀。\n前缀 spring.ai.bedrock.titan.embedding （在 BedrockTitanEmbeddingProperties 中定义）是用于配置 Titan 的嵌入模型实现的属性前缀。\n支持的值为： amazon.titan-embed-image-v1 、 amazon.titan-embed-text-v1 和 amazon.titan-embed-text-v2:0 。模型 ID 值也可以在 [ AWS Bedrock 文档的基本模型 ID]( https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids-arns.html) 中找到。\n运行时选项 # [ BedrockTitanEmbeddingOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/titan/[BedrockTitanEmbeddingOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/titan/BedrockTitanEmbeddingOptions.java)) 提供模型配置，例如 input-type 。启动时，可以使用 BedrockTitanEmbeddingModel(api).withInputType(type) 方法或 spring.ai.bedrock.titan.embedding.input-type 属性配置默认选项。\n在运行时，您可以通过向 EmbeddingRequest 调用添加新的、特定于请求的选项来覆盖默认选项。例如，要覆盖特定请求的默认温度：\nEmbeddingResponse embeddingResponse = embeddingModel.call( new EmbeddingRequest(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;), BedrockTitanEmbeddingOptions.builder() .withInputType(InputType.TEXT) .build())); 样品控制器 # [ 创建]( https://start.spring.io/)一个新的 Spring Boot 项目并将 spring-ai-starter-model-bedrock 添加到您的 pom（或 gradle）依赖项中。\n在 src/main/resources 目录下添加一个 application.properties 文件，以启用和配置 Titan Embedding 模型：\nspring.ai.bedrock.aws.region=eu-central-1 spring.ai.bedrock.aws.access-key=${AWS_ACCESS_KEY_ID} spring.ai.bedrock.aws.secret-key=${AWS_SECRET_ACCESS_KEY} spring.ai.model.embedding=bedrock-titan 这将创建一个 EmbeddingController 实现，您可以将其注入到您的类中。这是一个使用聊天模型生成文本的简单 @Controller 类的示例。\n@RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(\u0026#34;/ai/embedding\u0026#34;) public Map embed(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { EmbeddingResponse embeddingResponse = this.embeddingModel.embedForResponse(List.of(message)); return Map.of(\u0026#34;embedding\u0026#34;, embeddingResponse); } } 手动配置 # BedrockTitanEmbeddingModel 实现了 EmbeddingModel ，并使用 [ Low-level TitanEmbeddingBedrockApi Client](#low-level-api) 连接到 Bedrock Titan 服务。\n将 spring-ai-bedrock 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-bedrock\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-bedrock\u0026#39; } 接下来，创建一个 [ BedrockTitanEmbeddingModel]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/titan/[BedrockTitanEmbeddingModel](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/titan/BedrockTitanEmbeddingModel.java).java) 并将其用于文本嵌入：\nvar titanEmbeddingApi = new TitanEmbeddingBedrockApi( TitanEmbeddingModel.TITAN_EMBED_IMAGE_V1.id(), Region.US_EAST_1.id()); var embeddingModel = new BedrockTitanEmbeddingModel(this.titanEmbeddingApi); EmbeddingResponse embeddingResponse = this.embeddingModel .embedForResponse(List.of(\u0026#34;Hello World\u0026#34;)); // NOTE titan does not support batch embedding. 低级 TitanEmbeddingBedrockApi 客户端 # [ TitanEmbeddingBedrockApi]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/titan/api/[TitanEmbeddingBedrockApi](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/titan/api/TitanEmbeddingBedrockApi.java).java) 在 AWS Bedrock [ Titan Embedding 模型]( https://docs.aws.amazon.com/bedrock/latest/userguide/titan-multiemb-models.html)之上提供了轻量级 Java 客户端。\n以下类图说明了 TitanEmbeddingBedrockApi 接口和构建块：\nTitanEmbeddingBedrockApi 支持 amazon.titan-embed-image-v1 和 amazon.titan-embed-image-v1 模型，用于单次和批量嵌入计算。\n以下是以编程方式使用 API 的简单代码片段：\nTitanEmbeddingBedrockApi titanEmbedApi = new TitanEmbeddingBedrockApi( TitanEmbeddingModel.TITAN_EMBED_TEXT_V1.id(), Region.US_EAST_1.id()); TitanEmbeddingRequest request = TitanEmbeddingRequest.builder() .withInputText(\u0026#34;I like to eat apples.\u0026#34;) .build(); TitanEmbeddingResponse response = this.titanEmbedApi.embedding(this.request); 要嵌入图像，您需要将其转换为 base64 格式：\nTitanEmbeddingBedrockApi titanEmbedApi = new TitanEmbeddingBedrockApi( TitanEmbeddingModel.TITAN_EMBED_IMAGE_V1.id(), Region.US_EAST_1.id()); byte[] image = new DefaultResourceLoader() .getResource(\u0026#34;classpath:/spring_framework.png\u0026#34;) .getContentAsByteArray(); TitanEmbeddingRequest request = TitanEmbeddingRequest.builder() .withInputImage(Base64.getEncoder().encodeToString(this.image)) .build(); TitanEmbeddingResponse response = this.titanEmbedApi.embedding(this.request); "},{"id":32,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E9%80%82%E5%BA%A6/%E9%80%82%E5%BA%A6/","title":"适度","section":"适度","content":" 适度 # 介绍 # Spring AI 支持 Mistral AI 推出的全新审核服务，该服务由 Mistral Moderation 模型提供支持。它能够从多个策略维度检测有害文本内容。点击此[ 链接]( https://docs.mistral.ai/capabilities/guardrailing/) ，了解更多关于 Mistral AI 审核模型的信息。\n先决条件 # 自动配置 # Spring AI 为 Mistral AI Moderation 模型提供了 Spring Boot 自动配置功能。要启用此功能，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-mistral-ai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件：\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-mistral-ai\u0026#39; } 适度属性 # 连接属性 # 前缀 spring.ai.mistralai 用作允许您连接到 Mistral AI 的属性前缀。\n配置属性 # 前缀 spring.ai.mistralai.moderation 用作配置 Mistral AI 调节模型的属性前缀。\n运行时选项 # MistralAiModerationOptions 类提供了发出审核请求时使用的选项。启动时，将使用 spring.ai.mistralai.moderation 指定的选项，但您可以在运行时覆盖这些选项。\n例如：\nMistralAiModerationOptions moderationOptions = MistralAiModerationOptions.builder() .model(\u0026#34;mistral-moderation-latest\u0026#34;) .build(); ModerationPrompt moderationPrompt = new ModerationPrompt(\u0026#34;Text to be moderated\u0026#34;, this.moderationOptions); ModerationResponse response = mistralAiModerationModel.call(this.moderationPrompt); // Access the moderation results Moderation moderation = moderationResponse.getResult().getOutput(); // Print general information System.out.println(\u0026#34;Moderation ID: \u0026#34; + moderation.getId()); System.out.println(\u0026#34;Model used: \u0026#34; + moderation.getModel()); // Access the moderation results (there\u0026#39;s usually only one, but it\u0026#39;s a list) for (ModerationResult result : moderation.getResults()) { System.out.println(\u0026#34;\\nModeration Result:\u0026#34;); System.out.println(\u0026#34;Flagged: \u0026#34; + result.isFlagged()); // Access categories Categories categories = this.result.getCategories(); System.out.println(\u0026#34;\\nCategories:\u0026#34;); System.out.println(\u0026#34;Law: \u0026#34; + categories.isLaw()); System.out.println(\u0026#34;Financial: \u0026#34; + categories.isFinancial()); System.out.println(\u0026#34;PII: \u0026#34; + categories.isPii()); System.out.println(\u0026#34;Sexual: \u0026#34; + categories.isSexual()); System.out.println(\u0026#34;Hate: \u0026#34; + categories.isHate()); System.out.println(\u0026#34;Harassment: \u0026#34; + categories.isHarassment()); System.out.println(\u0026#34;Self-Harm: \u0026#34; + categories.isSelfHarm()); System.out.println(\u0026#34;Sexual/Minors: \u0026#34; + categories.isSexualMinors()); System.out.println(\u0026#34;Hate/Threatening: \u0026#34; + categories.isHateThreatening()); System.out.println(\u0026#34;Violence/Graphic: \u0026#34; + categories.isViolenceGraphic()); System.out.println(\u0026#34;Self-Harm/Intent: \u0026#34; + categories.isSelfHarmIntent()); System.out.println(\u0026#34;Self-Harm/Instructions: \u0026#34; + categories.isSelfHarmInstructions()); System.out.println(\u0026#34;Harassment/Threatening: \u0026#34; + categories.isHarassmentThreatening()); System.out.println(\u0026#34;Violence: \u0026#34; + categories.isViolence()); // Access category scores CategoryScores scores = this.result.getCategoryScores(); System.out.println(\u0026#34;\\nCategory Scores:\u0026#34;); System.out.println(\u0026#34;Law: \u0026#34; + scores.getLaw()); System.out.println(\u0026#34;Financial: \u0026#34; + scores.getFinancial()); System.out.println(\u0026#34;PII: \u0026#34; + scores.getPii()); System.out.println(\u0026#34;Sexual: \u0026#34; + scores.getSexual()); System.out.println(\u0026#34;Hate: \u0026#34; + scores.getHate()); System.out.println(\u0026#34;Harassment: \u0026#34; + scores.getHarassment()); System.out.println(\u0026#34;Self-Harm: \u0026#34; + scores.getSelfHarm()); System.out.println(\u0026#34;Sexual/Minors: \u0026#34; + scores.getSexualMinors()); System.out.println(\u0026#34;Hate/Threatening: \u0026#34; + scores.getHateThreatening()); System.out.println(\u0026#34;Violence/Graphic: \u0026#34; + scores.getViolenceGraphic()); System.out.println(\u0026#34;Self-Harm/Intent: \u0026#34; + scores.getSelfHarmIntent()); System.out.println(\u0026#34;Self-Harm/Instructions: \u0026#34; + scores.getSelfHarmInstructions()); System.out.println(\u0026#34;Harassment/Threatening: \u0026#34; + scores.getHarassmentThreatening()); System.out.println(\u0026#34;Violence: \u0026#34; + scores.getViolence()); } 手动配置 # 将 spring-ai-mistral-ai 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-mistral-ai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件：\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-mistral-ai\u0026#39; } 接下来，创建一个 MistralAiModerationModel：\nMistralAiModerationApi mistralAiModerationApi = new MistralAiModerationApi(System.getenv(\u0026#34;MISTRAL_AI_API_KEY\u0026#34;)); MistralAiModerationModel mistralAiModerationModel = new MistralAiModerationModel(this.mistralAiModerationApi); MistralAiModerationOptions moderationOptions = MistralAiModerationOptions.builder() .model(\u0026#34;mistral-moderation-latest\u0026#34;) .build(); ModerationPrompt moderationPrompt = new ModerationPrompt(\u0026#34;Text to be moderated\u0026#34;, this.moderationOptions); ModerationResponse response = this.mistralAiModerationModel.call(this.moderationPrompt); 示例代码 # MistralAiModerationModelIT 测试提供了一些有关如何使用该库的常规示例。您可以参考此测试获取更详细的使用示例。\n"},{"id":33,"href":"/docs/%E5%8F%82%E8%80%83/%E7%9F%A2%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/apache-cassandra-%E7%9F%A2%E9%87%8F%E5%AD%98%E5%82%A8/","title":"Apache Cassandra 矢量存储","section":"矢量数据库","content":" Apache Cassandra 矢量存储 # 本节将引导您设置 CassandraVectorStore 来存储文档嵌入并执行相似性搜索。\n什么是 Apache Cassandra？ # [ Apache Cassandra®]( https://cassandra.apache.org) 是一个真正的开源分布式数据库，以线性可扩展性、成熟的容错性和低延迟而闻名，使其成为关键任务事务数据的完美平台。\n其向量相似性搜索 (VSS) 基于 JVector 库，可确保一流的性能和相关性。\nApache Cassandra 中的向量搜索非常简单：\nSELECT content FROM table ORDER BY content_vector ANN OF query_embedding; 您可以[ 在此处]( https://cassandra.apache.org/doc/latest/cassandra/getting-started/vector-search-quickstart.html)阅读更多相关文档。\n这个 Spring AI Vector Store 旨在适用于全新的 RAG 应用程序，并且能够在现有数据和表格的基础上进行改造。\n该存储还可用于现有数据库中的非 RAG 用例，例如语义搜索、地理邻近搜索等。\n存储将根据其配置自动创建或增强架构。如果您不想修改架构，请使用 initializeSchema 配置存储。\n根据 Spring Boot 标准，使用 spring-boot-autoconfigure 时， initializeSchema 默认为 false ，并且您必须通过在 application.properties 文件中设置 …​initialize-schema=true 来选择加入模式创建/修改。\n什么是 JVector？ # [ JVector]( https://github.com/jbellis/jvector) 是一个纯 Java 嵌入式矢量搜索引擎。\n它与其他 HNSW 向量相似性搜索实现相比，具有以下优势：\n算法快速。JVector 使用受 DiskANN 和相关研究启发的最先进的图形算法，可提供高召回率和低延迟。 执行速度快。JVector 使用 Panama SIMD API 来加速索引构建和查询。 内存高效。JVector 使用乘积量化来压缩向量，以便它们在搜索期间可以保留在内存中。 磁盘感知。JVector 的磁盘布局旨在在查询时执行最少的必要 iops。 并发。索引构建线性扩展至至少 32 个线程。线程数加倍，构建时间减半。 增量式。在构建索引时进行查询。添加向量和在搜索结果中找到向量之间没有延迟。 易于嵌入。API 的设计旨在方便人们在生产环境中嵌入。 先决条件 # 依赖项 # 将这些依赖项添加到您的项目：\n仅对于 Cassandra Vector Store： \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-cassandra-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者，对于 RAG 应用程序中所需的一切（使用默认的 ONNX 嵌入模型）： \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-cassandra\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 配置属性 # 您可以在 Spring Boot 配置中使用以下属性来自定义 Apache Cassandra 向量存储。\n用法 # 基本用法 # 创建一个 CassandraVectorStore 实例作为 Spring Bean：\n@Bean public VectorStore vectorStore(CqlSession session, EmbeddingModel embeddingModel) { return CassandraVectorStore.builder(embeddingModel) .session(session) .keyspace(\u0026#34;my_keyspace\u0026#34;) .table(\u0026#34;my_vectors\u0026#34;) .build(); } 一旦有了向量存储实例，就可以添加文档并执行搜索：\n// Add documents vectorStore.add(List.of( new Document(\u0026#34;1\u0026#34;, \u0026#34;content1\u0026#34;, Map.of(\u0026#34;key1\u0026#34;, \u0026#34;value1\u0026#34;)), new Document(\u0026#34;2\u0026#34;, \u0026#34;content2\u0026#34;, Map.of(\u0026#34;key2\u0026#34;, \u0026#34;value2\u0026#34;)) )); // Search with filters List\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch( SearchRequest.query(\u0026#34;search text\u0026#34;) .withTopK(5) .withSimilarityThreshold(0.7f) .withFilterExpression(\u0026#34;metadata.key1 == \u0026#39;value1\u0026#39;\u0026#34;) ); 高级配置 # 对于更复杂的用例，您可以在 Spring Bean 中配置其他设置：\n@Bean public VectorStore vectorStore(CqlSession session, EmbeddingModel embeddingModel) { return CassandraVectorStore.builder(embeddingModel) .session(session) .keyspace(\u0026#34;my_keyspace\u0026#34;) .table(\u0026#34;my_vectors\u0026#34;) // Configure primary keys .partitionKeys(List.of( new SchemaColumn(\u0026#34;id\u0026#34;, DataTypes.TEXT), new SchemaColumn(\u0026#34;category\u0026#34;, DataTypes.TEXT) )) .clusteringKeys(List.of( new SchemaColumn(\u0026#34;timestamp\u0026#34;, DataTypes.TIMESTAMP) )) // Add metadata columns with optional indexing .addMetadataColumns( new SchemaColumn(\u0026#34;category\u0026#34;, DataTypes.TEXT, SchemaColumnTags.INDEXED), new SchemaColumn(\u0026#34;score\u0026#34;, DataTypes.DOUBLE) ) // Customize column names .contentColumnName(\u0026#34;text\u0026#34;) .embeddingColumnName(\u0026#34;vector\u0026#34;) // Performance tuning .fixedThreadPoolExecutorSize(32) // Schema management .initializeSchema(true) // Custom batching strategy .batchingStrategy(new TokenCountBatchingStrategy()) .build(); } 连接配置 # 有两种方法可以配置与 Cassandra 的连接：\n使用注入的 CqlSession（推荐）： @Bean public VectorStore vectorStore(CqlSession session, EmbeddingModel embeddingModel) { return CassandraVectorStore.builder(embeddingModel) .session(session) .keyspace(\u0026#34;my_keyspace\u0026#34;) .table(\u0026#34;my_vectors\u0026#34;) .build(); } 直接在构建器中使用连接详细信息： @Bean public VectorStore vectorStore(EmbeddingModel embeddingModel) { return CassandraVectorStore.builder(embeddingModel) .contactPoint(new InetSocketAddress(\u0026#34;localhost\u0026#34;, 9042)) .localDatacenter(\u0026#34;datacenter1\u0026#34;) .keyspace(\u0026#34;my_keyspace\u0026#34;) .build(); } 元数据过滤 # 您可以将通用的、可移植的元数据过滤器与 CassandraVectorStore 结合使用。元数据列要可搜索，必须设置为主键或已建立 SAI 索引。要使非主键列被索引，请使用 SchemaColumnTags.INDEXED 配置元数据列。\n例如，您可以使用文本表达语言：\nvectorStore.similaritySearch( SearchRequest.builder().query(\u0026#34;The World\u0026#34;) .topK(5) .filterExpression(\u0026#34;country in [\u0026#39;UK\u0026#39;, \u0026#39;NL\u0026#39;] \u0026amp;\u0026amp; year \u0026gt;= 2020\u0026#34;).build()); 或者以编程方式使用表达式 DSL：\nFilter.Expression f = new FilterExpressionBuilder() .and( f.in(\u0026#34;country\u0026#34;, \u0026#34;UK\u0026#34;, \u0026#34;NL\u0026#34;), f.gte(\u0026#34;year\u0026#34;, 2020) ).build(); vectorStore.similaritySearch( SearchRequest.builder().query(\u0026#34;The World\u0026#34;) .topK(5) .filterExpression(f).build()); 便携式过滤表达式会自动转换为 [ CQL 查询]( https://cassandra.apache.org/doc/latest/cassandra/developing/cql/index.html) 。\n高级示例：维基百科数据集上的向量存储 # 以下示例演示了如何在现有架构上使用存储。这里我们使用来自 [ github.com/datastax-labs/colbert-wikipedia-data](https:// github.com/datastax-labs/colbert-wikipedia-data) 项目的架构，该项目附带已为您矢量化的完整维基百科数据集。\n首先，在 Cassandra 数据库中创建模式：\nwget https://s.apache.org/colbert-wikipedia-schema-cql -O colbert-wikipedia-schema.cql cqlsh -f colbert-wikipedia-schema.cql 然后使用构建器模式配置存储：\n@Bean public VectorStore vectorStore(CqlSession session, EmbeddingModel embeddingModel) { List\u0026lt;SchemaColumn\u0026gt; partitionColumns = List.of( new SchemaColumn(\u0026#34;wiki\u0026#34;, DataTypes.TEXT), new SchemaColumn(\u0026#34;language\u0026#34;, DataTypes.TEXT), new SchemaColumn(\u0026#34;title\u0026#34;, DataTypes.TEXT) ); List\u0026lt;SchemaColumn\u0026gt; clusteringColumns = List.of( new SchemaColumn(\u0026#34;chunk_no\u0026#34;, DataTypes.INT), new SchemaColumn(\u0026#34;bert_embedding_no\u0026#34;, DataTypes.INT) ); List\u0026lt;SchemaColumn\u0026gt; extraColumns = List.of( new SchemaColumn(\u0026#34;revision\u0026#34;, DataTypes.INT), new SchemaColumn(\u0026#34;id\u0026#34;, DataTypes.INT) ); return CassandraVectorStore.builder() .session(session) .embeddingModel(embeddingModel) .keyspace(\u0026#34;wikidata\u0026#34;) .table(\u0026#34;articles\u0026#34;) .partitionKeys(partitionColumns) .clusteringKeys(clusteringColumns) .contentColumnName(\u0026#34;body\u0026#34;) .embeddingColumnName(\u0026#34;all_minilm_l6_v2_embedding\u0026#34;) .indexName(\u0026#34;all_minilm_l6_v2_ann\u0026#34;) .initializeSchema(false) .addMetadataColumns(extraColumns) .primaryKeyTranslator((List\u0026lt;Object\u0026gt; primaryKeys) -\u0026gt; { if (primaryKeys.isEmpty()) { return \u0026#34;test§¶0\u0026#34;; } return String.format(\u0026#34;%s§¶%s\u0026#34;, primaryKeys.get(2), primaryKeys.get(3)); }) .documentIdTranslator((id) -\u0026gt; { String[] parts = id.split(\u0026#34;§¶\u0026#34;); String title = parts[0]; int chunk_no = parts.length \u0026gt; 1 ? Integer.parseInt(parts[1]) : 0; return List.of(\u0026#34;simplewiki\u0026#34;, \u0026#34;en\u0026#34;, title, chunk_no, 0); }) .build(); } @Bean public EmbeddingModel embeddingModel() { // default is ONNX all-MiniLM-L6-v2 which is what we want return new TransformersEmbeddingModel(); } 加载完整的维基百科数据集 # 要加载完整的维基百科数据集：\n访问 Native Client # Cassandra Vector Store 实现通过 getNativeClient() 方法提供对底层本机 Cassandra 客户端（ CqlSession ）的访问：\nCassandraVectorStore vectorStore = context.getBean(CassandraVectorStore.class); Optional\u0026lt;CqlSession\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { CqlSession session = nativeClient.get(); // Use the native client for Cassandra-specific operations } 本机客户端使您可以访问可能无法通过 VectorStore 接口公开的 Cassandra 特定功能和操作。\n"},{"id":34,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B%E4%B8%8A%E4%B8%8B%E6%96%87%E5%8D%8F%E8%AE%AEmcp/mcp-%E5%AE%9E%E7%94%A8%E7%A8%8B%E5%BA%8F/","title":"MCP 实用程序","section":"模型上下文协议（MCP）","content":" MCP 实用程序 # MCP 实用程序为将模型上下文协议 (MCP) 与 Spring AI 应用程序集成提供了基础支持。这些实用程序支持 Spring AI 工具系统与 MCP 服务器之间的无缝通信，并支持同步和异步操作。它们通常用于以编程方式配置和交互 MCP 客户端和服务器。如需更精简的配置，请考虑使用启动器。\nToolCallback 实用程序 # 工具回调适配器 # 将 MCP 工具适配到 Spring AI 的工具接口，支持同步和异步执行。\n工具回调提供程序 # 从 MCP 客户端发现并提供 MCP 工具。\nToolCallbacks 到 ToolSpecifications # 将 Spring AI 工具回调转换为 MCP 工具规范：\nMCP 客户端到 ToolCallbacks # 从 MCP 客户端获取工具回调\n原生图像支持 # McpHints 类为 MCP 模式类提供 GraalVM 原生镜像提示。该类会在构建原生镜像时自动为 MCP 模式类注册所有必要的反射提示。\n"},{"id":35,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B-api/mistral-ai-%E5%B5%8C%E5%85%A5/","title":"Mistral AI 嵌入","section":"嵌入模型 API","content":" Mistral AI 嵌入 # Spring AI 支持 Mistral AI 的文本嵌入模型。嵌入是文本的矢量表示，它通过段落在高维向量空间中的位置来捕捉其语义。Mistral AI 嵌入 API 提供了尖端、先进的文本嵌入方法，可用于多种 NLP 任务。\n先决条件 # 您需要使用 MistralAI 创建一个 API 来访问 MistralAI 嵌入模型。\n在 [ MistralAI 注册页面]( https://auth.mistral.ai/ui/registration)创建一个帐户，并在 [ API 密钥页面]( https://console.mistral.ai/api-keys/)上生成令牌。\nSpring AI 项目定义了一个名为 spring.ai.mistralai.api-key 的配置属性，您应该将其设置为从 console.mistralai.ai 获取的 API Key 的值。\n您可以在 application.properties 文件中设置此配置属性：\nspring.ai.mistralai.api-key=\u0026lt;your-mistralai-api-key\u0026gt; 为了在处理 API 密钥等敏感信息时增强安全性，您可以使用 Spring 表达式语言 (SpEL) 来引用环境变量：\n# In application.yml spring: ai: mistralai: api-key: ${MISTRALAI_API_KEY} # In your environment or .env file export MISTRALAI_API_KEY=\u0026lt;your-mistralai-api-key\u0026gt; 您还可以在应用程序代码中以编程方式设置此配置：\n// Retrieve API key from a secure source or environment variable String apiKey = System.getenv(\u0026#34;MISTRALAI_API_KEY\u0026#34;); 添加存储库和 BOM # Spring AI 的构件已发布在 Maven Central 和 Spring Snapshot 仓库中。请参阅 [ “构件仓库”](../../getting-started.html#artifact-repositories) 部分，将这些仓库添加到您的构建系统中。\n为了帮助管理依赖项，Spring AI 提供了 BOM（物料清单），以确保在整个项目中使用一致版本的 Spring AI。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # Spring AI 为 MistralAI 嵌入模型提供了 Spring Boot 自动配置功能。要启用此功能，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-mistral-ai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-mistral-ai\u0026#39; } 嵌入属性 # 重试属性 # 前缀 spring.ai.retry 用作属性前缀，可让您配置 Mistral AI Embedding 模型的重试机制。\n连接属性 # 前缀 spring.ai.mistralai 用作允许您连接到 MistralAI 的属性前缀。\n配置属性 # 前缀 spring.ai.mistralai.embedding 是配置 MistralAI 的 EmbeddingModel 实现的属性前缀。\n运行时选项 # [ MistralAiEmbeddingOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-mistral-ai/src/main/java/org/springframework/ai/mistralai/[MistralAiEmbeddingOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-mistral-ai/src/main/java/org/springframework/ai/mistralai/MistralAiEmbeddingOptions.java)) 提供 MistralAI 配置，例如要使用的模型等。\n也可以使用 spring.ai.mistralai.embedding.options 属性来配置默认选项。\n启动时，使用 MistralAiEmbeddingModel 构造函数设置所有嵌入请求的默认选项。运行时，您可以使用 MistralAiEmbeddingOptions 实例作为 EmbeddingRequest 的一部分来覆盖默认选项。\n例如，要覆盖特定请求的默认模型名称：\nEmbeddingResponse embeddingResponse = embeddingModel.call( new EmbeddingRequest(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;), MistralAiEmbeddingOptions.builder() .withModel(\u0026#34;Different-Embedding-Model-Deployment-Name\u0026#34;) .build())); 样品控制器 # 这将创建一个 EmbeddingModel 实现，您可以将其注入到您的类中。这是一个使用 EmbeddingModel 实现的简单 @Controller 类的示例。\nspring.ai.mistralai.api-key=YOUR_API_KEY spring.ai.mistralai.embedding.options.model=mistral-embed @RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(\u0026#34;/ai/embedding\u0026#34;) public Map embed(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { var embeddingResponse = this.embeddingModel.embedForResponse(List.of(message)); return Map.of(\u0026#34;embedding\u0026#34;, embeddingResponse); } } 手动配置 # 如果您不使用 Spring Boot，则可以手动配置 OpenAI 嵌入模型。为此，请将 spring-ai-mistral-ai 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-mistral-ai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-mistral-ai\u0026#39; } 接下来，创建一个 MistralAiEmbeddingModel 实例并使用它来计算两个输入文本之间的相似度：\nvar mistralAiApi = new MistralAiApi(System.getenv(\u0026#34;MISTRAL_AI_API_KEY\u0026#34;)); var embeddingModel = new MistralAiEmbeddingModel(this.mistralAiApi, MistralAiEmbeddingOptions.builder() .withModel(\u0026#34;mistral-embed\u0026#34;) .withEncodingFormat(\u0026#34;float\u0026#34;) .build()); EmbeddingResponse embeddingResponse = this.embeddingModel .embedForResponse(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;)); MistralAiEmbeddingOptions 提供嵌入请求的配置信息。Options 类提供了一个 builder() ，方便用户轻松创建选项。\n"},{"id":36,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B-api/%E4%BA%BA%E7%B1%BB%E5%AD%A6-3-%E8%81%8A%E5%A4%A9/","title":"人类学 3 聊天","section":"聊天模型 API","content":" 人类学 3 聊天 # [ Anthropic Claude]( https://www.anthropic.com/) 是一系列基础 AI 模型，可用于各种应用。对于开发者和企业，您可以利用 API 访问权限，直接在 [ Anthropic 的 AI 基础架构]( https://www.anthropic.com/api)上进行构建。\nSpring AI 支持 Anthropic [ Messaging API]( https://docs.anthropic.com/claude/reference/messages_post) 进行同步和流文本生成。\n先决条件 # 您需要在 Anthropic 门户上创建一个 API 密钥。\n在 [ Anthropic API 仪表板上]( https://console.anthropic.com/dashboard)创建一个帐户，并在[ 获取 API 密钥]( https://console.anthropic.com/settings/keys)页面上生成 API 密钥。\nSpring AI 项目定义了一个名为 spring.ai.anthropic.api-key 的配置属性，您应该将其设置为从 anthropic.com 获取的 API Key 的值。\n您可以在 application.properties 文件中设置此配置属性：\nspring.ai.anthropic.api-key=\u0026lt;your-anthropic-api-key\u0026gt; 为了在处理 API 密钥等敏感信息时增强安全性，您可以使用 Spring 表达式语言 (SpEL) 引用自定义环境变量：\n# In application.yml spring: ai: anthropic: api-key: ${ANTHROPIC_API_KEY} # In your environment or .env file export ANTHROPIC_API_KEY=\u0026lt;your-anthropic-api-key\u0026gt; 您还可以在应用程序代码中以编程方式设置此配置：\n// Retrieve API key from a secure source or environment variable String apiKey = System.getenv(\u0026#34;ANTHROPIC_API_KEY\u0026#34;); 添加存储库和 BOM # Spring AI 的构件已发布在 Maven Central 和 Spring Snapshot 仓库中。请参阅 [ “构件仓库”](../../getting-started.html#artifact-repositories) 部分，将这些仓库添加到您的构建系统中。\n为了帮助管理依赖项，Spring AI 提供了 BOM（物料清单），以确保在整个项目中使用一致版本的 Spring AI。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # Spring AI 为 Anthropic Chat Client 提供了 Spring Boot 自动配置功能。要启用此功能，请将以下依赖项添加到项目的 Maven pom.xml 或 Gradle build.gradle 文件中：\n聊天属性 # 重试属性 # 前缀 spring.ai.retry 用作属性前缀，可让您配置 Anthropic 聊天模型的重试机制。\n连接属性 # 前缀 spring.ai.anthropic 用作允许您连接到 Anthropic 的属性前缀。\n配置属性 # 前缀 spring.ai.anthropic.chat 是允许您为 Anthropic 配置聊天模型实现的属性前缀。\n运行时选项 # [ AnthropicChatOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-anthropic/src/main/java/org/springframework/ai/anthropic/[AnthropicChatOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-anthropic/src/main/java/org/springframework/ai/anthropic/AnthropicChatOptions.java)) 提供模型配置，例如要使用的模型、温度、最大令牌数等。\n启动时，可以使用 AnthropicChatModel(api, options) 构造函数或 spring.ai.anthropic.chat.options.* 属性配置默认选项。\n在运行时，您可以通过向 Prompt 调用添加新的、特定于请求的选项来覆盖默认选项。例如，要覆盖特定请求的默认模型和温度：\nChatResponse response = chatModel.call( new Prompt( \u0026#34;Generate the names of 5 famous pirates.\u0026#34;, AnthropicChatOptions.builder() .model(\u0026#34;claude-3-7-sonnet-latest\u0026#34;) .temperature(0.4) .build() )); 工具/函数调用 # 您可以使用 AnthropicChatModel 注册自定义 Java 工具，并让 Anthropic Claude 模型智能地选择输出包含参数的 JSON 对象，以调用一个或多个已注册的函数。这是一种将 LLM 功能与外部工具和 API 连接起来的强大技术。了解更多关于[ 工具调用的](../tools.html)信息。\n多式联运 # 多模态性是指模型能够同时理解和处理来自各种来源的信息，包括文本、pdf、图像、数据格式。\n图片 # 目前，Anthropic Claude 3 支持 images 的 base64 源类型，以及 image/jpeg 、 image/png 、 image/gif 和 image/webp 媒体类型。更多信息请参阅 [ Vision 指南]( https://docs.anthropic.com/claude/docs/vision) 。Anthropic Claude 3.5 Sonnet 还支持 application/pdf 文件的 pdf 源类型。\nSpring AI 的 Message 接口通过引入 Media 类型来支持多模态 AI 模型。此类型包含消息中媒体附件的数据和信息，使用 Spring 的 org.springframework.util.MimeType 和 java.lang.Object 作为原始媒体数据。\n下面是从 [ AnthropicChatModelIT.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-anthropic/src/test/java/org/springframework/ai/anthropic/[AnthropicChatModelIT.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-anthropic/src/test/java/org/springframework/ai/anthropic/AnthropicChatModelIT.java)) 中提取的简单代码示例，演示了用户文本与图像的组合。\nvar imageData = new ClassPathResource(\u0026#34;/multimodal.test.png\u0026#34;); var userMessage = new UserMessage(\u0026#34;Explain what do you see on this picture?\u0026#34;, List.of(new Media(MimeTypeUtils.IMAGE_PNG, this.imageData))); ChatResponse response = chatModel.call(new Prompt(List.of(this.userMessage))); logger.info(response.getResult().getOutput().getContent()); 它将 multimodal.test.png 图像作为输入：\n以及文本消息“解释一下你在图片上看到了什么？”，并生成类似这样的回应：\n从 Sonnet 3.5 开始提供 [ PDF 支持（测试版）]( https://docs.anthropic.com/en/docs/build-with-claude/pdf-support) 。使用 application/pdf 媒体类型将 PDF 文件附加到消息中：\nvar pdfData = new ClassPathResource(\u0026#34;/spring-ai-reference-overview.pdf\u0026#34;); var userMessage = new UserMessage( \u0026#34;You are a very professional document summarization specialist. Please summarize the given document.\u0026#34;, List.of(new Media(new MimeType(\u0026#34;application\u0026#34;, \u0026#34;pdf\u0026#34;), pdfData))); var response = this.chatModel.call(new Prompt(List.of(userMessage))); 样品控制器 # [ 创建]( https://start.spring.io/)一个新的 Spring Boot 项目并将 spring-ai-starter-model-anthropic 添加到您的 pom（或 gradle）依赖项中。\n在 src/main/resources 目录下添加一个 application.properties 文件，以启用和配置 Anthropic 聊天模型：\nspring.ai.anthropic.api-key=YOUR_API_KEY spring.ai.anthropic.chat.options.model=claude-3-5-sonnet-latest spring.ai.anthropic.chat.options.temperature=0.7 spring.ai.anthropic.chat.options.max-tokens=450 这将创建一个 AnthropicChatModel 实现，您可以将其注入到您的类中。这是一个使用聊天模型生成文本的简单 @Controller 类的示例。\n@RestController public class ChatController { private final AnthropicChatModel chatModel; @Autowired public ChatController(AnthropicChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, this.chatModel.call(message)); } @GetMapping(\u0026#34;/ai/generateStream\u0026#34;) public Flux\u0026lt;ChatResponse\u0026gt; generateStream(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { Prompt prompt = new Prompt(new UserMessage(message)); return this.chatModel.stream(prompt); } } 手动配置 # AnthropicChatModel 实现了 ChatModel 和 StreamingChatModel ，并使用 [ Low-level AnthropicApi Client](#low-level-api) 连接到 Anthropic 服务。\n将 spring-ai-anthropic 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-anthropic\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-anthropic\u0026#39; } 接下来，创建一个 AnthropicChatModel 并将其用于文本生成：\nvar anthropicApi = new AnthropicApi(System.getenv(\u0026#34;ANTHROPIC_API_KEY\u0026#34;)); var chatModel = new AnthropicChatModel(this.anthropicApi, AnthropicChatOptions.builder() .model(\u0026#34;claude-3-opus-20240229\u0026#34;) .temperature(0.4) .maxTokens(200) .build()); ChatResponse response = this.chatModel.call( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); // Or with streaming responses Flux\u0026lt;ChatResponse\u0026gt; response = this.chatModel.stream( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); AnthropicChatOptions 提供聊天请求的配置信息。AnthropicChatOptions.Builder 是一个流畅的选项构建 AnthropicChatOptions.Builder 。\n低级 AnthropicApi 客户端 # [ AnthropicApi]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-anthropic/src/main/java/org/springframework/ai/anthropic/api/[AnthropicApi](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-anthropic/src/main/java/org/springframework/ai/anthropic/api/AnthropicApi.java).java) 为 [ Anthropic Message API]( https://docs.anthropic.com/claude/reference/messages_post) 提供了轻量级 Java 客户端。\n以下类图说明了 AnthropicApi 聊天接口和构建块：\n以下是以编程方式使用 API 的简单代码片段：\nAnthropicApi anthropicApi = new AnthropicApi(System.getenv(\u0026#34;ANTHROPIC_API_KEY\u0026#34;)); AnthropicMessage chatCompletionMessage = new AnthropicMessage( List.of(new ContentBlock(\u0026#34;Tell me a Joke?\u0026#34;)), Role.USER); // Sync request ResponseEntity\u0026lt;ChatCompletionResponse\u0026gt; response = this.anthropicApi .chatCompletionEntity(new ChatCompletionRequest(AnthropicApi.ChatModel.CLAUDE_3_OPUS.getValue(), List.of(this.chatCompletionMessage), null, 100, 0.8, false)); // Streaming request Flux\u0026lt;StreamResponse\u0026gt; response = this.anthropicApi .chatCompletionStream(new ChatCompletionRequest(AnthropicApi.ChatModel.CLAUDE_3_OPUS.getValue(), List.of(this.chatCompletionMessage), null, 100, 0.8, true)); 请关注 [ AnthropicApi.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-anthropic/src/main/java/org/springframework/ai/anthropic/api/[AnthropicApi.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-anthropic/src/main/java/org/springframework/ai/anthropic/api/AnthropicApi.java)) 的 JavaDoc 以获取更多信息。\n低级 API 示例 # AnthropicApiIT.java 测试提供了一些如何使用轻量级库的一般示例。 "},{"id":37,"href":"/docs/%E5%8F%82%E8%80%83/","title":"参考","section":"Docs","content":" 参考 # "},{"id":38,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E5%9B%BE%E5%83%8F%E6%A8%A1%E5%9E%8B-api/","title":"图像模型 API","section":"模型","content":" 图像模型 API # Spring Image Model API 旨在提供一个简单易用的接口，用于与各种专门用于图像生成的 [ AI 模型](../concepts.html#_models)进行交互，使开发人员能够以最少的代码更改在不同的图像相关模型之间切换。这种设计符合 Spring 的模块化和可互换性理念，确保开发人员能够快速地使其应用程序适应与图像处理相关的各种 AI 功能。\n此外，借助 ImagePrompt （用于输入封装）和 ImageResponse （用于输出处理）等配套类的支持，图像模型 API 统一了与专用于图像生成的 AI 模型的通信。它管理了请求准备和响应解析的复杂性，为图像生成功能提供了直接且简化的 API 交互。\nSpring 图像模型 API 建立在 Spring AI Generic Model API 之上，提供特定于图像的抽象和实现。\nAPI 概述 # 本节提供了 Spring Image Model API 接口和相关类的指南。\n图像模型 # 以下是 [ ImageModel]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/image/[ImageModel](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/image/ImageModel.java).java) 接口定义：\n@FunctionalInterface public interface ImageModel extends Model\u0026lt;ImagePrompt, ImageResponse\u0026gt; { ImageResponse call(ImagePrompt request); } 图像提示 # [[ImagePrompt](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/image/ImagePrompt.java)](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/image/[ImagePrompt](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/image/ImagePrompt.java).java) 是一个 ModelRequest ，它封装了一系列 [ ImageMes​​sage]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/image/ImageMessage.java) 对象和可选的模型请求选项。以下列表展示了 [[ImagePrompt](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/image/ImagePrompt.java)](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/image/[ImagePrompt](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/image/ImagePrompt.java).java) 类的精简版本，不包括构造函数和其他实用方法：\npublic class ImagePrompt implements ModelRequest\u0026lt;List\u0026lt;ImageMessage\u0026gt;\u0026gt; { private final List\u0026lt;ImageMessage\u0026gt; messages; private ImageOptions imageModelOptions; @Override public List\u0026lt;ImageMessage\u0026gt; getInstructions() {...} @Override public ImageOptions getOptions() {...} // constructors and utility methods omitted } 图像消息 # ImageMessage ​​sage 类封装了要使用的文本以及文本对生成图像的影响权重。对于支持权重的模型，权重可以是正值，也可以是负值。\npublic class ImageMessage { private String text; private Float weight; public String getText() {...} public Float getWeight() {...} // constructors and utility methods omitted } 图像选项 # 表示可传递给图像生成模型的选项。ImageOptions 接口扩展 ModelOptions ImageOptions ，用于定义一些可传递给 AI 模型的可移植选项。\nImageOptions 接口定义如下：\npublic interface ImageOptions extends ModelOptions { Integer getN(); String getModel(); Integer getWidth(); Integer getHeight(); String getResponseFormat(); // openai - url or base64 : stability ai byte[] or base64 } 此外，每个模型特定的 ImageModel 实现都可以有自己的选项，这些选项可以传递给 AI 模型。例如，OpenAI 图像生成模型就有自己的选项，例如 quality 、 style 等。\n这是一个强大的功能，允许开发人员在启动应用程序时使用特定于模型的选项，然后在运行时使用 ImagePrompt 覆盖它们。\n图像响应 # ImageResponse 类的结构如下：\npublic class ImageResponse implements ModelResponse\u0026lt;ImageGeneration\u0026gt; { private final ImageResponseMetadata imageResponseMetadata; private final List\u0026lt;ImageGeneration\u0026gt; imageGenerations; @Override public ImageGeneration getResult() { // get the first result } @Override public List\u0026lt;ImageGeneration\u0026gt; getResults() {...} @Override public ImageResponseMetadata getMetadata() {...} // other methods omitted } [ ImageResponse]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/image/[ImageResponse](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/image/ImageResponse.java).java) 类保存 AI 模型的输出，每个 ImageGeneration 实例包含由单个提示产生的多个潜在输出之一。\nImageResponse 类还带有一个 ImageResponseMetadata 对象，该对象保存有关 AI 模型响应的元数据。\n图像生成 # 最后， [ ImageGeneration]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/image/[ImageGeneration](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/image/ImageGeneration.java).java) 类从 ModelResult 扩展以表示有关此结果的输出响应和相关元数据：\npublic class ImageGeneration implements ModelResult\u0026lt;Image\u0026gt; { private ImageGenerationMetadata imageGenerationMetadata; private Image image; @Override public Image getOutput() {...} @Override public ImageGenerationMetadata getMetadata() {...} // other methods omitted } 可用的实现 # 为以下模型提供者提供了 ImageModel 实现：\nOpenAI 图像生成 Azure OpenAI 图像生成 千帆图像生成 稳定性 AI 图像生成 ZhiPuAI 图像生成 API 文档 # 您可以[ 在此处]( https://docs.spring.io/spring-ai/docs/current-SNAPSHOT/)找到 Javadoc。\n反馈和贡献 # 该项目的 [ GitHub 讨论]( https://github.com/spring-projects/spring-ai/discussions)是发送反馈的好地方。\n"},{"id":39,"href":"/docs/%E6%8C%87%E5%8D%97/%E6%9E%84%E5%BB%BA%E6%9C%89%E6%95%88%E4%BB%A3%E7%90%86/","title":"构建有效代理","section":"指南","content":" 构建有效代理 # 代理系统 # 该研究出版物对两种类型的代理系统进行了重要的架构区分：\n关键在于，虽然完全自主的代理看似诱人，但工作流通常能为定义明确的任务提供更好的可预测性和一致性。这完全符合企业对可靠性和可维护性至关重要的需求。\n让我们来看看 Spring AI 如何通过五种基本模式实现这些概念，每种模式都服务于特定的用例：\n1. 链式工作流程 # 链式工作流模式体现了将复杂任务分解为更简单、更易于管理的步骤的原则。\n何时使用：\n具有明确顺序步骤的任务 当你想以延迟换取更高的准确度时 每一步都建立在前一步的输出之上 以下是 Spring AI 实现的一个实际示例：\npublic class ChainWorkflow { private final ChatClient chatClient; private final String[] systemPrompts; public String chain(String userInput) { String response = userInput; for (String prompt : systemPrompts) { String input = String.format(\u0026#34;{%s}\\n {%s}\u0026#34;, prompt, response); response = chatClient.prompt(input).call().content(); } return response; } } 此实施体现了几个关键原则：\n每一步都有重点责任 一个步骤的输出成为下一个步骤的输入 该链易于扩展和维护 2. 并行化工作流程 # LLM 可以同时执行任务并通过编程方式汇总其输出。\n何时使用：\n处理大量类似但独立的物品 需要多个独立视角的任务 当处理时间至关重要且任务可并行时 List\u0026lt;String\u0026gt; parallelResponse = new ParallelizationWorkflow(chatClient) .parallel( \u0026#34;Analyze how market changes will impact this stakeholder group.\u0026#34;, List.of( \u0026#34;Customers: ...\u0026#34;, \u0026#34;Employees: ...\u0026#34;, \u0026#34;Investors: ...\u0026#34;, \u0026#34;Suppliers: ...\u0026#34; ), 4 ); 3. 路由工作流程 # 路由模式实现了智能任务分配，可以针对不同类型的输入进行专门处理。\n何时使用：\n具有不同输入类别的复杂任务 当不同的输入需要专门处理时 当分类能够准确处理时 @Autowired private ChatClient chatClient; RoutingWorkflow workflow = new RoutingWorkflow(chatClient); Map\u0026lt;String, String\u0026gt; routes = Map.of( \u0026#34;billing\u0026#34;, \u0026#34;You are a billing specialist. Help resolve billing issues...\u0026#34;, \u0026#34;technical\u0026#34;, \u0026#34;You are a technical support engineer. Help solve technical problems...\u0026#34;, \u0026#34;general\u0026#34;, \u0026#34;You are a customer service representative. Help with general inquiries...\u0026#34; ); String input = \u0026#34;My account was charged twice last week\u0026#34;; String response = workflow.route(input, routes); 何时使用：\n无法提前预测子任务的复杂任务 需要不同方法或视角的任务 需要适应性解决问题的情况 public class OrchestratorWorkersWorkflow { public WorkerResponse process(String taskDescription) { // 1. Orchestrator analyzes task and determines subtasks OrchestratorResponse orchestratorResponse = // ... // 2. Workers process subtasks in parallel List\u0026lt;String\u0026gt; workerResponses = // ... // 3. Results are combined into final response return new WorkerResponse(/*...*/); } } 使用示例：\nChatClient chatClient = // ... initialize chat client OrchestratorWorkersWorkflow workflow = new OrchestratorWorkersWorkflow(chatClient); WorkerResponse response = workflow.process( \u0026#34;Generate both technical and user-friendly documentation for a REST API endpoint\u0026#34; ); System.out.println(\u0026#34;Analysis: \u0026#34; + response.analysis()); System.out.println(\u0026#34;Worker Outputs: \u0026#34; + response.workerResponses()); 5. 评估器-优化器 # 何时使用：\n存在明确的评估标准 迭代改进提供可衡量的价值 任务受益于多轮批评 public class EvaluatorOptimizerWorkflow { public RefinedResponse loop(String task) { Generation generation = generate(task, context); EvaluationResponse evaluation = evaluate(generation.response(), task); return new RefinedResponse(finalSolution, chainOfThought); } } 使用示例：\nChatClient chatClient = // ... initialize chat client EvaluatorOptimizerWorkflow workflow = new EvaluatorOptimizerWorkflow(chatClient); RefinedResponse response = workflow.loop( \u0026#34;Create a Java class implementing a thread-safe counter\u0026#34; ); System.out.println(\u0026#34;Final Solution: \u0026#34; + response.solution()); System.out.println(\u0026#34;Evolution: \u0026#34; + response.chainOfThought()); Spring AI 的实现优势 # Spring AI 对这些模式的实现提供了几个与 Anthropic 的建议一致的好处：\n模型可移植性 # \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-openai-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 结构化输出 # EvaluationResponse response = chatClient.prompt(prompt) .call() .entity(EvaluationResponse.class); 一致的 API # 不同 LLM 提供商之间的统一接口 内置错误处理和重试 灵活的提示管理 最佳实践和建议 # 从简单开始 从基本工作流程开始，然后再增加复杂性 使用满足您要求的最简单的模式 仅在需要时添加复杂性 可靠性设计 实施清晰的错误处理 尽可能使用类型安全的响应 在每一步建立验证 考虑权衡 平衡延迟与准确性 评估何时使用并行处理 在固定工作流程和动态代理之间进行选择 未来工作 # 这些指南将进行更新，以探索如何构建将这些基础模式与复杂功能相结合的更高级的代理：\n图案组合\n结合多种模式来创建更强大的工作流程 构建利用每种模式优势的混合系统 创建能够适应不断变化的需求的灵活架构 高级代理内存管理\n实现跨对话的持久记忆 有效管理上下文窗口 制定长期知识保留策略 工具和模型上下文协议 (MCP) 集成\n通过标准化接口利用外部工具 实施 MCP 以增强模型交互 构建可扩展的代理架构 结论 # Anthropic 的研究见解与 Spring AI 的实际实现相结合，为构建有效的基于 LLM 的系统提供了强大的框架。\n通过遵循这些模式和原则，开发人员可以创建强大、可维护且有效的 AI 应用程序，提供真正的价值，同时避免不必要的复杂性。\n关键在于记住，有时最简单的解决方案才是最有效的。从基本模式入手，彻底了解你的用例，并且只有当复杂性能够明显提升系统性能或功能时才添加。\n"},{"id":40,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E5%9B%BE%E5%83%8F%E6%A8%A1%E5%9E%8B-api/%E7%A8%B3%E5%AE%9A%E6%80%A7-ai-%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/","title":"稳定性 AI 图像生成","section":"图像模型 API","content":" 稳定性 AI 图像生成 # Spring AI 支持 Stability AI 的[ 文本到图像生成模型]( https://platform.stability.ai/docs/api-reference#tag/v1generation) 。\n先决条件 # 您需要创建 Stability AI 的 API 密钥才能访问其 AI 模型。请按照其[ 入门文档]( https://platform.stability.ai/docs/getting-started/authentication)获取 API 密钥。\nSpring AI 项目定义了一个名为 spring.ai.stabilityai.api-key 的配置属性，您应该将其设置为从 Stability AI 获取的 API Key 的值。\n您可以在 application.properties 文件中设置此配置属性：\nspring.ai.stabilityai.api-key=\u0026lt;your-stabilityai-api-key\u0026gt; 为了在处理 API 密钥等敏感信息时增强安全性，您可以使用 Spring 表达式语言 (SpEL) 引用自定义环境变量：\n# In application.yml spring: ai: stabilityai: api-key: ${STABILITYAI_API_KEY} # In your environment or .env file export STABILITYAI_API_KEY=\u0026lt;your-stabilityai-api-key\u0026gt; 您还可以在应用程序代码中以编程方式设置此配置：\n// Retrieve API key from a secure source or environment variable String apiKey = System.getenv(\u0026#34;STABILITYAI_API_KEY\u0026#34;); 自动配置 # Spring AI 为 Stability AI 图像生成客户端提供了 Spring Boot 自动配置功能。要启用此功能，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-stability-ai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-stability-ai\u0026#39; } 图像生成属性 # 前缀 spring.ai.stabilityai 用作允许您连接到 Stability AI 的属性前缀。\n前缀 spring.ai.stabilityai.image 是属性前缀，可让您为 Stability AI 配置 ImageModel 实现。\n运行时选项 # [ StabilityAiImageOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-stabilityai/src/main/java/org/springframework/ai/stabilityai/api/[StabilityAiImageOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-stabilityai/src/main/java/org/springframework/ai/stabilityai/api/StabilityAiImageOptions.java)) 提供模型配置，例如要使用的模型、样式、大小等。\n启动时，可以使用 StabilityAiImageModel(StabilityAiApi stabilityAiApi, StabilityAiImageOptions options) 构造函数配置默认选项。或者，也可以使用前面描述的 spring.ai.openai.image.options.* 属性。\n在运行时，您可以通过向 ImagePrompt 调用添加新的、特定于请求的选项来覆盖默认选项。例如，要覆盖 Stability AI 特定的选项（例如质量和要创建的图像数量），请使用以下代码示例：\nImageResponse response = stabilityaiImageModel.call( new ImagePrompt(\u0026#34;A light cream colored mini golden doodle\u0026#34;, StabilityAiImageOptions.builder() .stylePreset(\u0026#34;cinematic\u0026#34;) .N(4) .height(1024) .width(1024).build()) ); "},{"id":41,"href":"/docs/%E5%8F%82%E8%80%83/%E7%BB%93%E6%9E%84%E5%8C%96%E8%BE%93%E5%87%BA%E8%BD%AC%E6%8D%A2%E5%99%A8/","title":"结构化输出转换器","section":"参考","content":" 结构化输出转换器 # LLM 生成结构化输出的能力对于依赖可靠解析输出值的下游应用程序至关重要。开发人员希望快速将 AI 模型的结果转换为可传递给其他应用程序函数和方法的数据类型，例如 JSON、XML 或 Java 类。\nSpring AI Structured Output Converters 有助于将 LLM 输出转换为结构化格式。如下图所示，此方法围绕 LLM 文本补全端点运行：\n使用通用补全 API 从大型语言模型 (LLM) 生成结构化输出需要仔细处理输入和输出。结构化输出转换器在 LLM 调用前后起着至关重要的作用，确保实现所需的输出结构。\n在 LLM 调用之前，转换器会将格式指令附加到提示中，为模型提供生成所需输出结构的明确指导。这些指令充当蓝图，塑造模型的响应以符合指定的格式。\nLLM 调用后，转换器会获取模型的输出文本，并将其转换为结构化类型的实例。此转换过程包括解析原始文本输出并将其映射到相应的结构化数据表示形式，例如 JSON、XML 或特定于域的数据结构。\n结构化输出 API # StructuredOutputConverter 接口允许您获取结构化输出，例如将基于文本的 AI 模型输出映射到 Java 类或值数组。接口定义如下：\npublic interface StructuredOutputConverter\u0026lt;T\u0026gt; extends Converter\u0026lt;String, T\u0026gt;, FormatProvider { } 它结合了 Spring [ Converter\u0026lt;String、T\u0026gt;]( https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/core/convert/converter/Converter.html) 接口和 FormatProvider 接口\npublic interface FormatProvider { String getFormat(); } 下图显示了使用结构化输出 API 时的数据流。\nFormatProvider 为 AI 模型提供特定的格式指南，使其能够生成文本输出，并使用 Converter 将其转换为指定的目标类型 T 以下是此类格式指令的示例：\n格式说明通常使用 [ PromptTemplate](prompt.html#_prompttemplate) 附加到用户输入的末尾，如下所示：\nStructuredOutputConverter outputConverter = ... String userInputTemplate = \u0026#34;\u0026#34;\u0026#34; ... user text input .... {format} \u0026#34;\u0026#34;\u0026#34;; // user input with a \u0026#34;format\u0026#34; placeholder. Prompt prompt = new Prompt( new PromptTemplate( this.userInputTemplate, Map.of(..., \u0026#34;format\u0026#34;, outputConverter.getFormat()) // replace the \u0026#34;format\u0026#34; placeholder with the converter\u0026#39;s format. ).createMessage()); Converter\u0026lt;String, T\u0026gt; 负责将模型中的输出文本转换为指定类型 T 的实例。\n可用的转换器 # 目前，Spring AI 提供 AbstractConversionServiceOutputConverter 、 AbstractMessageOutputConverter 、 BeanOutputConverter 、 MapOutputConverter 和 ListOutputConverter 实现：\nAbstractConversionServiceOutputConverter - 提供预配置的 GenericConversionService ，用于将 LLM 输出转换为所需格式。不提供默认的 FormatProvider 实现。 AbstractMessageOutputConverter - 提供预配置的 MessageConverter ，用于将 LLM 输出转换为所需格式。不提供默认的 FormatProvider 实现。 BeanOutputConverter - 此转换器配置了指定的 Java 类（例如 Bean）或 ParameterizedTypeReference ，并采用 FormatProvider 实现，指示 AI 模型生成符合 DRAFT_2020_12 的 JSON 响应，该响应 JSON Schema 源自指定的 Java 类。随后，它利用 ObjectMapper 将 JSON 输出反序列化为目标类的 Java 对象实例。 MapOutputConverter - 通过 FormatProvider 实现扩展了 AbstractMessageOutputConverter 的功能，该实现可指导 AI 模型生成符合 RFC8259 标准的 JSON 响应。此外，它还包含一个转换器实现，该实现利用提供的 MessageConverter 将 JSON 负载转换为 java.util.Map\u0026lt;String, Object\u0026gt; 实例。 ListOutputConverter - 扩展了 AbstractConversionServiceOutputConverter ，并包含一个专为逗号分隔列表输出而定制的 FormatProvider 实现。该转换器实现使用提供的 ConversionService 将模型文本输出转换为 java.util.List 。 使用转换器 # 以下部分提供了如何使用可用的转换器生成结构化输出的指南。\nBean 输出转换器 # 下面的示例展示如何使用 BeanOutputConverter 生成演员的电影作品。\n代表演员电影作品的目标记录：\nrecord ActorsFilms(String actor, List\u0026lt;String\u0026gt; movies) { } 以下是如何使用高级、流畅的 ChatClient API 应用 BeanOutputConverter：\nActorsFilms actorsFilms = ChatClient.create(chatModel).prompt() .user(u -\u0026gt; u.text(\u0026#34;Generate the filmography of 5 movies for {actor}.\u0026#34;) .param(\u0026#34;actor\u0026#34;, \u0026#34;Tom Hanks\u0026#34;)) .call() .entity(ActorsFilms.class); 或者直接使用低级 ChatModel API：\nBeanOutputConverter\u0026lt;ActorsFilms\u0026gt; beanOutputConverter = new BeanOutputConverter\u0026lt;\u0026gt;(ActorsFilms.class); String format = this.beanOutputConverter.getFormat(); String actor = \u0026#34;Tom Hanks\u0026#34;; String template = \u0026#34;\u0026#34;\u0026#34; Generate the filmography of 5 movies for {actor}. {format} \u0026#34;\u0026#34;\u0026#34;; Generation generation = chatModel.call( new PromptTemplate(this.template, Map.of(\u0026#34;actor\u0026#34;, this.actor, \u0026#34;format\u0026#34;, this.format)).create()).getResult(); ActorsFilms actorsFilms = this.beanOutputConverter.convert(this.generation.getOutput().getText()); 生成的架构中的属性排序 # BeanOutputConverter 通过 @JsonPropertyOrder 注解支持在生成的 JSON Schema 中自定义属性排序。此注解允许您指定属性在 Schema 中出现的精确顺序，而无需考虑它们在类或记录中的声明顺序。\n例如，为了确保 ActorsFilms 记录中的属性的特定顺序：\n@JsonPropertyOrder({\u0026#34;actor\u0026#34;, \u0026#34;movies\u0026#34;}) record ActorsFilms(String actor, List\u0026lt;String\u0026gt; movies) {} 此注释适用于记录和常规 Java 类。\n通用 Bean 类型 # 使用 ParameterizedTypeReference 构造函数指定更复杂的目标类结构。例如，要表示演员及其电影作品列表：\nList\u0026lt;ActorsFilms\u0026gt; actorsFilms = ChatClient.create(chatModel).prompt() .user(\u0026#34;Generate the filmography of 5 movies for Tom Hanks and Bill Murray.\u0026#34;) .call() .entity(new ParameterizedTypeReference\u0026lt;List\u0026lt;ActorsFilms\u0026gt;\u0026gt;() {}); 或者直接使用低级 ChatModel API：\nBeanOutputConverter\u0026lt;List\u0026lt;ActorsFilms\u0026gt;\u0026gt; outputConverter = new BeanOutputConverter\u0026lt;\u0026gt;( new ParameterizedTypeReference\u0026lt;List\u0026lt;ActorsFilms\u0026gt;\u0026gt;() { }); String format = this.outputConverter.getFormat(); String template = \u0026#34;\u0026#34;\u0026#34; Generate the filmography of 5 movies for Tom Hanks and Bill Murray. {format} \u0026#34;\u0026#34;\u0026#34;; Prompt prompt = new PromptTemplate(this.template, Map.of(\u0026#34;format\u0026#34;, this.format)).create(); Generation generation = chatModel.call(this.prompt).getResult(); List\u0026lt;ActorsFilms\u0026gt; actorsFilms = this.outputConverter.convert(this.generation.getOutput().getText()); 地图输出转换器 # 以下代码片段展示了如何使用 MapOutputConverter 将模型输出转换为地图中的数字列表。\nMap\u0026lt;String, Object\u0026gt; result = ChatClient.create(chatModel).prompt() .user(u -\u0026gt; u.text(\u0026#34;Provide me a List of {subject}\u0026#34;) .param(\u0026#34;subject\u0026#34;, \u0026#34;an array of numbers from 1 to 9 under they key name \u0026#39;numbers\u0026#39;\u0026#34;)) .call() .entity(new ParameterizedTypeReference\u0026lt;Map\u0026lt;String, Object\u0026gt;\u0026gt;() {}); 或者直接使用低级 ChatModel API：\nMapOutputConverter mapOutputConverter = new MapOutputConverter(); String format = this.mapOutputConverter.getFormat(); String template = \u0026#34;\u0026#34;\u0026#34; Provide me a List of {subject} {format} \u0026#34;\u0026#34;\u0026#34;; Prompt prompt = new PromptTemplate(this.template, Map.of(\u0026#34;subject\u0026#34;, \u0026#34;an array of numbers from 1 to 9 under they key name \u0026#39;numbers\u0026#39;\u0026#34;, \u0026#34;format\u0026#34;, this.format)).create(); Generation generation = chatModel.call(this.prompt).getResult(); Map\u0026lt;String, Object\u0026gt; result = this.mapOutputConverter.convert(this.generation.getOutput().getText()); 列表输出转换器 # 以下代码片段显示如何使用 ListOutputConverter 将模型输出转换为冰淇淋口味列表。\nList\u0026lt;String\u0026gt; flavors = ChatClient.create(chatModel).prompt() .user(u -\u0026gt; u.text(\u0026#34;List five {subject}\u0026#34;) .param(\u0026#34;subject\u0026#34;, \u0026#34;ice cream flavors\u0026#34;)) .call() .entity(new ListOutputConverter(new DefaultConversionService())); 或者直接使用低级 ChatModel API ：\nListOutputConverter listOutputConverter = new ListOutputConverter(new DefaultConversionService()); String format = this.listOutputConverter.getFormat(); String template = \u0026#34;\u0026#34;\u0026#34; List five {subject} {format} \u0026#34;\u0026#34;\u0026#34;; Prompt prompt = new PromptTemplate(this.template, Map.of(\u0026#34;subject\u0026#34;, \u0026#34;ice cream flavors\u0026#34;, \u0026#34;format\u0026#34;, this.format)).create(); Generation generation = this.chatModel.call(this.prompt).getResult(); List\u0026lt;String\u0026gt; list = this.listOutputConverter.convert(this.generation.getOutput().getText()); 支持的 AI 模型 # 以下 AI 模型已经过测试，支持 List、Map 和 Bean 结构化输出。\n内置 JSON 模式 # 一些 AI 模型提供专用的配置选项来生成结构化（通常是 JSON）输出。\nOpenAI 结构化输出可以确保您的模型生成的响应严格符合您提供的 JSON 模式。您可以选择 JSON_OBJECT 或 JSON_SCHEMA ，前者保证模型生成的消息为有效的 JSON，后者提供指定的模式，确保模型生成的响应与您提供的模式匹配（ spring.ai.openai.chat.options.responseFormat 选项）。 Azure OpenAI - 提供 spring.ai.azure.openai.chat.options.responseFormat 个选项，用于指定模型必须输出的格式。设置为 { \u0026ldquo;type\u0026rdquo;: \u0026ldquo;json_object\u0026rdquo; } 可启用 JSON 模式，从而保证模型生成的消息是有效的 JSON。 Ollama - 提供 spring.ai.ollama.chat.options.format 选项来指定返回响应的格式。目前，唯一接受的值是 json 。 Mistral AI - 提供 spring.ai.mistralai.chat.options.responseFormat 选项来指定返回响应的格式。将其设置为 { \u0026ldquo;type\u0026rdquo;: \u0026ldquo;json_object\u0026rdquo; } 可启用 JSON 模式，从而保证模型生成的消息是有效的 JSON。 "},{"id":42,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B-api/azure-openai-%E8%81%8A%E5%A4%A9/","title":"Azure OpenAI 聊天","section":"聊天模型 API","content":" Azure OpenAI 聊天 # Azure 的 OpenAI 产品由 ChatGPT 提供支持，超越了传统的 OpenAI 功能，提供功能增强的 AI 驱动文本生成。Azure 提供了额外的 AI 安全性和负责任的 AI 功能，正如其最新更新中[ 所]( https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/announcing-new-ai-safety-amp-responsible-ai-features-in-azure/ba-p/3983686)强调的那样。\nAzure 通过将 AI 与一系列 Azure 服务相集成，为 Java 开发人员提供了充分发挥 AI 潜力的机会，其中包括 Azure 上的矢量存储等与 AI 相关的资源。\n先决条件 # Azure OpenAI 客户端提供三种连接选项：使用 Azure API 密钥或使用 OpenAI API 密钥，或使用 Microsoft Entra ID。\nAzure API 密钥和端点 # 要使用 API 密钥访问模型，请从 [ Azure 门户]( https://portal.azure.com)上的 Azure OpenAI 服务部分获取 Azure OpenAI endpoint 和 api-key 。\nSpring AI 定义了两个配置属性：\n您可以在 application.properties 或 application.yml 文件中设置这些配置属性：\nspring.ai.azure.openai.api-key=\u0026lt;your-azure-api-key\u0026gt; spring.ai.azure.openai.endpoint=\u0026lt;your-azure-endpoint-url\u0026gt; 为了在处理 API 密钥等敏感信息时增强安全性，您可以使用 Spring 表达式语言 (SpEL) 来引用自定义环境变量：\n# In application.yml spring: ai: azure: openai: api-key: ${AZURE_OPENAI_API_KEY} endpoint: ${AZURE_OPENAI_ENDPOINT} # In your environment or .env file export AZURE_OPENAI_API_KEY=\u0026lt;your-azure-openai-api-key\u0026gt; export AZURE_OPENAI_ENDPOINT=\u0026lt;your-azure-openai-endpoint-url\u0026gt; OpenAI 密钥 # 要使用 OpenAI 服务（而非 Azure）进行身份验证，请提供 OpenAI API 密钥。这将自动将端点设置为 [ api.openai.com/v1](https:// api.openai.com/v1) 。\n使用此方法时，将 spring.ai.azure.openai.chat.options.deployment-name 属性设置为您希望使用的 [ OpenAI 模型]( https://platform.openai.com/docs/models)的名称。\n在您的应用程序配置中：\nspring.ai.azure.openai.openai-api-key=\u0026lt;your-azure-openai-key\u0026gt; spring.ai.azure.openai.chat.options.deployment-name=\u0026lt;openai-model-name\u0026gt; 将环境变量与 SpEL 结合使用：\n# In application.yml spring: ai: azure: openai: openai-api-key: ${AZURE_OPENAI_API_KEY} chat: options: deployment-name: ${AZURE_OPENAI_MODEL_NAME} # In your environment or .env file export AZURE_OPENAI_API_KEY=\u0026lt;your-openai-key\u0026gt; export AZURE_OPENAI_MODEL_NAME=\u0026lt;openai-model-name\u0026gt; 对于使用 Microsoft Entra ID（以前称为 Azure Active Directory）进行无密钥身份验证，请仅设置 spring.ai.azure.openai.endpoint 配置属性，而不要设置上面提到的 api-key 属性。\n仅找到端点属性，您的应用程序将评估检索凭据的几种不同选项，并使用令牌凭据创建 OpenAIClient 实例。\n部署名称 # 要使用 Azure AI 应用程序，您需要通过 [ Azure AI 门户]( https://oai.azure.com/portal)创建 Azure AI 部署。在 Azure 中，每个客户端必须指定 Deployment Name 才能连接到 Azure OpenAI 服务。需要注意的是， Deployment Name 与您选择部署的模型不同。例如，名为“MyAiDeployment”的部署可以配置为使用 GPT 3.5 Turbo 模型或 GPT 4.0 模型。\n首先，请按照以下步骤使用默认设置创建部署：\n此 Azure 配置与 Spring Boot Azure AI Starter 及其自动配置功能的默认配置一致。如果您使用其他部署名称，请确保相应地更新配置属性：\nspring.ai.azure.openai.chat.options.deployment-name=\u0026lt;my deployment name\u0026gt; Azure OpenAI 和 OpenAI 的部署结构不同，导致 Azure OpenAI 客户端库中出现了一个名为 deploymentOrModelName 的属性。这是因为 OpenAI 中没有 Deployment Name ，只有 Model Name 。\n访问 OpenAI 模型 # 您可以将客户端配置为直接使用 OpenAI 而不是 Azure OpenAI 部署的模型。为此，您需要设置 spring.ai.azure.openai.openai-api-key=\u0026lt;Your OpenAI Key\u0026gt; 而不是 spring.ai.azure.openai.api-key=\u0026lt;Your Azure OpenAi Key\u0026gt; 。\n添加存储库和 BOM # Spring AI 的构件已发布在 Maven Central 和 Spring Snapshot 仓库中。请参阅 [ “构件仓库”](../../getting-started.html#artifact-repositories) 部分，将这些仓库添加到您的构建系统中。\n为了帮助管理依赖项，Spring AI 提供了 BOM（物料清单），以确保在整个项目中使用一致版本的 Spring AI。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # Spring AI 为 Azure OpenAI 聊天客户端提供 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 或 Gradle build.gradle 生成文件中：\nAzure OpenAI 聊天客户端是使用 Azure SDK 提供的 [ OpenAIClientBuilder]( https://github.com/Azure/azure-sdk-for-java/blob/main/sdk/openai/azure-ai-openai/src/main/java/com/azure/ai/openai/[OpenAIClientBuilder](https://github.com/Azure/azure-sdk-for-java/blob/main/sdk/openai/azure-ai-openai/src/main/java/com/azure/ai/openai/OpenAIClientBuilder.java).java) 创建的。Spring AI 允许通过提供 Azure[ OpenAIClientBuilder]( https://github.com/Azure/azure-sdk-for-java/blob/main/sdk/openai/azure-ai-openai/src/main/java/com/azure/ai/openai/[OpenAIClientBuilder](https://github.com/Azure/azure-sdk-for-java/blob/main/sdk/openai/azure-ai-openai/src/main/java/com/azure/ai/openai/OpenAIClientBuilder.java).java)Customizer bean 来自定义构建器。\n例如，可以使用定制器来更改默认响应超时：\n@Configuration public class AzureOpenAiConfig { @Bean public AzureOpenAIClientBuilderCustomizer responseTimeoutCustomizer() { return openAiClientBuilder -\u0026gt; { HttpClientOptions clientOptions = new HttpClientOptions() .setResponseTimeout(Duration.ofMinutes(5)); openAiClientBuilder.httpClient(HttpClient.createDefault(clientOptions)); }; } } 聊天属性 # 前缀 spring.ai.azure.openai 是配置与 Azure OpenAI 的连接的属性前缀。\n前缀 spring.ai.azure.openai.chat 是配置 Azure OpenAI 的 ChatModel 实现的属性前缀。\n运行时选项 # [ AzureOpenAiChatOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-azure-openai/src/main/java/org/springframework/ai/azure/openai/[AzureOpenAiChatOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-azure-openai/src/main/java/org/springframework/ai/azure/openai/AzureOpenAiChatOptions.java)) 提供模型配置，例如要使用的模型、温度、频率惩罚等。\n启动时，可以使用 AzureOpenAiChatModel(api, options) 构造函数或 spring.ai.azure.openai.chat.options.* 属性配置默认选项。\n在运行时，您可以通过向 Prompt 调用添加新的、特定于请求的选项来覆盖默认选项。例如，要覆盖特定请求的默认模型和温度：\nChatResponse response = chatModel.call( new Prompt( \u0026#34;Generate the names of 5 famous pirates.\u0026#34;, AzureOpenAiChatOptions.builder() .deploymentName(\u0026#34;gpt-4o\u0026#34;) .temperature(0.4) .build() )); 函数调用 # 您可以将自定义 Java 函数注册到 AzureOpenAiChatModel，并让模型智能地选择输出包含参数的 JSON 对象，以调用一个或多个已注册的函数。这是一种将 LLM 功能与外部工具和 API 连接起来的强大技术。了解更多关于[ 工具调用的](../tools.html)信息。\n多式联运 # 多模态是指模型能够同时理解和处理来自各种来源的信息，包括文本、图像、音频和其他数据格式。目前，Azure OpenAI gpt-4o 模型提供多模态支持。\nAzure OpenAI 可以将一系列 base64 编码的图像或图像 URL 添加到消息中。Spring AI 的 [ Message]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/messages/[Message](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/messages/Message.java).java) 接口通过引入 [ Media]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/model/[Media](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/model/Media.java).java) 类型来促进多模态 AI 模型的发展。该类型包含消息中媒体附件的数据和详细信息，并利用 Spring 的 org.springframework.util.MimeType 和 java.lang.Object 来存储原始媒体数据。\n下面是摘录自 [ OpenAiChatModelIT.java]( https://github.com/spring-projects/spring-ai/blob/c9a3e66f90187ce7eae7eb78c462ec622685de6c/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/chat/[OpenAiChatModelIT.java](https://github.com/spring-projects/spring-ai/blob/c9a3e66f90187ce7eae7eb78c462ec622685de6c/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/chat/OpenAiChatModelIT.java#L293)#L293) 的代码示例，说明了如何使用 GPT_4_O 模型将用户文本与图像融合。\nURL url = new URL(\u0026#34;https://docs.spring.io/spring-ai/reference/_images/multimodal.test.png\u0026#34;); String response = ChatClient.create(chatModel).prompt() .options(AzureOpenAiChatOptions.builder().deploymentName(\u0026#34;gpt-4o\u0026#34;).build()) .user(u -\u0026gt; u.text(\u0026#34;Explain what do you see on this picture?\u0026#34;).media(MimeTypeUtils.IMAGE_PNG, this.url)) .call() .content(); 它将 multimodal.test.png 图像作为输入：\n以及文本消息“解释一下你在图片上看到了什么？”，并生成如下响应：\n您还可以传递类路径资源而不是 URL，如下例所示\nResource resource = new ClassPathResource(\u0026#34;multimodality/multimodal.test.png\u0026#34;); String response = ChatClient.create(chatModel).prompt() .options(AzureOpenAiChatOptions.builder() .deploymentName(\u0026#34;gpt-4o\u0026#34;).build()) .user(u -\u0026gt; u.text(\u0026#34;Explain what do you see on this picture?\u0026#34;) .media(MimeTypeUtils.IMAGE_PNG, this.resource)) .call() .content(); 样品控制器 # [ 创建]( https://start.spring.io/)一个新的 Spring Boot 项目并将 spring-ai-starter-model-azure-openai 添加到您的 pom（或 gradle）依赖项中。\n在 src/main/resources 目录下添加一个 application.properties 文件，以启用和配置 OpenAi 聊天模型：\nspring.ai.azure.openai.api-key=YOUR_API_KEY spring.ai.azure.openai.endpoint=YOUR_ENDPOINT spring.ai.azure.openai.chat.options.deployment-name=gpt-4o spring.ai.azure.openai.chat.options.temperature=0.7 这将创建一个 AzureOpenAiChatModel 实现，您可以将其注入到您的类中。这是一个使用聊天模型生成文本的简单 @Controller 类的示例。\n@RestController public class ChatController { private final AzureOpenAiChatModel chatModel; @Autowired public ChatController(AzureOpenAiChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, this.chatModel.call(message)); } @GetMapping(\u0026#34;/ai/generateStream\u0026#34;) public Flux\u0026lt;ChatResponse\u0026gt; generateStream(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { Prompt prompt = new Prompt(new UserMessage(message)); return this.chatModel.stream(prompt); } } 手动配置 # AzureOpenAiChatModel 实现了 ChatModel 和 StreamingChatModel 并使用 [ Azure OpenAI Java Client]( https://learn.microsoft.com/en-us/java/api/overview/azure/ai-openai-readme?view=azure-java-preview) 。\n要启用它，请将 spring-ai-azure-openai 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-azure-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-azure-openai\u0026#39; } 接下来，创建一个 AzureOpenAiChatModel 实例并使用它来生成文本响应：\nvar openAIClientBuilder = new OpenAIClientBuilder() .credential(new AzureKeyCredential(System.getenv(\u0026#34;AZURE_OPENAI_API_KEY\u0026#34;))) .endpoint(System.getenv(\u0026#34;AZURE_OPENAI_ENDPOINT\u0026#34;)); var openAIChatOptions = AzureOpenAiChatOptions.builder() .deploymentName(\u0026#34;gpt-4o\u0026#34;) .temperature(0.4) .maxTokens(200) .build(); var chatModel = AzureOpenAiChatModel.builder() .openAIClientBuilder(openAIClientBuilder) .defaultOptions(openAIChatOptions) .build(); ChatResponse response = chatModel.call( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); // Or with streaming responses Flux\u0026lt;ChatResponse\u0026gt; streamingResponses = chatModel.stream( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); "},{"id":43,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B-api/minimax-%E8%81%8A%E5%A4%A9/","title":"MiniMax 聊天","section":"嵌入模型 API","content":" MiniMax 聊天 # Spring AI 支持 MiniMax 的各种 AI 语言模型。您可以与 MiniMax 语言模型进行交互，并基于 MiniMax 模型创建多语言对话助手。\n先决条件 # 您需要使用 MiniMax 创建一个 API 来访问 MiniMax 语言模型。\n在 [ MiniMax 注册页面]( https://www.minimaxi.com/login)创建一个帐户，并在 [ API Keys 页面]( https://www.minimaxi.com/user-center/basic-information/interface-key)生成令牌。\nSpring AI 项目定义了一个名为 spring.ai.minimax.api-key 的配置属性，您应该将其设置为从 API Keys 页面获取的 API Key 的值。\n您可以在 application.properties 文件中设置此配置属性：\nspring.ai.minimax.api-key=\u0026lt;your-minimax-api-key\u0026gt; 为了在处理 API 密钥等敏感信息时增强安全性，您可以使用 Spring 表达式语言 (SpEL) 来引用环境变量：\n# In application.yml spring: ai: minimax: api-key: ${MINIMAX_API_KEY} # In your environment or .env file export MINIMAX_API_KEY=\u0026lt;your-minimax-api-key\u0026gt; 您还可以在应用程序代码中以编程方式设置此配置：\n// Retrieve API key from a secure source or environment variable String apiKey = System.getenv(\u0026#34;MINIMAX_API_KEY\u0026#34;); 添加存储库和 BOM # Spring AI 的构件已发布在 Maven Central 和 Spring Snapshot 仓库中。请参阅 [ “构件仓库”](../../getting-started.html#artifact-repositories) 部分，将这些仓库添加到您的构建系统中。\n为了帮助管理依赖项，Spring AI 提供了 BOM（物料清单），以确保在整个项目中使用一致版本的 Spring AI。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # Spring AI 为 Azure MiniMax 嵌入模型提供 Spring Boot 自动配置。 Spring AI offers Spring Boot auto-configuration for the Azure MiniMax Embedding Model. 要启用它，请将以下依赖项添加到项目的 Maven pom.xml 文件：To enable it, add the following dependency to your project\u0026rsquo;s Maven pom.xml file:\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-minimax\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-minimax\u0026#39; } 嵌入属性 # 重试属性 # 前缀 spring.ai.retry 用作属性前缀，可让您配置 MiniMax Embedding 模型的重试机制。\n连接属性 # 前缀 spring.ai.minimax 用作允许您连接到 MiniMax 的属性前缀。\n配置属性 # 前缀 spring.ai.minimax.embedding 是配置 MiniMax 的 EmbeddingModel 实现的属性前缀。\n运行时选项 # [ MiniMaxEmbeddingOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-minimax/src/main/java/org/springframework/ai/minimax/[MiniMaxEmbeddingOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-minimax/src/main/java/org/springframework/ai/minimax/MiniMaxEmbeddingOptions.java)) 提供 MiniMax 配置，例如要使用的模型等。\n也可以使用 spring.ai.minimax.embedding.options 属性来配置默认选项。\n启动时，使用 MiniMaxEmbeddingModel 构造函数设置所有嵌入请求的默认选项。运行时，您可以使用 MiniMaxEmbeddingOptions 实例作为 EmbeddingRequest 的一部分来覆盖默认选项。\n例如，要覆盖特定请求的默认模型名称：\nEmbeddingResponse embeddingResponse = embeddingModel.call( new EmbeddingRequest(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;), MiniMaxEmbeddingOptions.builder() .model(\u0026#34;Different-Embedding-Model-Deployment-Name\u0026#34;) .build())); 样品控制器 # 这将创建一个 EmbeddingModel 实现，您可以将其注入到您的类中。这是一个使用 EmbeddingC 实现的简单 @Controller 类的示例。\nspring.ai.minimax.api-key=YOUR_API_KEY spring.ai.minimax.embedding.options.model=embo-01 @RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(\u0026#34;/ai/embedding\u0026#34;) public Map embed(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { EmbeddingResponse embeddingResponse = this.embeddingModel.embedForResponse(List.of(message)); return Map.of(\u0026#34;embedding\u0026#34;, embeddingResponse); } } 手动配置 # 如果您不使用 Spring Boot，则可以手动配置 MiniMax 嵌入模型。为此，请将 spring-ai-minimax 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-minimax\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-minimax\u0026#39; } 接下来，创建一个 MiniMaxEmbeddingModel 实例并使用它来计算两个输入文本之间的相似度：\nvar miniMaxApi = new MiniMaxApi(System.getenv(\u0026#34;MINIMAX_API_KEY\u0026#34;)); var embeddingModel = new MiniMaxEmbeddingModel(minimaxApi, MetadataMode.EMBED, MiniMaxEmbeddingOptions.builder().model(\u0026#34;embo-01\u0026#34;).build()); EmbeddingResponse embeddingResponse = this.embeddingModel .embedForResponse(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;)); MiniMaxEmbeddingOptions 提供嵌入请求的配置信息。options 类提供了一个 builder() 方法，方便用户轻松创建选项。\n"},{"id":44,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E5%9B%BE%E5%83%8F%E6%A8%A1%E5%9E%8B-api/zhipuai-%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/","title":"ZhiPuAI 图像生成","section":"图像模型 API","content":" ZhiPuAI 图像生成 # Spring AI 支持来自 ZhiPuAI 的图像生成模型 CogView。\n先决条件 # 您需要使用 ZhiPuAI 创建一个 API 来访问 ZhiPu AI 语言模型。\n在[ 智普 AI 注册页面]( https://open.bigmodel.cn/login)创建账号，并在 [ API Keys 页面]( https://open.bigmodel.cn/usercenter/apikeys)生成 token。\nSpring AI 项目定义了一个名为 spring.ai.zhipuai.api-key 的配置属性，您应该将其设置为从 API Keys 页面获取的 API Key 的值。\n您可以在 application.properties 文件中设置此配置属性：\nspring.ai.zhipuai.api-key=\u0026lt;your-zhipuai-api-key\u0026gt; 为了在处理 API 密钥等敏感信息时增强安全性，您可以使用 Spring 表达式语言 (SpEL) 引用自定义环境变量：\n# In application.yml spring: ai: zhipuai: api-key: ${ZHIPUAI_API_KEY} # In your environment or .env file export ZHIPUAI_API_KEY=\u0026lt;your-zhipuai-api-key\u0026gt; 您还可以在应用程序代码中以编程方式设置此配置：\n// Retrieve API key from a secure source or environment variable String apiKey = System.getenv(\u0026#34;ZHIPUAI_API_KEY\u0026#34;); 添加存储库和 BOM # Spring AI 的构件已发布在 Maven Central 和 Spring Snapshot 仓库中。请参阅 [ “构件仓库”](../../getting-started.html#artifact-repositories) 部分，将这些仓库添加到您的构建系统中。\n为了帮助管理依赖项，Spring AI 提供了 BOM（物料清单），以确保在整个项目中使用一致版本的 Spring AI。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # Spring AI 为 ZhiPuAI 聊天客户端提供了 Spring Boot 自动配置功能。要启用此功能，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-zhipuai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-zhipuai\u0026#39; } 图像生成属性 # 前缀 spring.ai.zhipuai.image 是允许您为 ZhiPuAI 配置 ImageModel 实现的属性前缀。\n连接属性 # 前缀 spring.ai.zhipuai 用作允许您连接到 ZhiPuAI 的属性前缀。\n配置属性 # 重试属性 # 前缀 spring.ai.retry 用作属性前缀，可让您配置 ZhiPuAI Image 客户端的重试机制。\n运行时选项 # [ ZhiPuAiImageOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-zhipuai/src/main/java/org/springframework/ai/zhipuai/[ZhiPuAiImageOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-zhipuai/src/main/java/org/springframework/ai/zhipuai/ZhiPuAiImageOptions.java)) 提供模型配置，例如要使用的模型，质量，尺寸等。\n启动时，可以使用 ZhiPuAiImageModel(ZhiPuAiImageApi zhiPuAiImageApi) 构造函数和 withDefaultOptions(ZhiPuAiImageOptions defaultOptions) 方法配置默认选项。或者，也可以使用前面描述的 spring.ai.zhipuai.image.options.* 属性。\n在运行时，您可以通过向 ImagePrompt 调用添加新的、特定于请求的选项来覆盖默认选项。例如，要覆盖 ZhiPuAI 的特定选项（例如质量和要创建的图像数量），请使用以下代码示例：\nImageResponse response = zhiPuAiImageModel.call( new ImagePrompt(\u0026#34;A light cream colored mini golden doodle\u0026#34;, ZhiPuAiImageOptions.builder() .quality(\u0026#34;hd\u0026#34;) .N(4) .height(1024) .width(1024).build()) ); "},{"id":45,"href":"/docs/%E6%8C%87%E5%8D%97/%E4%BA%91%E7%BB%91%E5%AE%9A/","title":"云绑定","section":"指南","content":" 云绑定 # Spring AI 基于 [ spring-cloud-bindings]( https://github.com/spring-cloud/[spring-cloud-bindings](https://github.com/spring-cloud/spring-cloud-bindings)) 的基础，提供对云绑定的支持。这允许应用程序为提供程序指定绑定类型，然后使用通用格式表达属性。spring-ai 云绑定将处理这些属性，并将其绑定到 spring-ai 原生属性。\n例如，使用 OpenAi 时，绑定类型为 openai 。使用属性 spring.ai.cloud.bindings.openai.enabled 可以启用或禁用绑定处理器。默认情况下，指定绑定类型时将启用此属性。可以指定 api-key 、 uri 、 username 、 password 等配置，spring-ai 会将它们映射到所支持系统中的相应属性。\n要启用云绑定支持，请在应用程序中包含以下依赖项。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-spring-cloud-bindings\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-spring-cloud-bindings\u0026#39; } 可用的云绑定 # 以下是 spring-ai-spring-clou-bindings 模块中当前提供云绑定支持的组件：\n"},{"id":46,"href":"/docs/%E5%8F%82%E8%80%83/%E5%A4%9A%E6%A8%A1%E6%80%81-api/","title":"多模态 API","section":"参考","content":" 多模态 API # 人类能够同时处理多种数据输入模式的知识。我们的学习方式和经验都是多模态的。我们拥有的不仅仅是视觉、听觉和文本。\n与这些原则相反，机器学习通常专注于为处理单一模态而定制的专用模型。例如，我们开发了用于文本转语音或语音转文本等任务的音频模型，以及用于对象检测和分类等任务的计算机视觉模型。\n然而，新一波多模态大型语言模型开始涌现。例如，OpenAI 的 GPT-4o、谷歌的 Vertex AI Gemini 1.5、Anthropic 的 Claude3，以及开源产品 Llama3.2、LLaVA 和 BakLLaVA，它们能够接受多种输入，包括文本、图像、音频和视频，并通过集成这些输入来生成文本响应。\nSpring AI 多模态 # 多模态性是指模型同时理解和处理来自各种来源的信息的能力，包括文本、图像、音频和其他数据格式。\nSpring AI Message API 提供了支持多模式 LLM 所需的所有抽象。\nUserMessage 的 content 字段主要用于文本输入，而可选的 media 字段允许添加一个或多个不同模态的附加内容，例如图像、音频和视频。MimeType 指定模态类型。根据所使用的 LLM， Media 数据字段可以是作为 Resource 对象的原始媒体内容 MimeType 也可以是指向该内容的 URI 。\n例如，我们可以将下面的图片（ multimodal.test.png ）作为输入，并要求 LLM 解释它所看到的内容。\n对于大多数多模式 LLM，Spring AI 代码看起来像这样：\nvar imageResource = new ClassPathResource(\u0026#34;/multimodal.test.png\u0026#34;); var userMessage = new UserMessage( \u0026#34;Explain what do you see in this picture?\u0026#34;, // content new Media(MimeTypeUtils.IMAGE_PNG, this.imageResource)); // media ChatResponse response = chatModel.call(new Prompt(this.userMessage)); 或者使用流畅的 [ ChatClient](chatclient.html) API：\nString response = ChatClient.create(chatModel).prompt() .user(u -\u0026gt; u.text(\u0026#34;Explain what do you see on this picture?\u0026#34;) .media(MimeTypeUtils.IMAGE_PNG, new ClassPathResource(\u0026#34;/multimodal.test.png\u0026#34;))) .call() .content(); 并产生如下响应：\nSpring AI 为以下聊天模型提供多模式支持：\n人性克劳德3 Azure Open AI（例如 GPT-4o 模型） Mistral AI（例如 Mistral Pixtral 型号） Ollama（例如 LLaVA、BakLLaVA、Llama3.2 型号） OpenAI（例如 GPT-4 和 GPT-4o 模型） Vertex AI Gemini（例如 gemini-1.5-pro-001、gemini-1.5-flash-001 型号） "},{"id":47,"href":"/docs/%E6%8C%87%E5%8D%97/","title":"指南","section":"Docs","content":" 指南 # "},{"id":48,"href":"/docs/%E5%8F%82%E8%80%83/%E7%9F%A2%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/%E8%89%B2%E5%BA%A6/","title":"色度","section":"矢量数据库","content":" 色度 # 本节将引导您设置 Chroma VectorStore 来存储文档嵌入并执行相似性搜索。\n[ Chroma]( https://docs.trychroma.com/) 是一个开源的嵌入数据库。它提供了存储文档嵌入、内容和元数据的工具，并支持搜索这些嵌入，包括元数据过滤。\n先决条件 # 启动时，如果尚未配置， ChromaVectorStore 会创建所需的集合。\n自动配置 # Spring AI 为 Chroma 矢量存储提供了 Spring Boot 自动配置功能。要启用此功能，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-chroma\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-vector-store-chroma\u0026#39; } 向量存储实现可以为您初始化必要的模式，但您必须通过在适当的构造函数中指定 initializeSchema 布尔值或在 application.properties 文件中设置 …​initialize-schema=true 来选择加入。\n此外，您还需要一个已配置的 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) bean。有关更多信息，请参阅 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) 部分。\n以下是所需 bean 的示例：\n@Bean public EmbeddingModel embeddingModel() { // Can be any other EmbeddingModel implementation. return new OpenAiEmbeddingModel(OpenAiApi.builder().apiKey(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;)).build()); } 要连接到 Chroma，您需要提供实例的访问详细信息。您可以通过 Spring Boot 的 application.properties 提供简单的配置，\n# Chroma Vector Store connection properties spring.ai.vectorstore.chroma.client.host=\u0026lt;your Chroma instance host\u0026gt; spring.ai.vectorstore.chroma.client.port=\u0026lt;your Chroma instance port\u0026gt; spring.ai.vectorstore.chroma.client.key-token=\u0026lt;your access token (if configure)\u0026gt; spring.ai.vectorstore.chroma.client.username=\u0026lt;your username (if configure)\u0026gt; spring.ai.vectorstore.chroma.client.password=\u0026lt;your password (if configure)\u0026gt; # Chroma Vector Store collection properties spring.ai.vectorstore.chroma.initialize-schema=\u0026lt;true or false\u0026gt; spring.ai.vectorstore.chroma.collection-name=\u0026lt;your collection name\u0026gt; # Chroma Vector Store configuration properties # OpenAI API key if the OpenAI auto-configuration is used. spring.ai.openai.api.key=\u0026lt;OpenAI Api-key\u0026gt; 请查看向量存储的[ 配置参数](#_configuration_properties)列表，以了解默认值和配置选项。\n现在您可以在应用程序中自动连接 Chroma Vector Store 并使用它\n@Autowired VectorStore vectorStore; // ... List \u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); // Add the documents vectorStore.add(documents); // Retrieve documents similar to a query List\u0026lt;Document\u0026gt; results = this.vectorStore.similaritySearch(SearchRequest.builder().query(\u0026#34;Spring\u0026#34;).topK(5).build()); 配置属性 # 您可以在 Spring Boot 配置中使用以下属性来自定义矢量存储。\n元数据过滤 # 您还可以利用带有 ChromaVector 存储的通用、可移植元[ 数据过滤器]( https://docs.spring.io/spring-ai/reference/api/vectordbs.html#_metadata_filters) 。\n例如，您可以使用文本表达语言：\nvectorStore.similaritySearch( SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(\u0026#34;author in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; article_type == \u0026#39;blog\u0026#39;\u0026#34;).build()); 或者以编程方式使用 Filter.Expression DSL：\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(b.and( b.in(\u0026#34;john\u0026#34;, \u0026#34;jill\u0026#34;), b.eq(\u0026#34;article_type\u0026#34;, \u0026#34;blog\u0026#34;)).build()).build()); 例如，这个便携式过滤器表达式：\nauthor in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; article_type == \u0026#39;blog\u0026#39; 转换为专有的 Chroma 格式\n{\u0026#34;$and\u0026#34;:[ {\u0026#34;author\u0026#34;: {\u0026#34;$in\u0026#34;: [\u0026#34;john\u0026#34;, \u0026#34;jill\u0026#34;]}}, {\u0026#34;article_type\u0026#34;:{\u0026#34;$eq\u0026#34;:\u0026#34;blog\u0026#34;}}] } 手动配置 # 如果您希望手动配置 Chroma Vector Store，则可以通过在 Spring Boot 应用程序中创建 ChromaVectorStore bean 来实现。\n将这些依赖项添加到您的项目：* Chroma VectorStore。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-chroma-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; OpenAI：计算嵌入时必需。您可以使用任何其他嵌入模型实现。 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 示例代码 # 使用适当的 ChromaDB 授权配置创建 RestClient.Builder 实例并使用它来创建 ChromaApi 实例：\n@Bean public RestClient.Builder builder() { return RestClient.builder().requestFactory(new SimpleClientHttpRequestFactory()); } @Bean public ChromaApi chromaApi(RestClient.Builder restClientBuilder) { String chromaUrl = \u0026#34;http://localhost:8000\u0026#34;; ChromaApi chromaApi = new ChromaApi(chromaUrl, restClientBuilder); return chromaApi; } 通过将 Spring Boot OpenAI 启动器添加到您的项目来与 OpenAI 的嵌入集成。这将为您提供 Embeddings 客户端的实现：\n@Bean public VectorStore chromaVectorStore(EmbeddingModel embeddingModel, ChromaApi chromaApi) { return ChromaVectorStore.builder(chromaApi, embeddingModel) .collectionName(\u0026#34;TestCollection\u0026#34;) .initializeSchema(true) .build(); } 在您的主代码中，创建一些文档：\nList\u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); 将文档添加到您的矢量存储中：\nvectorStore.add(documents); 最后，检索与查询类似的文档：\nList\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch(\u0026#34;Spring\u0026#34;); 如果一切顺利，您应该检索包含文本“Spring AI rocks!!”的文档。\n本地运行 Chroma # docker run -it --rm --name chroma -p 8000:8000 ghcr.io/chroma-core/chroma:1.0.0 在 [ localhost:8000/api/v1](http:// localhost:8000/api/v1) 上启动色度商店\n"},{"id":49,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E9%9F%B3%E9%A2%91%E6%A8%A1%E5%9E%8B/","title":"音频模型","section":"模型","content":" 音频模型 # "},{"id":50,"href":"/docs/%E5%8F%82%E8%80%83/%E7%9F%A2%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/couchbase/","title":"Couchbase","section":"矢量数据库","content":" Couchbase # 本节将引导您设置 CouchbaseSearchVectorStore 来存储文档嵌入并使用 Couchbase 执行相似性搜索。\n[ Couchbase]( https://docs.couchbase.com/server/current/vector-search/vector-search.html) 是一个分布式 JSON 文档数据库，具备关系型数据库管理系统 (DBMS) 的所有功能。此外，它还允许用户使用基于向量的存储和检索来查询信息。\n先决条件 # 正在运行的 Couchbase 实例。以下选项可用：Couchbase * [ Docker]( https://hub.docker.com/_/couchbase/)\n[ Capella - Couchbase 即服务]( https://cloud.couchbase.com/) [ 本地安装 Couchbase]( https://www.couchbase.com/downloads/?family=couchbase-server) [ Couchbase Kubernetes 操作员]( https://www.couchbase.com/downloads/?family=open-source-kubernetes) 自动配置 # Spring AI 为 Couchbase 矢量存储提供了 Spring Boot 自动配置功能。要启用此功能，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-couchbase\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-couchbase-store-spring-boot-starter\u0026#39; } 向量存储实现可以使用默认选项为您初始化配置的存储桶、范围、集合和搜索索引，但您必须通过在适当的构造函数中指定 initializeSchema 布尔值来选择加入。\n请查看向量存储的[ 配置参数](#couchbasevector-properties)列表，以了解默认值和配置选项。\n此外，您还需要一个已配置的 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) bean。有关更多信息，请参阅 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) 部分。\n现在，您可以将 CouchbaseSearchVectorStore 自动连接为应用程序中的矢量存储。\n@Autowired VectorStore vectorStore; // ... List \u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); // Add the documents to Qdrant vectorStore.add(documents); // Retrieve documents similar to a query List\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch(SearchRequest.query(\u0026#34;Spring\u0026#34;).withTopK(5)); 配置属性 # 要连接到 Couchbase 并使用 CouchbaseSearchVectorStore ，您需要提供实例的访问详细信息。您可以通过 Spring Boot 的 application.properties 提供配置：\nspring.ai.openai.api-key=\u0026lt;key\u0026gt; spring.couchbase.connection-string=\u0026lt;conn_string\u0026gt; spring.couchbase.username=\u0026lt;username\u0026gt; spring.couchbase.password=\u0026lt;password\u0026gt; 如果您希望使用环境变量来存储密码或 API 密钥等敏感信息，则您有多种选择：\n选项 1：使用 Spring 表达语言（SpEL） # 您可以使用自定义环境变量名称，并使用 SpEL 在应用程序配置中引用它们：\n# In application.yml spring: ai: openai: api-key: ${OPENAI_API_KEY} couchbase: connection-string: ${COUCHBASE_CONN_STRING} username: ${COUCHBASE_USER} password: ${COUCHBASE_PASSWORD} # In your environment or .env file export OPENAI_API_KEY=\u0026lt;api-key\u0026gt; export COUCHBASE_CONN_STRING=\u0026lt;couchbase connection string like couchbase://localhost\u0026gt; export COUCHBASE_USER=\u0026lt;couchbase username\u0026gt; export COUCHBASE_PASSWORD=\u0026lt;couchbase password\u0026gt; 选项 2：以编程方式访问环境变量 # 或者，您可以在 Java 代码中访问环境变量：\nString apiKey = System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;); 这种方法使您可以灵活地命名环境变量，同时将敏感信息排除在应用程序配置文件之外。\nSpring Boot 的 Couchbase Cluster 自动配置功能将创建一个由 CouchbaseSearchVectorStore 使用的 bean 实例。\n以 spring.couchbase.* 开头的 Spring Boot 属性用于配置 Couchbase 集群实例：\n以 spring.ai.vectorstore.couchbase.* 前缀开头的属性用于配置 CouchbaseSearchVectorStore 。\n可以使用以下相似度函数：\nl2_范数 点积 可以使用以下索引优化：\n记起 延迟 有关每个内容的更多详细信息，请参阅 [ Couchbase 文档]( https://docs.couchbase.com/server/current/search/child-field-options-reference.html)中的矢量搜索。\n元数据过滤 # 您可以利用 Couchbase 商店的通用、可移植元[ 数据过滤器]( https://docs.spring.io/spring-ai/reference/api/vectordbs.html#_metadata_filters) 。\n例如，您可以使用文本表达语言：\nvectorStore.similaritySearch( SearchRequest.defaults() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .filterExpression(\u0026#34;author in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; article_type == \u0026#39;blog\u0026#39;\u0026#34;)); 或者以编程方式使用 Filter.Expression DSL：\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.defaults() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .filterExpression(b.and( b.in(\u0026#34;author\u0026#34;,\u0026#34;john\u0026#34;, \u0026#34;jill\u0026#34;), b.eq(\u0026#34;article_type\u0026#34;, \u0026#34;blog\u0026#34;)).build())); 手动配置 # 除了使用 Spring Boot 自动配置之外，您还可以手动配置 Couchbase 向量存储。为此，您需要将 spring-ai-couchbase-store 添加到您的项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-couchbase-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-couchbase-store\u0026#39; } 创建 Couchbase Cluster Bean。阅读 [ Couchbase 文档]( https://docs.couchbase.com/java-sdk/current/hello-world/start-using-sdk.html) ，了解有关自定义 Cluster 实例配置的更多详细信息。\n@Bean public Cluster cluster() { return Cluster.connect(\u0026#34;couchbase://localhost\u0026#34;, \u0026#34;username\u0026#34;, \u0026#34;password\u0026#34;); } 然后使用构建器模式创建 CouchbaseSearchVectorStore bean：\n@Bean public VectorStore couchbaseSearchVectorStore(Cluster cluster, EmbeddingModel embeddingModel, Boolean initializeSchema) { return CouchbaseSearchVectorStore .builder(cluster, embeddingModel) .bucketName(\u0026#34;test\u0026#34;) .scopeName(\u0026#34;test\u0026#34;) .collectionName(\u0026#34;test\u0026#34;) .initializeSchema(initializeSchema) .build(); } // This can be any EmbeddingModel implementation. @Bean public EmbeddingModel embeddingModel() { return new OpenAiEmbeddingModel(OpenAiApi.builder().apiKey(this.openaiKey).build()); } 限制 # "},{"id":51,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B-api/deepseek-%E8%81%8A%E5%A4%A9/","title":"DeepSeek 聊天","section":"聊天模型 API","content":" DeepSeek 聊天 # Spring AI 支持 DeepSeek 的各种 AI 语言模型。您可以与 DeepSeek 语言模型进行交互，并基于 DeepSeek 模型创建多语言对话助手。\n先决条件 # 您需要使用 DeepSeek 创建 API 密钥才能访问 DeepSeek 语言模型。\n在 [ DeepSeek 注册页面]( https://platform.deepseek.com/sign_up)创建一个帐户，并在 [ API Keys 页面]( https://platform.deepseek.com/api_keys)生成一个令牌。\nSpring AI 项目定义了一个名为 spring.ai.deepseek.api-key 的配置属性，您应该将其设置为从 API Keys 页面获取的 API Key 的值。\n您可以在 application.properties 文件中设置此配置属性：\nspring.ai.deepseek.api-key=\u0026lt;your-deepseek-api-key\u0026gt; 为了在处理 API 密钥等敏感信息时增强安全性，您可以使用 Spring 表达式语言 (SpEL) 引用自定义环境变量：\n# In application.yml spring: ai: deepseek: api-key: ${DEEPSEEK_API_KEY} # In your environment or .env file export DEEPSEEK_API_KEY=\u0026lt;your-deepseek-api-key\u0026gt; 您还可以在应用程序代码中以编程方式设置此配置：\n// Retrieve API key from a secure source or environment variable String apiKey = System.getenv(\u0026#34;DEEPSEEK_API_KEY\u0026#34;); 添加存储库和 BOM # Spring AI 工件已发布在 Spring Milestone 和 Snapshot 仓库中。请参阅 [ “工件仓库”](../../getting-started.html#artifact-repositories) 部分，将这些仓库添加到您的构建系统中。\n为了帮助管理依赖项，Spring AI 提供了 BOM（物料清单），以确保在整个项目中使用一致版本的 Spring AI。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # Spring AI 为 DeepSeek 聊天模型提供了 Spring Boot 自动配置功能。要启用此功能，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-deepseek\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-deepseek\u0026#39; } 聊天属性 # 重试属性 # 前缀 spring.ai.retry 用作属性前缀，可让您配置 DeepSeek Chat 模型的重试机制。\n连接属性 # 前缀 spring.ai.deepseek 用作允许您连接到 DeepSeek 的属性前缀。\n配置属性 # 前缀 spring.ai.deepseek.chat 是属性前缀，可让您配置 DeepSeek 的聊天模型实现。\n运行时选项 # [ DeepSeekChatOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-deepseek/src/main/java/org/springframework/ai/deepseek/[DeepSeekChatOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-deepseek/src/main/java/org/springframework/ai/deepseek/DeepSeekChatOptions.java)) 提供模型配置，例如要使用的模型、温度、频率惩罚等。\n启动时，可以使用 DeepSeekChatModel(api, options) 构造函数或 spring.ai.deepseek.chat.options.* 属性配置默认选项。\n在运行时，您可以通过向 Prompt 调用添加新的特定于请求的选项来覆盖默认选项。例如，要覆盖特定请求的默认模型和温度：\nChatResponse response = chatModel.call( new Prompt( \u0026#34;Generate the names of 5 famous pirates. Please provide the JSON response without any code block markers such as ```json```.\u0026#34;, DeepSeekChatOptions.builder() .withModel(DeepSeekApi.ChatModel.DEEPSEEK_CHAT.getValue()) .withTemperature(0.8f) .build() )); 样本控制器（自动配置） # [ 创建]( https://start.spring.io/)一个新的 Spring Boot 项目并将 spring-ai-starter-model-deepseek 添加到您的 pom（或 gradle）依赖项中。\n在 src/main/resources 目录下添加 application.properties 文件，用于启用和配置 DeepSeek Chat 模型：\nspring.ai.deepseek.api-key=YOUR_API_KEY spring.ai.deepseek.chat.options.model=deepseek-chat spring.ai.deepseek.chat.options.temperature=0.8 这将创建一个 DeepSeekChatModel 实现，您可以将其注入到您的类中。这是一个使用聊天模型生成文本的简单 @Controller 类的示例。\n@RestController public class ChatController { private final DeepSeekChatModel chatModel; @Autowired public ChatController(DeepSeekChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, chatModel.call(message)); } @GetMapping(\u0026#34;/ai/generateStream\u0026#34;) public Flux\u0026lt;ChatResponse\u0026gt; generateStream(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { var prompt = new Prompt(new UserMessage(message)); return chatModel.stream(prompt); } } 聊天前缀补全 # 聊天前缀补全遵循聊天补全 API，用户提供助手的前缀消息，以便模型完成消息的其余部分。\n使用前缀完成时，用户必须确保消息列表中的最后一条消息是 DeepSeekAssistantMessage。\n下面是一个完整的聊天前缀补全 Java 代码示例。在本例中，我们将助手的前缀消息设置为“python\\n”，以强制模型输出 Python 代码，并将停止参数设置为 [\u0026rsquo;\u0026rsquo;]，以防止模型进行额外的解释。\n@RestController public class CodeGenerateController { private final DeepSeekChatModel chatModel; @Autowired public ChatController(DeepSeekChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generatePythonCode\u0026#34;) public String generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Please write quick sort code\u0026#34;) String message) { UserMessage userMessage = new UserMessage(message); Message assistantMessage = DeepSeekAssistantMessage.prefixAssistantMessage(\u0026#34;```python\\\\n\u0026#34;); Prompt prompt = new Prompt(List.of(userMessage, assistantMessage), ChatOptions.builder().stopSequences(List.of(\u0026#34;```\u0026#34;)).build()); ChatResponse response = chatModel.call(prompt); return response.getResult().getOutput().getText(); } } 推理模型（deepseek-reasoner） # deepseek-reasoner 是由 DeepSeek 开发的推理模型。在得出最终答案之前，该模型会首先生成思维链 (CoT)，以提高其响应的准确性。我们的 API 允许用户访问 deepseek-reasoner 生成的 CoT 内容，以便他们查看、显示和提取这些内容。\n您可以使用 DeepSeekAssistantMessage 获取 deepseek-reasoner 生成的 CoT 内容。\npublic void deepSeekReasonerExample() { DeepSeekChatOptions promptOptions = DeepSeekChatOptions.builder() .model(DeepSeekApi.ChatModel.DEEPSEEK_REASONER.getValue()) .build(); Prompt prompt = new Prompt(\u0026#34;9.11 and 9.8, which is greater?\u0026#34;, promptOptions); ChatResponse response = chatModel.call(prompt); // Get the CoT content generated by deepseek-reasoner, only available when using deepseek-reasoner model DeepSeekAssistantMessage deepSeekAssistantMessage = (DeepSeekAssistantMessage) response.getResult().getOutput(); String reasoningContent = deepSeekAssistantMessage.getReasoningContent(); String text = deepSeekAssistantMessage.getText(); } 推理模型多轮对话 # 在每一轮对话中，模型都会输出 CoT（reasoning_content）和最终答案（content）。在下一轮对话中，前几轮的 CoT 不会被连接到上下文中，如下图所示：\n请注意，如果输入消息序列中包含 reasoning_content 字段，API 将返回 400 错误。因此，您应该在发出 API 请求之前，从 API 响应中移除 reasoning_content 字段，如 API 示例中所示。\npublic String deepSeekReasonerMultiRoundExample() { List\u0026lt;Message\u0026gt; messages = new ArrayList\u0026lt;\u0026gt;(); messages.add(new UserMessage(\u0026#34;9.11 and 9.8, which is greater?\u0026#34;)); DeepSeekChatOptions promptOptions = DeepSeekChatOptions.builder() .model(DeepSeekApi.ChatModel.DEEPSEEK_REASONER.getValue()) .build(); Prompt prompt = new Prompt(messages, promptOptions); ChatResponse response = chatModel.call(prompt); DeepSeekAssistantMessage deepSeekAssistantMessage = (DeepSeekAssistantMessage) response.getResult().getOutput(); String reasoningContent = deepSeekAssistantMessage.getReasoningContent(); String text = deepSeekAssistantMessage.getText(); messages.add(new AssistantMessage(Objects.requireNonNull(text))); messages.add(new UserMessage(\u0026#34;How many Rs are there in the word \u0026#39;strawberry\u0026#39;?\u0026#34;)); Prompt prompt2 = new Prompt(messages, promptOptions); ChatResponse response2 = chatModel.call(prompt2); DeepSeekAssistantMessage deepSeekAssistantMessage2 = (DeepSeekAssistantMessage) response2.getResult().getOutput(); String reasoningContent2 = deepSeekAssistantMessage2.getReasoningContent(); return deepSeekAssistantMessage2.getText(); } 手动配置 # DeepSeekChatModel 实现了 ChatModel 和 StreamingChatModel ，并使用 [ Low-level DeepSeekApi Client](#low-level-api) 连接到 DeepSeek 服务。\n将 spring-ai-deepseek 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-deepseek\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-deepseek\u0026#39; } 接下来，创建一个 DeepSeekChatModel 并使用它来生成文本：\nvar deepSeekApi = new DeepSeekApi(System.getenv(\u0026#34;DEEPSEEK_API_KEY\u0026#34;)); var chatModel = new DeepSeekChatModel(deepSeekApi, DeepSeekChatOptions.builder() .withModel(DeepSeekApi.ChatModel.DEEPSEEK_CHAT.getValue()) .withTemperature(0.4f) .withMaxTokens(200) .build()); ChatResponse response = chatModel.call( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); // Or with streaming responses Flux\u0026lt;ChatResponse\u0026gt; streamResponse = chatModel.stream( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); DeepSeekChatOptions 提供聊天请求的配置信息。DeepSeekChatOptions.Builder 是一个流畅的选项构建 DeepSeekChatOptions.Builder 。\n低级 DeepSeekApi 客户端 # [ DeepSeekApi]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-deepseek/src/main/java/org/springframework/ai/deepseek/api/[DeepSeekApi](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-deepseek/src/main/java/org/springframework/ai/deepseek/api/DeepSeekApi.java).java) 是 [ DeepSeek API]( https://platform.deepseek.com/api-docs/) 的轻量级 Java 客户端。\n下面是一个简单的代码片段，展示了如何以编程方式使用 API：\nDeepSeekApi deepSeekApi = new DeepSeekApi(System.getenv(\u0026#34;DEEPSEEK_API_KEY\u0026#34;)); ChatCompletionMessage chatCompletionMessage = new ChatCompletionMessage(\u0026#34;Hello world\u0026#34;, Role.USER); // Sync request ResponseEntity\u0026lt;ChatCompletion\u0026gt; response = deepSeekApi.chatCompletionEntity( new ChatCompletionRequest(List.of(chatCompletionMessage), DeepSeekApi.ChatModel.DEEPSEEK_CHAT.getValue(), 0.7f, false)); // Streaming request Flux\u0026lt;ChatCompletionChunk\u0026gt; streamResponse = deepSeekApi.chatCompletionStream( new ChatCompletionRequest(List.of(chatCompletionMessage), DeepSeekApi.ChatModel.DEEPSEEK_CHAT.getValue(), 0.7f, true)); 请关注 [ DeepSeekApi.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-deepseek/src/main/java/org/springframework/ai/deepseek/api/[DeepSeekApi.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-deepseek/src/main/java/org/springframework/ai/deepseek/api/DeepSeekApi.java)) 的 JavaDoc 以获取更多信息。\nDeepSeekApi 示例 # DeepSeekApiIT.java 测试提供了一些如何使用轻量级库的一般示例。 "},{"id":52,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B-api/oracle-%E4%BA%91%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD-oci-genai-%E5%B5%8C%E5%85%A5/","title":"Oracle 云基础设施 (OCI) GenAI 嵌入","section":"嵌入模型 API","content":" Oracle 云基础设施 (OCI) GenAI 嵌入 # [ OCI GenAI 服务]( https://www.oracle.com/artificial-intelligence/generative-ai/generative-ai-service/)提供按需模型或专用 AI 集群的文本嵌入。\n[ OCI 嵌入模型页面]( https://docs.oracle.com/en-us/iaas/Content/generative-ai/embed-models.htm)和 [ OCI 文本嵌入页面]( https://docs.oracle.com/en-us/iaas/Content/generative-ai/use-playground-embed.htm)提供了有关在 OCI 上使用和托管嵌入模型的详细信息。\n先决条件 # 添加存储库和 BOM # Spring AI 的构件已发布在 Maven Central 和 Spring Snapshot 仓库中。请参阅 [ “构件仓库”](../../getting-started.html#artifact-repositories) 部分，将这些仓库添加到您的构建系统中。\n为了帮助管理依赖项，Spring AI 提供了 BOM（物料清单），以确保在整个项目中使用一致版本的 Spring AI。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # Spring AI 为 OCI GenAI 嵌入客户端提供了 Spring Boot 自动配置功能。要启用此功能，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-oci-genai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-oci-genai\u0026#39; } 嵌入属性 # 前缀 spring.ai.oci.genai 是配置与 OCI GenAI 的连接的属性前缀。\n前缀 spring.ai.oci.genai.embedding 是配置 OCI GenAI 的 EmbeddingModel 实现的属性前缀\n运行时选项 # OCIEmbeddingOptions 提供嵌入请求的配置信息。OCIEmbeddingOptions 提供了一个构建 OCIEmbeddingOptions 来创建这些选项。\n在启动时，使用 OCIEmbeddingOptions 构造函数设置所有嵌入请求的默认选项。在运行时，您可以通过将 OCIEmbeddingOptions 实例传递给 EmbeddingRequest 请求来覆盖默认选项。\n例如，要覆盖特定请求的默认模型名称：\nEmbeddingResponse embeddingResponse = embeddingModel.call( new EmbeddingRequest(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;), OCIEmbeddingOptions.builder() .model(\u0026#34;my-other-embedding-model\u0026#34;) .build() )); 示例代码 # 这将创建一个 EmbeddingModel 实现，您可以将其注入到您的类中。这是一个使用 EmbeddingModel 实现的简单 @Controller 类的示例。\nspring.ai.oci.genai.embedding.model=\u0026lt;your model\u0026gt; spring.ai.oci.genai.embedding.compartment=\u0026lt;your model compartment\u0026gt; @RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(\u0026#34;/ai/embedding\u0026#34;) public Map embed(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { EmbeddingResponse embeddingResponse = this.embeddingModel.embedForResponse(List.of(message)); return Map.of(\u0026#34;embedding\u0026#34;, embeddingResponse); } } 手动配置 # 如果您不想使用 Spring Boot 自动配置，则可以在应用程序中手动配置 OCIEmbeddingModel 。为此，请将 spring-oci-genai-openai 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-oci-genai-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-oci-genai-openai\u0026#39; } 接下来，创建一个 OCIEmbeddingModel 实例并使用它来计算两个输入文本之间的相似度：\nfinal String EMBEDDING_MODEL = \u0026#34;cohere.embed-english-light-v2.0\u0026#34;; final String CONFIG_FILE = Paths.get(System.getProperty(\u0026#34;user.home\u0026#34;), \u0026#34;.oci\u0026#34;, \u0026#34;config\u0026#34;).toString(); final String PROFILE = \u0026#34;DEFAULT\u0026#34;; final String REGION = \u0026#34;us-chicago-1\u0026#34;; final String COMPARTMENT_ID = System.getenv(\u0026#34;OCI_COMPARTMENT_ID\u0026#34;); var authProvider = new ConfigFileAuthenticationDetailsProvider( this.CONFIG_FILE, this.PROFILE); var aiClient = GenerativeAiInferenceClient.builder() .region(Region.valueOf(this.REGION)) .build(this.authProvider); var options = OCIEmbeddingOptions.builder() .model(this.EMBEDDING_MODEL) .compartment(this.COMPARTMENT_ID) .servingMode(\u0026#34;on-demand\u0026#34;) .build(); var embeddingModel = new OCIEmbeddingModel(this.aiClient, this.options); List\u0026lt;Double\u0026gt; embedding = this.embeddingModel.embed(new Document(\u0026#34;How many provinces are in Canada?\u0026#34;)); "},{"id":53,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E5%9B%BE%E5%83%8F%E6%A8%A1%E5%9E%8B-api/%E5%8D%83%E5%B8%86%E5%BD%B1%E5%83%8F/","title":"千帆影像","section":"图像模型 API","content":" 千帆影像 # 此功能已移至 Spring AI 社区存储库。\n请访问 [ github.com/spring-ai-community/qianfan](https:// github.com/spring-ai-community/qianfan) 获取最新版本。\n"},{"id":54,"href":"/docs/%E5%8D%87%E7%BA%A7%E8%AF%B4%E6%98%8E/","title":"升级说明","section":"Docs","content":" 升级说明 # 升级到 1.0.0-SNAPSHOT # 概述 # 1.0.0-SNAPSHOT 版本对构件 ID、软件包名称和模块结构进行了重大更改。本节提供有关使用 SNAPSHOT 版本的具体指导。\n添加快照存储库 # 要使用 1.0.0-SNAPSHOT 版本，您需要将快照存储库添加到构建文件中。有关详细说明，请参阅入门指南中的 [ “快照 - 添加快照存储库”](getting-started.html#snapshots-add-snapshot-repositories) 部分。\n更新依赖管理 # 在构建配置中，将 Spring AI BOM 版本更新为 1.0.0-SNAPSHOT 。有关配置依赖项管理的详细说明，请参阅入门指南中的 [ “依赖项管理”](getting-started.html#dependency-management) 部分。\n工件 ID、包和模块变更 # 1.0.0-SNAPSHOT 包括对工件 ID、包名称和模块结构的更改。\n详情请参考：- [ 通用神器 ID 变更](#common-artifact-id-changes)\n[ 常见软件包变更](#common-package-changes) [ 通用模块结构](#common-module-structure) 升级到 1.0.0-RC1 # 您可以使用 OpenRewrite 配方自动升级到 1.0.0-RC1。此配方有助于应用此版本所需的许多代码更改。您可以在 [ Arconia Spring AI 迁移]( https://github.com/arconia-io/arconia-migrations/blob/main/docs/spring-ai.md)中找到配方和使用说明。\n重大变化 # 聊天客户端和顾问 # 影响最终用户代码的主要变化是：\n在 VectorStoreChatMemoryAdvisor 中： 常量 CHAT_MEMORY_RETRIEVE_SIZE_KEY 已重命名为 TOP_K 。 常量 DEFAULT_CHAT_MEMORY_RESPONSE_SIZE （值：100）已重命名为 DEFAULT_TOP_K ，其新默认值为 20。 常量 CHAT_MEMORY_RETRIEVE_SIZE_KEY 已重命名为 TOP_K 。 常量 DEFAULT_CHAT_MEMORY_RESPONSE_SIZE （值：100）已重命名为 DEFAULT_TOP_K ，其新默认值为 20。 常量 CHAT_MEMORY_CONVERSATION_ID_KEY 已重命名为 CONVERSATION_ID ，并从 AbstractChatMemoryAdvisor 移至 ChatMemory 接口。请更新您的导入以使用 org.springframework.ai.chat.memory.ChatMemory.CONVERSATION_ID 。 顾问中的自包含模板 # 执行提示增强的内置顾问已更新为使用自包含模板。目标是使每个顾问能够执行模板操作，而不会影响或受其他顾问中的模板和提示决策的影响。\n如果您为以下顾问提供自定义模板，则需要更新它们以确保包含所有预期的占位符。\nQuestionAnswerAdvisor 需要一个带有以下占位符的模板（查看更多详细信息 ）： 用于接收用户问题的 query 占位符。 一个 question_answer_context 占位符来接收检索到的上下文。 用于接收用户问题的 query 占位符。 一个 question_answer_context 占位符来接收检索到的上下文。 PromptChatMemoryAdvisor 需要一个带有以下占位符的模板（查看更多详细信息 ）： 接收原始系统消息的 instructions 占位符。 一个 memory 占位符，用于接收检索到的对话内存。 接收原始系统消息的 instructions 占位符。 一个 memory 占位符，用于接收检索到的对话内存。 VectorStoreChatMemoryAdvisor 需要一个带有以下占位符的模板（查看更多详细信息 ）： 接收原始系统消息的 instructions 占位符。 long_term_memory 占位符用于接收检索到的对话记忆。 接收原始系统消息的 instructions 占位符。 long_term_memory 占位符用于接收检索到的对话记忆。 可观察性 # 重构内容观察以使用日志记录而不是跟踪（ ca843e8 ） 用日志处理程序替换内容观察过滤器 重命名配置属性以更好地反映其用途： 为跟踪感知日志记录添加了 TracingAwareLoggingObservationHandler 已将 micrometer-tracing-bridge-otel 替换为 micrometer-tracing 删除基于事件的跟踪，转而采用直接记录 消除了对 OTel SDK 的直接依赖 将观察属性中的 includePrompt 重命名为 logPrompt （在 ChatClientBuilderProperties 、 ChatObservationProperties 和 ImageObservationProperties 中） 用日志处理程序替换内容观察过滤器 重命名配置属性以更好地反映其用途： 为跟踪感知日志记录添加了 TracingAwareLoggingObservationHandler 已将 micrometer-tracing-bridge-otel 替换为 micrometer-tracing 删除基于事件的跟踪，转而采用直接记录 消除了对 OTel SDK 的直接依赖 将观察属性中的 includePrompt 重命名为 logPrompt （在 ChatClientBuilderProperties 、 ChatObservationProperties 和 ImageObservationProperties 中） 聊天内存存储库模块和自动配置重命名 # 我们通过在整个代码库中添加存储库后缀，标准化了聊天记忆组件的命名模式。此更改会影响 Cassandra、JDBC 和 Neo4j 实现，从而影响到工件 ID、Java 包名称和类名称，以方便区分。\n工件 ID # 所有与内存相关的工件现在都遵循一致的模式：\nspring-ai-model-chat-memory- → spring-ai-model-chat-memory-repository- spring-ai-autoconfigure-model-chat-memory- → spring-ai-autoconfigure-model-chat-memory-repository- spring-ai-starter-model-chat-memory- → spring-ai-starter-model-chat-memory-repository- Java 包 # 包路径现在包括 .repository. 段 例如： org.springframework.ai.chat.memory.jdbc → org.springframework.ai.chat.memory.repository.jdbc 配置类 # 主自动配置类现在使用 Repository 后缀 例如： JdbcChatMemoryAutoConfiguration → JdbcChatMemoryRepositoryAutoConfiguration 特性 # 配置属性已从 spring.ai.chat.memory.…​ 重命名为 spring.ai.chat.memory.repository.…​ 需要迁移：\n更新您的 Maven/Gradle 依赖项以使用新的工件 ID。 更新使用旧包或类名的任何导入、类引用或配置。 消息聚合器重构 # 变化 # MessageAggregator 类已从 spring-ai-client-chat 模块中的 org.springframework.ai.chat.model 包移至 spring-ai-model 模块（相同的包名） 已从 MessageAggregator 中移除了 aggregateChatClientResponse 方法，并将其移至 org.springframework.ai.chat.client 包中的新类 ChatClientMessageAggregator 迁移指南 # 如果您直接使用 MessageAggregator 中的 aggregateChatClientResponse 方法，则需要使用新的 ChatClientMessageAggregator 类：\n// Before new MessageAggregator().aggregateChatClientResponse(chatClientResponses, aggregationHandler); // After new ChatClientMessageAggregator().aggregateChatClientResponse(chatClientResponses, aggregationHandler); 不要忘记添加适当的导入：\nimport org.springframework.ai.chat.client.ChatClientMessageAggregator; 沃森 # Watson AI 模型已被移除，因为它基于较旧的文本生成模型，而该模型被认为已经过时，因为有新的聊天生成模型可用。希望 Watson 能够在 Spring AI 的未来版本中重新出现。\nMoonShot 和 QianFan # Moonshot 和 Qianfan 已被移除，因为它们在中国境外无法访问。它们已被移至 Spring AI 社区代码库。\n移除向量存储 # 删除了 HanaDB 矢量存储自动配置（ f3b4624 ） 内存管理 # 删除了 CassandraChatMemory 实现（ 11e3c8f ） 简化了聊天内存顾问层次结构并删除了已弃用的 API（ 848a3fd ） 删除了 JdbcChatMemory 中的弃用内容 ( 356a68f ) 重构聊天内存存储库工件以提高清晰度（ 2d517ee ） 重构聊天内存存储库自动配置和 Spring Boot 启动器以提高清晰度（ f6dba1b ） 消息和模板 API # 删除了已弃用的 UserMessage 构造函数 ( 06edee4 ) 删除了已弃用的 PromptTemplate 构造函数 ( 722c77e ) 从 Media 中删除了已弃用的方法 ( 228ef10 ) 重构 StTemplateRenderer：将 supportStFunctions 重命名为 validStFunctions ( 0e15197 ) 移动 TemplateRender 接口后将其剩余的内容删除（ 52675d8 ） 其他客户端 API 变更 # 删除了 ChatClient 和 Advisors 中的弃用内容 ( 4fe74d8 ) 删除了 OllamaApi 和 AnthropicApi 中的弃用部分 ( 46be898 ) 软件包结构变化 # 删除了 spring-ai-model ( ebfa5b9 ) 中的包间依赖循环 将 MessageAggregator 移至 spring-ai-model 模块（ 54e5c07 ） 依赖项 # 删除了 spring-ai-openai 中未使用的 json-path 依赖项（ 9de13d1 ） 行为变化 # 为 Azure OpenAI 添加了 Entra ID 身份管理，并具有干净的自动配置（ 3dc86d3 ） 常规清理 # 删除了所有弃用的代码（ 76bee8c ）和（ b6ce7f3 ） 升级到 1.0.0-M8 # 您可以使用 OpenRewrite 配方自动升级到 1.0.0-M8。此配方有助于应用此版本所需的许多代码更改。您可以在 [ Arconia Spring AI 迁移]( https://github.com/arconia-io/arconia-migrations/blob/main/docs/spring-ai.md)中找到配方和使用说明。\n重大变化 # 从 Spring AI 1.0 M7 升级到 1.0 M8 时，之前注册过工具回调的用户会遇到重大变更，导致工具调用功能静默失败。这尤其会影响使用已弃用的 tools() 方法的代码。\n例子 # 以下是在 M7 中可以运行但在 M8 中不再按预期运行的代码示例：\n// This worked in M7 but silently fails in M8 ChatClient chatClient = new OpenAiChatClient(api) .tools(List.of( new Tool(\u0026#34;get_current_weather\u0026#34;, \u0026#34;Get the current weather in a given location\u0026#34;, new ToolSpecification.ToolParameter(\u0026#34;location\u0026#34;, \u0026#34;The city and state, e.g. San Francisco, CA\u0026#34;, true)) )) .toolCallbacks(List.of( new ToolCallback(\u0026#34;get_current_weather\u0026#34;, (toolName, params) -\u0026gt; { // Weather retrieval logic return Map.of(\u0026#34;temperature\u0026#34;, 72, \u0026#34;unit\u0026#34;, \u0026#34;fahrenheit\u0026#34;, \u0026#34;description\u0026#34;, \u0026#34;Sunny\u0026#34;); }) )); 解决方案 # 解决方案是使用 toolSpecifications() 方法，而不是已弃用的 tools() 方法：\n// This works in M8 ChatClient chatClient = new OpenAiChatClient(api) .toolSpecifications(List.of( new Tool(\u0026#34;get_current_weather\u0026#34;, \u0026#34;Get the current weather in a given location\u0026#34;, new ToolSpecification.ToolParameter(\u0026#34;location\u0026#34;, \u0026#34;The city and state, e.g. San Francisco, CA\u0026#34;, true)) )) .toolCallbacks(List.of( new ToolCallback(\u0026#34;get_current_weather\u0026#34;, (toolName, params) -\u0026gt; { // Weather retrieval logic return Map.of(\u0026#34;temperature\u0026#34;, 72, \u0026#34;unit\u0026#34;, \u0026#34;fahrenheit\u0026#34;, \u0026#34;description\u0026#34;, \u0026#34;Sunny\u0026#34;); }) )); 删除的实现和 API # 内存管理 # 删除了 CassandraChatMemory 实现（ 11e3c8f ） 简化了聊天内存顾问层次结构并删除了已弃用的 API（ 848a3fd ） 删除了 JdbcChatMemory 中的弃用内容 ( 356a68f ) 重构聊天内存存储库工件以提高清晰度（ 2d517ee ） 重构聊天内存存储库自动配置和 Spring Boot 启动器以提高清晰度（ f6dba1b ） 客户端 API # 删除了 ChatClient 和 Advisors 中的弃用内容 ( 4fe74d8 ) 聊天客户端工具调用的重大变更（ 5b7849d ） 删除了 OllamaApi 和 AnthropicApi 中的弃用部分 ( 46be898 ) 消息和模板 API # 删除了已弃用的 UserMessage 构造函数 ( 06edee4 ) 删除了已弃用的 PromptTemplate 构造函数 ( 722c77e ) 从 Media 中删除了已弃用的方法 ( 228ef10 ) 重构 StTemplateRenderer：将 supportStFunctions 重命名为 validStFunctions ( 0e15197 ) 移动 TemplateRender 接口后将其剩余的内容删除（ 52675d8 ） 模型实现 # 删除了 Watson 文本生成模型 ( 9e71b16 ) 删除了千帆代码（ bfcaad7 ） 删除了 HanaDB 矢量存储自动配置（ f3b4624 ） 从 OpenAiApi 中删除了 deepseek 选项 ( 59b36d1 ) 软件包结构变化 # 删除了 spring-ai-model ( ebfa5b9 ) 中的包间依赖循环 将 MessageAggregator 移至 spring-ai-model 模块（ 54e5c07 ） 依赖项 # 删除了 spring-ai-openai 中未使用的 json-path 依赖项（ 9de13d1 ） 行为变化 # 可观察性 # 重构内容观察以使用日志记录而不是跟踪（ ca843e8 ）\n用日志处理程序替换内容观察过滤器 重命名配置属性以更好地反映其用途： 为跟踪感知日志记录添加了 TracingAwareLoggingObservationHandler 已将 micrometer-tracing-bridge-otel 替换为 micrometer-tracing 删除基于事件的跟踪，转而采用直接记录 消除了对 OTel SDK 的直接依赖 将观察属性中的 includePrompt 重命名为 logPrompt （在 ChatClientBuilderProperties 、 ChatObservationProperties 和 ImageObservationProperties 中） 用日志处理程序替换内容观察过滤器\n重命名配置属性以更好地反映其用途：\n为跟踪感知日志记录添加了 TracingAwareLoggingObservationHandler\n已将 micrometer-tracing-bridge-otel 替换为 micrometer-tracing\n删除基于事件的跟踪，转而采用直接记录\n消除了对 OTel SDK 的直接依赖\n将观察属性中的 includePrompt 重命名为 logPrompt （在 ChatClientBuilderProperties 、 ChatObservationProperties 和 ImageObservationProperties 中）\n为 Azure OpenAI 添加了 Entra ID 身份管理，并具有干净的自动配置（ 3dc86d3 ）\n常规清理 # 删除了 1.0.0-M8 中的所有弃用内容 ( 76bee8c ) 常规弃用清理（ b6ce7f3 ） 升级到 1.0.0-M7 # 变更概述 # Spring AI 1.0.0-M7 是 RC1 和 GA 版本之前的最后一个里程碑版本。它对工件 ID、包名称和模块结构进行了一些重要更改，这些更改将在最终版本中保留。\n工件 ID、包和模块变更 # 1.0.0-M7 包含与 1.0.0-SNAPSHOT 相同的结构变化。\n详情请参考：- [ 通用神器 ID 变更](#common-artifact-id-changes)\n[ 常见软件包变更](#common-package-changes) [ 通用模块结构](#common-module-structure) MCP Java SDK 升级至 0.9.0 # Spring AI 1.0.0-M7 现使用 MCP Java SDK 0.9.0 版本，该版本与之前的版本相比有显著变化。如果您在应用程序中使用 MCP，则需要更新代码以适应这些变化。\n主要变化包括：\n接口重命名 # DefaultMcpSession → McpClientSession 或 McpServerSession 所有 *Registration 类别 → *Specification 类别 服务器创建更改 # 使用 McpServerTransportProvider 代替 ServerMcpTransport // Before ServerMcpTransport transport = new WebFluxSseServerTransport(objectMapper, \u0026#34;/mcp/message\u0026#34;); var server = McpServer.sync(transport) .serverInfo(\u0026#34;my-server\u0026#34;, \u0026#34;1.0.0\u0026#34;) .build(); // After McpServerTransportProvider transportProvider = new WebFluxSseServerTransportProvider(objectMapper, \u0026#34;/mcp/message\u0026#34;); var server = McpServer.sync(transportProvider) .serverInfo(\u0026#34;my-server\u0026#34;, \u0026#34;1.0.0\u0026#34;) .build(); 处理程序签名变更 # 所有处理程序现在都接收 exchange 参数作为其第一个参数：\n// Before .tool(calculatorTool, args -\u0026gt; new CallToolResult(\u0026#34;Result: \u0026#34; + calculate(args))) // After .tool(calculatorTool, (exchange, args) -\u0026gt; new CallToolResult(\u0026#34;Result: \u0026#34; + calculate(args))) 通过交易所与客户互动 # 以前在服务器上可用的方法现在可以通过交换对象访问：\n// Before ClientCapabilities capabilities = server.getClientCapabilities(); CreateMessageResult result = server.createMessage(new CreateMessageRequest(...)); // After ClientCapabilities capabilities = exchange.getClientCapabilities(); CreateMessageResult result = exchange.createMessage(new CreateMessageRequest(...)); Roots 变更处理程序 # // Before .rootsChangeConsumers(List.of( roots -\u0026gt; System.out.println(\u0026#34;Roots changed: \u0026#34; + roots) )) // After .rootsChangeHandlers(List.of( (exchange, roots) -\u0026gt; System.out.println(\u0026#34;Roots changed: \u0026#34; + roots) )) 有关迁移 MCP 代码的完整指南，请参阅 [ MCP 迁移指南]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-docs/src/main/antora/modules/ROOT/pages/mcp-migration.adoc) 。\n启用/禁用模型自动配置 # 先前用于启用/禁用模型自动配置的配置属性已被删除：\n默认情况下，如果在类路径中找到模型提供程序（例如 OpenAI、Ollama），则会启用其针对相关模型类型（聊天、嵌入等）的相应自动配置。如果同一模型类型存在多个提供程序（例如， spring-ai-openai-spring-boot-starter 和 spring-ai-ollama-spring-boot-starter ），您可以使用以下属性来选择应启用哪个提供程序的自动配置，从而有效地禁用该特定模型类型的其他提供程序。\n要完全禁用特定模型类型的自动配置，即使只存在一个提供程序，也请将相应的属性设置为与类路径上的任何提供程序都不匹配的值（例如， none 或 disabled ）。\n您可以参考 [[SpringAIModels](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/model/SpringAIModels.java)](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/model/[SpringAIModels](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/model/SpringAIModels.java).java) 枚举以获取知名提供程序值的列表。\n利用人工智能实现自动升级 # 您可以使用 Claude Code CLI 工具通过提供的提示自动将升级过程升级到 1.0.0-M7：\n跨版本的常见变更 # 神器 ID 变更 # Spring AI 启动器工件的命名模式已更改。您需要根据以下模式更新依赖项：\n模型启动器： spring-ai-{model}-spring-boot-starter → spring-ai-starter-model-{model} 矢量商店启动器： spring-ai-{store}-store-spring-boot-starter → spring-ai-starter-vector-store-{store} MCP 启动器： spring-ai-mcp-{type}-spring-boot-starter → spring-ai-starter-mcp-{type} 示例 # Spring AI 自动配置工件的变更 # Spring AI 自动配置已从单个整体式构件更改为每个模型、向量存储和其他组件的单独自动配置构件。此更改旨在最大限度地减少不同版本依赖库（例如 Google Protocol Buffers、Google RPC 等）冲突的影响。通过将自动配置分离为特定于组件的构件，您可以避免引入不必要的依赖项，并降低应用程序中发生版本冲突的风险。\n原始的整体工件不再可用：\n\u0026lt;!-- NO LONGER AVAILABLE --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-spring-boot-autoconfigure\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${project.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 相反，每个组件现在都有自己的自动配置工件，遵循以下模式：\n模型自动配置： spring-ai-autoconfigure-model-{model} 矢量存储自动配置： spring-ai-autoconfigure-vector-store-{store} MCP 自动配置： spring-ai-autoconfigure-mcp-{type} 新的自动配置工件示例 # 软件包名称变更 # 您的 IDE 应该协助重构到新的包位置。\nKeywordMetadataEnricher 和 SummaryMetadataEnricher 已从 org.springframework.ai.transformer 移至 org.springframework.ai.chat.transformer 。 Content 、 MediaContent 和 Media 已从 org.springframework.ai.model 移至 org.springframework.ai.content 。 模块结构 # 该项目的模块和构件结构经历了重大变化。之前， spring-ai-core 包含所有核心接口，但现在已拆分为专门的领域模块，以减少应用程序中不必要的依赖。\n基础模块，不依赖其他 Spring AI 模块。包含：- 核心领域模型（ Document 、 TextSplitter ）- JSON 实用程序和资源处理 - 结构化日志记录和可观察性支持\n提供 AI 能力抽象： - ChatModel 、 EmbeddingModel 和 ImageModel 等接口\n消息类型和提示模板 函数调用框架（ ToolDefinition 、 ToolCallback ） - 内容过滤和观察支持 spring-ai-矢量存储 # 统一向量数据库抽象： - 用于相似性搜索的 VectorStore 接口 - 使用类似 SQL 表达式的高级过滤 - 用于内存使用的 SimpleVectorStore - 对嵌入的批处理支持\nspring-ai-客户端聊天 # 高级对话式 AI API： - ChatClient 接口 - 通过 ChatMemory 实现对话持久化\n使用 OutputConverter 进行响应转换 基于顾问的拦截 同步和反应流支持 桥梁与 RAG 的矢量存储聊天： - QuestionAnswerAdvisor ：将上下文注入提示 - VectorStoreChatMemoryAdvisor ：存储/检索对话历史记录\nspring-ai-model-聊天-内存-cassandra # Apache Cassandra ChatMemory 持久性： - CassandraChatMemory 实现 - 使用 Cassandra 的 QueryBuilder 的类型安全 CQL ==== spring-ai-model-chat-memory-neo4j\nNeo4j 图形数据库持久化聊天对话。\n春艾拉格 # 检索增强生成的综合框架：- RAG 管道的模块化架构 - RetrievalAugmentationAdvisor 作为主要入口点 - 具有可组合组件的函数式编程原理\n依赖结构 # 依赖层次可以概括为：\nspring-ai-commons （基金会） spring-ai-model （依赖于 commons） spring-ai-vector-store 和 spring-ai-client-chat （均依赖于模型） spring-ai-advisors-vector-store 和 spring-ai-rag （依赖于客户端聊天和矢量存储） spring-ai-model-chat-memory-* 模块（依赖于客户端聊天） ToolContext 更改 # ToolContext 类已得到增强，支持显式和隐式工具解析。工具现在可以：\n从 1.0.0-M7 开始，仅当在提示中明确请求或在调用中明确包含工具时，工具才会包含在对模型的调用中。\n此外， ToolContext 类现已被标记为 final，无法再进行扩展。它原本不应被子类化。您可以在实例化 ToolContext 时以 Map\u0026lt;String, Object\u0026gt; 的形式添加所需的所有上下文数据。更多信息，请参阅 文档。\n升级到 1.0.0-M6 # Usage 接口和 DefaultUsage 实现的变更 # Usage 接口及其默认实现 DefaultUsage 经历了以下变化：\n所需行动 # 将所有对 getGenerationTokens() 的调用替换为 getCompletionTokens() 更新 DefaultUsage 构造函数调用： JSON Ser/Deser 更改 # 虽然 M6 保留了 generationTokens 字段 JSON 反序列化的向后兼容性，但该字段将在 M7 中被移除。所有使用旧字段名的持久化 JSON 文档都应更新为使用 completionTokens 。\n新 JSON 格式的示例：\n{ \u0026#34;promptTokens\u0026#34;: 100, \u0026#34;completionTokens\u0026#34;: 50, \u0026#34;totalTokens\u0026#34;: 150 } 工具调用的 FunctionCallingOptions 用法的变更 # 每个 ChatModel 实例在构造时都接受一个可选的 ChatOptions 或 FunctionCallingOptions 实例，可用于配置用于调用模型的默认工具。\n1.0.0-M6 之前：\n通过默认 FunctionCallingOptions 实例的 functions() 方法传递的任何工具都包含在从该 ChatModel 实例对模型的每次调用中，可能会被运行时选项覆盖。 通过默认 FunctionCallingOptions 实例的 functionCallbacks() 方法传递的任何工具仅可用于运行时动态解析（请参阅工具解析 ），但除非明确请求，否则永远不会包含在对模型的任何调用中。 启动 1.0.0-M6：\n通过 functions() 方法或默认 FunctionCallingOptions 的 functionCallbacks() 传递的任何工具 实例现在以相同的方式处理：它会包含在该 ChatModel 实例对模型的每次调用中，并可能被运行时选项覆盖。这样，在模型调用中包含工具的方式就保持一致，并避免了由于 functionCallbacks() 和所有其他选项之间的行为差​​异而导致的混淆。 如果您希望使工具可用于运行时动态解析，并且仅在明确请求时将其包含在模型的聊天请求中，则可以使用[ 工具解析](api/tools.html#_tool_resolution)中描述的策略之一。\n删除已弃用的 Amazon Bedrock 聊天模型 # 从 1.0.0-M6 开始，Spring AI 已过渡到使用 Amazon Bedrock 的 Converse API 来实现 Spring AI 中的所有聊天对话。除 Cohere 和 Titan 的 Embedding 模型外，所有 Amazon Bedrock Chat 模型均已移除。\n使用 Spring Boot 3.4.2 进行依赖管理的变更 # Spring AI 更新后使用 Spring Boot 3.4.2 进行依赖管理。您可以参考[ 此处]( https://github.com/spring-projects/spring-boot/blob/v3.4.2/spring-boot-project/spring-boot-dependencies/build.gradle)了解 Spring Boot 3.4.2 管理的依赖项\n所需行动 # 如果您要升级到 Spring Boot 3.4.2，请务必参考此文档，了解配置 REST 客户端所需的更改。需要注意的是，如果您的类路径中没有 HTTP 客户端库，这可能会导致使用 JdkClientHttpRequestFactory ，而之前应该使用 SimpleClientHttpRequestFactory 。要切换到使用 SimpleClientHttpRequestFactory ，您需要设置 spring.http.client.factory=simple 。 如果您使用的是不同版本的 Spring Boot（例如 Spring Boot 3.3.x）并且需要特定版本的依赖项，则可以在构建配置中覆盖它。 矢量存储 API 变更 # 在 1.0.0-M6 版本中， VectorStore 接口中的 delete 方法已修改为 void 操作，而不是返回 Optional\u0026lt;Boolean\u0026gt; 。如果您的代码之前检查了 delete 操作的返回值，则需要移除此检查。现在，如果删除失败，该操作会抛出异常，从而提供更直接的错误处理。\n1.0.0-M6 之前： # Optional\u0026lt;Boolean\u0026gt; result = vectorStore.delete(ids); if (result.isPresent() \u0026amp;\u0026amp; result.get()) { // handle successful deletion } 在 1.0.0-M6 及更高版本中： # vectorStore.delete(ids); // deletion successful if no exception is thrown 升级到 1.0.0.M5 # 为了保持一致性，向量构建器已被重构。 当前 VectorStore 实现构造函数已被弃用，请使用构建器模式。 VectorStore 实现包已移至唯一的包名称，以避免跨工件发生冲突。例如， org.springframework.ai.vectorstore 移至 org.springframework.ai.pgvector.vectorstore 。 升级到 1.0.0.RC3 # 便携式聊天选项（ frequencyPenalty 、 presencePenalty 、 temperature 、 topP ）的类型已从 Float 更改为 Double 。 升级到 1.0.0.M2 # Chroma 矢量存储的配置前缀已从 spring.ai.vectorstore.chroma.store 更改为 spring.ai.vectorstore.chroma ，以与其他矢量存储的命名约定保持一致。 能够 initialize-schema 属性的默认值现已设置为 false 。这意味着，如果希望在应用程序启动时创建模式，则应用程序现在需要明确选择在受支持的向量存储上启用模式初始化。并非所有向量存储都支持此属性。有关更多详细信息，请参阅相应的向量存储文档。以下是当前不支持 initialize-schema 属性的向量存储。 哈娜 松果 威维特 在《侏罗纪世界 2》基岩版中，聊天选项 countPenalty 、 frequencyPenalty 和 presencePenalty 已重命名为 countPenaltyOptions 、 frequencyPenaltyOptions 和 presencePenaltyOptions 。此外，聊天选项 stopSequences 的类型已从 String[] 更改为 List 。 在 Azure OpenAI 中，聊天选项 frequencyPenalty 和 presencePenalty 的类型 已从 Double 更改为 Float ，与所有其他实现一致。 升级到 1.0.0.M1 # 在即将发布 1.0.0 M1 的过程中，我们做出了几项重大变更。抱歉，这是为了更好的体验！\nChatClient 变更 # 此次重大改动在于，将“旧版” ChatClient 功能迁移至 ChatModel 。“新版” ChatClient 现在接受 ChatModel 的实例。此举旨在支持流畅的 API，以类似于 Spring 生态系统中其他客户端类（例如 RestClient 、 WebClient 和 JdbcClient ）的风格创建和执行提示。有关流畅 API 的更多信息，请参阅 JavaDoc，正式的参考文档即将发布。\n我们将“旧”的 `````ModelClient` 重命名为 Model ，并重命名了实现类，例如将 ``ImageClient`` 重命名为 ImageModel。Model实现代表了在 Spring AI API 和底层 AIModel```` API 之间进行转换的可移植层。\n一个新的包 model ，包含接口和基类，支持为任何输入/输出数据类型组合创建 AI 模型客户端。目前，聊天和图像模型包已实现此功能。我们将很快将嵌入包更新到此新模型。\n一种新的“可移植选项”设计模式。我们希望在 ModelCall 中尽可能地实现跨不同聊天 AI 模型的可移植性。它包含一组通用的生成选项，以及特定于模型提供者的选项。我们使用了一种“鸭子类型”方法。模型包中的 ModelOptions 是一个标记接口，指示此类的实现将为模型提供选项。请参阅 ImageOptions ，它是一个子接口，定义了跨所有文本→图像 ImageModel 实现的可移植选项。StabilityAiImageOptions 和 OpenAiImageOptions 则提供了特定于每个模型提供者的选项。所有选项类均通过流畅的 API 构建器创建，所有选项类都可以传递到可移植的 ImageModel API 中。这些选项数据类型用于 ImageModel 实现 StabilityAiImageOptions 自动配置/配置属性。\n神器名称变更 # 重命名的 POM 工件名称： - spring-ai-qdrant → spring-ai-qdrant-store - spring-ai-cassandra → spring-ai-cassandra-store - spring-ai-pinecone → spring-ai-pinecone-store - spring-ai-redis → spring-ai-redis-store - spring-ai-qdrant → spring-ai-qdrant-store - spring-ai-gemfire → spring-ai-gemfire-store - spring-ai-azure-vector-store-spring-boot-starter → spring-ai-azure-store-spring-boot-starter - spring-ai-redis-spring-boot-starter → spring-ai-starter-vector-store-redis\n升级到 0.8.1 # 之前的 spring-ai-vertex-ai 已重命名为 spring-ai-vertex-ai-palm2 并且 spring-ai-vertex-ai-spring-boot-starter 已重命名为 spring-ai-vertex-ai-palm2-spring-boot-starter 。\n因此，您需要将依赖关系从\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-vertex-ai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 到\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-vertex-ai-palm2\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 并且 Palm2 型号的相关 Boot 启动器已从\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-vertex-ai-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 到\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-vertex-ai-palm2-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 重命名课程（2024 年 3 月 1 日） 升级到 0.8.0 # 2024 年 1 月 24 日更新 # 将 prompt 和 messages 以及 metadata 包移动到 org.springframework.ai.chat 的子包中 新功能是文本转图片客户端。类包括 OpenAiImageModel 和 StabilityAiImageModel 。使用方法请参阅集成测试，文档即将发布。 一个新的包 model ，包含接口和基类，支持为任何输入/输出数据类型组合创建 AI 模型客户端。目前，聊天和图像模型包已实现此功能。我们将很快将嵌入包更新到此新模型。 一种新的“可移植选项”设计模式。我们希望在 ModelCall 中尽可能地实现跨不同聊天 AI 模型的可移植性。它包含一组通用的生成选项，以及特定于模型提供者的选项。我们使用了一种“鸭子类型”方法。模型包中的 ModelOptions 是一个标记接口，指示此类的实现将为模型提供选项。请参阅 ImageOptions ，它是一个子接口，定义了跨所有文本→图像 ImageModel 实现的可移植选项。StabilityAiImageOptions 和 OpenAiImageOptions 则提供了特定于每个模型提供者的选项。所有选项类均通过流畅的 API 构建器创建，所有选项类都可以传递到可移植的 ImageModel API 中。这些选项数据类型用于 ImageModel 实现 StabilityAiImageOptions 自动配置/配置属性。 2024 年 1 月 13 日更新 # 以下 OpenAi 自动配置聊天属性已更改\n从 spring.ai.openai.model 到 spring.ai.openai.chat.options.model 。 从 spring.ai.openai.temperature 到 spring.ai.openai.chat.options.temperature 。 查找有关 OpenAi 属性的更新文档： [ docs.spring.io/spring-ai/reference/api/chat/openai-chat.html](https:// docs.spring.io/spring-ai/reference/api/chat/openai-chat.html)\n2023年12月27日更新 # 将 SimplePersistentVectorStore 和 InMemoryVectorStore 合并为 SimpleVectorStore * 将 InMemoryVectorStore 替换为 SimpleVectorStore\n2023年12月20日更新 # 重构 Ollama 客户端及相关类和包名\n将 org.springframework.ai.ollama.client.OllamaClient 替换为 org.springframework.ai.ollama.OllamaModelCall。 OllamaChatClient 方法签名已改变。 将 org.springframework.ai.autoconfigure.ollama.OllamaProperties 重命名为 org.springframework.ai.model.ollama.autoconfigure.OllamaChatProperties ，并将后缀更改为： spring.ai.ollama.chat 。部分属性也进行了修改。 2023年12月19日更新 # 重命名 AiClient 及相关类和包名称\n将 AiClient 重命名为 ChatClient 将 AiResponse 重命名为 ChatResponse 将 AiStreamClient 重命名为 StreamingChatClient 将包 org.sf.ai.client 重命名为 org.sf.ai.chat 重命名工件 ID\n嵌入到 spring-ai-transformers 中的 transformers-embedding 将 Maven 模块从顶级目录和 embedding-clients 端子目录移动到单个 models 目录下。\n2023年12月1日 # 我们正在转换项目的组 ID：\n发件人 ： org.springframework.experimental.ai 至 ： org.springframework.ai 工件仍将托管在快照存储库中，如下所示。\n主分支将迁移至 0.8.0-SNAPSHOT 版本。该版本将在一两周内不稳定。如果您不想使用最新版，请使用 0.7.1-SNAPSHOT 版本。\n您可以像以前一样访问 0.7.1-SNAPSHOT 工件，并且仍然可以访问 0.7.1-SNAPSHOT 文档 。\n0.7.1-SNAPSHOT 依赖项 # "},{"id":55,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/","title":"模型","section":"参考","content":" 模型 # 介绍 # Spring AI API 涵盖了广泛的功能。每个主要功能都有其专门的章节进行详细介绍。为了提供概述，以下是可用的关键功能：\nAI 模型 API # 跨 AI 提供商的可移植 Model API 适用于 Chat 、 Text to Image 、 Audio Transcription 、 Text to Speech 和 Embedding 模型。支持 synchronous 和 stream API 选项。此外，还支持下拉访问模型特定功能。\n支持 OpenAI、微软、亚马逊、谷歌、亚马逊 Bedrock、Hugging Face 等公司的 AI 模型。\n矢量存储 API # 可跨多个提供商的可移植 Vector Store API ，包括新颖的 SQL-like metadata filter API 该 API 也具有可移植性。目前支持 14 个矢量数据库。\n工具调用 API # Spring AI 可以轻松地让 AI 模型以 @Tool 注释方法或 POJO java.util.Function 对象的形式调用您的服务。\n查看 Spring AI [ Tool Calling](tools.html) 文档。\n自动配置 # Spring Boot 自动配置和 AI 模型和矢量存储的启动器。\nETL 数据工程 # 数据工程的 ETL 框架。这为将数据加载到矢量数据库提供了基础，有助于实现检索增强生成模式，使您能够将数据带入 AI 模型并纳入其响应中。\n反馈和贡献 # 该项目的 [ GitHub 讨论]( https://github.com/spring-projects/spring-ai/discussions)是发送反馈的好地方。\n"},{"id":56,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E9%80%82%E5%BA%A6/","title":"适度","section":"模型","content":" 适度 # "},{"id":57,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B-api/docker-%E6%A8%A1%E5%9E%8B%E8%BF%90%E8%A1%8C%E8%80%85%E8%81%8A%E5%A4%A9/","title":"Docker 模型运行者聊天","section":"聊天模型 API","content":" Docker 模型运行者聊天 # [ Docker Model Runner]( https://docs.docker.com/desktop/features/model-runner/) 是一个 AI 推理引擎，提供来自[ 不同提供商]( https://hub.docker.com/u/ai)的多种模型。\nSpring AI 通过重用现有的 [ OpenAI](openai-chat.html) 支持的 ChatClient 与 Docker Model Runner 集成。为此，请将基本 URL 设置为 [[localhost:12434/engines](http://localhost:12434/engines)](http://[localhost:12434/engines](http://localhost:12434/engines)) ，并选择提供的 [ LLM 模型]( https://hub.docker.com/u/ai)之一。\n查看 [ DockerModelRunnerWithOpenAiChatModelIT.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/chat/proxy/[DockerModelRunnerWithOpenAiChatModelIT.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/chat/proxy/DockerModelRunnerWithOpenAiChatModelIT.java)) 测试，了解如何将 Docker Model Runner 与 Spring AI 结合使用的示例。\n先决条件 # 下载适用于 Mac 4.40.0 的 Docker Desktop。 选择以下选项之一来启用模型运行器：\n选项 1：\n启用模型运行器 docker desktop enable model-runner \u0026ndash;tcp 12434 。 将 base-url 设置为 localhost:12434/engines 选项 2：\n启用模型运行器 docker desktop enable model-runner 。 使用 Testcontainers 并设置 base-url 如下： @Container private static final SocatContainer socat = new SocatContainer().withTarget(80, \u0026#34;model-runner.docker.internal\u0026#34;); @Bean public OpenAiApi chatCompletionApi() { var baseUrl = \u0026#34;http://%s:%d/engines\u0026#34;.formatted(socat.getHost(), socat.getMappedPort(80)); return OpenAiApi.builder().baseUrl(baseUrl).apiKey(\u0026#34;test\u0026#34;).build(); } 您可以通过阅读[ 使用 Docker 博客文章在本地运行 LLM]( https://www.docker.com/blog/run-llms-locally/) 来了解有关 Docker Model Runner 的更多信息。\n自动配置 # Spring AI 为 OpenAI Chat Client 提供了 Spring Boot 自动配置功能。要启用此功能，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者将以下内容添加到您的 Gradle build.gradle 构建文件中。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-openai\u0026#39; } 聊天属性 # 重试属性 # 前缀 spring.ai.retry 用作属性前缀，可让您配置 OpenAI 聊天模型的重试机制。\n连接属性 # 前缀 spring.ai.openai 用作允许您连接到 OpenAI 的属性前缀。\n配置属性 # 前缀 spring.ai.openai.chat 是允许您为 OpenAI 配置聊天模型实现的属性前缀。\n运行时选项 # [ OpenAiChatOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/[OpenAiChatOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/OpenAiChatOptions.java)) 提供模型配置，例如要使用的模型、温度、频率惩罚等。\n启动时，可以使用 OpenAiChatModel(api, options) 构造函数或 spring.ai.openai.chat.options.* 属性配置默认选项。\n在运行时，您可以通过向 Prompt 调用添加新的、特定于请求的选项来覆盖默认选项。例如，要覆盖特定请求的默认模型和温度：\nChatResponse response = chatModel.call( new Prompt( \u0026#34;Generate the names of 5 famous pirates.\u0026#34;, OpenAiChatOptions.builder() .model(\u0026#34;ai/gemma3:4B-F16\u0026#34;) .build() )); 函数调用 # Docker Model Runner 在选择支持它的模型时支持工具/函数调用。\n您可以将自定义 Java 函数注册到 ChatModel 中，并让提供的模型智能地选择输出包含参数的 JSON 对象，以调用一个或多个已注册的函数。这是一种将 LLM 功能与外部工具和 API 连接起来的强大技术。\n工具示例 # 下面是一个如何使用 Docker Model Runner 函数调用 Spring AI 的简单示例：\nspring.ai.openai.api-key=test spring.ai.openai.base-url=http://localhost:12434/engines spring.ai.openai.chat.options.model=ai/gemma3:4B-F16 @SpringBootApplication public class DockerModelRunnerLlmApplication { public static void main(String[] args) { SpringApplication.run(DockerModelRunnerLlmApplication.class, args); } @Bean CommandLineRunner runner(ChatClient.Builder chatClientBuilder) { return args -\u0026gt; { var chatClient = chatClientBuilder.build(); var response = chatClient.prompt() .user(\u0026#34;What is the weather in Amsterdam and Paris?\u0026#34;) .functions(\u0026#34;weatherFunction\u0026#34;) // reference by bean name. .call() .content(); System.out.println(response); }; } @Bean @Description(\u0026#34;Get the weather in location\u0026#34;) public Function\u0026lt;WeatherRequest, WeatherResponse\u0026gt; weatherFunction() { return new MockWeatherService(); } public static class MockWeatherService implements Function\u0026lt;WeatherRequest, WeatherResponse\u0026gt; { public record WeatherRequest(String location, String unit) {} public record WeatherResponse(double temp, String unit) {} @Override public WeatherResponse apply(WeatherRequest request) { double temperature = request.location().contains(\u0026#34;Amsterdam\u0026#34;) ? 20 : 25; return new WeatherResponse(temperature, request.unit); } } } 在此示例中，当模型需要天气信息时，它将自动调用 weatherFunction bean，该 bean 可以获取实时天气数据。预期响应为：“阿姆斯特丹的当前气温为 20 摄氏度，巴黎的当前气温为 25 摄氏度。”\n阅读有关 OpenAI [ 函数调用的]( https://docs.spring.io/spring-ai/reference/api/chat/functions/openai-chat-functions.html)更多信息。\n样品控制器 # [ 创建]( https://start.spring.io/)一个新的 Spring Boot 项目并将 spring-ai-starter-model-openai 添加到您的 pom（或 gradle）依赖项中。\n在 src/main/resources 目录下添加一个 application.properties 文件，以启用和配置 OpenAi 聊天模型：\nspring.ai.openai.api-key=test spring.ai.openai.base-url=http://localhost:12434/engines spring.ai.openai.chat.options.model=ai/gemma3:4B-F16 # Docker Model Runner doesn\u0026#39;t support embeddings, so we need to disable them. spring.ai.openai.embedding.enabled=false 这是一个使用聊天模型进行文本生成的简单 @Controller 类的示例。\n@RestController public class ChatController { private final OpenAiChatModel chatModel; @Autowired public ChatController(OpenAiChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, this.chatModel.call(message)); } @GetMapping(\u0026#34;/ai/generateStream\u0026#34;) public Flux\u0026lt;ChatResponse\u0026gt; generateStream(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { Prompt prompt = new Prompt(new UserMessage(message)); return this.chatModel.stream(prompt); } } "},{"id":58,"href":"/docs/%E5%8F%82%E8%80%83/%E7%9F%A2%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/elasticsearch/","title":"Elasticsearch","section":"矢量数据库","content":" Elasticsearch # 本节将引导您设置 Elasticsearch VectorStore 来存储文档嵌入并执行相似性搜索。\n[ Elasticsearch]( https://www.elastic.co/elasticsearch) 是一个基于 Apache Lucene 库的开源搜索和分析引擎。\n先决条件 # 正在运行的 Elasticsearch 实例。以下选项可用：\n自管理 Elasticsearch 弹性云 自动配置 # Spring AI 为 Elasticsearch 矢量存储提供了 Spring Boot 自动配置功能。要启用此功能，请将以下依赖项添加到项目的 Maven pom.xml 或 Gradle build.gradle 构建文件中：\n向量存储实现可以为您初始化必要的模式，但您必须通过在相应的构造函数中指定 initializeSchema 布尔值或在 application.properties 文件中设置 …​initialize-schema=true 来选择启用。或者，您可以选择退出初始化，并使用 Elasticsearch 客户端手动创建索引，如果索引需要高级映射或其他配置，这种方法会很有用。\n请查看向量存储的[ 配置参数](#elasticsearchvector-properties)列表，了解默认值和配置选项。您也可以通过配置 ElasticsearchVectorStoreOptions bean 来设置这些属性。\n此外，您还需要一个已配置的 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) bean。有关更多信息，请参阅 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) 部分。\n现在，您可以将 ElasticsearchVectorStore 自动连接为应用程序中的向量存储。\n@Autowired VectorStore vectorStore; // ... List \u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); // Add the documents to Elasticsearch vectorStore.add(documents); // Retrieve documents similar to a query List\u0026lt;Document\u0026gt; results = this.vectorStore.similaritySearch(SearchRequest.builder().query(\u0026#34;Spring\u0026#34;).topK(5).build()); 配置属性 # 要连接到 Elasticsearch 并使用 ElasticsearchVectorStore ，您需要提供实例的访问详细信息。您可以通过 Spring Boot 的 application.yml 提供简单的配置，\nspring: elasticsearch: uris: \u0026lt;elasticsearch instance URIs\u0026gt; username: \u0026lt;elasticsearch username\u0026gt; password: \u0026lt;elasticsearch password\u0026gt; ai: vectorstore: elasticsearch: initialize-schema: true index-name: custom-index dimensions: 1536 similarity: cosine 以 spring.elasticsearch.* 开头的 Spring Boot 属性用于配置 Elasticsearch 客户端：\n以 spring.ai.vectorstore.elasticsearch.* 开头的属性用于配置 ElasticsearchVectorStore ：\n可以使用以下相似度函数：\ncosine - 默认值，适用于大多数用例。测量向量之间的余弦相似度。 l2_norm - 向量之间的欧氏距离。值越低，相似度越高。 dot_product - 标准化向量的最佳性能（例如，OpenAI 嵌入）。 有关每个内容的更多详细信息，请参阅 [ Elasticsearch 文档]( https://www.elastic.co/guide/en/elasticsearch/reference/master/dense-vector.html#dense-vector-params)中的密集向量。\n元数据过滤 # 您还可以利用 Elasticsearch 的通用、可移植元[ 数据过滤器](../vectordbs.html#metadata-filters) 。\n例如，您可以使用文本表达语言：\nvectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(\u0026#34;author in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; \u0026#39;article_type\u0026#39; == \u0026#39;blog\u0026#39;\u0026#34;).build()); 或者以编程方式使用 Filter.Expression DSL：\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(b.and( b.in(\u0026#34;author\u0026#34;, \u0026#34;john\u0026#34;, \u0026#34;jill\u0026#34;), b.eq(\u0026#34;article_type\u0026#34;, \u0026#34;blog\u0026#34;)).build()).build()); 例如，这个便携式过滤器表达式：\nauthor in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; \u0026#39;article_type\u0026#39; == \u0026#39;blog\u0026#39; 转换为专有的 Elasticsearch 过滤器格式：\n(metadata.author:john OR jill) AND metadata.article_type:blog 手动配置 # 除了使用 Spring Boot 自动配置之外，您还可以手动配置 Elasticsearch 向量存储。为此，您需要将 spring-ai-elasticsearch-store 添加到您的项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-elasticsearch-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-elasticsearch-store\u0026#39; } 创建一个 Elasticsearch RestClient bean。阅读 [ Elasticsearch 文档]( https://www.elastic.co/guide/en/elasticsearch/client/java-api-client/current/java-rest-low-usage-initialization.html) ，了解有关自定义 RestClient 配置的更多详细信息。\n@Bean public RestClient restClient() { return RestClient.builder(new HttpHost(\u0026#34;\u0026lt;host\u0026gt;\u0026#34;, 9200, \u0026#34;http\u0026#34;)) .setDefaultHeaders(new Header[]{ new BasicHeader(\u0026#34;Authorization\u0026#34;, \u0026#34;Basic \u0026lt;encoded username and password\u0026gt;\u0026#34;) }) .build(); } 然后使用构建器模式创建 ElasticsearchVectorStore bean：\n@Bean public VectorStore vectorStore(RestClient restClient, EmbeddingModel embeddingModel) { ElasticsearchVectorStoreOptions options = new ElasticsearchVectorStoreOptions(); options.setIndexName(\u0026#34;custom-index\u0026#34;); // Optional: defaults to \u0026#34;spring-ai-document-index\u0026#34; options.setSimilarity(COSINE); // Optional: defaults to COSINE options.setDimensions(1536); // Optional: defaults to model dimensions or 1536 return ElasticsearchVectorStore.builder(restClient, embeddingModel) .options(options) // Optional: use custom options .initializeSchema(true) // Optional: defaults to false .batchingStrategy(new TokenCountBatchingStrategy()) // Optional: defaults to TokenCountBatchingStrategy .build(); } // This can be any EmbeddingModel implementation @Bean public EmbeddingModel embeddingModel() { return new OpenAiEmbeddingModel(new OpenAiApi(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;))); } 访问 Native Client # Elasticsearch Vector Store 实现通过 getNativeClient() 方法提供对底层原生 Elasticsearch 客户端（ ElasticsearchClient ）的访问：\nElasticsearchVectorStore vectorStore = context.getBean(ElasticsearchVectorStore.class); Optional\u0026lt;ElasticsearchClient\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { ElasticsearchClient client = nativeClient.get(); // Use the native client for Elasticsearch-specific operations } 本机客户端使您可以访问可能无法通过 VectorStore 接口公开的 Elasticsearch 特定功能和操作。\n"},{"id":59,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B-api/ollama-%E5%B5%8C%E5%85%A5/","title":"Ollama 嵌入","section":"嵌入模型 API","content":" Ollama 嵌入 # 使用 [ Ollama]( https://ollama.ai/) ，您可以在本地运行各种 [ AI 模型]( https://ollama.com/search?c=embedding) ，并从中生成嵌入。嵌入是一个浮点数向量（列表）。两个向量之间的距离衡量它们的关联性。距离越小，关联性越高；距离越大，关联性越低。\nOllamaEmbeddingModel 实现利用了 Ollama [ Embeddings API]( https://github.com/ollama/ollama/blob/main/docs/api.md#generate-embeddings) 端点。\n先决条件 # 首先，您需要访问 Ollama 实例。有以下几种选择：\n在本地机器上下载并安装 Ollama 。 通过 Testcontainers 配置并运行 Ollama 。 通过 Kubernetes 服务绑定绑定到 Ollama 实例。 您可以从 [ Ollama 模型库中]( https://ollama.com/search?c=embedding)提取您想要在应用程序中使用的模型：\nollama pull \u0026lt;model-name\u0026gt; 您还可以从数千个免费的 [ GGUF Hugging Face Models]( https://huggingface.co/models?library=gguf\u0026sort=trending) 中挑选任意一个：\nollama pull hf.co/\u0026lt;username\u0026gt;/\u0026lt;model-repository\u0026gt; 或者，您可以启用自动下载任何所需模型的选项： [ 自动拉取模型](#auto-pulling-models) 。\n自动配置 # Spring AI 为 Azure Ollama 嵌入模型提供 Spring Boot 自动配置。要启用它，请将以下依赖项添加到 Maven pom.xml 或 Gradle build.gradle 构建文件中：\n基本属性 # 前缀 spring.ai.ollama 是配置与 Ollama 连接的属性前缀\n以下是初始化 Ollama 集成和[ 自动拉取模型的](#auto-pulling-models)属性。\n嵌入属性 # 前缀 `spring.ai.ollama.embedding.options``` 是配置 Ollama 嵌入模型的属性前缀。它包含 Ollama 请求（高级）参数，例如 model、keep-alive和truncate，以及 Ollama 模型options`` 属性。\n以下是 Ollama 嵌入模型的高级请求参数：\n其余 options 属性基于 [ Ollama 有效参数和值]( https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values)以及 [ Ollama 类型]( https://github.com/ollama/ollama/blob/main/api/types.go) 。默认值基于： [ Ollama 类型]( https://github.com/ollama/ollama/blob/main/api/types.go)默认值 。\n运行时选项 # [ OllamaOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-ollama/src/main/java/org/springframework/ai/ollama/api/[OllamaOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-ollama/src/main/java/org/springframework/ai/ollama/api/OllamaOptions.java)) 提供 Ollama 配置，例如要使用的模型、低级 GPU 和 CPU 调整等。\n也可以使用 spring.ai.ollama.embedding.options 属性来配置默认选项。\n启动时，使用 OllamaEmbeddingModel(OllamaApi ollamaApi, ``OllamaOptions`` defaultOptions) 配置所有嵌入请求的默认选项。运行时，您可以使用 OllamaOptions 实例作为 EmbeddingRequest 的一部分来覆盖默认选项。\n例如，要覆盖特定请求的默认模型名称：\nEmbeddingResponse embeddingResponse = embeddingModel.call( new EmbeddingRequest(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;), OllamaOptions.builder() .model(\u0026#34;Different-Embedding-Model-Deployment-Name\u0026#34;)) .truncates(false) .build()); 自动拉动模型 # Spring AI Ollama 可以在您的 Ollama 实例中不可用时自动拉取模型。此功能对于开发和测试以及将应用程序部署到新环境特别有用。\n拉模型的策略有三种：\nalways （在 PullModelStrategy.ALWAYS 中定义）：始终拉取模型，即使它已经可用。这有助于确保您使用的是最新版本的模型。 when_missing （在 PullModelStrategy.WHEN_MISSING 中定义）：仅当模型不可用时才提取该模型。这可能会导致使用旧版本的模型。 never （在 PullModelStrategy.NEVER 中定义）：从不自动拉动模型。 所有通过配置属性和默认选项定义的模型都可以在启动时自动拉取。您可以使用配置属性配置拉取策略、超时时间和最大重试次数：\nspring: ai: ollama: init: pull-model-strategy: always timeout: 60s max-retries: 1 您可以在启动时初始化其他模型，这对于运行时动态使用的模型很有用：\nspring: ai: ollama: init: pull-model-strategy: always embedding: additional-models: - mxbai-embed-large - nomic-embed-text 如果您希望仅将拉取策略应用于特定类型的模型，则可以从初始化任务中排除嵌入模型：\nspring: ai: ollama: init: pull-model-strategy: always embedding: include: false 此配置将把拉取策略应用于除嵌入模型之外的所有模型。\nHuggingFace 模型 # Ollama 可以立即访问所有 [ GGUF Hugging 人脸]( https://huggingface.co/models?library=gguf\u0026sort=trending)嵌入模型。您可以按名称 ollama pull hf.co/\u0026lt;username\u0026gt;/\u0026lt;model-repository\u0026gt; 拉取以下任意模型，也可以配置自动拉取策略： [ 自动拉取模型](#auto-pulling-models) ：\nspring.ai.ollama.embedding.options.model=hf.co/mixedbread-ai/mxbai-embed-large-v1 spring.ai.ollama.init.pull-model-strategy=always spring.ai.ollama.embedding.options.model ：指定要使用的 Hugging Face GGUF 模型 。 spring.ai.ollama.init.pull-model-strategy=always ：（可选）在启动时启用自动模型拉取。对于生产环境，您应该预先下载模型以避免延迟： ollama pull hf.co/mixedbread-ai/mxbai-embed-large-v1 。 样品控制器 # 这将创建一个 EmbeddingModel 实现，您可以将其注入到您的类中。这是一个使用 EmbeddingModel 实现的简单 @Controller 类的示例。\n@RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(\u0026#34;/ai/embedding\u0026#34;) public Map embed(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { EmbeddingResponse embeddingResponse = this.embeddingModel.embedForResponse(List.of(message)); return Map.of(\u0026#34;embedding\u0026#34;, embeddingResponse); } } 手动配置 # 如果你不使用 Spring Boot，则可以手动配置 OllamaEmbeddingModel 。为此，请将 spring-ai-ollama 依赖项添加到项目的 Maven pom.xml 或 Gradle build.gradle 构建文件中：\n接下来，创建一个 OllamaEmbeddingModel 实例，并使用它使用专用的 chroma/all-minilm-l6-v2-f32 嵌入模型来计算两个输入文本的嵌入：\nvar ollamaApi = OllamaApi.builder().build(); var embeddingModel = new OllamaEmbeddingModel(this.ollamaApi, OllamaOptions.builder() .model(OllamaModel.MISTRAL.id()) .build()); EmbeddingResponse embeddingResponse = this.embeddingModel.call( new EmbeddingRequest(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;), OllamaOptions.builder() .model(\u0026#34;chroma/all-minilm-l6-v2-f32\u0026#34;)) .truncate(false) .build()); OllamaOptions 为所有嵌入请求提供配置信息。\n"},{"id":60,"href":"/docs/%E5%8F%82%E8%80%83/%E8%81%8A%E5%A4%A9%E8%AE%B0%E5%BF%86/","title":"聊天记忆","section":"参考","content":" 聊天记忆 # 大型语言模型 (LLM) 是无状态的，这意味着它们不会保留先前交互的信息。当您希望在多个交互之间维护上下文或状态时，这可能会造成限制。为了解决这个问题，Spring AI 提供了聊天记忆功能，允许您使用 LLM 跨多个交互存储和检索信息。\nChatMemory 抽象允许您实现各种类型的内存以支持不同的用例。消息的底层存储由 ChatMemoryRepository 处理，其唯一职责是存储和检索消息。由 ChatMemory 实现决定保留哪些消息以及何时删除它们。策略示例包括保留最后 N 条消息、将消息保留一段时间或将消息保留到一定的令牌限制。\n在选择记忆类型之前，必须了解聊天记忆和聊天历史之间的区别。\n聊天记忆 。大型语言模型保留并用于在整个对话过程中保持语境感知的信息。 聊天记录 。整个对话历史记录，包括用户和模型之间交换的所有消息。 ChatMemory 抽象旨在管理聊天内存 。它允许您存储和检索与当前对话上下文相关的消息。但是，它并非存储聊天历史记录的最佳选择。如果您需要维护所有交换消息的完整记录，则应考虑使用其他方法，例如依赖 Spring Data 来高效地存储和检索完整的聊天历史记录。\n快速入门 # Spring AI 会自动配置一个 ChatMemory bean，您可以在应用程序中直接使用它。默认情况下，它使用内存存储库 ( InMemoryChatMemoryRepository ) 来存储消息，并使用 MessageWindowChatMemory 实现来管理对话历史记录。如果已配置其他存储库（例如 Cassandra、JDBC 或 Neo4j），Spring AI 将改用该存储库。\n@Autowired ChatMemory chatMemory; 以下部分将进一步描述 Spring AI 中可用的不同内存类型和存储库。\n内存类型 # ChatMemory 抽象允许您实现各种类型的内存，以适应不同的用例。内存类型的选择会显著影响应用程序的性能和行为。本节介绍 Spring AI 提供的内置内存类型及其特性。\n消息窗口聊天记忆 # MessageWindowChatMemory 会维护一个消息窗口，其大小不超过指定的最大限制。当消息数量超过上限时，较旧的消息会被删除，但系统消息会被保留。默认窗口大小为 20 条消息。\nMessageWindowChatMemory memory = MessageWindowChatMemory.builder() .maxMessages(10) .build(); 这是 Spring AI 用于自动配置 ChatMemory bean 的默认消息类型。\n内存存储 # Spring AI 提供了 ChatMemoryRepository 抽象来存储聊天记忆。本节介绍 Spring AI 提供的内置存储库及其使用方法，但您也可以根据需要实现自己的存储库。\n内存存储库 # InMemoryChatMemoryRepository 使用 ConcurrentHashMap 将消息存储在内存中。\n默认情况下，如果尚未配置其他存储库，Spring AI 会自动配置一个 InMemoryChatMemoryRepository 类型的 ChatMemoryRepository bean，您可以在应用程序中直接使用它。\n@Autowired ChatMemoryRepository chatMemoryRepository; 如果您希望手动创建 InMemoryChatMemoryRepository ，您可以按如下方式操作：\nChatMemoryRepository repository = new InMemoryChatMemoryRepository(); JdbcChatMemoryRepository 是一个内置实现，使用 JDBC 将消息存储在关系数据库中。它开箱即用地支持多种数据库，适用于需要持久存储聊天记忆的应用程序。\n首先，向您的项目添加以下依赖项：\nSpring AI 为 JdbcChatMemoryRepository 提供了自动配置，您可以在应用程序中直接使用它。\n@Autowired JdbcChatMemoryRepository chatMemoryRepository; ChatMemory chatMemory = MessageWindowChatMemory.builder() .chatMemoryRepository(chatMemoryRepository) .maxMessages(10) .build(); 如果您希望手动创建 JdbcChatMemoryRepository ，您可以通过提供 JdbcTemplate 实例和 JdbcChatMemoryRepositoryDialect 来实现：\nChatMemoryRepository chatMemoryRepository = JdbcChatMemoryRepository.builder() .jdbcTemplate(jdbcTemplate) .dialect(new PostgresChatMemoryDialect()) .build(); ChatMemory chatMemory = MessageWindowChatMemory.builder() .chatMemoryRepository(chatMemoryRepository) .maxMessages(10) .build(); 支持的数据库和方言抽象 # Spring AI 通过方言抽象支持多种关系数据库。以下数据库是开箱即用的：\nMySQL/MariaDB SQL 服务器 数据库 使用 ```JdbcChatMemoryRepositoryDialect.from(DataSource)` 时，可以从 JDBC 网址自动检测正确的方言。您可以通过实现 JdbcChatMemoryRepositoryDialect`` 接口来扩展对其他数据库的支持。\n配置属性 # 模式初始化 # 自动配置将在启动时自动创建 SPRING_AI_CHAT_MEMORY 表，并使用数据库供应商特定的 SQL 脚本。默认情况下，模式初始化仅适用于嵌入式数据库（H2、HSQL、Derby 等）。\n您可以使用 spring.ai.chat.memory.repository.jdbc.initialize-schema 属性控制架构初始化：\nspring.ai.chat.memory.repository.jdbc.initialize-schema=embedded # Only for embedded DBs (default) spring.ai.chat.memory.repository.jdbc.initialize-schema=always # Always initialize spring.ai.chat.memory.repository.jdbc.initialize-schema=never # Never initialize (useful with Flyway/Liquibase) 要覆盖架构脚本位置，请使用：\nspring.ai.chat.memory.repository.jdbc.schema=classpath:/custom/path/schema-mysql.sql 扩展方言 # 要添加对新数据库的支持，请实现 JdbcChatMemoryRepositoryDialect 接口并提供用于选择、插入和删除消息的 SQL。然后，您可以将自定义方言传递给存储库构建器。\nChatMemoryRepository chatMemoryRepository = JdbcChatMemoryRepository.builder() .jdbcTemplate(jdbcTemplate) .dialect(new MyCustomDbDialect()) .build(); CassandraChatMemory 存储库 # CassandraChatMemoryRepository 使用 Apache Cassandra 存储消息。它适用于需要持久存储聊天内存的应用程序，尤其适用于需要高可用性、持久性、可扩展性以及利用生存时间 (TTL) 功能的应用程序。\nCassandraChatMemoryRepository 采用时间序列模式，记录所有过去的聊天窗口，这对于治理和审计非常有价值。建议将生存时间设置为某个值，例如三年。\n要首先使用 CassandraChatMemoryRepository ，请将依赖项添加到您的项目中：\nSpring AI 为 CassandraChatMemoryRepository 提供了自动配置，您可以在应用程序中直接使用它。\n@Autowired CassandraChatMemoryRepository chatMemoryRepository; ChatMemory chatMemory = MessageWindowChatMemory.builder() .chatMemoryRepository(chatMemoryRepository) .maxMessages(10) .build(); 如果您希望手动创建 CassandraChatMemoryRepository ，您可以通过提供 CassandraChatMemoryRepositoryConfig 实例来实现：\nChatMemoryRepository chatMemoryRepository = CassandraChatMemoryRepository .create(CassandraChatMemoryConfig.builder().withCqlSession(cqlSession)); ChatMemory chatMemory = MessageWindowChatMemory.builder() .chatMemoryRepository(chatMemoryRepository) .maxMessages(10) .build(); 配置属性 # 模式初始化 # 自动配置将自动创建 ai_chat_memory 表。\n您可以通过将属性 spring.ai.chat.memory.repository.cassandra.initialize-schema 设置为 false 来禁用架构初始化。\nNeo4jChatMemoryRepository 是一个内置实现，它使用 Neo4j 将聊天消息作为节点和关系存储在属性图数据库中。它适用于希望利用 Neo4j 的图功能实现聊天记忆持久化的应用程序。\n首先，向您的项目添加以下依赖项：\nSpring AI 为 Neo4jChatMemoryRepository 提供了自动配置，您可以在应用程序中直接使用它。\n@Autowired Neo4jChatMemoryRepository chatMemoryRepository; ChatMemory chatMemory = MessageWindowChatMemory.builder() .chatMemoryRepository(chatMemoryRepository) .maxMessages(10) .build(); 如果您希望手动创建 Neo4jChatMemoryRepository ，则可以通过提供 Neo4j Driver 实例来实现：\nChatMemoryRepository chatMemoryRepository = Neo4jChatMemoryRepository.builder() .driver(driver) .build(); ChatMemory chatMemory = MessageWindowChatMemory.builder() .chatMemoryRepository(chatMemoryRepository) .maxMessages(10) .build(); 配置属性 # 索引初始化 # Neo4j 代码库将自动确保为对话 ID 和消息索引创建索引，以优化性能。如果您使用自定义标签，系统也会为这些标签创建索引。无需初始化架构，但您应确保您的应用程序可以访问 Neo4j 实例。\n聊天客户端中的内存 # 使用 ChatClient API 时，您可以提供 ChatMemory 实现来维护跨多个交互的对话上下文。\nSpring AI 提供了一些内置的 Advisor，您可以根据需要使用它们来配置 ChatClient 的内存行为。\nMessageChatMemoryAdvisor 。此顾问程序使用提供的 ChatMemory 实现来管理对话内存。每次交互时，它都会从内存中检索对话历史记录，并将其作为消息集合包含在提示中。 PromptChatMemoryAdvisor 。此顾问程序使用提供的 ChatMemory 实现来管理对话内存。每次交互时，它都会从内存中检索对话历史记录，并将其以纯文本形式附加到系统提示中。 VectorStoreChatMemoryAdvisor 。此顾问程序使用提供的 VectorStore 实现来管理对话内存。每次交互时，它都会从向量存储中检索对话历史记录，并将其以纯文本形式附加到系统消息中。 例如，如果您想将 MessageWindowChatMemory 与 MessageChatMemoryAdvisor 一起使用，您可以按如下方式配置它：\nChatMemory chatMemory = MessageWindowChatMemory.builder().build(); ChatClient chatClient = ChatClient.builder(chatModel) .defaultAdvisors(MessageChatMemoryAdvisor.builder(chatMemory).build()) .build(); 当调用 ChatClient 时，内存将由 MessageChatMemoryAdvisor 自动管理。对话历史记录将根据指定的对话 ID 从内存中检索：\nString conversationId = \u0026#34;007\u0026#34;; chatClient.prompt() .user(\u0026#34;Do I have license to code?\u0026#34;) .advisors(a -\u0026gt; a.param(ChatMemory.CONVERSATION_ID, conversationId)) .call() .content(); 自定义模板 # PromptChatMemoryAdvisor 使用默认模板，将检索到的对话记忆添加到系统消息中。您可以通过 .promptTemplate() 构建器方法提供自己的 PromptTemplate 对象来自定义此行为。\n自定义的 PromptTemplate 可以使用任何 TemplateRenderer 实现（默认情况下，它使用基于 [ StringTemplate]( https://www.stringtemplate.org/) 引擎的 StPromptTemplate ）。重要的要求是模板必须包含以下两个占位符：\n接收原始系统消息的 instructions 占位符。 一个 memory 占位符，用于接收检索到的对话内存。 自定义模板 # VectorStoreChatMemoryAdvisor 使用默认模板，将检索到的对话记忆添加到系统消息中。您可以通过 .promptTemplate() 构建器方法提供自己的 PromptTemplate 对象来自定义此行为。\n自定义的 PromptTemplate 可以使用任何 TemplateRenderer 实现（默认情况下，它使用基于 [ StringTemplate]( https://www.stringtemplate.org/) 引擎的 StPromptTemplate ）。重要的要求是模板必须包含以下两个占位符：\n接收原始系统消息的 instructions 占位符。 long_term_memory 占位符用于接收检索到的对话记忆。 聊天模型中的内存 # 如果您直接使用 ChatModel 而不是 ChatClient ，则可以明确管理内存：\n// Create a memory instance ChatMemory chatMemory = MessageWindowChatMemory.builder().build(); String conversationId = \u0026#34;007\u0026#34;; // First interaction UserMessage userMessage1 = new UserMessage(\u0026#34;My name is James Bond\u0026#34;); chatMemory.add(conversationId, userMessage1); ChatResponse response1 = chatModel.call(new Prompt(chatMemory.get(conversationId))); chatMemory.add(conversationId, response1.getResult().getOutput()); // Second interaction UserMessage userMessage2 = new UserMessage(\u0026#34;What is my name?\u0026#34;); chatMemory.add(conversationId, userMessage2); ChatResponse response2 = chatModel.call(new Prompt(chatMemory.get(conversationId))); chatMemory.add(conversationId, response2.getResult().getOutput()); // The response will contain \u0026#34;James Bond\u0026#34; "},{"id":61,"href":"/docs/%E5%8F%82%E8%80%83/%E7%9F%A2%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/gemfire-%E7%9F%A2%E9%87%8F%E5%95%86%E5%BA%97/","title":"GemFire 矢量商店","section":"矢量数据库","content":" GemFire 矢量商店 # 本节将引导您设置 GemFireVectorStore 来存储文档嵌入并执行相似性搜索。\n[ GemFire]( https://tanzu.vmware.com/gemfire) 是一个分布式内存键值存储系统，能够以极快的速度执行读写操作。它提供高可用性的并行消息队列、持续可用性以及事件驱动的架构，让您能够动态扩展而无需停机。随着您的数据规模需求不断增长以支持高性能实时应用，[ GemFire]( https://tanzu.vmware.com/gemfire) 可以轻松实现线性扩展。\n[ GemFire VectorDB]( https://docs.vmware.com/en/VMware-GemFire-VectorDB/1.0/gemfire-vectordb/overview.html) 扩展了 GemFire 的功能，作为一个多功能矢量数据库，可以高效地存储、检索和执行矢量相似性搜索。\n先决条件 # 自动配置 # 将 GemFire VectorStore Spring Boot 启动器添加到项目的 Maven 构建文件 pom.xml ：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-gemfire\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 文件\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-vector-store-gemfire\u0026#39; } 配置属性 # 您可以在 Spring Boot 配置中使用以下属性来进一步配置 GemFireVectorStore 。\n手动配置 # 如果只使用 GemFireVectorStore 而不使用 Spring Boot 的自动配置，请将以下依赖项添加到项目的 Maven pom.xml 中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-gemfire-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 对于 Gradle 用户，请将以下内容添加到 build.gradle 文件的依赖项块下，以便仅使用 GemFireVectorStore ：\n用法 # 下面是一个创建 GemfireVectorStore 实例而不是使用 AutoConfiguration 的示例\n@Bean public GemFireVectorStore vectorStore(EmbeddingModel embeddingModel) { return GemFireVectorStore.builder(embeddingModel) .host(\u0026#34;localhost\u0026#34;) .port(7071) .indexName(\u0026#34;my-vector-index\u0026#34;) .initializeSchema(true) .build(); } 在您的应用程序中，创建一些文档： List\u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;country\u0026#34;, \u0026#34;UK\u0026#34;, \u0026#34;year\u0026#34;, 2020)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;, Map.of()), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;country\u0026#34;, \u0026#34;NL\u0026#34;, \u0026#34;year\u0026#34;, 2023))); 将文档添加到向量存储： vectorStore.add(documents); 并使用相似性搜索检索文档： List\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch( SearchRequest.builder().query(\u0026#34;Spring\u0026#34;).topK(5).build()); 您应该检索包含文本“Spring AI rocks!!”的文档。\n您还可以使用相似度阈值来限制结果的数量：\nList\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch( SearchRequest.builder().query(\u0026#34;Spring\u0026#34;).topK(5) .similarityThreshold(0.5d).build()); "},{"id":62,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B-api/google-vertexai/","title":"Google VertexAI","section":"聊天模型 API","content":" Google VertexAI # [ VertexAI API]( https://cloud.google.com/vertex-ai/docs/reference) 以最少的机器学习专业知识和努力提供高质量的定制机器学习模型。\nSpring AI 通过以下客户端提供与 VertexAI API 的集成：\nVertexAI Gemini 聊天 "},{"id":63,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B-api/transformersonnx%E5%B5%8C%E5%85%A5/","title":"Transformers（ONNX）嵌入","section":"嵌入模型 API","content":" Transformers（ONNX）嵌入 # `TransformersEmbeddingModel``` 是 EmbeddingModel`` 实现，它使用选定的[ 句子转换器]( https://www.sbert.net/)在本地计算[ 句子嵌入]( https://www.sbert.net/examples/applications/computing-embeddings/README.html#sentence-embeddings-with-transformers) 。\n您可以使用任何 [ HuggingFace 嵌入模型]( https://huggingface.co/spaces/mteb/leaderboard) 。\n它使用[ 预先训练的]( https://www.sbert.net/docs/pretrained_models.html)变压器模型，序列化为[ 开放神经网络交换 (ONNX)]( https://onnx.ai/) 格式。\n[ Deep Java 库]( https://djl.ai/)和 Microsoft [ ONNX Java 运行时]( https://onnxruntime.ai/docs/get-started/with-java.html)库用于运行 ONNX 模型并计算 Java 中的嵌入。\n先决条件 # 为了在 Java 中运行，我们需要将 Tokenizer 和 Transformer 模型序列化为 ONNX 格式。\n使用 [ optimal-cli]( https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model#exporting-a-model-to-onnx-using-the-cli) 进行序列化 - 一种快速实现此目的的方法是使用 [ optimal-cli]( https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model#exporting-a-model-to-onnx-using-the-cli) 命令行工具。以下代码片段准备了一个 Python 虚拟环境，安装了所需的软件包，并使用 optimum-cli 序列化（例如导出）指定的模型：\npython3 -m venv venv source ./venv/bin/activate (venv) pip install --upgrade pip (venv) pip install optimum onnx onnxruntime sentence-transformers (venv) optimum-cli export onnx --model sentence-transformers/all-MiniLM-L6-v2 onnx-output-folder 该代码片段将 [ sentence-transformers/all-MiniLM-L6-v2]( https://huggingface.co/[sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)) 转换器导出到 onnx-output-folder 文件夹。后者包含嵌入模型使用的 tokenizer.json 和 model.onnx 文件。\n您可以选择任何 huggingface 转换器标识符或提供直接文件路径来代替 all-MiniLM-L6-v2。\n自动配置 # Spring AI 为 ONNX Transformer 嵌入模型提供了 Spring Boot 自动配置功能。要启用此功能，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-transformers\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-transformers\u0026#39; } 要配置它，请使用 spring.ai.embedding.transformer.* 属性。\n例如，将其添加到您的 application.properties 文件以使用 [ intfloat/e5-small-v2]( https://huggingface.co/[intfloat/e5-small-v2](https://huggingface.co/intfloat/e5-small-v2)) 文本嵌入模型配置客户端：\n受支持的属性的完整列表如下：\n嵌入属性 # 错误和特殊情况 # 手动配置 # 如果您不使用 Spring Boot，则可以手动配置 Onnx Transformers 嵌入模型。为此，请将 spring-ai-transformers 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-transformers\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 然后创建一个新的 TransformersEmbeddingModel 实例并使用 setTokenizerResource(tokenizerJsonUri) 和 setModelResource(modelOnnxUri) 方法设置导出的 tokenizer.json 和 model.onnx 文件的 URI。（支持 classpath: 、 file: 或 https: URI 模式）。\n如果未明确设置模型， TransformersEmbeddingModel 默认为 [ sentence-transformers/all-MiniLM-L6-v2]( https://huggingface.co/[sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)) ：\n以下代码片段说明了如何手动使用 TransformersEmbeddingModel ：\nTransformersEmbeddingModel embeddingModel = new TransformersEmbeddingModel(); // (optional) defaults to classpath:/onnx/all-MiniLM-L6-v2/tokenizer.json embeddingModel.setTokenizerResource(\u0026#34;classpath:/onnx/all-MiniLM-L6-v2/tokenizer.json\u0026#34;); // (optional) defaults to classpath:/onnx/all-MiniLM-L6-v2/model.onnx embeddingModel.setModelResource(\u0026#34;classpath:/onnx/all-MiniLM-L6-v2/model.onnx\u0026#34;); // (optional) defaults to ${java.io.tmpdir}/spring-ai-onnx-model // Only the http/https resources are cached by default. embeddingModel.setResourceCacheDirectory(\u0026#34;/tmp/onnx-zoo\u0026#34;); // (optional) Set the tokenizer padding if you see an errors like: // \u0026#34;ai.onnxruntime.OrtException: Supplied array is ragged, ...\u0026#34; embeddingModel.setTokenizerOptions(Map.of(\u0026#34;padding\u0026#34;, \u0026#34;true\u0026#34;)); embeddingModel.afterPropertiesSet(); List\u0026lt;List\u0026lt;Double\u0026gt;\u0026gt; embeddings = this.embeddingModel.embed(List.of(\u0026#34;Hello world\u0026#34;, \u0026#34;World is big\u0026#34;)); 第一次 embed() 调用会下载大型 ONNX 模型并将其缓存在本地文件系统中。因此，第一次调用可能需要比平时更长的时间。请使用 #setResourceCacheDirectory(\u0026lt;path\u0026gt;) 方法设置存储 ONNX 模型的本地文件夹。默认缓存文件夹为 ${java.io.tmpdir}/spring-ai-onnx-model 。\n将 TransformersEmbeddingModel 创建为 Bean 更加方便（也是首选）。这样就不必手动调用 afterPropertiesSet() 了。\n@Bean public EmbeddingModel embeddingModel() { return new TransformersEmbeddingModel(); } "},{"id":64,"href":"/docs/%E5%8F%82%E8%80%83/%E5%B7%A5%E5%85%B7%E8%B0%83%E7%94%A8/","title":"工具调用","section":"参考","content":" 工具调用 # 工具调用 （也称为函数调用 ）是 AI 应用程序中的一种常见模式，允许模型与一组 API 或工具进行交互，从而增强其功能。\n工具主要用于：\n信息检索 。此类工具可用于从外部来源（例如数据库、Web 服务、文件系统或 Web 搜索引擎）检索信息。其目标是增强模型的知识，使其能够回答原本无法回答的问题。因此，它们可用于检索增强生成 (RAG) 场景。例如，可以使用工具检索给定位置的当前天气、检索最新新闻文章或查询数据库中的特定记录。 采取行动 。此类别中的工具可用于在软件系统中采取行动，例如发送电子邮件、在数据库中创建新记录、提交表单或触发工作流。其目标是自动化原本需要人工干预或明确编程的任务。例如，可以使用工具为与聊天机器人交互的客户预订航班、在网页上填写表单，或在代码生成场景中基于自动化测试 (TDD) 实现 Java 类。 尽管我们通常将工具调用称为模型功能，但实际上工具调用逻辑是由客户端应用程序提供的。模型只能请求工具调用并提供输入参数，而应用程序负责根据输入参数执行工具调用并返回结果。模型永远无法访问任何作为工具提供的 API，这是一个至关重要的安全考虑因素。\nSpring AI 提供了便捷的 API 来定义工具、解析来自模型的工具调用请求以及执行工具调用。以下部分概述了 Spring AI 中的工具调用功能。\n快速入门 # 让我们看看如何在 Spring AI 中开始使用工具调用。我们将实现两个简单的工具：一个用于信息检索，一个用于执行操作。信息检索工具将用于获取用户时区的当前日期和时间。操作工具将用于设置指定时间的闹钟。\n信息检索 # AI 模型无法访问实时信息。任何假设模型能够感知当前日期或天气预报等信息的问题都无法由模型回答。不过，我们可以提供一个工具来检索这些信息，并让模型在需要访问实时信息时调用此工具。\n让我们在 DateTimeTools 类中实现一个工具，用于获取用户时区的当前日期和时间。该工具不接受任何参数。Spring 框架中的 LocaleContextHolder 可以提供用户的时区信息。该工具将被定义为一个带有 @Tool 注解的方法。为了帮助模型理解是否以及何时调用此工具，我们将提供该工具功能的详细描述。\nimport java.time.LocalDateTime; import org.springframework.ai.tool.annotation.Tool; import org.springframework.context.i18n.LocaleContextHolder; class DateTimeTools { @Tool(description = \u0026#34;Get the current date and time in the user\u0026#39;s timezone\u0026#34;) String getCurrentDateTime() { return LocalDateTime.now().atZone(LocaleContextHolder.getTimeZone().toZoneId()).toString(); } } 接下来，让我们将该工具提供给模型。在本例中，我们将使用 ChatClient 与模型交互。我们将通过 tools() 方法传递一个 DateTimeTools 实例，将该工具提供给模型。当模型需要获取当前日期和时间时，它会请求调用该工具。在内部， ChatClient 将调用该工具并将结果返回给模型，然后模型将使用工具调用结果生成对原始问题的最终响应。\nChatModel chatModel = ... String response = ChatClient.create(chatModel) .prompt(\u0026#34;What day is tomorrow?\u0026#34;) .tools(new DateTimeTools()) .call() .content(); System.out.println(response); 输出将是类似这样的：\nTomorrow is 2015-10-21. 您可以重试再次提出相同的问题。这次，不要向模型提供该工具。输出将类似于：\nI am an AI and do not have access to real-time information. Please provide the current date so I can accurately determine what day tomorrow will be. 如果没有该工具，模型就不知道如何回答问题，因为它没有能力确定当前的日期和时间。\n采取行动 # AI 模型可以用来生成实现特定目标的计划。例如，一个模型可以生成预订丹麦之旅的计划。然而，该模型并不具备执行该计划的能力。这时，工具就派上用场了：它们可以用来执行模型生成的计划。\n在上一个示例中，我们使用了一个工具来确定当前日期和时间。在本例中，我们将定义第二个工具，用于在特定时间设置闹钟。目标是设置从现在起 10 分钟后的闹钟，因此我们需要向模型提供这两个工具来完成此任务。\n我们将新工具添加到与之前相同的 DateTimeTools 类中。新工具将接受一个参数，即 ISO-8601 格式的时间。然后，该工具将向控制台打印一条消息，指示已为指定时间设置闹钟。与之前一样，该工具被定义为带有 @Tool 注解的方法，我们也使用 @Tool 注解来提供详细描述，以帮助模型了解何时以及如何使用该工具。\nimport java.time.LocalDateTime; import java.time.format.DateTimeFormatter; import org.springframework.ai.tool.annotation.Tool; import org.springframework.context.i18n.LocaleContextHolder; class DateTimeTools { @Tool(description = \u0026#34;Get the current date and time in the user\u0026#39;s timezone\u0026#34;) String getCurrentDateTime() { return LocalDateTime.now().atZone(LocaleContextHolder.getTimeZone().toZoneId()).toString(); } @Tool(description = \u0026#34;Set a user alarm for the given time, provided in ISO-8601 format\u0026#34;) void setAlarm(String time) { LocalDateTime alarmTime = LocalDateTime.parse(time, DateTimeFormatter.ISO_DATE_TIME); System.out.println(\u0026#34;Alarm set for \u0026#34; + alarmTime); } } 接下来，让我们将这两个工具都提供给模型。我们将使用 ChatClient 与模型交互。我们将通过 tools() 方法传递一个 DateTimeTools 实例，将这两个工具提供给模型。当我们要求设置 10 分钟后的闹钟时，模型首先需要知道当前的日期和时间。然后，它将使用当前日期和时间计算闹钟时间。最后，它将使用闹钟工具设置闹钟。在内部， ChatClient 将处理来自模型的任何工具调用请求，并将任何工具调用的执行结果返回给它，以便模型可以生成最终的响应。\nChatModel chatModel = ... String response = ChatClient.create(chatModel) .prompt(\u0026#34;Can you set an alarm 10 minutes from now?\u0026#34;) .tools(new DateTimeTools()) .call() .content(); System.out.println(response); 在应用程序日志中，您可以检查闹钟是否已在正确的时间设置。\n概述 # Spring AI 通过一组灵活的抽象来支持工具调用，这些抽象允许您以一致的方式定义、解析和执行工具。本节概述了 Spring AI 中工具调用的主要概念和组件。\n工具是工具调用的基石，它们由 ToolCallback 接口建模。Spring AI 内置了从方法和函数指定 ToolCallback 的支持，但您也可以定义自己的 ToolCallback 实现来支持更多用例。\nChatModel 实现透明地将工具调用请求分发给相应的 ToolCallback 实现，并将工具调用结果返回给模型，最终由模型生成响应。ChatModel 使用 ToolCallingManager 接口来实现上述操作，该接口负责管理工具执行的生命周期。\nChatClient 和 ChatModel 都接受 ToolCallback 对象列表，以使工具可供模型和最终将执行它们的 ToolCallingManager 使用。\n除了直接传递 ToolCallback 对象之外，您还可以传递工具名称列表，该列表将使用 ToolCallbackResolver 接口动态解析。\n以下部分将详细介绍所有这些概念和 API，包括如何自定义和扩展它们以支持更多用例。\n方法作为工具 # Spring AI 提供了内置支持，以两种方式从方法中指定工具（即 ToolCallback ：）：\n声明式地使用 @Tool 注释 以编程方式，使用低级 MethodToolCallback 实现。 声明式规范： @Tool # 您可以通过使用 @Tool 注释将方法转变为工具。\nclass DateTimeTools { @Tool(description = \u0026#34;Get the current date and time in the user\u0026#39;s timezone\u0026#34;) String getCurrentDateTime() { return LocalDateTime.now().atZone(LocaleContextHolder.getTimeZone().toZoneId()).toString(); } } @Tool 注释允许您提供有关该工具的关键信息：\nname ：工具的名称。如果未提供，则使用方法名称。AI 模型使用此名称来识别调用该工具的工具。因此，同一个类中不允许有两个同名的工具。对于特定的聊天请求，该名称在模型可用的所有工具中必须是唯一的。 description ：工具的描述，模型可以通过它来了解何时以及如何调用该工具。如果未提供，则方法名称将用作工具描述。但是，强烈建议提供详细的描述，因为这对于模型理解工具的用途及其使用方法至关重要。如果描述不充分，可能会导致模型在应该使用工具时不使用它，或者错误地使用它。 returnDirect ：工具结果是否应直接返回给客户端或传递回模型。有关更多详细信息，请参阅 “直接返回” 。 resultConverter ： ToolCallResultConverter 实现，用于将工具调用的结果转换为 String object ，并发送回 AI 模型。更多详情，请参阅结果转换 。 方法可以是静态的，也可以是实例的，并且可以具有任意可见性（公共、受保护、包级私有或私有）。包含该方法的类可以是顶级类或嵌套类，也可以具有任意可见性（只要它在你计划实例化它的地方可以访问）。\n您可以为该方法定义任意数量的参数（包括无参数），并且支持大多数类型（基元、POJO、枚举、列表、数组、映射等）。同样，该方法可以返回大多数类型，包括 void 。如果该方法返回值，则返回类型必须是可序列化类型，因为结果将被序列化并发送回模型。\nSpring AI 会自动为带有 @Tool 注解的方法的输入参数生成 JSON 模式。模型会使用该模式来了解如何调用工具并准备工具请求。 @ToolParam 注解可用于提供有关输入参数的附加信息，例如描述或参数是必需的还是可选的。默认情况下，所有输入参数都被视为必需的。\nimport java.time.LocalDateTime; import java.time.format.DateTimeFormatter; import org.springframework.ai.tool.annotation.Tool; import org.springframework.ai.tool.annotation.ToolParam; class DateTimeTools { @Tool(description = \u0026#34;Set a user alarm for the given time\u0026#34;) void setAlarm(@ToolParam(description = \u0026#34;Time in ISO-8601 format\u0026#34;) String time) { LocalDateTime alarmTime = LocalDateTime.parse(time, DateTimeFormatter.ISO_DATE_TIME); System.out.println(\u0026#34;Alarm set for \u0026#34; + alarmTime); } } @ToolParam 注释允许您提供有关工具参数的关键信息：\ndescription ：参数的描述，模型可以通过它来更好地理解如何使用参数。例如，参数应该采用什么格式、允许使用哪些值等等。 required ：该参数是必需的还是可选的。默认情况下，所有参数都被视为必需的。 如果参数被注释为 @Nullable ，它将被视为可选，除非使用 @ToolParam 注释明确标记为必需。\n除了 @ToolParam 注解之外，您还可以使用 Swagger 的 @Schema 注解或 Jackson 的 @JsonProperty 注解。更多详情，请参阅 [ JSON Schema](#_json_schema) 。\n向 ChatClient 添加工具 # 使用声明式规范方法时，您可以在调用 ChatClient 时将工具类实例传递给 tools() 方法。此类工具仅适用于添加它们的特定聊天请求。\nChatClient.create(chatModel) .prompt(\u0026#34;What day is tomorrow?\u0026#34;) .tools(new DateTimeTools()) .call() .content(); 在底层， ChatClient 会根据工具类实例中每个带有 @Tool 注解的方法生成一个 ToolCallback ，并将其传递给模型。如果您希望自行生成 ToolCallback ，可以使用 ToolCallbacks 实用程序类。\nToolCallback[] dateTimeTools = ToolCallbacks.from(new DateTimeTools()); 向 ChatClient 添加默认工具 # 使用声明式规范方法时，您可以通过将工具类实例传递给 defaultTools() 方法，向 ChatClient.Builder 添加默认工具。如果同时提供了默认工具和运行时工具，则运行时工具将完全覆盖默认工具。\nChatModel chatModel = ... ChatClient chatClient = ChatClient.builder(chatModel) .defaultTools(new DateTimeTools()) .build(); 向 ChatModel 添加工具 # 使用声明式规范方法时，您可以将工具类实例传递给 ToolCallingChatOptions 的 toolCallbacks() 方法，用于调用 ChatModel 。此类工具仅适用于添加它们的特定聊天请求。\nChatModel chatModel = ... ToolCallback[] dateTimeTools = ToolCallbacks.from(new DateTimeTools()); ChatOptions chatOptions = ToolCallingChatOptions.builder() .toolCallbacks(dateTimeTools) .build(); Prompt prompt = new Prompt(\u0026#34;What day is tomorrow?\u0026#34;, chatOptions); chatModel.call(prompt); 向 ChatModel 添加默认工具 # 使用声明式规范方法时，您可以在构造时将工具类实例传递给用于创建 ChatModel 的 ToolCallingChatOptions 实例的 toolCallbacks() 方法，从而向 ChatModel 添加默认工具。如果同时提供了默认工具和运行时工具，则运行时工具将完全覆盖默认工具。\nToolCallback[] dateTimeTools = ToolCallbacks.from(new DateTimeTools()); ChatModel chatModel = OllamaChatModel.builder() .ollamaApi(OllamaApi.builder().build()) .defaultOptions(ToolCallingChatOptions.builder() .toolCallbacks(dateTimeTools) .build()) .build(); 程序规范： MethodToolCallback # 您可以通过以编程方式构建 MethodToolCallback 将方法转变为工具。\nclass DateTimeTools { String getCurrentDateTime() { return LocalDateTime.now().atZone(LocaleContextHolder.getTimeZone().toZoneId()).toString(); } } ```MethodToolCallback.Builder` 允许您构建 MethodToolCallback`` 实例并提供有关该工具的关键信息：\ntoolDefinition ：定义工具名称、描述和输入架构的 ToolDefinition 实例。您可以使用 ToolDefinition.Builder 类构建它。必需。 toolMetadata ： ToolMetadata 实例，用于定义其他设置，例如是否应将结果直接返回给客户端，以及要使用的结果转换器。您可以使用 ToolMetadata.Builder 类来构建它。 toolMethod ：表示工具方法的 Method 实例。必需。 toolObject ：包含工具方法的对象实例。如果方法是静态的，则可以省略此参数。 toolCallResultConverter ：用于将工具调用结果转换为 String 对象并发送回 AI 模型的 ToolCallResultConverter 实例。如果未提供，则将使用默认转换器 ( DefaultToolCallResultConverter )。 ```ToolDefinition.Builder` 允许您构建 ToolDefinition`` 实例并定义工具名称、描述和输入模式：\nname ：工具的名称。如果未提供，则使用方法名称。AI 模型使用此名称来识别调用该工具的工具。因此，同一个类中不允许有两个同名的工具。对于特定的聊天请求，该名称在模型可用的所有工具中必须是唯一的。 description ：工具的描述，模型可以通过它来了解何时以及如何调用该工具。如果未提供，则方法名称将用作工具描述。但是，强烈建议提供详细的描述，因为这对于模型理解工具的用途及其使用方法至关重要。如果描述不充分，可能会导致模型在应该使用工具时不使用它，或者错误地使用它。 inputSchema ：工具输入参数的 JSON 架构。如果未提供，系统将根据方法参数自动生成架构。您可以使用 @ToolParam 注解提供有关输入参数的其他信息，例如描述或参数是必需还是可选。默认情况下，所有输入参数都被视为必需。有关更多详细信息，请参阅 JSON 架构 。 ```ToolMetadata.Builder` 允许您构建 ToolMetadata`` 实例并为该工具定义其他设置：\nreturnDirect ：工具结果是否应直接返回给客户端或传递回模型。有关更多详细信息，请参阅 “直接返回” 。 Method method = ReflectionUtils.findMethod(DateTimeTools.class, \u0026#34;getCurrentDateTime\u0026#34;); ToolCallback toolCallback = MethodToolCallback.builder() .toolDefinition(ToolDefinition.builder(method) .description(\u0026#34;Get the current date and time in the user\u0026#39;s timezone\u0026#34;) .build()) .toolMethod(method) .toolObject(new DateTimeTools()) .build(); 方法可以是静态的，也可以是实例的，并且可以具有任意可见性（公共、受保护、包级私有或私有）。包含该方法的类可以是顶级类或嵌套类，也可以具有任意可见性（只要它在你计划实例化它的地方可以访问）。\n您可以为该方法定义任意数量的参数（包括无参数），并且支持大多数类型（基元、POJO、枚举、列表、数组、映射等）。同样，该方法可以返回大多数类型，包括 void 。如果该方法返回值，则返回类型必须是可序列化类型，因为结果将被序列化并发送回模型。\n如果该方法是静态的，则可以省略 toolObject() 方法，因为它不是必需的。\nclass DateTimeTools { static String getCurrentDateTime() { return LocalDateTime.now().atZone(LocaleContextHolder.getTimeZone().toZoneId()).toString(); } } Method method = ReflectionUtils.findMethod(DateTimeTools.class, \u0026#34;getCurrentDateTime\u0026#34;); ToolCallback toolCallback = MethodToolCallback.builder() .toolDefinition(ToolDefinition.builder(method) .description(\u0026#34;Get the current date and time in the user\u0026#39;s timezone\u0026#34;) .build()) .toolMethod(method) .build(); Spring AI 将自动为该方法的输入参数生成 JSON 模式。模型将使用该模式来了解如何调用工具并准备工具请求。 @ToolParam 注解可用于提供有关输入参数的附加信息，例如描述或参数是必需的还是可选的。默认情况下，所有输入参数都被视为必需的。\nimport java.time.LocalDateTime; import java.time.format.DateTimeFormatter; import org.springframework.ai.tool.annotation.ToolParam; class DateTimeTools { void setAlarm(@ToolParam(description = \u0026#34;Time in ISO-8601 format\u0026#34;) String time) { LocalDateTime alarmTime = LocalDateTime.parse(time, DateTimeFormatter.ISO_DATE_TIME); System.out.println(\u0026#34;Alarm set for \u0026#34; + alarmTime); } } @ToolParam 注释允许您提供有关工具参数的关键信息：\ndescription ：参数的描述，模型可以通过它来更好地理解如何使用参数。例如，参数应该采用什么格式、允许使用哪些值等等。 required ：该参数是必需的还是可选的。默认情况下，所有参数都被视为必需的。 如果参数被注释为 @Nullable ，它将被视为可选，除非使用 @ToolParam 注释明确标记为必需。\n除了 @ToolParam 注解之外，您还可以使用 Swagger 的 @Schema 注解或 Jackson 的 @JsonProperty 注解。更多详情，请参阅 [ JSON Schema](#_json_schema) 。\n向 ChatClient 和 ChatModel 添加工具 # 使用编程规范方法时，您可以将 MethodToolCallback 实例传递给 ChatClient 的 tools() 方法。该工具仅适用于其所添加到的特定聊天请求。\nToolCallback toolCallback = ... ChatClient.create(chatModel) .prompt(\u0026#34;What day is tomorrow?\u0026#34;) .tools(toolCallback) .call() .content(); 向 ChatClient 添加默认工具 # 使用编程式规范方法时，您可以通过将 MethodToolCallback 实例传递给 defaultTools() 方法，将默认工具添加到 ChatClient.Builder 。如果同时提供了默认工具和运行时工具，则运行时工具将完全覆盖默认工具。\nChatModel chatModel = ... ToolCallback toolCallback = ... ChatClient chatClient = ChatClient.builder(chatModel) .defaultTools(toolCallback) .build(); 向 ChatModel 添加工具 # 使用编程式规范方法时，您可以将 MethodToolCallback 实例传递给 ToolCallingChatOptions 的 toolCallbacks() 方法，用于调用 ChatModel 。该工具仅适用于其所添加到的特定聊天请求。\nChatModel chatModel = ... ToolCallback toolCallback = ... ChatOptions chatOptions = ToolCallingChatOptions.builder() .toolCallbacks(toolCallback) .build(): Prompt prompt = new Prompt(\u0026#34;What day is tomorrow?\u0026#34;, chatOptions); chatModel.call(prompt); 向 ChatModel 添加默认工具 # 使用编程式规范方法时，您可以在构造时将 MethodToolCallback 实例传递给用于创建 ChatModel 的 ToolCallingChatOptions 实例的 toolCallbacks() 方法，从而向 ChatModel 添加默认工具。如果同时提供了默认工具和运行时工具，则运行时工具将完全覆盖默认工具。\nToolCallback toolCallback = ... ChatModel chatModel = OllamaChatModel.builder() .ollamaApi(OllamaApi.builder().build()) .defaultOptions(ToolCallingChatOptions.builder() .toolCallbacks(toolCallback) .build()) .build(); 方法工具限制 # 以下类型目前不支持作为工具使用的方法的参数或返回类型：\n异步类型（例如 CompletableFuture 、 Future ） 反应类型（例如 Flow 、 Mono 、 Flux ） 功能类型（例如 Function 、 Supplier 、 Consumer ）。 使用基于函数的工具规范方法支持函数类型。有关更多详细信息，请参阅 [ “函数作为工具”](#_functions_as_tools) 。\n函数作为工具 # Spring AI 提供了对从函数指定工具的内置支持，可以通过编程方式使用低级 FunctionToolCallback 实现，也可以通过运行时解析的 @Bean 动态指定。\n程序规范： FunctionToolCallback # 您可以通过以编程方式构建 FunctionToolCallback 将功能类型（ Function 、 Supplier 、 Consumer 或 BiFunction ）转变为工具。\npublic class WeatherService implements Function\u0026lt;WeatherRequest, WeatherResponse\u0026gt; { public WeatherResponse apply(WeatherRequest request) { return new WeatherResponse(30.0, Unit.C); } } public enum Unit { C, F } public record WeatherRequest(String location, Unit unit) {} public record WeatherResponse(double temp, Unit unit) {} ```FunctionToolCallback.Builder` 允许您构建 FunctionToolCallback`` 实例并提供有关该工具的关键信息：\nname ：工具的名称。AI 模型使用此名称来识别调用该工具的工具。因此，同一上下文中不允许存在两个同名的工具。对于特定聊天请求，该名称在模型可用的所有工具中必须是唯一的。必填。 toolFunction ：表示工具方法（ Function 、 Supplier 、 Consumer 或 BiFunction ）的函数对象。必需。 description ：工具的描述，模型可以通过它来了解何时以及如何调用该工具。如果未提供，则方法名称将用作工具描述。但是，强烈建议提供详细的描述，因为这对于模型理解工具的用途及其使用方法至关重要。如果描述不充分，可能会导致模型在应该使用工具时不使用它，或者错误地使用它。 inputType ：函数输入的类型。必需。 inputSchema ：工具输入参数的 JSON 架构。如果未提供，系统将根据 inputType 自动生成架构。您可以使用 @ToolParam 注解提供有关输入参数的其他信息，例如描述或参数是必需还是可选。默认情况下，所有输入参数都被视为必需。有关更多详细信息，请参阅 JSON 架构 。 toolMetadata ： ToolMetadata 实例，用于定义其他设置，例如是否应将结果直接返回给客户端，以及要使用的结果转换器。您可以使用 ToolMetadata.Builder 类来构建它。 toolCallResultConverter ：用于将工具调用结果转换为 String 对象并发送回 AI 模型的 ToolCallResultConverter 实例。如果未提供，则将使用默认转换器 ( DefaultToolCallResultConverter )。 ```ToolMetadata.Builder` 允许您构建 ToolMetadata`` 实例并为该工具定义其他设置：\nreturnDirect ：工具结果是否应直接返回给客户端或传递回模型。有关更多详细信息，请参阅 “直接返回” 。 ToolCallback toolCallback = FunctionToolCallback .builder(\u0026#34;currentWeather\u0026#34;, new WeatherService()) .description(\u0026#34;Get the weather in location\u0026#34;) .inputType(WeatherRequest.class) .build(); 函数的输入和输出可以是 Void 或 POJO。输入和输出 POJO 必须是可序列化的，因为结果将被序列化并发送回模型。函数以及输入和输出类型必须是公共的。\n向 ChatClient 添加工具 # 使用编程规范方法时，您可以将 FunctionToolCallback 实例传递给 ChatClient 的 tools() 方法。该工具仅适用于其所添加到的特定聊天请求。\nToolCallback toolCallback = ... ChatClient.create(chatModel) .prompt(\u0026#34;What\u0026#39;s the weather like in Copenhagen?\u0026#34;) .tools(toolCallback) .call() .content(); 向 ChatClient 添加默认工具 # 使用编程式规范方法时，您可以通过将 FunctionToolCallback 实例传递给 defaultTools() 方法，将默认工具添加到 ChatClient.Builder 。如果同时提供了默认工具和运行时工具，则运行时工具将完全覆盖默认工具。\nChatModel chatModel = ... ToolCallback toolCallback = ... ChatClient chatClient = ChatClient.builder(chatModel) .defaultTools(toolCallback) .build(); 向 ChatModel 添加工具 # 使用编程规范方法时，您可以将 FunctionToolCallback 实例传递给 ToolCallingChatOptions 的 toolCallbacks() 方法。该工具仅适用于其所添加到的特定聊天请求。\nChatModel chatModel = ... ToolCallback toolCallback = ... ChatOptions chatOptions = ToolCallingChatOptions.builder() .toolCallbacks(toolCallback) .build(): Prompt prompt = new Prompt(\u0026#34;What\u0026#39;s the weather like in Copenhagen?\u0026#34;, chatOptions); chatModel.call(prompt); 向 ChatModel 添加默认工具 # 使用编程式规范方法时，您可以在构造时将 FunctionToolCallback 实例传递给用于创建 ChatModel 的 ToolCallingChatOptions 实例的 toolCallbacks() 方法，从而将默认工具添加到 ChatModel 中。如果同时提供了默认工具和运行时工具，则运行时工具将完全覆盖默认工具。\nToolCallback toolCallback = ... ChatModel chatModel = OllamaChatModel.builder() .ollamaApi(OllamaApi.builder().build()) .defaultOptions(ToolCallingChatOptions.builder() .toolCallbacks(toolCallback) .build()) .build(); 动态规范： @Bean # 您不必以编程方式指定工具，而是可以将工具定义为 Spring bean，并让 Spring AI 在运行时使用 ToolCallbackResolver 接口（通过 SpringBeanToolCallbackResolver 实现）动态解析它们。此选项使您可以使用任何 Function 、 Supplier 、 Consumer 或 BiFunction bean 作为工具。bean 名称将用作工具名称，Spring Framework 中的 @Description 注释可用于提供工具的描述，模型使用这些信息来了解何时以及如何调用该工具。如果您不提供描述，则方法名称将用作工具描述。但是，强烈建议提供详细的描述，因为这对于模型理解工具的用途及其使用方法至关重要。如果无法提供良好的描述，则可能导致模型在应该使用工具时不使用它或错误地使用它。\n@Configuration(proxyBeanMethods = false) class WeatherTools { WeatherService weatherService = new WeatherService(); @Bean @Description(\u0026#34;Get the weather in location\u0026#34;) Function\u0026lt;WeatherRequest, WeatherResponse\u0026gt; currentWeather() { return weatherService; } } 工具输入参数的 [ JSON 架构](#_json_schema)将自动生成。您可以使用 @ToolParam 注解提供有关输入参数的其他信息，例如描述或参数是必需的还是可选的。默认情况下，所有输入参数都被视为必需的。有关更多详细信息，请参阅 [ JSON 架构](#_json_schema) 。\nrecord WeatherRequest(@ToolParam(description = \u0026#34;The name of a city or a country\u0026#34;) String location, Unit unit) {} 这种工具指定方法的缺点是无法保证类型安全，因为工具解析是在运行时完成的。为了缓解这个问题，您可以使用 @Bean 注解显式指定工具名称，并将值存储在常量中，这样您就可以在聊天请求中使用它，而不必对工具名称进行硬编码。\n@Configuration(proxyBeanMethods = false) class WeatherTools { public static final String CURRENT_WEATHER_TOOL = \u0026#34;currentWeather\u0026#34;; @Bean(CURRENT_WEATHER_TOOL) @Description(\u0026#34;Get the weather in location\u0026#34;) Function\u0026lt;WeatherRequest, WeatherResponse\u0026gt; currentWeather() { ... } } 向 ChatClient 添加工具 # 使用动态规范方法时，您可以将工具名称（即函数 bean 名称）传递给 ChatClient 的 tools() 方法。该工具仅适用于其所添加到的特定聊天请求。\nChatClient.create(chatModel) .prompt(\u0026#34;What\u0026#39;s the weather like in Copenhagen?\u0026#34;) .tools(\u0026#34;currentWeather\u0026#34;) .call() .content(); 向 ChatClient 添加默认工具 # 使用动态规范方法时，您可以通过将工具名称传递给 defaultTools() 方法，将默认工具添加到 ChatClient.Builder 。如果同时提供了默认工具和运行时工具，则运行时工具将完全覆盖默认工具。\nChatModel chatModel = ... ChatClient chatClient = ChatClient.builder(chatModel) .defaultTools(\u0026#34;currentWeather\u0026#34;) .build(); 向 ChatModel 添加工具 # 使用动态规范方法时，您可以将工具名称传递给用于调用 ChatModel 的 ToolCallingChatOptions 的 toolNames() 方法。该工具仅适用于添加到的特定聊天请求。\nChatModel chatModel = ... ChatOptions chatOptions = ToolCallingChatOptions.builder() .toolNames(\u0026#34;currentWeather\u0026#34;) .build(): Prompt prompt = new Prompt(\u0026#34;What\u0026#39;s the weather like in Copenhagen?\u0026#34;, chatOptions); chatModel.call(prompt); 向 ChatModel 添加默认工具 # 使用动态规范方法时，您可以在构造时将默认工具添加到 ChatModel 中，只需将工具名称传递给用于创建 ChatModel ToolCallingChatOptions 实例的 toolNames() 方法即可。如果同时提供了默认工具和运行时工具，则运行时工具将完全覆盖默认工具。\nChatModel chatModel = OllamaChatModel.builder() .ollamaApi(OllamaApi.builder().build()) .defaultOptions(ToolCallingChatOptions.builder() .toolNames(\u0026#34;currentWeather\u0026#34;) .build()) .build(); 函数工具限制 # 对于用作工具的函数，当前不支持以下类型作为输入或输出类型：\n原始类型 集合类型（例如 List 、 Map 、 Array 、 Set ） 异步类型（例如 CompletableFuture 、 Future ） 反应类型（例如 Flow 、 Mono 、 Flux ）。 使用基于方法的工具规范方法支持原始类型和集合。有关更多详细信息，请参阅 [ “方法即工具”](#_methods_as_tools) 。\n工具规格 # 在 Spring AI 中，工具通过 ToolCallback 接口建模。在前面的部分中，我们了解了如何使用 Spring AI 提供的内置支持，通过方法和函数定义工具（请参阅[ 方法即工具](#_methods_as_tools)和[ 函数即工具](#_functions_as_tools) ）。本节将深入探讨工具规范，以及如何自定义和扩展它以支持更多用例。\n工具回调 # ToolCallback 接口提供了一种定义可由 AI 模型调用的工具的方法，包括定义和执行逻辑。当您想从头定义工具时，它是主要实现的接口。例如，您可以从 MCP 客户端（使用模型上下文协议）或 ChatClient （用于构建模块化代理应用程序）定义 ToolCallback 。\n该接口提供以下方法：\npublic interface ToolCallback { /** * Definition used by the AI model to determine when and how to call the tool. */ ToolDefinition getToolDefinition(); /** * Metadata providing additional information on how to handle the tool. */ ToolMetadata getToolMetadata(); /** * Execute tool with the given input and return the result to send back to the AI model. */ String call(String toolInput); /** * Execute tool with the given input and context, and return the result to send back to the AI model. */ String call(String toolInput, ToolContext tooContext); } Spring AI 为工具方法（ MethodToolCallback ）和工具函数（ FunctionToolCallback ）提供了内置实现。\n工具定义 # ToolDefinition 接口提供了 AI 模型了解工具可用性所需的信息，包括工具名称、描述和输入模式。每个 ToolCallback 实现都必须提供一个 ToolDefinition 实例来定义该工具。\n该接口提供以下方法：\npublic interface ToolDefinition { /** * The tool name. Unique within the tool set provided to a model. */ String name(); /** * The tool description, used by the AI model to determine what the tool does. */ String description(); /** * The schema of the parameters used to call the tool. */ String inputSchema(); } ```ToolDefinition.Builder` 允许您使用默认实现（ DefaultToolDefinition）构建ToolDefinition`` 实例。\nToolDefinition toolDefinition = ToolDefinition.builder() .name(\u0026#34;currentWeather\u0026#34;) .description(\u0026#34;Get the weather in location\u0026#34;) .inputSchema(\u0026#34;\u0026#34;\u0026#34; { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;unit\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;enum\u0026#34;: [\u0026#34;C\u0026#34;, \u0026#34;F\u0026#34;] } }, \u0026#34;required\u0026#34;: [\u0026#34;location\u0026#34;, \u0026#34;unit\u0026#34;] } \u0026#34;\u0026#34;\u0026#34;) .build(); 方法工具定义 # 从方法构建工具时，系统会自动生成 ToolDefinition 。如果您希望自行生成 ToolDefinition ，可以使用这个便捷的构建器。\nMethod method = ReflectionUtils.findMethod(DateTimeTools.class, \u0026#34;getCurrentDateTime\u0026#34;); ToolDefinition toolDefinition = ToolDefinition.from(method); 从方法生成的 ToolDefinition 包含方法名称（作为工具名称）、方法名称（作为工具描述）以及方法输入参数的 JSON 架构。如果方法带有 @Tool 注解，则工具名称和描述将从注解中获取（如果已设置）。\n如果您希望明确提供部分或全部属性，则可以使用 ```ToolDefinition.Builder` 来构建自定义 ToolDefinition`` 实例。\nMethod method = ReflectionUtils.findMethod(DateTimeTools.class, \u0026#34;getCurrentDateTime\u0026#34;); ToolDefinition toolDefinition = ToolDefinition.builder(method) .name(\u0026#34;currentDateTime\u0026#34;) .description(\u0026#34;Get the current date and time in the user\u0026#39;s timezone\u0026#34;) .inputSchema(JsonSchemaGenerator.generateForMethodInput(method)) .build(); 功能工具定义 # 从函数构建工具时，系统会自动生成 ToolDefinition 。使用 ```FunctionToolCallback.Builder` 构建 FunctionToolCallback`` 实例时，您可以提供用于生成 ToolDefinition 工具名称、描述和输入架构。更多详情，请参阅 [ “函数即工具”](#_functions_as_tools) 。\nJSON 模式 # 在向 AI 模型提供工具时，模型需要了解用于调用该工具的输入类型的模式。该模式用于了解如何调用该工具并准备工具请求。Spring AI 通过 JsonSchemaGenerator 类内置了为工具生成输入类型的 JSON 模式的支持。该模式作为 ToolDefinition 的一部分提供。\nJsonSchemaGenerator 类在底层用于为方法或函数的输入参数生成 JSON 模式，可以使用 [ “方法即工具”](#_methods_as_tools) 和 [ “函数即工具”](#_functions_as_tools) 中描述的任何策略。JSON 模式生成逻辑支持一系列注解，您可以在方法和函数的输入参数上使用这些注解来自定义生成的模式。\n本节介绍在为工具的输入参数生成 JSON 模式时可以自定义的两个主要选项：描述和所需状态。\n描述 # 除了提供工具本身的描述外，您还可以提供工具输入参数的描述。该描述可用于提供有关输入参数的关键信息，例如参数应采用的格式、允许的值等等。这有助于模型理解输入模式及其使用方法。Spring AI 内置支持使用以下注解之一生成输入参数的描述：\n来自 Spring AI 的 @ToolParam(description = \u0026ldquo;…​\u0026rdquo;) 来自杰克逊的 @JsonClassDescription(description = \u0026ldquo;…​\u0026rdquo;) 来自杰克逊的 @JsonPropertyDescription(description = \u0026ldquo;…​\u0026rdquo;) 来自 Swagger 的 @Schema(description = \u0026ldquo;…​\u0026rdquo;) 。 这种方法适用于方法和函数，并且可以递归地将其用于嵌套类型。\nimport java.time.LocalDateTime; import java.time.format.DateTimeFormatter; import org.springframework.ai.tool.annotation.Tool; import org.springframework.ai.tool.annotation.ToolParam; import org.springframework.context.i18n.LocaleContextHolder; class DateTimeTools { @Tool(description = \u0026#34;Set a user alarm for the given time\u0026#34;) void setAlarm(@ToolParam(description = \u0026#34;Time in ISO-8601 format\u0026#34;) String time) { LocalDateTime alarmTime = LocalDateTime.parse(time, DateTimeFormatter.ISO_DATE_TIME); System.out.println(\u0026#34;Alarm set for \u0026#34; + alarmTime); } } 必需/可选 # 默认情况下，每个输入参数都被视为必需参数，这会强制 AI 模型在调用该工具时为其提供一个值。但是，您可以使用以下注释之一（按以下优先顺序）将输入参数设为可选参数：\nSpring AI 中的 @ToolParam(required = false) 来自杰克逊的 @JsonProperty(required = false) 来自 Swagger 的 @Schema(required = false) Spring 框架中的 @Nullable 。 这种方法适用于方法和函数，并且可以递归地将其用于嵌套类型。\nclass CustomerTools { @Tool(description = \u0026#34;Update customer information\u0026#34;) void updateCustomerInfo(Long id, String name, @ToolParam(required = false) String email) { System.out.println(\u0026#34;Updated info for customer with id: \u0026#34; + id); } } 结果转换 # 工具调用的结果使用 ToolCallResultConverter 进行序列化，然后发送回 AI 模型。ToolCallResultConverter ToolCallResultConverter 提供了一种将工具调用结果转换为 String 对象的方法。\n该接口提供以下方法：\n@FunctionalInterface public interface ToolCallResultConverter { /** * Given an Object returned by a tool, convert it to a String compatible with the * given class type. */ String convert(@Nullable Object result, @Nullable Type returnType); } 结果必须是可序列化类型。默认情况下，结果使用 Jackson ( `DefaultToolCallResultConverter``` ) 序列化为 JSON，但您可以通过提供自己的 ToolCallResultConverter`` 实现来自定义序列化过程。\nSpring AI 在方法和函数工具中都依赖于 ToolCallResultConverter 。\n方法工具调用结果转换 # 当使用声明式方法从方法构建工具时，您可以通过设置 @Tool 注释的 resultConverter() 属性来提供用于该工具的自定义 ToolCallResultConverter 。\nclass CustomerTools { @Tool(description = \u0026#34;Retrieve customer information\u0026#34;, resultConverter = CustomToolCallResultConverter.class) Customer getCustomerInfo(Long id) { return customerRepository.findById(id); } } 如果使用编程方法，您可以通过设置 MethodToolCallback.Builder 的 resultConverter() 属性来提供用于该工具的自定义 ToolCallResultConverter 。\n请参阅[ 方法作为工具](#_methods_as_tools)以了解更多详细信息。\n函数工具调用结果转换 # 当使用编程方法从函数构建工具时，您可以通过设置 FunctionToolCallback.Builder 的 resultConverter() 属性来提供用于该工具的自定义 ToolCallResultConverter 。\n请参阅[ 函数作为工具](#_functions_as_tools)以了解更多详细信息。\n工具上下文 # Spring AI 支持通过 ToolContext API 向工具传递额外的上下文信息。此功能允许您提供额外的用户提供数据，这些数据可在工具执行过程中与 AI 模型传递的工具参数一起使用。\nclass CustomerTools { @Tool(description = \u0026#34;Retrieve customer information\u0026#34;) Customer getCustomerInfo(Long id, ToolContext toolContext) { return customerRepository.findById(id, toolContext.get(\u0026#34;tenantId\u0026#34;)); } } 调用 ChatClient 时， ToolContext 会填充用户提供的数据。\nChatModel chatModel = ... String response = ChatClient.create(chatModel) .prompt(\u0026#34;Tell me more about the customer with ID 42\u0026#34;) .tools(new CustomerTools()) .toolContext(Map.of(\u0026#34;tenantId\u0026#34;, \u0026#34;acme\u0026#34;)) .call() .content(); System.out.println(response); 同样，您可以在直接调用 ChatModel 时定义工具上下文数据。\nChatModel chatModel = ... ToolCallback[] customerTools = ToolCallbacks.from(new CustomerTools()); ChatOptions chatOptions = ToolCallingChatOptions.builder() .toolCallbacks(customerTools) .toolContext(Map.of(\u0026#34;tenantId\u0026#34;, \u0026#34;acme\u0026#34;)) .build(); Prompt prompt = new Prompt(\u0026#34;Tell me more about the customer with ID 42\u0026#34;, chatOptions); chatModel.call(prompt); 如果在默认选项和运行时选项中都设置了 toolContext 选项，则生成的 ToolContext 将是两者的合并，其中运行时选项优先于默认选项。\n直接退货 # 默认情况下，工具调用的结果会作为响应发送回模型。然后，模型可以使用该结果继续对话。\n在某些情况下，您更愿意将结果直接返回给调用者，而不是将其发送回模型。例如，如果您构建了一个依赖于 RAG 工具的代理，您可能希望将结果直接返回给调用者，而不是将其发送回模型进行不必要的后处理。或者，您可能拥有某些工具，应该终止代理的推理循环。\n每个 ToolCallback 实现都可以定义工具调用的结果应该直接返回给调用者还是返回给模型。默认情况下，结果会返回给模型。但您可以根据工具更改此行为。\nToolCallingManager 负责管理工具执行生命周期，并负责处理与工具关联的 returnDirect 属性。如果该属性设置为 true ，则工具调用的结果将直接返回给调用者。否则，结果将发送回模型。\n方法直接返回 # 当使用声明式方法从方法构建工具时，可以通过将 @Tool 注释的 returnDirect 属性设置为 true 来标记工具将结果直接返回给调用者。\nclass CustomerTools { @Tool(description = \u0026#34;Retrieve customer information\u0026#34;, returnDirect = true) Customer getCustomerInfo(Long id) { return customerRepository.findById(id); } } 如果使用编程方法，您可以通过 ToolMetadata 接口设置 returnDirect 属性并将其传递给 MethodToolCallback.Builder 。\nToolMetadata toolMetadata = ToolMetadata.builder() .returnDirect(true) .build(); 请参阅[ 方法作为工具](#_methods_as_tools)以了解更多详细信息。\n函数直接返回 # 当使用编程方法从函数构建工具时，您可以通过 ToolMetadata 接口设置 returnDirect 属性并将其传递给 FunctionToolCallback.Builder 。\nToolMetadata toolMetadata = ToolMetadata.builder() .returnDirect(true) .build(); 请参阅[ 函数作为工具](#_functions_as_tools)以了解更多详细信息。\n工具执行 # 工具执行是使用提供的输入参数调用工具并返回结果的过程。工具执行由 ToolCallingManager 接口处理，该接口负责管理工具执行的生命周期。\npublic interface ToolCallingManager { /** * Resolve the tool definitions from the model\u0026#39;s tool calling options. */ List\u0026lt;ToolDefinition\u0026gt; resolveToolDefinitions(ToolCallingChatOptions chatOptions); /** * Execute the tool calls requested by the model. */ ToolExecutionResult executeToolCalls(Prompt prompt, ChatResponse chatResponse); } 如果您使用任何 Spring AI Spring Boot Starters， `DefaultToolCallingManager````` 是 ToolCallingManager接口的自动配置实现。您可以通过提供自己的ToolCallingManager```` bean 来自定义工具执行行为。\n@Bean ToolCallingManager toolCallingManager() { return ToolCallingManager.builder().build(); } 默认情况下，Spring AI 会在每个 ChatModel 实现内部透明地管理工具执行生命周期。但您可以选择退出此行为并自行控制工具执行。本节将介绍这两种场景。\n框架控制的工具执行 # 使用默认行为时，Spring AI 会自动拦截来自模型的任何工具调用请求，调用该工具并将结果返回给模型。所有这一切都由每个 ChatModel 实现使用 ToolCallingManager 为您透明地完成。\n确定工具调用是否有资格执行的逻辑由 ToolExecutionEligibilityPredicate 接口处理。默认情况下，工具执行资格通过检查 ToolCallingChatOptions 的 internalToolExecutionEnabled 属性是否设置为 true （默认值）以及 ChatResponse 是否包含任何工具调用来确定。\npublic class DefaultToolExecutionEligibilityPredicate implements ToolExecutionEligibilityPredicate { @Override public boolean test(ChatOptions promptOptions, ChatResponse chatResponse) { return ToolCallingChatOptions.isInternalToolExecutionEnabled(promptOptions) \u0026amp;\u0026amp; chatResponse != null \u0026amp;\u0026amp; chatResponse.hasToolCalls(); } } 您可以在创建 ChatModel bean 时提供 ToolExecutionEligibilityPredicate 的自定义实现。\n用户控制的工具执行 # 在某些情况下，您可能希望自行控制工具执行生命周期。您可以将 ToolCallingChatOptions 的 internalToolExecutionEnabled 属性设置为 false 来实现。\n当您使用此选项调用 ChatModel 时，工具的执行将委托给调用者，让您完全控制工具执行的生命周期。您负责检查 ChatResponse 中的工具调用，并使用 ToolCallingManager 执行它们。\n以下示例演示了用户控制工具执行方法的最小实现：\nChatModel chatModel = ... ToolCallingManager toolCallingManager = ToolCallingManager.builder().build(); ChatOptions chatOptions = ToolCallingChatOptions.builder() .toolCallbacks(new CustomerTools()) .internalToolExecutionEnabled(false) .build(); Prompt prompt = new Prompt(\u0026#34;Tell me more about the customer with ID 42\u0026#34;, chatOptions); ChatResponse chatResponse = chatModel.call(prompt); while (chatResponse.hasToolCalls()) { ToolExecutionResult toolExecutionResult = toolCallingManager.executeToolCalls(prompt, chatResponse); prompt = new Prompt(toolExecutionResult.conversationHistory(), chatOptions); chatResponse = chatModel.call(prompt); } System.out.println(chatResponse.getResult().getOutput().getText()); 接下来的示例展示了用户控制工具执行方法与 ChatMemory API 结合使用的最小实现：\nToolCallingManager toolCallingManager = DefaultToolCallingManager.builder().build(); ChatMemory chatMemory = MessageWindowChatMemory.builder().build(); String conversationId = UUID.randomUUID().toString(); ChatOptions chatOptions = ToolCallingChatOptions.builder() .toolCallbacks(ToolCallbacks.from(new MathTools())) .internalToolExecutionEnabled(false) .build(); Prompt prompt = new Prompt( List.of(new SystemMessage(\u0026#34;You are a helpful assistant.\u0026#34;), new UserMessage(\u0026#34;What is 6 * 8?\u0026#34;)), chatOptions); chatMemory.add(conversationId, prompt.getInstructions()); Prompt promptWithMemory = new Prompt(chatMemory.get(conversationId), chatOptions); ChatResponse chatResponse = chatModel.call(promptWithMemory); chatMemory.add(conversationId, chatResponse.getResult().getOutput()); while (chatResponse.hasToolCalls()) { ToolExecutionResult toolExecutionResult = toolCallingManager.executeToolCalls(promptWithMemory, chatResponse); chatMemory.add(conversationId, toolExecutionResult.conversationHistory() .get(toolExecutionResult.conversationHistory().size() - 1)); promptWithMemory = new Prompt(chatMemory.get(conversationId), chatOptions); chatResponse = chatModel.call(promptWithMemory); chatMemory.add(conversationId, chatResponse.getResult().getOutput()); } UserMessage newUserMessage = new UserMessage(\u0026#34;What did I ask you earlier?\u0026#34;); chatMemory.add(conversationId, newUserMessage); ChatResponse newResponse = chatModel.call(new Prompt(chatMemory.get(conversationId))); 异常处理 # 当工具调用失败时，异常会以 ToolExecutionException 形式传播，您可以捕获该异常来处理错误。 ToolExecutionExceptionProcessor 可用于处理 ToolExecutionException ，并产生两种结果：要么生成一条错误消息并发送回 AI 模型，要么抛出一个异常并由调用方处理。\n@FunctionalInterface public interface ToolExecutionExceptionProcessor { /** * Convert an exception thrown by a tool to a String that can be sent back to the AI * model or throw an exception to be handled by the caller. */ String process(ToolExecutionException exception); } 如果您使用任何 Spring AI Spring Boot Starters，则 Default``ToolExecutionExceptionProcessor``` 是 ``ToolExecutionExceptionProcessor`` 接口的自动配置实现。默认情况下，错误消息会发送回模型。您可以使用 DefaultToolExecutionExceptionProcessor``` 构造函数将 alwaysThrow属性设置为 ````true```` 或false`` 。如果 true ，则会抛出异常，而不是将错误消息发送回模型。\n@Bean ToolExecutionExceptionProcessor toolExecutionExceptionProcessor() { return new DefaultToolExecutionExceptionProcessor(true); } 默认 ToolCallingManager ( DefaultToolCallingManager ) 在内部使用 ToolExecutionExceptionProcessor 来处理[ 工具执行](#_tool_execution)期间的异常。有关[ 工具执行](#_tool_execution)生命周期的更多详细信息，请参阅[ 工具执行](#_tool_execution) 。\n工具分辨率 # 将工具传递给模型的主要方法是在调用 ChatClient 或 ChatModel 时提供 ToolCallback ，使用[ 方法作为工具](#_methods_as_tools)和[ 函数作为工具](#_functions_as_tools)中描述的策略之一。\n但是，Spring AI 还支持使用 ToolCallbackResolver 接口在运行时动态解析工具。\npublic interface ToolCallbackResolver { /** * Resolve the {@link ToolCallback} for the given tool name. */ @Nullable ToolCallback resolve(String toolName); } 使用此方法时：\n在客户端，您向 ChatClient 或 ChatModel 提供工具名称，而不是 ToolCallback 。 在服务器端， ToolCallbackResolver 实现负责将工具名称解析为相应的 ToolCallback 实例。 默认情况下，Spring AI 依赖于 `DelegatingToolCallbackResolver``` ，它将工具解析委托给 ToolCallbackResolver`` 实例列表：\nSpringBeanToolCallbackResolver 解析 Function 、 Supplier 、 Consumer 或 BiFunction 类型的 Spring Bean 中的工具。有关更多详细信息，请参阅动态规范： @Bean 。 StaticToolCallbackResolver 从 ToolCallback 实例的静态列表中解析工具。使用 Spring Boot 自动配置时，此解析器会自动配置应用程序上下文中定义的所有 ToolCallback 类型的 bean。 如果您依赖 Spring Boot 自动配置，则可以通过提供自定义 ToolCallbackResolver bean 来定制解析逻辑。\n@Bean ToolCallbackResolver toolCallbackResolver(List\u0026lt;FunctionCallback\u0026gt; toolCallbacks) { StaticToolCallbackResolver staticToolCallbackResolver = new StaticToolCallbackResolver(toolCallbacks); return new DelegatingToolCallbackResolver(List.of(staticToolCallbackResolver)); } ToolCallbackResolver 由 ToolCallingManager 内部使用，用于在运行时动态解析工具，支持[ 框架控制的工具执行](#_framework_controlled_tool_execution)和[ 用户控制的工具执行](#_user_controlled_tool_execution) 。\n可观察性 # 工具调用包含 spring.ai.tool 观察的可观察性支持，用于测量完成时间并传播跟踪信息。请参阅[ 工具调用可观察性](../observability/index.html#_tool_calling) 。\nSpring AI 可以选择将工具调用参数和结果导出为 span 属性，但由于敏感性原因，默认情况下禁用此功能。详情： [ 工具调用参数和结果数据](../observability/index.html#_tool_call_arguments_and_result_data) 。\n日志记录 # 工具调用功能的所有主要操作均以 DEBUG 级别记录日志。您可以通过将 org.springframework.ai 包的日志级别设置为 DEBUG 来启用日志记录。\n"},{"id":65,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B-api/groq-%E8%81%8A%E5%A4%A9/","title":"Groq 聊天","section":"聊天模型 API","content":" Groq 聊天 # [ Groq]( https://groq.com/) 是一种基于 LPU™ 的超快 AI 推理引擎，支持各种 [ AI 模型]( https://console.groq.com/docs/models) ，支持 Tool/Function Calling 并公开与 OpenAI API 兼容的端点。\nSpring AI 通过重用现有的 [ OpenAI](openai-chat.html) 客户端与 [ Groq]( https://groq.com/) 集成。为此，您需要获取 [ Groq]( https://groq.com/) Api Key ，将 base-url 设置为 [ api.groq.com/openai](https:// api.groq.com/openai) ，并选择提供的 [ Groq]( https://groq.com/) 模型之一。\n查看 [ GroqWithOpenAiChatModelIT.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/chat/proxy/[GroqWithOpenAiChatModelIT.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/chat/proxy/GroqWithOpenAiChatModelIT.java)) 测试，了解将 Groq 与 Spring AI 结合使用的示例。\n先决条件 # 创建 API 密钥 ：访问此处创建 API 密钥。Spring AI 项目定义了一个名为 spring.ai.openai.api-key 的配置属性，您应该将其设置为从 groq.com 获取的 API Key 的值。 设置 Groq URL ：您必须将 spring.ai.openai.base-url 属性设置为 api.groq.com/openai 。 选择 Groq 模型 ：使用 spring.ai.openai.chat.model= 属性从可用的 Groq 模型中进行选择。 您可以在 application.properties 文件中设置这些配置属性：\nspring.ai.openai.api-key=\u0026lt;your-groq-api-key\u0026gt; spring.ai.openai.base-url=https://api.groq.com/openai spring.ai.openai.chat.model=llama3-70b-8192 为了在处理 API 密钥等敏感信息时增强安全性，您可以使用 Spring 表达式语言 (SpEL) 来引用自定义环境变量：\n# In application.yml spring: ai: openai: api-key: ${GROQ_API_KEY} base-url: ${GROQ_BASE_URL} chat: model: ${GROQ_MODEL} # In your environment or .env file export GROQ_API_KEY=\u0026lt;your-groq-api-key\u0026gt; export GROQ_BASE_URL=https://api.groq.com/openai export GROQ_MODEL=llama3-70b-8192 您还可以在应用程序代码中以编程方式设置这些配置：\n// Retrieve configuration from secure sources or environment variables String apiKey = System.getenv(\u0026#34;GROQ_API_KEY\u0026#34;); String baseUrl = System.getenv(\u0026#34;GROQ_BASE_URL\u0026#34;); String model = System.getenv(\u0026#34;GROQ_MODEL\u0026#34;); 添加存储库和 BOM # Spring AI 的构件已发布在 Maven Central 和 Spring Snapshot 仓库中。请参阅 [ “构件仓库”](../../getting-started.html#artifact-repositories) 部分，将这些仓库添加到您的构建系统中。\n为了帮助管理依赖项，Spring AI 提供了 BOM（物料清单），以确保在整个项目中使用一致版本的 Spring AI。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # Spring AI 为 OpenAI Chat Client 提供了 Spring Boot 自动配置功能。要启用此功能，请将以下依赖项添加到项目的 Maven pom.xml 或 Gradle build.gradle 构建文件中：\n聊天属性 # 重试属性 # 前缀 spring.ai.retry 用作属性前缀，可让您配置 OpenAI 聊天模型的重试机制。\n连接属性 # 前缀 spring.ai.openai 用作允许您连接到 OpenAI 的属性前缀。\n配置属性 # 前缀 spring.ai.openai.chat 是允许您为 OpenAI 配置聊天模型实现的属性前缀。\n运行时选项 # [ OpenAiChatOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/[OpenAiChatOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/OpenAiChatOptions.java)) 提供模型配置，例如要使用的模型、温度、频率惩罚等。\n启动时，可以使用 OpenAiChatModel(api, options) 构造函数或 spring.ai.openai.chat.options.* 属性配置默认选项。\n在运行时，您可以通过向 Prompt 调用添加新的、特定于请求的选项来覆盖默认选项。例如，要覆盖特定请求的默认模型和温度：\nChatResponse response = chatModel.call( new Prompt( \u0026#34;Generate the names of 5 famous pirates.\u0026#34;, OpenAiChatOptions.builder() .model(\u0026#34;mixtral-8x7b-32768\u0026#34;) .temperature(0.4) .build() )); 函数调用 # 当选择其中一个工具/功能支持模型时，Groq API 端点支持[ 工具/功能调用]( https://console.groq.com/docs/tool-use) 。\n您可以将自定义 Java 函数注册到 ChatModel 中，并让提供的 Groq 模型智能地选择输出包含参数的 JSON 对象，以调用一个或多个已注册的函数。这是一种将 LLM 功能与外部工具和 API 连接起来的强大技术。\n工具示例 # 下面是一个如何使用 Groq 函数调用和 Spring AI 的简单示例：\n@SpringBootApplication public class GroqApplication { public static void main(String[] args) { SpringApplication.run(GroqApplication.class, args); } @Bean CommandLineRunner runner(ChatClient.Builder chatClientBuilder) { return args -\u0026gt; { var chatClient = chatClientBuilder.build(); var response = chatClient.prompt() .user(\u0026#34;What is the weather in Amsterdam and Paris?\u0026#34;) .functions(\u0026#34;weatherFunction\u0026#34;) // reference by bean name. .call() .content(); System.out.println(response); }; } @Bean @Description(\u0026#34;Get the weather in location\u0026#34;) public Function\u0026lt;WeatherRequest, WeatherResponse\u0026gt; weatherFunction() { return new MockWeatherService(); } public static class MockWeatherService implements Function\u0026lt;WeatherRequest, WeatherResponse\u0026gt; { public record WeatherRequest(String location, String unit) {} public record WeatherResponse(double temp, String unit) {} @Override public WeatherResponse apply(WeatherRequest request) { double temperature = request.location().contains(\u0026#34;Amsterdam\u0026#34;) ? 20 : 25; return new WeatherResponse(temperature, request.unit); } } } 在此示例中，当模型需要天气信息时，它将自动调用 weatherFunction bean，该 bean 可以获取实时天气数据。预期响应如下：“阿姆斯特丹的天气当前为 20 摄氏度，巴黎的天气当前为 25 摄氏度。”\n阅读有关 OpenAI [ 函数调用的]( https://docs.spring.io/spring-ai/reference/api/chat/functions/openai-chat-functions.html)更多信息。\n多式联运 # 样品控制器 # [ 创建]( https://start.spring.io/)一个新的 Spring Boot 项目并将 spring-ai-starter-model-openai 添加到您的 pom（或 gradle）依赖项中。\n在 src/main/resources 目录下添加一个 application.properties 文件，以启用和配置 OpenAi 聊天模型：\nspring.ai.openai.api-key=\u0026lt;GROQ_API_KEY\u0026gt; spring.ai.openai.base-url=https://api.groq.com/openai spring.ai.openai.chat.options.model=llama3-70b-8192 spring.ai.openai.chat.options.temperature=0.7 这将创建一个 OpenAiChatModel 实现，您可以将其注入到您的类中。这是一个使用聊天模型进行文本生成的简单 @Controller 类的示例。\n@RestController public class ChatController { private final OpenAiChatModel chatModel; @Autowired public ChatController(OpenAiChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, this.chatModel.call(message)); } @GetMapping(\u0026#34;/ai/generateStream\u0026#34;) public Flux\u0026lt;ChatResponse\u0026gt; generateStream(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { Prompt prompt = new Prompt(new UserMessage(message)); return this.chatModel.stream(prompt); } } 手动配置 # OpenAiChatModel 实现了 ChatModel 和 StreamingChatModel ，并使用 [ [low-level-api]](#low-level-api) 连接到 OpenAI 服务。\n将 spring-ai-openai 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-openai\u0026#39; } 接下来，创建一个 OpenAiChatModel 并将其用于文本生成：\nvar openAiApi = new OpenAiApi(\u0026#34;https://api.groq.com/openai\u0026#34;, System.getenv(\u0026#34;GROQ_API_KEY\u0026#34;)); var openAiChatOptions = OpenAiChatOptions.builder() .model(\u0026#34;llama3-70b-8192\u0026#34;) .temperature(0.4) .maxTokens(200) .build(); var chatModel = new OpenAiChatModel(this.openAiApi, this.openAiChatOptions); ChatResponse response = this.chatModel.call( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); // Or with streaming responses Flux\u0026lt;ChatResponse\u0026gt; response = this.chatModel.stream( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); OpenAiChatOptions 提供聊天请求的配置信息。OpenAiChatOptions.Builder 是一个流畅的选项构建 OpenAiChatOptions.Builder 。\n"},{"id":66,"href":"/docs/%E5%8F%82%E8%80%83/%E7%9F%A2%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/mariadb-%E5%90%91%E9%87%8F%E5%AD%98%E5%82%A8/","title":"MariaDB 向量存储","section":"矢量数据库","content":" MariaDB 向量存储 # 本节将引导您设置 MariaDBVectorStore 来存储文档嵌入并执行相似性搜索。\n[ MariaDB Vector]( https://mariadb.org/projects/mariadb-vector/) 是 MariaDB 11.7 的一部分，支持存储和搜索机器学习生成的嵌入。它使用向量索引提供高效的向量相似性搜索功能，支持余弦相似度和欧氏距离度量。\n先决条件 # 一个正在运行的 MariaDB (11.7+) 实例。以下选项可用： Docker 镜像 MariaDB 服务器 Docker 镜像 MariaDB 服务器 如果需要，可以使用 EmbeddingModel 的 API 密钥来生成 MariaDBVectorStore 存储的嵌入。 自动配置 # Spring AI 为 MariaDB 矢量存储提供了 Spring Boot 自动配置功能。要启用此功能，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-mariadb\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-vector-store-mariadb\u0026#39; } 向量存储实现可以为您初始化所需的模式，但您必须通过在适当的构造函数中指定 initializeSchema 布尔值或在 application.properties 文件中设置 …​initialize-schema=true 来选择加入。\n此外，您还需要一个已配置的 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) bean。有关更多信息，请参阅 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) 部分。\n例如，要使用 [ OpenAI EmbeddingModel](../embeddings/openai-embeddings.html) ，请添加以下依赖项：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 现在您可以在应用程序中自动连接 MariaDBVectorStore ：\n@Autowired VectorStore vectorStore; // ... List\u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); // Add the documents to MariaDB vectorStore.add(documents); // Retrieve documents similar to a query List\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch(SearchRequest.builder().query(\u0026#34;Spring\u0026#34;).topK(5).build()); 配置属性 # 要连接到 MariaDB 并使用 MariaDBVectorStore ，您需要提供实例的访问详细信息。可以通过 Spring Boot 的 application.yml 提供简单的配置：\nspring: datasource: url: jdbc:mariadb://localhost/db username: myUser password: myPassword ai: vectorstore: mariadb: initialize-schema: true distance-type: COSINE dimensions: 1536 以 spring.ai.vectorstore.mariadb.* 开头的属性用于配置 MariaDBVectorStore ：\n手动配置 # 除了使用 Spring Boot 自动配置之外，您还可以手动配置 MariaDB 向量存储。为此，您需要向项目添加以下依赖项：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-jdbc\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.mariadb.jdbc\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mariadb-java-client\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;runtime\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-mariadb-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 然后使用构建器模式创建 MariaDBVectorStore bean：\n@Bean public VectorStore vectorStore(JdbcTemplate jdbcTemplate, EmbeddingModel embeddingModel) { return MariaDBVectorStore.builder(jdbcTemplate, embeddingModel) .dimensions(1536) // Optional: defaults to 1536 .distanceType(MariaDBDistanceType.COSINE) // Optional: defaults to COSINE .schemaName(\u0026#34;mydb\u0026#34;) // Optional: defaults to null .vectorTableName(\u0026#34;custom_vectors\u0026#34;) // Optional: defaults to \u0026#34;vector_store\u0026#34; .contentFieldName(\u0026#34;text\u0026#34;) // Optional: defaults to \u0026#34;content\u0026#34; .embeddingFieldName(\u0026#34;embedding\u0026#34;) // Optional: defaults to \u0026#34;embedding\u0026#34; .idFieldName(\u0026#34;doc_id\u0026#34;) // Optional: defaults to \u0026#34;id\u0026#34; .metadataFieldName(\u0026#34;meta\u0026#34;) // Optional: defaults to \u0026#34;metadata\u0026#34; .initializeSchema(true) // Optional: defaults to false .schemaValidation(true) // Optional: defaults to false .removeExistingVectorStoreTable(false) // Optional: defaults to false .maxDocumentBatchSize(10000) // Optional: defaults to 10000 .build(); } // This can be any EmbeddingModel implementation @Bean public EmbeddingModel embeddingModel() { return new OpenAiEmbeddingModel(new OpenAiApi(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;))); } 元数据过滤 # 您可以利用 MariaDB Vector 存储的通用、可移植元[ 数据过滤器](../vectordbs.html#metadata-filters) 。\n例如，您可以使用文本表达语言：\nvectorStore.similaritySearch( SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(\u0026#34;author in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; article_type == \u0026#39;blog\u0026#39;\u0026#34;).build()); 或者以编程方式使用 Filter.Expression DSL：\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(b.and( b.in(\u0026#34;author\u0026#34;, \u0026#34;john\u0026#34;, \u0026#34;jill\u0026#34;), b.eq(\u0026#34;article_type\u0026#34;, \u0026#34;blog\u0026#34;)).build()).build()); 访问 Native Client # MariaDB Vector Store 实现通过 getNativeClient() 方法提供对底层本机 JDBC 客户端（ JdbcTemplate ）的访问：\nMariaDBVectorStore vectorStore = context.getBean(MariaDBVectorStore.class); Optional\u0026lt;JdbcTemplate\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { JdbcTemplate jdbc = nativeClient.get(); // Use the native client for MariaDB-specific operations } 本机客户端使您可以访问可能无法通过 VectorStore 接口公开的 MariaDB 特定功能和操作。\n"},{"id":67,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B-api/openai-%E5%B5%8C%E5%85%A5/","title":"OpenAI 嵌入","section":"嵌入模型 API","content":" OpenAI 嵌入 # Spring AI 支持 OpenAI 的文本嵌入模型。OpenAI 的文本嵌入用于衡量文本字符串的相关性。嵌入是一个浮点数向量（列表）。两个向量之间的距离衡量它们的相关性。距离越小，相关性越高；距离越大，相关性越低。\n先决条件 # 您需要使用 OpenAI 创建一个 API 来访问 OpenAI 嵌入模型。\n在 [ OpenAI 注册页面]( https://platform.openai.com/signup)创建一个帐户，并在 [ API 密钥页面]( https://platform.openai.com/account/api-keys)上生成令牌。\nSpring AI 项目定义了一个名为 spring.ai.openai.api-key 的配置属性，您应该将其设置为从 openai.com 获取的 API Key 的值。\n您可以在 application.properties 文件中设置此配置属性：\nspring.ai.openai.api-key=\u0026lt;your-openai-api-key\u0026gt; 为了在处理 API 密钥等敏感信息时增强安全性，您可以使用 Spring 表达式语言 (SpEL) 来引用环境变量：\n# In application.yml spring: ai: openai: api-key: ${OPENAI_API_KEY} # In your environment or .env file export OPENAI_API_KEY=\u0026lt;your-openai-api-key\u0026gt; 您还可以在应用程序代码中以编程方式设置此配置：\n// Retrieve API key from a secure source or environment variable String apiKey = System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;); 添加存储库和 BOM # Spring AI 的构件已发布在 Maven Central 和 Spring Snapshot 仓库中。请参阅 [ “构件仓库”](../../getting-started.html#artifact-repositories) 部分，将这些仓库添加到您的构建系统中。\n为了帮助管理依赖项，Spring AI 提供了 BOM（物料清单），以确保在整个项目中使用一致版本的 Spring AI。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # Spring AI 为 OpenAI 嵌入模型提供了 Spring Boot 自动配置功能。要启用此功能，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-openai\u0026#39; } 嵌入属性 # 重试属性 # 前缀 spring.ai.retry 用作属性前缀，可让您配置 OpenAI Embedding 模型的重试机制。\n连接属性 # 前缀 spring.ai.openai 用作允许您连接到 OpenAI 的属性前缀。\n配置属性 # 前缀 spring.ai.openai.embedding 是配置 OpenAI 的 EmbeddingModel 实现的属性前缀。\n运行时选项 # [ OpenAiEmbeddingOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/[OpenAiEmbeddingOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/OpenAiEmbeddingOptions.java)) 提供 OpenAI 配置，例如要使用的模型等。\n也可以使用 spring.ai.openai.embedding.options 属性来配置默认选项。\n启动时，使用 OpenAiEmbeddingModel 构造函数设置所有嵌入请求的默认选项。运行时，您可以使用 OpenAiEmbeddingOptions 实例作为 EmbeddingRequest 的一部分来覆盖默认选项。\n例如，要覆盖特定请求的默认模型名称：\nEmbeddingResponse embeddingResponse = embeddingModel.call( new EmbeddingRequest(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;), OpenAiEmbeddingOptions.builder() .model(\u0026#34;Different-Embedding-Model-Deployment-Name\u0026#34;) .build())); 样品控制器 # 这将创建一个 EmbeddingModel 实现，您可以将其注入到您的类中。这是一个使用 EmbeddingModel 实现的简单 @Controller 类的示例。\nspring.ai.openai.api-key=YOUR_API_KEY spring.ai.openai.embedding.options.model=text-embedding-ada-002 @RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(\u0026#34;/ai/embedding\u0026#34;) public Map embed(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { EmbeddingResponse embeddingResponse = this.embeddingModel.embedForResponse(List.of(message)); return Map.of(\u0026#34;embedding\u0026#34;, embeddingResponse); } } 手动配置 # 如果您不使用 Spring Boot，则可以手动配置 OpenAI 嵌入模型。为此，请将 spring-ai-openai 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-openai\u0026#39; } 接下来，创建一个 OpenAiEmbeddingModel 实例并使用它来计算两个输入文本之间的相似度：\nvar openAiApi = OpenAiApi.builder() .apiKey(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;)) .build(); var embeddingModel = new OpenAiEmbeddingModel( this.openAiApi, MetadataMode.EMBED, OpenAiEmbeddingOptions.builder() .model(\u0026#34;text-embedding-ada-002\u0026#34;) .user(\u0026#34;user-6\u0026#34;) .build(), RetryUtils.DEFAULT_RETRY_TEMPLATE); EmbeddingResponse embeddingResponse = this.embeddingModel .embedForResponse(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;)); OpenAiEmbeddingOptions 提供嵌入请求的配置信息。api 和 options 类提供了一个 builder() ，方便用户轻松创建选项。\n"},{"id":68,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B%E4%B8%8A%E4%B8%8B%E6%96%87%E5%8D%8F%E8%AE%AEmcp/","title":"模型上下文协议（MCP）","section":"参考","content":" 模型上下文协议（MCP） # [ 模型上下文协议]( https://modelcontextprotocol.org/docs/concepts/architecture) (MCP) 是一种标准化协议，使 AI 模型能够以结构化的方式与外部工具和资源进行交互。它支持多种传输机制，从而提供跨不同环境的灵活性。\n[ MCP Java SDK]( https://modelcontextprotocol.io/sdk/java) 提供了模型上下文协议的 Java 实现，支持通过同步和异步通信模式与 AI 模型和工具进行标准化交互。\nSpring AI MCP 通过 Spring Boot 集成扩展了 MCP Java SDK，提供[ 客户端](mcp-client-boot-starter-docs.html)和[ 服务器](mcp-server-boot-starter-docs.html)启动器。使用 [ Spring Initializer]( https://start.spring.io) 引导您的 AI 应用程序，使其支持 MCP。\nMCP Java SDK 架构 # Java MCP 实现遵循三层架构：\n有关使用低级 MCP 客户端/服务器 API 的详细实施指南，请参阅 [ MCP Java SDK 文档]( https://modelcontextprotocol.io/sdk/java) 。如需使用 Spring Boot 进行简化设置，请使用下文所述的 MCP Boot Starters。\nSpring AI MCP 集成 # Spring AI 通过以下 Spring Boot 启动器提供 MCP 集成：\n客户端启动器 # spring-ai-starter-mcp-client - 核心启动器提供 STDIO 和基于 HTTP 的 SSE 支持 spring-ai-starter-mcp-client-webflux - 基于 WebFlux 的 SSE 传输实现 服务器启动器 # spring-ai-starter-mcp-server - 具有 STDIO 传输支持的核心服务器 spring-ai-starter-mcp-server-webmvc - 基于 Spring MVC 的 SSE 传输实现 spring-ai-starter-mcp-server-webflux - 基于 WebFlux 的 SSE 传输实现 其他资源 # MCP 客户端启动启动器文档 MCP 服务器启动启动器文档 MCP 实用程序文档 模型上下文协议规范 "},{"id":69,"href":"/docs/%E5%8F%82%E8%80%83/%E7%9F%A2%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/milvus/","title":"Milvus","section":"矢量数据库","content":" Milvus # [ Milvus]( https://milvus.io/) 是一个开源向量数据库，在数据科学和机器学习领域备受关注。其突出特点之一在于对向量索引和查询的强大支持。[ Milvus]( https://milvus.io/) 采用先进的尖端算法来加速搜索过程，即使在处理海量数据集时，也能高效地检索相似的向量。\n先决条件 # 正在运行的 Milvus 实例。有以下选项可供选择： Milvus 单机版 ：Docker、Operator、Helm、DEB/RPM、Docker Compose。 Milvus Cluster ：操作员、Helm。 Milvus 单机版 ：Docker、Operator、Helm、DEB/RPM、Docker Compose。 Milvus Cluster ：操作员、Helm。 如果需要， EmbeddingModel 的 API 密钥可以生成 MilvusVectorStore 存储的嵌入。 依赖项 # 然后将 Milvus VectorStore 启动启动器依赖项添加到您的项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-milvus\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-vector-store-milvus\u0026#39; } 向量存储实现可以为您初始化必要的模式，但您必须通过在适当的构造函数中指定 initializeSchema 布尔值或在 application.properties 文件中设置 …​initialize-schema=true 来选择加入。\nVector Store 还需要一个 EmbeddingModel 实例来计算文档的嵌入。你可以从可用的 EmbeddingModel 实现中选择一个。\n要连接并配置 MilvusVectorStore ，您需要提供实例的访问详细信息。您可以通过 Spring Boot 的 application.yml 提供简单的配置。\n现在您可以在应用程序中自动连接 Milvus 矢量存储并使用它\n@Autowired VectorStore vectorStore; // ... List \u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); // Add the documents to Milvus Vector Store vectorStore.add(documents); // Retrieve documents similar to a query List\u0026lt;Document\u0026gt; results = this.vectorStore.similaritySearch(SearchRequest.builder().query(\u0026#34;Spring\u0026#34;).topK(5).build()); 手动配置 # 除了使用 Spring Boot 自动配置之外，您还可以手动配置 MilvusVectorStore 。要将以下依赖项添加到您的项目：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-milvus-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 要在您的应用程序中配置 MilvusVectorStore，您可以使用以下设置：\n@Bean public VectorStore vectorStore(MilvusServiceClient milvusClient, EmbeddingModel embeddingModel) { return MilvusVectorStore.builder(milvusClient, embeddingModel) .collectionName(\u0026#34;test_vector_store\u0026#34;) .databaseName(\u0026#34;default\u0026#34;) .indexType(IndexType.IVF_FLAT) .metricType(MetricType.COSINE) .batchingStrategy(new TokenCountBatchingStrategy()) .initializeSchema(true) .build(); } @Bean public MilvusServiceClient milvusClient() { return new MilvusServiceClient(ConnectParam.newBuilder() .withAuthorization(\u0026#34;minioadmin\u0026#34;, \u0026#34;minioadmin\u0026#34;) .withUri(milvusContainer.getEndpoint()) .build()); } 元数据过滤 # 您可以利用 Milvus 商店的通用、可移植[ 元数据过滤器]( https://docs.spring.io/spring-ai/reference/api/vectordbs.html#_metadata_filters) 。\n例如，您可以使用文本表达语言：\nvectorStore.similaritySearch( SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(\u0026#34;author in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; article_type == \u0026#39;blog\u0026#39;\u0026#34;).build()); 或者以编程方式使用 Filter.Expression DSL：\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(b.and( b.in(\u0026#34;author\u0026#34;,\u0026#34;john\u0026#34;, \u0026#34;jill\u0026#34;), b.eq(\u0026#34;article_type\u0026#34;, \u0026#34;blog\u0026#34;)).build()).build()); 使用 MilvusSearchRequest # MilvusSearchRequest 扩展了 SearchRequest，允许您使用 Milvus 特定的搜索参数，例如原生表达式和搜索参数 JSON。\nMilvusSearchRequest request = MilvusSearchRequest.milvusBuilder() .query(\u0026#34;sample query\u0026#34;) .topK(5) .similarityThreshold(0.7) .nativeExpression(\u0026#34;metadata[\\\u0026#34;age\\\u0026#34;] \u0026gt; 30\u0026#34;) // Overrides filterExpression if both are set .filterExpression(\u0026#34;age \u0026lt;= 30\u0026#34;) // Ignored if nativeExpression is set .searchParamsJson(\u0026#34;{\\\u0026#34;nprobe\\\u0026#34;:128}\u0026#34;) .build(); List results = vectorStore.similaritySearch(request); 这使得使用 Milvus 特定的搜索功能时具有更大的灵活性。\nMilvusSearchRequest 中 nativeExpression 和 searchParamsJson 的重要性 # 这两个参数增强了 Milvus 的搜索精度并确保了最佳的查询性能：\nnativeExpression ：使用 Milvus 的原生过滤表达式启用额外的过滤功能。 [ Milvus 过滤]( https://milvus.io/docs/boolean.md)\n例子：\nMilvusSearchRequest request = MilvusSearchRequest.milvusBuilder() .query(\u0026#34;sample query\u0026#34;) .topK(5) .nativeExpression(\u0026#34;metadata[\u0026#39;category\u0026#39;] == \u0026#39;science\u0026#39;\u0026#34;) .build(); searchParamsJson ：使用 IVF_FLAT（Milvus 的默认索引）时调整搜索行为至关重要。 [ Milvus 矢量索引]( https://milvus.io/docs/index.md?tab=floating)\n默认情况下， IVF_FLAT 要求设置 nprobe 才能获得准确的结果。如果未指定， nprobe 默认为 1 ，这可能导致召回率较低，甚至搜索结果为零。\n例子：\nMilvusSearchRequest request = MilvusSearchRequest.milvusBuilder() .query(\u0026#34;sample query\u0026#34;) .topK(5) .searchParamsJson(\u0026#34;{\\\u0026#34;nprobe\\\u0026#34;:128}\u0026#34;) .build(); 使用 nativeExpression 可确保高级过滤，而 searchParamsJson 可防止因默认 nprobe 值较低而导致的无效搜索。\nMilvus VectorStore 属性 # 您可以在 Spring Boot 配置中使用以下属性来自定义 Milvus 矢量存储。\n启动 Milvus 商店 # 从 src/test/resources/ 文件夹运行：\ndocker-compose up 清洁环境：\ndocker-compose down; rm -Rf ./volumes 然后连接到 [ http://localhost:19530]( http://localhost:19530) 上的向量存储或进行管理 [ http://localhost:9001]( http://localhost:9001) （用户： minioadmin ，密码： minioadmin ）\n故障排除 # 如果 Docker 抱怨资源不足，则执行：\ndocker system prune --all --force --volumes 访问 Native Client # Milvus Vector Store 实现通过 getNativeClient() 方法提供对底层原生 Milvus 客户端（ MilvusServiceClient ）的访问：\nMilvusVectorStore vectorStore = context.getBean(MilvusVectorStore.class); Optional\u0026lt;MilvusServiceClient\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { MilvusServiceClient client = nativeClient.get(); // Use the native client for Milvus-specific operations } 本机客户端使您可以访问可能无法通过 VectorStore 界面公开的 Milvus 特定功能和操作。\n"},{"id":70,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B-api/postgresml-%E5%B5%8C%E5%85%A5/","title":"PostgresML 嵌入","section":"嵌入模型 API","content":" PostgresML 嵌入 # Spring AI 支持 PostgresML 文本嵌入模型。\n嵌入是文本的数值表示形式。它们用于将单词和句子表示为向量（数字数组）。嵌入可以用来查找相似的文本片段，方法是使用距离度量比较数值向量的相似性；或者，由于大多数算法无法直接使用文本，因此嵌入可以用作其他机器学习模型的输入特征。\n许多预训练的 LLM 可用于在 PostgresML 中从文本生成嵌入。您可以浏览所有可用的[ 模型]( https://huggingface.co/models?library=sentence-transformers) ，在 Hugging Face 上找到最佳解决方案。\n添加存储库和 BOM # Spring AI 的构件已发布在 Maven Central 和 Spring Snapshot 仓库中。请参阅 [ “构件仓库”](../../getting-started.html#artifact-repositories) 部分，将这些仓库添加到您的构建系统中。\n为了帮助管理依赖项，Spring AI 提供了 BOM（物料清单），以确保在整个项目中使用一致版本的 Spring AI。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # Spring AI 为 Azure PostgresML 嵌入模型提供 Spring Boot 自动配置。 Spring AI offers Spring Boot auto-configuration for the Azure PostgresML Embedding Model. 若要启用此功能，请将以下依赖项添加到项目的 Maven pom.xml 文件：To enable it, add the following dependency to your project\u0026rsquo;s Maven pom.xml file:\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-postgresml-embedding\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-postgresml-embedding\u0026#39; } 使用 spring.ai.postgresml.embedding.options.* 属性配置您的 PostgresMlEmbeddingModel 。链接\n嵌入属性 # 前缀 spring.ai.postgresml.embedding 是配置 PostgresML 嵌入的 EmbeddingModel 实现的属性前缀。\n运行时选项 # 使用 [ PostgresMlEmbeddingOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/postgresml/[PostgresMlEmbeddingOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/postgresml/PostgresMlEmbeddingOptions.java)) 配置 PostgresMlEmbeddingModel 的选项，例如要使用的模型等。\n启动时，您可以将 PostgresMlEmbeddingOptions 传递给 PostgresMlEmbeddingModel 构造函数来配置用于所有嵌入请求的默认选项。\n在运行时，您可以使用 EmbeddingRequest 中的 PostgresMlEmbeddingOptions 覆盖默认选项。\n例如，要覆盖特定请求的默认模型名称：\nEmbeddingResponse embeddingResponse = embeddingModel.call( new EmbeddingRequest(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;), PostgresMlEmbeddingOptions.builder() .transformer(\u0026#34;intfloat/e5-small\u0026#34;) .vectorType(VectorType.PG_ARRAY) .kwargs(Map.of(\u0026#34;device\u0026#34;, \u0026#34;gpu\u0026#34;)) .build())); 样品控制器 # 这将创建一个 EmbeddingModel 实现，您可以将其注入到您的类中。这是一个使用 EmbeddingModel 实现的简单 @Controller 类的示例。\nspring.ai.postgresml.embedding.options.transformer=distilbert-base-uncased spring.ai.postgresml.embedding.options.vectorType=PG_ARRAY spring.ai.postgresml.embedding.options.metadataMode=EMBED spring.ai.postgresml.embedding.options.kwargs.device=cpu @RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(\u0026#34;/ai/embedding\u0026#34;) public Map embed(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { EmbeddingResponse embeddingResponse = this.embeddingModel.embedForResponse(List.of(message)); return Map.of(\u0026#34;embedding\u0026#34;, embeddingResponse); } } 手动配置 # 除了使用 Spring Boot 自动配置外，您还可以手动创建 PostgresMlEmbeddingModel 。为此，请将 spring-ai-postgresml 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-postgresml\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-postgresml\u0026#39; } 接下来，创建一个 PostgresMlEmbeddingModel 实例并使用它来计算两个输入文本之间的相似度：\nvar jdbcTemplate = new JdbcTemplate(dataSource); // your posgresml data source PostgresMlEmbeddingModel embeddingModel = new PostgresMlEmbeddingModel(this.jdbcTemplate, PostgresMlEmbeddingOptions.builder() .transformer(\u0026#34;distilbert-base-uncased\u0026#34;) // huggingface transformer model name. .vectorType(VectorType.PG_VECTOR) //vector type in PostgreSQL. .kwargs(Map.of(\u0026#34;device\u0026#34;, \u0026#34;cpu\u0026#34;)) // optional arguments. .metadataMode(MetadataMode.EMBED) // Document metadata mode. .build()); embeddingModel.afterPropertiesSet(); // initialize the jdbc template and database. EmbeddingResponse embeddingResponse = this.embeddingModel .embedForResponse(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;)); @Bean public EmbeddingModel embeddingModel(JdbcTemplate jdbcTemplate) { return new PostgresMlEmbeddingModel(jdbcTemplate, PostgresMlEmbeddingOptions.builder() .... .build()); } "},{"id":71,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B-api/%E6%8B%A5%E6%8A%B1%E8%84%B8%E8%81%8A%E5%A4%A9/","title":"拥抱脸聊天","section":"聊天模型 API","content":" 拥抱脸聊天 # Hugging Face 文本生成推理 (TGI) 是一种专门的部署解决方案，用于在云端提供大型语言模型 (LLM)，并通过 API 访问它们。TGI 通过持续批处理、令牌流和高效的内存管理等功能，为文本生成任务提供优化的性能。\n先决条件 # 您需要在 Hugging Face 上创建一个推理端点，并创建一个 API 令牌来访问该端点。更多详情请点击[ 此处]( https://huggingface.co/docs/inference-endpoints/index) 。\nSpring AI 项目定义了两个配置属性：\n您可以[ 在此处]( https://ui.endpoints.huggingface.co/)的推理端点 UI 上找到您的推理端点 URL。\n您可以在 application.properties 文件中设置这些配置属性：\nspring.ai.huggingface.chat.api-key=\u0026lt;your-huggingface-api-key\u0026gt; spring.ai.huggingface.chat.url=\u0026lt;your-inference-endpoint-url\u0026gt; 为了在处理 API 密钥等敏感信息时增强安全性，您可以使用 Spring 表达式语言 (SpEL) 来引用自定义环境变量：\n# In application.yml spring: ai: huggingface: chat: api-key: ${HUGGINGFACE_API_KEY} url: ${HUGGINGFACE_ENDPOINT_URL} # In your environment or .env file export HUGGINGFACE_API_KEY=\u0026lt;your-huggingface-api-key\u0026gt; export HUGGINGFACE_ENDPOINT_URL=\u0026lt;your-inference-endpoint-url\u0026gt; 您还可以在应用程序代码中以编程方式设置这些配置：\n// Retrieve API key and endpoint URL from secure sources or environment variables String apiKey = System.getenv(\u0026#34;HUGGINGFACE_API_KEY\u0026#34;); String endpointUrl = System.getenv(\u0026#34;HUGGINGFACE_ENDPOINT_URL\u0026#34;); 添加存储库和 BOM # Spring AI 的构件已发布在 Maven Central 和 Spring Snapshot 仓库中。请参阅 [ “构件仓库”](../../getting-started.html#artifact-repositories) 部分，将这些仓库添加到您的构建系统中。\n为了帮助管理依赖项，Spring AI 提供了 BOM（物料清单），以确保在整个项目中使用一致版本的 Spring AI。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # Spring AI 为 Hugging Face Chat Client 提供了 Spring Boot 自动配置功能。要启用此功能，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-huggingface\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-huggingface\u0026#39; } 聊天属性 # 前缀 spring.ai.huggingface 是允许您配置 Hugging Face 的聊天模型实现的属性前缀。\n样本控制器（自动配置） # [ 创建]( https://start.spring.io/)一个新的 Spring Boot 项目并将 spring-ai-starter-model-huggingface 添加到您的 pom（或 gradle）依赖项中。\n在 src/main/resources 目录下添加一个 application.properties 文件，以启用和配置 Hugging Face 聊天模型：\nspring.ai.huggingface.chat.api-key=YOUR_API_KEY spring.ai.huggingface.chat.url=YOUR_INFERENCE_ENDPOINT_URL 这将创建一个 HuggingfaceChatModel 实现，您可以将其注入到您的类中。这是一个使用聊天模型生成文本的简单 @Controller 类的示例。\n@RestController public class ChatController { private final HuggingfaceChatModel chatModel; @Autowired public ChatController(HuggingfaceChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, this.chatModel.call(message)); } } 手动配置 # HuggingfaceChatModel 实现了 ChatModel 接口，并使用 [ [low-level-api]](#low-level-api) 连接到 Hugging Face 推理端点。\n将 spring-ai-huggingface 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-huggingface\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-huggingface\u0026#39; } 接下来，创建一个 HuggingfaceChatModel 并将其用于文本生成：\nHuggingfaceChatModel chatModel = new HuggingfaceChatModel(apiKey, url); ChatResponse response = this.chatModel.call( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); System.out.println(response.getGeneration().getResult().getOutput().getContent()); "},{"id":72,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90/","title":"检索增强生成","section":"参考","content":" 检索增强生成 # 检索增强生成 (RAG) 是一种有助于克服大型语言模型在长篇内容、事实准确性和上下文感知方面的局限性的技术。\nSpring AI 通过提供模块化架构来支持 RAG，该架构允许您自己构建自定义 RAG 流或使用 Advisor API 使用开箱即用的 RAG 流。\n顾问 # Spring AI 使用 Advisor API 为常见的 RAG 流提供开箱即用的支持。\n要使用 QuestionAnswerAdvisor 或 RetrievalAugmentationAdvisor ，您需要将 spring-ai-advisors-vector-store 依赖项添加到您的项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-advisors-vector-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 问答顾问 # 向量数据库存储的是 AI 模型无法感知的数据。当用户问题发送到 AI 模型时， QuestionAnswerAdvisor 会查询向量数据库，查找与用户问题相关的文档。\n来自向量数据库的响应被附加到用户文本中，为 AI 模型生成响应提供上下文。\n假设您已经将数据加载到 VectorStore 中，则可以通过向 ChatClient 提供 QuestionAnswerAdvisor 实例来执行检索增强生成 (RAG)。\nChatResponse response = ChatClient.builder(chatModel) .build().prompt() .advisors(new QuestionAnswerAdvisor(vectorStore)) .user(userText) .call() .chatResponse(); 在此示例中， QuestionAnswerAdvisor 将对 Vector 数据库中的所有文档执行相似性搜索。为了限制搜索的文档类型， SearchRequest 采用了类似 SQL 的过滤表达式，该表达式可在所有 VectorStores 之间移植。\n此过滤表达式可以在创建 QuestionAnswerAdvisor 时进行配置，因此将始终适用于所有 ChatClient 请求，或者可以在每个请求运行时提供。\n下面介绍如何创建 QuestionAnswerAdvisor 实例，其中阈值为 0.8 ，并返回前 6 结果。\nvar qaAdvisor = QuestionAnswerAdvisor.builder(vectorStore) .searchRequest(SearchRequest.builder().similarityThreshold(0.8d).topK(6).build()) .build(); 动态过滤表达式 # 使用 FILTER_EXPRESSION 顾问上下文参数在运行时更新 SearchRequest 过滤表达式：\nChatClient chatClient = ChatClient.builder(chatModel) .defaultAdvisors(QuestionAnswerAdvisor.builder(vectorStore) .searchRequest(SearchRequest.builder().build()) .build()) .build(); // Update filter expression at runtime String content = this.chatClient.prompt() .user(\u0026#34;Please answer my question XYZ\u0026#34;) .advisors(a -\u0026gt; a.param(QuestionAnswerAdvisor.FILTER_EXPRESSION, \u0026#34;type == \u0026#39;Spring\u0026#39;\u0026#34;)) .call() .content(); FILTER_EXPRESSION 参数允许您根据提供的表达式动态过滤搜索结果。\n自定义模板 # QuestionAnswerAdvisor 使用默认模板，通过检索到的文档来扩充用户问题。您可以通过 .promptTemplate() 构建器方法提供自己的 PromptTemplate 对象来自定义此行为。\n自定义的 PromptTemplate 可以使用任何 TemplateRenderer 实现（默认情况下，它使用基于 [ StringTemplate]( https://www.stringtemplate.org/) 引擎的 StPromptTemplate ）。重要的要求是模板必须包含以下两个占位符：\n用于接收用户问题的 query 占位符。 一个 question_answer_context 占位符来接收检索到的上下文。 PromptTemplate customPromptTemplate = PromptTemplate.builder() .renderer(StTemplateRenderer.builder().startDelimiterToken(\u0026#39;\u0026lt;\u0026#39;).endDelimiterToken(\u0026#39;\u0026gt;\u0026#39;).build()) .template(\u0026#34;\u0026#34;\u0026#34; \u0026lt;query\u0026gt; Context information is below. --------------------- \u0026lt;question_answer_context\u0026gt; --------------------- Given the context information and no prior knowledge, answer the query. Follow these rules: 1. If the answer is not in the context, just say that you don\u0026#39;t know. 2. Avoid statements like \u0026#34;Based on the context...\u0026#34; or \u0026#34;The provided information...\u0026#34;. \u0026#34;\u0026#34;\u0026#34;) .build(); String question = \u0026#34;Where does the adventure of Anacletus and Birba take place?\u0026#34;; QuestionAnswerAdvisor qaAdvisor = QuestionAnswerAdvisor.builder(vectorStore) .promptTemplate(customPromptTemplate) .build(); String response = ChatClient.builder(chatModel).build() .prompt(question) .advisors(qaAdvisor) .call() .content(); 检索增强顾问 # Spring AI 包含一个 [ RAG 模块库](#modules) ，您可以使用它 RetrievalAugmentation``Advisor``` 构建自己的 RAG 流程。RetrievalAugmentationAdvisor``` 是一个 Advisor`` ，它基于模块化架构，为最常见的 RAG 流程提供开箱即用的实现。\n顺序 RAG 流 # 天真的 RAG # Advisor retrievalAugmentationAdvisor = RetrievalAugmentationAdvisor.builder() .documentRetriever(VectorStoreDocumentRetriever.builder() .similarityThreshold(0.50) .vectorStore(vectorStore) .build()) .build(); String answer = chatClient.prompt() .advisors(retrievalAugmentationAdvisor) .user(question) .call() .content(); 默认情况下， RetrievalAugmentationAdvisor 不允许检索到的上下文为空。如果出现这种情况，它会指示模型不回答用户查询。您可以按如下方式允许空上下文。\nAdvisor retrievalAugmentationAdvisor = RetrievalAugmentationAdvisor.builder() .documentRetriever(VectorStoreDocumentRetriever.builder() .similarityThreshold(0.50) .vectorStore(vectorStore) .build()) .queryAugmenter(ContextualQueryAugmenter.builder() .allowEmptyContext(true) .build()) .build(); String answer = chatClient.prompt() .advisors(retrievalAugmentationAdvisor) .user(question) .call() .content(); VectorStoreDocumentRetriever 接受 FilterExpression ，以便根据元数据过滤搜索结果。您可以在实例化 VectorStoreDocumentRetriever 时或在每次请求运行时使用 FILTER_EXPRESSION advisor 上下文参数提供一个 FilterExpression 参数。\nAdvisor retrievalAugmentationAdvisor = RetrievalAugmentationAdvisor.builder() .documentRetriever(VectorStoreDocumentRetriever.builder() .similarityThreshold(0.50) .vectorStore(vectorStore) .build()) .build(); String answer = chatClient.prompt() .advisors(retrievalAugmentationAdvisor) .advisors(a -\u0026gt; a.param(VectorStoreDocumentRetriever.FILTER_EXPRESSION, \u0026#34;type == \u0026#39;Spring\u0026#39;\u0026#34;)) .user(question) .call() .content(); 有关更多信息，请参阅 [ VectorStoreDocumentRetriever](#_vectorstoredocumentretriever) 。\n高级 RAG # Advisor retrievalAugmentationAdvisor = RetrievalAugmentationAdvisor.builder() .queryTransformers(RewriteQueryTransformer.builder() .chatClientBuilder(chatClientBuilder.build().mutate()) .build()) .documentRetriever(VectorStoreDocumentRetriever.builder() .similarityThreshold(0.50) .vectorStore(vectorStore) .build()) .build(); String answer = chatClient.prompt() .advisors(retrievalAugmentationAdvisor) .user(question) .call() .content(); 您还可以使用 DocumentPostProcessor API 对检索到的文档进行后处理，然后再将其传递给模型。例如，您可以使用该接口根据检索到的文档与查询的相关性对其进行重新排序，移除不相关或冗余的文档，或者压缩每个文档的内容以减少噪音和冗余。\n模块 # Spring AI 实现了一种模块化 RAG 架构，其灵感来自于论文“ [ 模块化 RAG：将 RAG 系统转变为类似乐高的可重构框架]( https://arxiv.org/abs/2407.21059) ”中详述的模块化概念。\n预检索 # 预检索模块负责处理用户查询以获得最佳的检索结果。\n查询转换 # 用于转换输入查询以使其更有效地执行检索任务的组件，解决诸如格式不正确的查询、歧义的术语、复杂的词汇或不受支持的语言等挑战。\n压缩查询转换器 # CompressionQueryTransformer 使用大型语言模型将对话历史和后续查询压缩为独立查询，以捕捉对话的本质。\n当对话历史很长并且后续查询与对话上下文相关时，此转换器很有用。\nQuery query = Query.builder() .text(\u0026#34;And what is its second largest city?\u0026#34;) .history(new UserMessage(\u0026#34;What is the capital of Denmark?\u0026#34;), new AssistantMessage(\u0026#34;Copenhagen is the capital of Denmark.\u0026#34;)) .build(); QueryTransformer queryTransformer = CompressionQueryTransformer.builder() .chatClientBuilder(chatClientBuilder) .build(); Query transformedQuery = queryTransformer.transform(query); 可以通过构建器中提供的 promptTemplate() 方法自定义此组件使用的提示。\n重写查询转换器 # RewriteQueryTransformer 使用大型语言模型重写用户查询，以便在查询目标系统（例如向量存储或 Web 搜索引擎）时提供更好的结果。\n当用户查询冗长、模糊或包含可能影响搜索结果质量的不相关信息时，此转换器很有用。\nQuery query = new Query(\u0026#34;I\u0026#39;m studying machine learning. What is an LLM?\u0026#34;); QueryTransformer queryTransformer = RewriteQueryTransformer.builder() .chatClientBuilder(chatClientBuilder) .build(); Query transformedQuery = queryTransformer.transform(query); 可以通过构建器中提供的 promptTemplate() 方法自定义此组件使用的提示。\n翻译查询转换器 # TranslationQueryTransformer 使用大型语言模型将查询翻译成用于生成文档嵌入的嵌入模型所支持的目标语言。如果查询已经是目标语言，则返回原样。如果查询的语言未知，则也返回原样。\n当嵌入模型在特定语言上进行训练并且用户查询使用不同的语言时，此转换器很有用。\nQuery query = new Query(\u0026#34;Hvad er Danmarks hovedstad?\u0026#34;); QueryTransformer queryTransformer = TranslationQueryTransformer.builder() .chatClientBuilder(chatClientBuilder) .targetLanguage(\u0026#34;english\u0026#34;) .build(); Query transformedQuery = queryTransformer.transform(query); 可以通过构建器中提供的 promptTemplate() 方法自定义此组件使用的提示。\n查询扩展 # 用于将输入查询扩展为查询列表的组件，通过提供替代查询公式或将复杂问题分解为更简单的子查询来解决诸如格式不良的查询等挑战。\n多查询扩展器 # MultiQueryExpander 使用大型语言模型将查询扩展为多个语义不同的变体，以捕捉不同的视角，有助于检索额外的上下文信息并增加找到相关结果的机会。\nMultiQueryExpander queryExpander = MultiQueryExpander.builder() .chatClientBuilder(chatClientBuilder) .numberOfQueries(3) .build(); List\u0026lt;Query\u0026gt; queries = queryExpander.expand(new Query(\u0026#34;How to run a Spring Boot app?\u0026#34;)); 默认情况下， MultiQueryExpander 会将原始查询包含在扩展查询列表中。您可以通过构建器中的 includeOriginal 方法禁用此行为。\nMultiQueryExpander queryExpander = MultiQueryExpander.builder() .chatClientBuilder(chatClientBuilder) .includeOriginal(false) .build(); 可以通过构建器中提供的 promptTemplate() 方法自定义此组件使用的提示。\n检索 # 检索模块负责查询向量存储等数据系统并检索最相关的文档。\n文档搜索 # 负责从底层数据源（例如搜索引擎、向量存储、数据库或知识图）检索 Documents 组件。\nVectorStoreDocumentRetriever 从向量存储中检索与输入查询语义相似的文档。它支持基于元数据、相似度阈值和 Top-k 结果的过滤。\nDocumentRetriever retriever = VectorStoreDocumentRetriever.builder() .vectorStore(vectorStore) .similarityThreshold(0.73) .topK(5) .filterExpression(new FilterExpressionBuilder() .eq(\u0026#34;genre\u0026#34;, \u0026#34;fairytale\u0026#34;) .build()) .build(); List\u0026lt;Document\u0026gt; documents = retriever.retrieve(new Query(\u0026#34;What is the main character of the story?\u0026#34;)); 过滤表达式可以是静态的，也可以是动态的。对于动态过滤表达式，你可以传递一个 Supplier 。\nDocumentRetriever retriever = VectorStoreDocumentRetriever.builder() .vectorStore(vectorStore) .filterExpression(() -\u0026gt; new FilterExpressionBuilder() .eq(\u0026#34;tenant\u0026#34;, TenantContextHolder.getTenantIdentifier()) .build()) .build(); List\u0026lt;Document\u0026gt; documents = retriever.retrieve(new Query(\u0026#34;What are the KPIs for the next semester?\u0026#34;)); 您还可以通过 Query API 使用 FILTER_EXPRESSION 参数提供特定于请求的过滤表达式。如果同时提供了特定于请求和特定于检索器的过滤表达式，则特定于请求的过滤表达式优先。\nQuery query = Query.builder() .text(\u0026#34;Who is Anacletus?\u0026#34;) .context(Map.of(VectorStoreDocumentRetriever.FILTER_EXPRESSION, \u0026#34;location == \u0026#39;Whispering Woods\u0026#39;\u0026#34;)) .build(); List\u0026lt;Document\u0026gt; retrievedDocuments = documentRetriever.retrieve(query); 文档连接 # 该组件用于将基于多个查询从多个数据源检索到的文档合并为一个文档集合。作为合并过程的一部分，它还可以处理重复文档和互惠排名策略。\nConcatenationDocumentJoiner 会将基于多个查询从多个数据源检索到的文档连接起来，形成一个文档集合。如果出现重复文档，则保留第一个文档。每个文档的分数保持不变。\nMap\u0026lt;Query, List\u0026lt;List\u0026lt;Document\u0026gt;\u0026gt;\u0026gt; documentsForQuery = ... DocumentJoiner documentJoiner = new ConcatenationDocumentJoiner(); List\u0026lt;Document\u0026gt; documents = documentJoiner.join(documentsForQuery); 检索后 # 后检索模块负责处理检索到的文档以获得最佳的生成结果。\n文档后期处理 # 用于根据查询对检索到的文档进行后处理的组件，解决了诸如中间丢失 、模型的上下文长度限制等挑战，以及需要减少检索到的信息中的噪音和冗余。\n例如，它可以根据文档与查询的相关性对文档进行排名，删除不相关或冗余的文档，或者压缩每个文档的内容以减少噪音和冗余。\n一代 # 生成模块负责根据用户查询和检索到的文档生成最终响应。\n查询增强 # 用于使用附加数据扩充输入查询的组件，有助于为大型语言模型提供回答用户查询所需的必要上下文。\n上下文查询增强器 # ContextualQueryAugmenter 使用所提供文档内容中的上下文数据来增强用户查询。\nQueryAugmenter queryAugmenter = ContextualQueryAugmenter.builder().build(); 默认情况下， ContextualQueryAugmenter 不允许检索到的上下文为空。当这种情况发生时，它会指示模型不回答用户查询。\n您可以启用 allowEmptyContext 选项，以允许模型即使检索到的上下文为空也能生成响应。\nQueryAugmenter queryAugmenter = ContextualQueryAugmenter.builder() .allowEmptyContext(true) .build(); 可以通过构建器中提供的 promptTemplate() 和 emptyContextPromptTemplate() 方法自定义此组件使用的提示。\n"},{"id":73,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B-api/mistral-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%81%8A%E5%A4%A9/","title":"Mistral 人工智能聊天","section":"聊天模型 API","content":" Mistral 人工智能聊天 # Spring AI 支持 Mistral AI 的各种 AI 语言模型。您可以与 Mistral AI 语言模型进行交互，并基于 Mistral 模型创建多语言对话助手。\n先决条件 # 您需要使用 Mistral AI 创建一个 API 来访问 Mistral AI 语言模型。\n在 [ Mistral AI 注册页面]( https://auth.mistral.ai/ui/registration)创建一个帐户，并在 [ API Keys 页面]( https://console.mistral.ai/api-keys/)上生成令牌。\nSpring AI 项目定义了一个名为 spring.ai.mistralai.api-key 的配置属性，您应该将其设置为从 console.mistralai.ai 获取的 API Key 的值。\n您可以在 application.properties 文件中设置此配置属性：\nspring.ai.mistralai.api-key=\u0026lt;your-mistralai-api-key\u0026gt; 为了在处理 API 密钥等敏感信息时增强安全性，您可以使用 Spring 表达式语言 (SpEL) 引用自定义环境变量：\n# In application.yml spring: ai: mistralai: api-key: ${MISTRALAI_API_KEY} # In your environment or .env file export MISTRALAI_API_KEY=\u0026lt;your-mistralai-api-key\u0026gt; 您还可以在应用程序代码中以编程方式设置此配置：\n// Retrieve API key from a secure source or environment variable String apiKey = System.getenv(\u0026#34;MISTRALAI_API_KEY\u0026#34;); 添加存储库和 BOM # Spring AI 的构件已发布在 Maven Central 和 Spring Snapshot 仓库中。请参阅 [ “构件仓库”](../../getting-started.html#artifact-repositories) 部分，将这些仓库添加到您的构建系统中。\n为了帮助管理依赖项，Spring AI 提供了 BOM（物料清单），以确保在整个项目中使用一致版本的 Spring AI。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # Spring AI 为 Mistral AI 聊天客户端提供了 Spring Boot 自动配置功能。要启用此功能，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-mistral-ai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-mistral-ai\u0026#39; } 聊天属性 # 重试属性 # 前缀 spring.ai.retry 用作属性前缀，可让您配置 Mistral AI 聊天模型的重试机制。\n连接属性 # 前缀 spring.ai.mistralai 用作允许您连接到 OpenAI 的属性前缀。\n配置属性 # 前缀 spring.ai.mistralai.chat 是允许您配置 Mistral AI 聊天模型实现的属性前缀。\n运行时选项 # [ MistralAiChatOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-mistral-ai/src/main/java/org/springframework/ai/mistralai/[MistralAiChatOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-mistral-ai/src/main/java/org/springframework/ai/mistralai/MistralAiChatOptions.java)) 提供模型配置，例如要使用的模型、温度、频率惩罚等。\n启动时，可以使用 MistralAiChatModel(api, options) 构造函数或 spring.ai.mistralai.chat.options.* 属性配置默认选项。\n在运行时，您可以通过向 Prompt 调用添加新的特定于请求的选项来覆盖默认选项。例如，要覆盖特定请求的默认模型和温度：\nChatResponse response = chatModel.call( new Prompt( \u0026#34;Generate the names of 5 famous pirates.\u0026#34;, MistralAiChatOptions.builder() .model(MistralAiApi.ChatModel.LARGE.getValue()) .temperature(0.5) .build() )); 函数调用 # 您可以使用 MistralAiChatModel 注册自定义 Java 函数，并让 Mistral AI 模型智能地选择输出包含参数的 JSON 对象，以调用一个或多个已注册的函数。这是一种将 LLM 功能与外部工具和 API 连接起来的强大技术。了解更多关于[ 工具调用的](../tools.html)信息。\n多式联运 # 多模态是指模型能够同时理解和处理来自各种来源的信息，包括文本、图像、音频和其他数据格式。Mistral AI 支持文本和视觉模态。\n想象 # 提供[ 视觉]( https://docs.mistral.ai/capabilities/vision/)多模态支持的 Mistral AI 模型包括 pixtral-large-latest 。有关更多信息，请参阅[ 视觉]( https://docs.mistral.ai/capabilities/vision/)指南。\nMistral AI [用户[ 消息](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-client-chat/src/main/java/org/springframework/ai/chat/messages/Message.java) API]( https://docs.mistral.ai/api/#tag/chat/operation/chat_completion_v1_chat_completions_post) 可以将一系列 base64 编码的图片或图片 URL 添加到[ 消息]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-client-chat/src/main/java/org/springframework/ai/chat/messages/Message.java)中。Spring AI 的[ 消息]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-client-chat/src/main/java/org/springframework/ai/chat/messages/Message.java)接口通过引入[ 媒体]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/model/Media.java)类型来促进多模态 AI 模型的发展。此类型包含[ 消息]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-client-chat/src/main/java/org/springframework/ai/chat/messages/Message.java)中[ 媒体]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/model/Media.java)附件的数据和详细信息，并利用 Spring 的 org.springframework.util.MimeType 和 org.springframework.core.io.Resource 来存储原始[ 媒体]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/model/Media.java)数据。\n下面是摘录自 MistralAiChatModelIT.java 的代码示例，展示了用户文本与图像的融合。\nvar imageResource = new ClassPathResource(\u0026#34;/multimodal.test.png\u0026#34;); var userMessage = new UserMessage(\u0026#34;Explain what do you see on this picture?\u0026#34;, new Media(MimeTypeUtils.IMAGE_PNG, this.imageResource)); ChatResponse response = chatModel.call(new Prompt(this.userMessage, ChatOptions.builder().model(MistralAiApi.ChatModel.PIXTRAL_LARGE.getValue()).build())); 或者等效的图像 URL：\nvar userMessage = new UserMessage(\u0026#34;Explain what do you see on this picture?\u0026#34;, new Media(MimeTypeUtils.IMAGE_PNG, URI.create(\u0026#34;https://docs.spring.io/spring-ai/reference/_images/multimodal.test.png\u0026#34;))); ChatResponse response = chatModel.call(new Prompt(this.userMessage, ChatOptions.builder().model(MistralAiApi.ChatModel.PIXTRAL_LARGE.getValue()).build())); 该示例展示了一个将 multimodal.test.png 图像作为输入的模型：\n以及文本消息“解释一下你在图片上看到了什么？”，并生成如下的回应：\nOpenAI API 兼容性 # Mistral 与 OpenAI API 兼容，您可以使用 [ Spring AI OpenAI](openai-chat.html) 客户端与 Mistral 通信。为此，您需要将 OpenAI 基本网址配置为 Mistral AI 平台： spring.ai.openai.chat.base-url=https://api.mistral.ai ，选择一个 Mistral 模型： spring.ai.openai.chat.options.model=mistral-small-latest ，并设置 Mistral AI API 密钥： spring.ai.openai.chat.api-key=\u0026lt;YOUR MISTRAL API KEY 。\n检查 [ MistralWithOpenAiChatModelIT.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/chat/proxy/[MistralWithOpenAiChatModelIT.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/chat/proxy/MistralWithOpenAiChatModelIT.java)) 测试，了解通过 Spring AI OpenAI 使用 Mistral 的示例。\n样本控制器（自动配置） # [ 创建]( https://start.spring.io/)一个新的 Spring Boot 项目并将 spring-ai-starter-model-mistral-ai 添加到您的 pom（或 gradle）依赖项中。\n在 src/main/resources 目录下添加 application.properties 文件，用于启用和配置 Mistral AI 聊天模型：\nspring.ai.mistralai.api-key=YOUR_API_KEY spring.ai.mistralai.chat.options.model=mistral-small spring.ai.mistralai.chat.options.temperature=0.7 这将创建一个 MistralAiChatModel 实现，您可以将其注入到您的类中。这是一个使用聊天模型生成文本的简单 @RestController 类的示例。\n@RestController public class ChatController { private final MistralAiChatModel chatModel; @Autowired public ChatController(MistralAiChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map\u0026lt;String,String\u0026gt; generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, this.chatModel.call(message)); } @GetMapping(\u0026#34;/ai/generateStream\u0026#34;) public Flux\u0026lt;ChatResponse\u0026gt; generateStream(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { var prompt = new Prompt(new UserMessage(message)); return this.chatModel.stream(prompt); } } 手动配置 # MistralAiChatModel 实现了 ChatModel 和 StreamingChatModel ，并使用 [ Low-level MistralAiApi Client](#low-level-api) 连接到 Mistral AI 服务。\n将 spring-ai-mistral-ai 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-mistral-ai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-mistral-ai\u0026#39; } 接下来，创建一个 MistralAiChatModel 并将其用于文本生成：\nvar mistralAiApi = new MistralAiApi(System.getenv(\u0026#34;MISTRAL_AI_API_KEY\u0026#34;)); var chatModel = new MistralAiChatModel(this.mistralAiApi, MistralAiChatOptions.builder() .model(MistralAiApi.ChatModel.LARGE.getValue()) .temperature(0.4) .maxTokens(200) .build()); ChatResponse response = this.chatModel.call( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); // Or with streaming responses Flux\u0026lt;ChatResponse\u0026gt; response = this.chatModel.stream( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); MistralAiChatOptions 提供聊天请求的配置信息。MistralAiChatOptions.Builder 是一个流畅的选项构建 MistralAiChatOptions.Builder 。\n低级 MistralAiApi 客户端 # [ MistralAiApi]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-mistral-ai/src/main/java/org/springframework/ai/mistralai/api/[MistralAiApi](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-mistral-ai/src/main/java/org/springframework/ai/mistralai/api/MistralAiApi.java).java) 为 [ Mistral AI API]( https://docs.mistral.ai/api/) 提供了轻量级 Java 客户端。\n下面是一个简单的代码片段，展示了如何以编程方式使用 API：\nMistralAiApi mistralAiApi = new MistralAiApi(System.getenv(\u0026#34;MISTRAL_AI_API_KEY\u0026#34;)); ChatCompletionMessage chatCompletionMessage = new ChatCompletionMessage(\u0026#34;Hello world\u0026#34;, Role.USER); // Sync request ResponseEntity\u0026lt;ChatCompletion\u0026gt; response = this.mistralAiApi.chatCompletionEntity( new ChatCompletionRequest(List.of(this.chatCompletionMessage), MistralAiApi.ChatModel.LARGE.getValue(), 0.8, false)); // Streaming request Flux\u0026lt;ChatCompletionChunk\u0026gt; streamResponse = this.mistralAiApi.chatCompletionStream( new ChatCompletionRequest(List.of(this.chatCompletionMessage), MistralAiApi.ChatModel.LARGE.getValue(), 0.8, true)); 请关注 [ MistralAiApi.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-mistral-ai/src/main/java/org/springframework/ai/mistralai/api/[MistralAiApi.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-mistral-ai/src/main/java/org/springframework/ai/mistralai/api/MistralAiApi.java)) 的 JavaDoc 以获取更多信息。\nMistralAiApi 样本 # MistralAiApiIT.java 测试提供了一些如何使用轻量级库的一般示例。 PaymentStatusFunctionCallingIT.java 测试展示了如何使用低级 API 调用工具函数。该测试基于 Mistral AI 函数调用教程。 "},{"id":74,"href":"/docs/%E5%8F%82%E8%80%83/%E7%9F%A2%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/mongodb-atlas/","title":"MongoDB Atlas","section":"矢量数据库","content":" MongoDB Atlas # 本节将引导您将 MongoDB Atlas 设置为矢量存储以与 Spring AI 一起使用。\n什么是 MongoDB Atlas？ # [ MongoDB Atlas]( https://www.mongodb.com/products/platform/atlas-database) 是 MongoDB 推出的完全托管云数据库，可在 AWS、Azure 和 GCP 上使用。Atlas 支持对 MongoDB 文档数据进行原生向量搜索和全文搜索。\n[ MongoDB Atlas 向量搜索]( https://www.mongodb.com/products/platform/atlas-vector-search)允许您将向量嵌入存储在 MongoDB 文档中，创建向量搜索索引，并使用近似最近邻算法（分层可导航小世界）执行 KNN 搜索。您可以在 MongoDB 聚合阶段使用 $vectorSearch 聚合运算符对向量嵌入执行搜索。\n先决条件 # 运行 MongoDB 6.0.11、7.0.2 或更高版本的 Atlas 集群。要开始使用 MongoDB Atlas，您可以按照此处的说明操作。确保您的 IP 地址包含在 Atlas 项目的访问列表中。 正在运行且启用了向量搜索的 MongoDB Atlas 实例 配置了向量搜索索引的集合 具有 id（字符串）、内容（字符串）、元数据（文档）和嵌入（向量）字段的集合架构 索引和集合操作的适当访问权限 自动配置 # Spring AI 为 MongoDB Atlas 矢量存储提供了 Spring Boot 自动配置功能。要启用此功能，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-mongodb-atlas\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件：\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-vector-store-mongodb-atlas\u0026#39; } 向量存储实现可以为您初始化必要的架构，但您必须通过在 application.properties 文件中设置 spring.ai.vectorstore.mongodb.initialize-schema=true 来选择加入。或者，您可以选择退出初始化，并使用 MongoDB Atlas UI、Atlas Administration API 或 Atlas CLI 手动创建索引。如果索引需要高级映射或其他配置，这种方法会非常有用。\n请查看向量存储的[ 配置参数](#mongodbvector-properties)列表，以了解默认值和配置选项。\n此外，您还需要一个已配置的 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) bean。有关更多信息，请参阅 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) 部分。\n现在您可以将 MongoDBAtlasVectorStore 自动连接为应用程序中的矢量存储：\n@Autowired VectorStore vectorStore; // ... List\u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); // Add the documents to MongoDB Atlas vectorStore.add(documents); // Retrieve documents similar to a query List\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch(SearchRequest.builder().query(\u0026#34;Spring\u0026#34;).topK(5).build()); 配置属性 # 要连接到 MongoDB Atlas 并使用 MongoDBAtlasVectorStore ，您需要提供实例的访问详细信息。您可以通过 Spring Boot 的 application.yml 提供一个简单的配置：\nspring: data: mongodb: uri: \u0026lt;mongodb atlas connection string\u0026gt; database: \u0026lt;database name\u0026gt; ai: vectorstore: mongodb: initialize-schema: true collection-name: custom_vector_store index-name: custom_vector_index path-name: custom_embedding metadata-fields-to-filter: author,year 以 spring.ai.vectorstore.mongodb.* 开头的属性用于配置 MongoDBAtlasVectorStore ：\n手动配置 # 除了使用 Spring Boot 自动配置之外，您还可以手动配置 MongoDB Atlas 向量存储。为此，您需要将 spring-ai-mongodb-atlas-store 添加到您的项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-mongodb-atlas-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件：\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-mongodb-atlas-store\u0026#39; } 创建一个 MongoTemplate bean：\n@Bean public MongoTemplate mongoTemplate() { return new MongoTemplate(MongoClients.create(\u0026#34;\u0026lt;mongodb atlas connection string\u0026gt;\u0026#34;), \u0026#34;\u0026lt;database name\u0026gt;\u0026#34;); } 然后使用构建器模式创建 MongoDBAtlasVectorStore bean：\n@Bean public VectorStore vectorStore(MongoTemplate mongoTemplate, EmbeddingModel embeddingModel) { return MongoDBAtlasVectorStore.builder(mongoTemplate, embeddingModel) .collectionName(\u0026#34;custom_vector_store\u0026#34;) // Optional: defaults to \u0026#34;vector_store\u0026#34; .vectorIndexName(\u0026#34;custom_vector_index\u0026#34;) // Optional: defaults to \u0026#34;vector_index\u0026#34; .pathName(\u0026#34;custom_embedding\u0026#34;) // Optional: defaults to \u0026#34;embedding\u0026#34; .numCandidates(500) // Optional: defaults to 200 .metadataFieldsToFilter(List.of(\u0026#34;author\u0026#34;, \u0026#34;year\u0026#34;)) // Optional: defaults to empty list .initializeSchema(true) // Optional: defaults to false .batchingStrategy(new TokenCountBatchingStrategy()) // Optional: defaults to TokenCountBatchingStrategy .build(); } // This can be any EmbeddingModel implementation @Bean public EmbeddingModel embeddingModel() { return new OpenAiEmbeddingModel(new OpenAiApi(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;))); } 元数据过滤 # 您还可以利用 MongoDB Atlas 的通用、可移植[ 元数据过滤器](../vectordbs.html#metadata-filters) 。\n例如，您可以使用文本表达语言：\nvectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(5) .similarityThreshold(0.7) .filterExpression(\u0026#34;author in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; article_type == \u0026#39;blog\u0026#39;\u0026#34;).build()); 或者以编程方式使用 Filter.Expression DSL：\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(5) .similarityThreshold(0.7) .filterExpression(b.and( b.in(\u0026#34;author\u0026#34;, \u0026#34;john\u0026#34;, \u0026#34;jill\u0026#34;), b.eq(\u0026#34;article_type\u0026#34;, \u0026#34;blog\u0026#34;)).build()).build()); 例如，这个便携式过滤器表达式：\nauthor in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; article_type == \u0026#39;blog\u0026#39; 转换为专有的 MongoDB Atlas 过滤器格式：\n{ \u0026#34;$and\u0026#34;: [ { \u0026#34;$or\u0026#34;: [ { \u0026#34;metadata.author\u0026#34;: \u0026#34;john\u0026#34; }, { \u0026#34;metadata.author\u0026#34;: \u0026#34;jill\u0026#34; } ] }, { \u0026#34;metadata.article_type\u0026#34;: \u0026#34;blog\u0026#34; } ] } 教程和代码示例 # 要开始使用 Spring AI 和 MongoDB：\n请参阅 Spring AI Integration 的入门指南 。 有关使用 Spring AI 和 MongoDB 演示检索增强生成 (RAG) 的全面代码示例，请参阅此详细教程 。 访问 Native Client # MongoDB Atlas Vector Store 实现通过 getNativeClient() 方法提供对底层本机 MongoDB 客户端（ MongoClient ）的访问：\nMongoDBAtlasVectorStore vectorStore = context.getBean(MongoDBAtlasVectorStore.class); Optional\u0026lt;MongoClient\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { MongoClient client = nativeClient.get(); // Use the native client for MongoDB-specific operations } 本机客户端使您可以访问可能无法通过 VectorStore 接口公开的 MongoDB 特定功能和操作。\n"},{"id":75,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B-api/%E5%8D%83%E5%B8%86%E5%B5%8C%E5%85%A5/","title":"千帆嵌入","section":"嵌入模型 API","content":" 千帆嵌入 # 此功能已移至 Spring AI 社区存储库。\n请访问 [ github.com/spring-ai-community/qianfan](https:// github.com/spring-ai-community/qianfan) 获取最新版本。\n"},{"id":76,"href":"/docs/%E5%8F%82%E8%80%83/%E8%AF%84%E4%BC%B0%E6%B5%8B%E8%AF%95/","title":"评估测试","section":"参考","content":" 评估测试 # 测试人工智能应用程序需要评估生成的内容，以确保人工智能模型没有产生幻觉反应。\n评估响应结果的一种方法是使用 AI 模型本身进行评估。选择最佳的 AI 模型进行评估，该模型可能与生成响应结果的模型不同。\nSpring AI 用于评估响应的接口是 Evaluator ，定义如下：\n@FunctionalInterface public interface Evaluator { EvaluationResponse evaluate(EvaluationRequest evaluationRequest); } 评估的输入是 EvaluationRequest 定义为\npublic class EvaluationRequest { private final String userText; private final List\u0026lt;Content\u0026gt; dataList; private final String responseContent; public EvaluationRequest(String userText, List\u0026lt;Content\u0026gt; dataList, String responseContent) { this.userText = userText; this.dataList = dataList; this.responseContent = responseContent; } ... } userText ：来自用户的原始输入，以 String 形式 dataList ：上下文数据，例如来自检索增强生成的数据，附加到原始输入。 responseContent ：AI 模型的响应内容（ String 相关性评估器 # `RelevancyEvaluator``` 是 Evaluator`` 接口的一个实现，旨在评估 AI 生成的响应与所提供上下文的相关性。此评估器通过确定 AI 模型的响应是否与用户输入（就检索到的上下文而言）相关，来帮助评估 RAG 流程的质量。\n评估基于用户输入、AI 模型的响应以及上下文信息。它使用提示模板询问 AI 模型响应是否与用户输入和上下文相关。\n这是 RelevancyEvaluator 使用的默认提示模板：\nYour task is to evaluate if the response for the query is in line with the context information provided. You have two options to answer. Either YES or NO. Answer YES, if the response for the query is in line with context information otherwise NO. Query: {query} Response: {response} Context: {context} Answer: 在集成测试中的使用 # 以下是在集成测试中使用 RelevancyEvaluator 的示例，使用 RetrievalAugmentationAdvisor 验证 RAG 流的结果：\n@Test void evaluateRelevancy() { String question = \u0026#34;Where does the adventure of Anacletus and Birba take place?\u0026#34;; RetrievalAugmentationAdvisor ragAdvisor = RetrievalAugmentationAdvisor.builder() .documentRetriever(VectorStoreDocumentRetriever.builder() .vectorStore(pgVectorStore) .build()) .build(); ChatResponse chatResponse = ChatClient.builder(chatModel).build() .prompt(question) .advisors(ragAdvisor) .call() .chatResponse(); EvaluationRequest evaluationRequest = new EvaluationRequest( // The original user question question, // The retrieved context from the RAG flow chatResponse.getMetadata().get(RetrievalAugmentationAdvisor.DOCUMENT_CONTEXT), // The AI model\u0026#39;s response chatResponse.getResult().getOutput().getText() ); RelevancyEvaluator evaluator = new RelevancyEvaluator(ChatClient.builder(chatModel)); EvaluationResponse evaluationResponse = evaluator.evaluate(evaluationRequest); assertThat(evaluationResponse.isPass()).isTrue(); } 您可以在 Spring AI 项目中找到多个集成[[[ 测试](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/QuestionAnswerAdvisorIT.java)](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/RetrievalAugmentationAdvisorIT.java)]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/QuestionAnswerAdvisorIT.java)，这些[[[ 测试](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/QuestionAnswerAdvisorIT.java)](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/RetrievalAugmentationAdvisorIT.java)]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/QuestionAnswerAdvisorIT.java)使用 RelevancyEvaluator 来[[[ 测试](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/QuestionAnswerAdvisorIT.java)](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/RetrievalAugmentationAdvisorIT.java)]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/QuestionAnswerAdvisorIT.java) QuestionAnswerAdvisor （参见[[[ 测试](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/QuestionAnswerAdvisorIT.java)](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/RetrievalAugmentationAdvisorIT.java)]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/QuestionAnswerAdvisorIT.java) ）和 RetrievalAugmentationAdvisor （参见[[[ 测试](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/QuestionAnswerAdvisorIT.java)](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/RetrievalAugmentationAdvisorIT.java)]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/QuestionAnswerAdvisorIT.java) ）的功能。\n自定义模板 # RelevancyEvaluator 使用默认模板来提示 AI 模型进行评估。您可以通过 .promptTemplate() 构建器方法提供自己的 PromptTemplate 对象来自定义此行为。\n自定义的 PromptTemplate 可以使用任何 TemplateRenderer 实现（默认情况下，它使用基于 [ StringTemplate]( https://www.stringtemplate.org/) 引擎的 StPromptTemplate ）。重要的要求是模板必须包含以下占位符：\n用于接收用户问题的 query 占位符。 一个 response 占位符，用于接收 AI 模型的响应。 用于接收上下文信息的 context 占位符。 事实核查评估员 # FactCheckingEvaluator 是 Evaluator 接口的另一个实现，旨在根据提供的上下文评估 AI 生成的响应的事实准确性。该评估器通过验证给定的语句（声明）是否在逻辑上得到提供的上下文（文档）的支持，帮助检测并减少 AI 输出中的错觉。\n“索赔”和“文件”将提交给人工智能模型进行评估。目前已有更小、更高效的人工智能模型专门用于此目的，例如 Bespoke 的 Minicheck，与 GPT-4 等旗舰模型相比，它有助于降低执行这些检查的成本。Minicheck 也可通过 Ollama 使用。\n用法 # FactCheckingEvaluator 构造函数以 ChatClient.Builder 作为参数：\npublic FactCheckingEvaluator(ChatClient.Builder chatClientBuilder) { this.chatClientBuilder = chatClientBuilder; } 评估人员使用以下提示模板进行事实核查：\nDocument: {document} Claim: {claim} 其中 {document} 是上下文信息， {claim} 是需要评估的 AI 模型的响应。\n例子 # 下面是如何将 FactCheckingEvaluator 与基于 Ollama 的 ChatModel（特别是 Bespoke-Minicheck 模型）一起使用的示例：\n@Test void testFactChecking() { // Set up the Ollama API OllamaApi ollamaApi = new OllamaApi(\u0026#34;http://localhost:11434\u0026#34;); ChatModel chatModel = new OllamaChatModel(ollamaApi, OllamaOptions.builder().model(BESPOKE_MINICHECK).numPredict(2).temperature(0.0d).build()) // Create the FactCheckingEvaluator var factCheckingEvaluator = new FactCheckingEvaluator(ChatClient.builder(chatModel)); // Example context and claim String context = \u0026#34;The Earth is the third planet from the Sun and the only astronomical object known to harbor life.\u0026#34;; String claim = \u0026#34;The Earth is the fourth planet from the Sun.\u0026#34;; // Create an EvaluationRequest EvaluationRequest evaluationRequest = new EvaluationRequest(context, Collections.emptyList(), claim); // Perform the evaluation EvaluationResponse evaluationResponse = factCheckingEvaluator.evaluate(evaluationRequest); assertFalse(evaluationResponse.isPass(), \u0026#34;The claim should not be supported by the context\u0026#34;); } "},{"id":77,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B-api/google-vertexai/","title":"Google VertexAI","section":"嵌入模型 API","content":" Google VertexAI # "},{"id":78,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B-api/minimax-%E8%81%8A%E5%A4%A9/","title":"MiniMax 聊天","section":"聊天模型 API","content":" MiniMax 聊天 # Spring AI 支持 MiniMax 的各种 AI 语言模型。您可以与 MiniMax 语言模型进行交互，并基于 MiniMax 模型创建多语言对话助手。\n先决条件 # 您需要使用 MiniMax 创建一个 API 来访问 MiniMax 语言模型。\n在 [ MiniMax 注册页面]( https://www.minimaxi.com/login)创建一个帐户，并在 [ API Keys 页面]( https://www.minimaxi.com/user-center/basic-information/interface-key)生成令牌。\nSpring AI 项目定义了一个名为 spring.ai.minimax.api-key 的配置属性，您应该将其设置为从 API Keys 页面获取的 API Key 的值。\n您可以在 application.properties 文件中设置此配置属性：\nspring.ai.minimax.api-key=\u0026lt;your-minimax-api-key\u0026gt; 为了在处理 API 密钥等敏感信息时增强安全性，您可以使用 Spring 表达式语言 (SpEL) 来引用环境变量：\n# In application.yml spring: ai: minimax: api-key: ${MINIMAX_API_KEY} # In your environment or .env file export MINIMAX_API_KEY=\u0026lt;your-minimax-api-key\u0026gt; 您还可以在应用程序代码中以编程方式设置此配置：\n// Retrieve API key from a secure source or environment variable String apiKey = System.getenv(\u0026#34;MINIMAX_API_KEY\u0026#34;); 添加存储库和 BOM # Spring AI 的构件已发布在 Maven Central 和 Spring Snapshot 仓库中。请参阅 [ “构件仓库”](../../getting-started.html#artifact-repositories) 部分，将这些仓库添加到您的构建系统中。\n为了帮助管理依赖项，Spring AI 提供了 BOM（物料清单），以确保在整个项目中使用一致版本的 Spring AI。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # Spring AI 为 MiniMax 聊天客户端提供了 Spring Boot 自动配置功能。要启用此功能，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-minimax\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-minimax\u0026#39; } 聊天属性 # 重试属性 # 前缀 spring.ai.retry 用作属性前缀，可让您配置 MiniMax 聊天模型的重试机制。\n连接属性 # 前缀 spring.ai.minimax 用作允许您连接到 MiniMax 的属性前缀。\n配置属性 # 前缀 spring.ai.minimax.chat 是允许您配置 MiniMax 的聊天模型实现的属性前缀。\n运行时选项 # [ MiniMaxChatOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-minimax/src/main/java/org/springframework/ai/minimax/[MiniMaxChatOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-minimax/src/main/java/org/springframework/ai/minimax/MiniMaxChatOptions.java)) 提供模型配置，例如要使用的模型、温度、频率惩罚等。\n启动时，可以使用 MiniMaxChatModel(api, options) 构造函数或 spring.ai.minimax.chat.options.* 属性配置默认选项。\n在运行时，您可以通过向 Prompt 调用添加新的、特定于请求的选项来覆盖默认选项。例如，要覆盖特定请求的默认模型和温度：\nChatResponse response = chatModel.call( new Prompt( \u0026#34;Generate the names of 5 famous pirates.\u0026#34;, MiniMaxChatOptions.builder() .model(MiniMaxApi.ChatModel.ABAB_6_5_S_Chat.getValue()) .temperature(0.5) .build() )); 样品控制器 # [ 创建]( https://start.spring.io/)一个新的 Spring Boot 项目并将 spring-ai-starter-model-minimax 添加到您的 pom（或 gradle）依赖项中。\n在 src/main/resources 目录下添加一个 application.properties 文件，以启用和配置 MiniMax 聊天模型：\nspring.ai.minimax.api-key=YOUR_API_KEY spring.ai.minimax.chat.options.model=abab6.5g-chat spring.ai.minimax.chat.options.temperature=0.7 这将创建一个 MiniMaxChatModel 实现，您可以将其注入到您的类中。这是一个使用聊天模型生成文本的简单 @Controller 类的示例。\n@RestController public class ChatController { private final MiniMaxChatModel chatModel; @Autowired public ChatController(MiniMaxChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, this.chatModel.call(message)); } @GetMapping(\u0026#34;/ai/generateStream\u0026#34;) public Flux\u0026lt;ChatResponse\u0026gt; generateStream(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { var prompt = new Prompt(new UserMessage(message)); return this.chatModel.stream(prompt); } } 手动配置 # MiniMaxChatModel 实现了 ChatModel 和 StreamingChatModel ，并使用 [ Low-level MiniMaxApi Client](#low-level-api) 连接到 MiniMax 服务。\n将 spring-ai-minimax 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-minimax\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-minimax\u0026#39; } 接下来，创建一个 MiniMaxChatModel 并将其用于文本生成：\nvar miniMaxApi = new MiniMaxApi(System.getenv(\u0026#34;MINIMAX_API_KEY\u0026#34;)); var chatModel = new MiniMaxChatModel(this.miniMaxApi, MiniMaxChatOptions.builder() .model(MiniMaxApi.ChatModel.ABAB_6_5_S_Chat.getValue()) .temperature(0.4) .maxTokens(200) .build()); ChatResponse response = this.chatModel.call( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); // Or with streaming responses Flux\u0026lt;ChatResponse\u0026gt; streamResponse = this.chatModel.stream( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); MiniMaxChatOptions 提供聊天请求的配置信息。MiniMaxChatOptions.Builder 是一个流畅的选项构建 MiniMaxChatOptions.Builder 。\n低级 MiniMaxApi 客户端 # [ MiniMaxApi]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-minimax/src/main/java/org/springframework/ai/minimax/api/[MiniMaxApi](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-minimax/src/main/java/org/springframework/ai/minimax/api/MiniMaxApi.java).java) 为 [ MiniMax API]( https://www.minimaxi.com/document/guides/chat-model/V2) 提供了轻量级 Java 客户端。\n以下是以编程方式使用 API 的简单代码片段：\nMiniMaxApi miniMaxApi = new MiniMaxApi(System.getenv(\u0026#34;MINIMAX_API_KEY\u0026#34;)); ChatCompletionMessage chatCompletionMessage = new ChatCompletionMessage(\u0026#34;Hello world\u0026#34;, Role.USER); // Sync request ResponseEntity\u0026lt;ChatCompletion\u0026gt; response = this.miniMaxApi.chatCompletionEntity( new ChatCompletionRequest(List.of(this.chatCompletionMessage), MiniMaxApi.ChatModel.ABAB_6_5_S_Chat.getValue(), 0.7f, false)); // Streaming request Flux\u0026lt;ChatCompletionChunk\u0026gt; streamResponse = this.miniMaxApi.chatCompletionStream( new ChatCompletionRequest(List.of(this.chatCompletionMessage), MiniMaxApi.ChatModel.ABAB_6_5_S_Chat.getValue(), 0.7f, true)); 请关注 [ MiniMaxApi.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-minimax/src/main/java/org/springframework/ai/minimax/api/[MiniMaxApi.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-minimax/src/main/java/org/springframework/ai/minimax/api/MiniMaxApi.java)) 的 JavaDoc 以获取更多信息。\nWebSearch 聊天 # MiniMax 型号支持网页搜索功能。网页搜索功能允许您在网络上搜索信息，并在聊天回复中返回结果。\n关于网络搜索，请关注 [ MiniMax ChatCompletion]( https://platform.minimaxi.com/document/ChatCompletion%20v2) 以获取更多信息。\n以下是如何使用网络搜索的简单片段：\nUserMessage userMessage = new UserMessage( \u0026#34;How many gold medals has the United States won in total at the 2024 Olympics?\u0026#34;); List\u0026lt;Message\u0026gt; messages = new ArrayList\u0026lt;\u0026gt;(List.of(this.userMessage)); List\u0026lt;MiniMaxApi.FunctionTool\u0026gt; functionTool = List.of(MiniMaxApi.FunctionTool.webSearchFunctionTool()); MiniMaxChatOptions options = MiniMaxChatOptions.builder() .model(MiniMaxApi.ChatModel.ABAB_6_5_S_Chat.value) .tools(this.functionTool) .build(); // Sync request ChatResponse response = chatModel.call(new Prompt(this.messages, this.options)); // Streaming request Flux\u0026lt;ChatResponse\u0026gt; streamResponse = chatModel.stream(new Prompt(this.messages, this.options)); MiniMaxApi 示例 # MiniMaxApiIT.java 测试提供了一些如何使用轻量级库的一般示例。 MiniMaxApiToolFunctionCallIT.java 测试展示了如何使用低级 API 调用工具函数。\u0026gt; "},{"id":79,"href":"/docs/%E5%8F%82%E8%80%83/%E7%9F%A2%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/neo4j/","title":"Neo4j","section":"矢量数据库","content":" Neo4j # 本节将引导您设置 Neo4jVectorStore 来存储文档嵌入并执行相似性搜索。\n[ Neo4j]( https://neo4j.com) 是一个开源 NoSQL 图形数据库。它是一个完全事务性 (ACID) 数据库，以图的形式存储数据，这些图由节点组成，并通过关系连接。受现实世界结构的启发，它能够对复杂数据提供高性能查询，同时保持开发人员的直观和简单。\n[ Neo4j 的向量搜索]( https://neo4j.com/docs/cypher-manual/current/indexes-for-vector-search/)功能允许用户从大型数据集中查询向量嵌入。嵌入是数据对象（例如文本、图像、音频或文档）的数值表示。嵌入可以存储在节点属性中，并可使用 db.index.vector.queryNodes() 函数进行查询。这些索引由 Lucene 提供支持，它使用分层可导航小世界图 (HNSW) 对向量场执行 k-ANN 近似最近邻 (k-ANN) 查询。\n先决条件 # 正在运行的 Neo4j (5.15+) 实例。以下选项可用： Docker 镜像 Neo4j 桌面 Neo4j 服务器实例 Docker 镜像 Neo4j 桌面 Neo4j 服务器实例 如果需要，可以使用 EmbeddingModel 的 API 密钥来生成 Neo4jVectorStore 存储的嵌入。 自动配置 # Spring AI 为 Neo4j 矢量存储提供了 Spring Boot 自动配置功能。要启用此功能，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-neo4j\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-vector-store-neo4j\u0026#39; } 请查看矢量存储的[ 配置属性](#neo4jvector-properties)列表，以了解默认值和配置选项。\n向量存储实现可以为您初始化必要的模式，但您必须通过在适当的构造函数中指定 initializeSchema 布尔值或在 application.properties 文件中设置 …​initialize-schema=true 来选择加入。\n此外，您还需要一个已配置的 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) bean。有关更多信息，请参阅 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) 部分。\n现在，您可以将 Neo4jVectorStore 自动连接为应用程序中的矢量存储。\n@Autowired VectorStore vectorStore; // ... List\u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); // Add the documents to Neo4j vectorStore.add(documents); // Retrieve documents similar to a query List\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch(SearchRequest.builder().query(\u0026#34;Spring\u0026#34;).topK(5).build()); 配置属性 # 要连接到 Neo4j 并使用 Neo4jVectorStore ，您需要提供实例的访问详细信息。您可以通过 Spring Boot 的 application.yml 提供一个简单的配置：\nspring: neo4j: uri: \u0026lt;neo4j instance URI\u0026gt; authentication: username: \u0026lt;neo4j username\u0026gt; password: \u0026lt;neo4j password\u0026gt; ai: vectorstore: neo4j: initialize-schema: true database-name: neo4j index-name: custom-index embedding-dimension: 1536 distance-type: cosine 以 spring.neo4j.* 开头的 Spring Boot 属性用于配置 Neo4j 客户端：\n以 spring.ai.vectorstore.neo4j.* 开头的属性用于配置 Neo4jVectorStore ：\n可以使用以下距离函数：\ncosine - 默认值，适用于大多数用例。测量向量之间的余弦相似度。 euclidean - 向量之间的欧几里得距离。值越低，相似度越高。 手动配置 # 除了使用 Spring Boot 自动配置之外，您还可以手动配置 Neo4j 向量存储。为此，您需要将 spring-ai-neo4j-store 添加到您的项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-neo4j-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-neo4j-store\u0026#39; } 创建一个 Neo4j Driver bean。阅读 [ Neo4j 文档]( https://neo4j.com/docs/java-manual/current/client-applications/) ，了解有关自定义驱动程序配置的更多详细信息。\n@Bean public Driver driver() { return GraphDatabase.driver(\u0026#34;neo4j://\u0026lt;host\u0026gt;:\u0026lt;bolt-port\u0026gt;\u0026#34;, AuthTokens.basic(\u0026#34;\u0026lt;username\u0026gt;\u0026#34;, \u0026#34;\u0026lt;password\u0026gt;\u0026#34;)); } 然后使用构建器模式创建 Neo4jVectorStore bean：\n@Bean public VectorStore vectorStore(Driver driver, EmbeddingModel embeddingModel) { return Neo4jVectorStore.builder(driver, embeddingModel) .databaseName(\u0026#34;neo4j\u0026#34;) // Optional: defaults to \u0026#34;neo4j\u0026#34; .distanceType(Neo4jDistanceType.COSINE) // Optional: defaults to COSINE .embeddingDimension(1536) // Optional: defaults to 1536 .label(\u0026#34;Document\u0026#34;) // Optional: defaults to \u0026#34;Document\u0026#34; .embeddingProperty(\u0026#34;embedding\u0026#34;) // Optional: defaults to \u0026#34;embedding\u0026#34; .indexName(\u0026#34;custom-index\u0026#34;) // Optional: defaults to \u0026#34;spring-ai-document-index\u0026#34; .initializeSchema(true) // Optional: defaults to false .batchingStrategy(new TokenCountBatchingStrategy()) // Optional: defaults to TokenCountBatchingStrategy .build(); } // This can be any EmbeddingModel implementation @Bean public EmbeddingModel embeddingModel() { return new OpenAiEmbeddingModel(new OpenAiApi(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;))); } 元数据过滤 # 您还可以利用 Neo4j 存储的通用、可移植元[ 数据过滤器](../vectordbs.html#metadata-filters) 。\n例如，您可以使用文本表达语言：\nvectorStore.similaritySearch( SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(\u0026#34;author in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; \u0026#39;article_type\u0026#39; == \u0026#39;blog\u0026#39;\u0026#34;).build()); 或者以编程方式使用 Filter.Expression DSL：\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(b.and( b.in(\u0026#34;author\u0026#34;, \u0026#34;john\u0026#34;, \u0026#34;jill\u0026#34;), b.eq(\u0026#34;article_type\u0026#34;, \u0026#34;blog\u0026#34;)).build()).build()); 例如，这个便携式过滤器表达式：\nauthor in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; \u0026#39;article_type\u0026#39; == \u0026#39;blog\u0026#39; 转换为专有的 Neo4j 过滤器格式：\nnode.`metadata.author` IN [\u0026#34;john\u0026#34;,\u0026#34;jill\u0026#34;] AND node.`metadata.\u0026#39;article_type\u0026#39;` = \u0026#34;blog\u0026#34; 访问 Native Client # Neo4j Vector Store 实现通过 getNativeClient() 方法提供对底层本机 Neo4j 客户端（ Driver ）的访问：\nNeo4jVectorStore vectorStore = context.getBean(Neo4jVectorStore.class); Optional\u0026lt;Driver\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { Driver driver = nativeClient.get(); // Use the native client for Neo4j-specific operations } 本机客户端使您可以访问可能无法通过 VectorStore 界面公开的 Neo4j 特定功能和操作。\n"},{"id":80,"href":"/docs/%E5%8F%82%E8%80%83/%E7%9F%A2%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/","title":"矢量数据库","section":"参考","content":" 矢量数据库 # 矢量数据库是一种特殊类型的数据库，在人工智能应用中发挥着至关重要的作用。\n在向量数据库中，查询与传统的关系型数据库不同。它们执行相似性搜索，而不是精确匹配。当给定一个向量作为查询时，向量数据库会返回与查询向量“相似”的向量。关于如何在高层计算这种相似性的更多细节，请参阅[ 向量相似性](vectordbs/understand-vectordbs.html#vectordbs-similarity) 。\n向量数据库用于将您的数据与 AI 模型集成。使用向量数据库的第一步是将您的数据加载到向量数据库中。然后，当用户查询要发送到 AI 模型时，系统会首先检索一组类似的文档。这些文档随后作为用户问题的上下文，并与用户的查询一起发送到 AI 模型。这项技术称为[ 检索增强生成 (RAG)](../concepts.html#concept-rag) 。\n以下部分描述了使用多个矢量数据库实现的 Spring AI 接口以及一些高级示例用法。\n最后一部分旨在揭开矢量数据库中相似性搜索的底层方法的神秘面纱。\nAPI 概述 # 本节作为 Spring AI 框架内的 VectorStore 接口及其相关类的指南。\nSpring AI 提供了一个抽象的 API，用于通过 VectorStore 接口与矢量数据库进行交互。\n以下是 VectorStore 接口定义：\npublic interface VectorStore extends DocumentWriter { default String getName() { return this.getClass().getSimpleName(); } void add(List\u0026lt;Document\u0026gt; documents); void delete(List\u0026lt;String\u0026gt; idList); void delete(Filter.Expression filterExpression); default void delete(String filterExpression) { ... }; List\u0026lt;Document\u0026gt; similaritySearch(String query); List\u0026lt;Document\u0026gt; similaritySearch(SearchRequest request); default \u0026lt;T\u0026gt; Optional\u0026lt;T\u0026gt; getNativeClient() { return Optional.empty(); } } 以及相关的 SearchRequest 构建器：\npublic class SearchRequest { public static final double SIMILARITY_THRESHOLD_ACCEPT_ALL = 0.0; public static final int DEFAULT_TOP_K = 4; private String query = \u0026#34;\u0026#34;; private int topK = DEFAULT_TOP_K; private double similarityThreshold = SIMILARITY_THRESHOLD_ACCEPT_ALL; @Nullable private Filter.Expression filterExpression; public static Builder from(SearchRequest originalSearchRequest) { return builder().query(originalSearchRequest.getQuery()) .topK(originalSearchRequest.getTopK()) .similarityThreshold(originalSearchRequest.getSimilarityThreshold()) .filterExpression(originalSearchRequest.getFilterExpression()); } public static class Builder { private final SearchRequest searchRequest = new SearchRequest(); public Builder query(String query) { Assert.notNull(query, \u0026#34;Query can not be null.\u0026#34;); this.searchRequest.query = query; return this; } public Builder topK(int topK) { Assert.isTrue(topK \u0026gt;= 0, \u0026#34;TopK should be positive.\u0026#34;); this.searchRequest.topK = topK; return this; } public Builder similarityThreshold(double threshold) { Assert.isTrue(threshold \u0026gt;= 0 \u0026amp;\u0026amp; threshold \u0026lt;= 1, \u0026#34;Similarity threshold must be in [0,1] range.\u0026#34;); this.searchRequest.similarityThreshold = threshold; return this; } public Builder similarityThresholdAll() { this.searchRequest.similarityThreshold = 0.0; return this; } public Builder filterExpression(@Nullable Filter.Expression expression) { this.searchRequest.filterExpression = expression; return this; } public Builder filterExpression(@Nullable String textExpression) { this.searchRequest.filterExpression = (textExpression != null) ? new FilterExpressionTextParser().parse(textExpression) : null; return this; } public SearchRequest build() { return this.searchRequest; } } public String getQuery() {...} public int getTopK() {...} public double getSimilarityThreshold() {...} public Filter.Expression getFilterExpression() {...} } 要将数据插入矢量数据库，请将其封装在 Document 对象中。Document 类封装了来自 Document 源（例如 PDF 或 Word 文档）的内容，并包含以字符串形式表示的文本。它还包含键值对形式的元数据，包括文件名等详细信息。\n文本内容插入向量数据库后，会使用[ 嵌入]( https://en.wikipedia.org/wiki/Word2vec)模型将其转换为数值数组（或 float[] 类型），即向量[ 嵌入]( https://en.wikipedia.org/wiki/Word2vec)。Word2Vec、 [ GLoVE]( https://en.wikipedia.org/wiki/GloVe_(machine_learning)) 和 [ BERT]( https://en.wikipedia.org/wiki/[BERT](https://en.wikipedia.org/wiki/BERT_(language_model))_(language_model)) 等[ 嵌入]( https://en.wikipedia.org/wiki/Word2vec)模型，以及 OpenAI 的 text-embedding-ada-002 等[ 嵌入]( https://en.wikipedia.org/wiki/Word2vec)模型，可用于将单词、句子或段落转换为这些向量[ 嵌入]( https://en.wikipedia.org/wiki/Word2vec)。\n向量数据库的作用是存储这些嵌入，并方便进行相似性搜索。它本身并不生成嵌入。要创建向量嵌入，应该使用 EmbeddingModel 。\n接口中的 similaritySearch 方法允许检索与给定查询字符串相似的文档。您可以使用以下参数对这些方法进行微调：\nk ：一个整数，指定最多返回多少个相似文档。这通常被称为“前 K 个”搜索或“K 最近邻”（KNN）。 threshold ：一个介于 0 到 1 之间的双精度值，值越接近 1，相似度越高。默认情况下，例如，如果您将阈值设置为 0.75，则仅返回相似度高于此值的文档。 Filter.Expression ：用于传递流畅的 DSL（领域特定语言）表达式的类，其功能类似于 SQL 中的“where”子句，但它仅适用于 Document 的元数据键值对。 filterExpression ：基于 ANTLR4 的外部 DSL，接受字符串形式的过滤表达式。例如，对于 country、year 和 isActive 等元数据键，您可以使用如下表达式： country == \u0026lsquo;UK\u0026rsquo; \u0026amp;\u0026amp; year \u0026gt;= 2020 \u0026amp;\u0026amp; isActive == true. 在[ 元数据过滤器](#metadata-filters)部分中查找有关 Filter.Expression 的更多信息。\n模式初始化 # 某些向量存储要求在使用前初始化其后端架构。默认情况下，它不会为您初始化。您必须选择加入，方法是为相应的构造函数参数传递一个 boolean ，或者，如果使用 Spring Boot，则在 application.properties 或 application.yml 中将相应的 initialize-schema 属性设置为 true 。有关具体的属性名称，请查看您正在使用的向量存储的文档。\n批处理策略 # 使用向量存储时，通常需要嵌入大量文档。虽然一次调用即可嵌入所有文档看似简单，但这种方法可能会导致问题。嵌入模型将文本作为标记处理，并且具有最大标记限制，通常称为上下文窗口大小。此限制限制了单个嵌入请求中可处理的文本量。尝试在一次调用中嵌入过多的标记可能会导致错误或嵌入被截断。\n为了解决此令牌限制问题，Spring AI 实现了批处理策略。此方法将大量文档分解为适合嵌入模型最大上下文窗口的较小批次。批处理不仅解决了令牌限制问题，还可以提高性能并更有效地利用 API 速率限制。\nSpring AI 通过 BatchingStrategy 接口提供此功能，该接口允许根据文档的标记计数以子批次的形式处理文档。\n核心 BatchingStrategy 接口定义如下：\npublic interface BatchingStrategy { List\u0026lt;List\u0026lt;Document\u0026gt;\u0026gt; batch(List\u0026lt;Document\u0026gt; documents); } 该接口定义了一个方法 batch ，该方法接受文档列表并返回文档批次列表。\n默认实现 # Spring AI 提供了一个名为 TokenCountBatchingStrategy 的默认实现。此策略根据文档的 token 计数对文档进行批处理，确保每个批次不超过计算的最大输入 token 计数。\nTokenCountBatchingStrategy 的主要特点：\n该策略估计每个文档的标记数，将它们分组为批次，且不超过最大输入标记数，如果单个文档超过此限制，则抛出异常。\n您还可以自定义 TokenCountBatchingStrategy ，以更好地满足您的特定需求。这可以通过在 Spring Boot @Configuration 类中使用自定义参数创建新实例来实现。\n以下是如何创建自定义 TokenCountBatchingStrategy bean 的示例：\n@Configuration public class EmbeddingConfig { @Bean public BatchingStrategy customTokenCountBatchingStrategy() { return new TokenCountBatchingStrategy( EncodingType.CL100K_BASE, // Specify the encoding type 8000, // Set the maximum input token count 0.1 // Set the reserve percentage ); } } 在此配置中：\n默认情况下，此构造函数使用 Document.DEFAULT_CONTENT_FORMATTER 进行内容格式化，并使用 MetadataMode.NONE 进行元数据处理。如果您需要自定义这些参数，可以使用包含其他参数的完整构造函数。\n一旦定义，此自定义 TokenCountBatchingStrategy bean 将由应用程序中的 EmbeddingModel 实现自动使用，取代默认策略。\nTokenCountBatchingStrategy 内部使用 TokenCountEstimator （具体来说是 JTokkitTokenCountEstimator ）来计算 token 数量，从而实现高效的批处理。这确保了根据指定的编码类型进行准确的 token 估算。\n此外， TokenCountBatchingStrategy 还允许您传入自定义的 TokenCountEstimator 接口实现，从而提供更高的灵活性。此功能使您能够根据特定需求定制令牌计数策略。例如：\nTokenCountEstimator customEstimator = new YourCustomTokenCountEstimator(); TokenCountBatchingStrategy strategy = new TokenCountBatchingStrategy( this.customEstimator, 8000, // maxInputTokenCount 0.1, // reservePercentage Document.DEFAULT_CONTENT_FORMATTER, MetadataMode.NONE ); 使用自动截断 # 某些嵌入模型（例如 Vertex AI 文本嵌入）支持 auto_truncate 功能。启用此功能后，模型会静默截断超出最大大小的文本输入并继续处理；禁用此功能后，模型会针对过大的输入抛出显式错误。\n将自动截断与批处理策略结合使用时，您必须将批处理策略配置为输入标记数远高于模型的实际最大值。这可以防止批处理策略针对大型文档引发异常，从而允许嵌入模型在内部处理截断。\n自动截断配置 # 启用自动截断时，请将批处理策略的最大输入标记数设置为远高于模型的实际限制。这可以防止批处理策略针对大型文档引发异常，从而允许嵌入模型在内部处理截断。\n以下是使用具有自动截断和自定义 BatchingStrategy Vertex AI 并在 PgVectorStore 中使用它们的示例配置：\n@Configuration public class AutoTruncationEmbeddingConfig { @Bean public VertexAiTextEmbeddingModel vertexAiEmbeddingModel( VertexAiEmbeddingConnectionDetails connectionDetails) { VertexAiTextEmbeddingOptions options = VertexAiTextEmbeddingOptions.builder() .model(VertexAiTextEmbeddingOptions.DEFAULT_MODEL_NAME) .autoTruncate(true) // Enable auto-truncation .build(); return new VertexAiTextEmbeddingModel(connectionDetails, options); } @Bean public BatchingStrategy batchingStrategy() { // Only use a high token limit if auto-truncation is enabled in your embedding model. // Set a much higher token count than the model actually supports // (e.g., 132,900 when Vertex AI supports only up to 20,000) return new TokenCountBatchingStrategy( EncodingType.CL100K_BASE, 132900, // Artificially high limit 0.1 // 10% reserve ); } @Bean public VectorStore vectorStore(JdbcTemplate jdbcTemplate, EmbeddingModel embeddingModel, BatchingStrategy batchingStrategy) { return PgVectorStore.builder(jdbcTemplate, embeddingModel) // other properties omitted here .build(); } } 在此配置中：\n为什么有效 # 这种方法有效是因为：\n最佳实践 # 使用自动截断时：\n将批处理策略的最大输入令牌数设置为至少比模型的实际限制大 5-10 倍，以避免批处理策略过早出现异常。 监控日志中嵌入模型的截断警告（注意：并非所有模型都会记录截断事件）。 考虑静默截断对嵌入质量的影响。 使用样本文档进行测试，以确保截断的嵌入仍然满足您的要求。 由于它是非标准的，因此请为未来的维护者记录此配置。 Spring Boot 自动配置 # 如果您使用 Spring Boot 自动配置，则必须提供自定义 BatchingStrategy bean 来覆盖 Spring AI 附带的默认 bean：\n@Bean public BatchingStrategy customBatchingStrategy() { // This bean will override the default BatchingStrategy return new TokenCountBatchingStrategy( EncodingType.CL100K_BASE, 132900, // Much higher than model\u0026#39;s actual limit 0.1 ); } 应用程序上下文中此 bean 的存在将自动替换所有向量存储使用的默认批处理策略。\n定制实现 # 虽然 TokenCountBatchingStrategy 提供了健壮的默认实现，但您可以自定义批处理策略以满足您的特定需求。这可以通过 Spring Boot 的自动配置来完成。\n要自定义批处理策略，请在 Spring Boot 应用程序中定义 BatchingStrategy bean：\n@Configuration public class EmbeddingConfig { @Bean public BatchingStrategy customBatchingStrategy() { return new CustomBatchingStrategy(); } } 然后，应用程序中的 EmbeddingModel 实现将自动使用此自定义 BatchingStrategy 。\nVectorStore 实现 # 这些是 VectorStore 接口的可用实现：\nAzure 矢量搜索 - Azure 矢量存储。 Apache Cassandra - Apache Cassandra 矢量存储。 Chroma 矢量商店 - Chroma 矢量商店。 Elasticsearch 矢量存储 - Elasticsearch 矢量存储。 GemFire 矢量商店 - GemFire 矢量商店。 MariaDB 矢量存储 - MariaDB 矢量存储。 Milvus 矢量商店 - Milvus 矢量商店. MongoDB Atlas 矢量存储 - MongoDB Atlas 矢量存储。 Neo4j 矢量存储 - Neo4j 矢量存储。 OpenSearch 矢量存储 - OpenSearch 矢量存储。 Oracle 向量存储 - Oracle 数据库向量存储。 PgVector Store - PostgreSQL/PGVector 向量存储。 Pinecone 矢量商店 - PineCone 矢量商店。 Qdrant 矢量存储 - Qdrant 矢量存储。 Redis 矢量存储 - Redis 矢量存储。 SAP Hana 矢量存储 - SAP HANA 矢量存储。 Typesense 矢量商店 - Typesense 矢量商店。 Weaviate 矢量商店 - Weaviate 矢量商店。 SimpleVectorStore - 持久向量存储的简单实现，适合教育目的。 未来版本可能会支持更多实现。\n如果您有一个需要 Spring AI 支持的矢量数据库，请在 GitHub 上打开一个问题，或者更好的是，提交一个带有实现的拉取请求。\n有关每个 VectorStore 实现的信息可在本章的小节中找到。\n示例用法 # 要计算向量数据库的嵌入，您需要选择与所使用的高级 AI 模型相匹配的嵌入模型。\n例如，对于 OpenAI 的 ChatGPT，我们使用 OpenAiEmbeddingModel 和名为 text-embedding-ada-002 的模型。\nSpring Boot starter 对 OpenAI 的自动配置使得 EmbeddingModel 的实现可以在 Spring 应用程序上下文中用于依赖注入。\n将数据加载到矢量存储中的一般用法是在批处理作业中执行的，首先将数据加载到 Spring AI 的 Document 类中，然后调用 save 方法。\n给定一个源文件的 String 引用，该文件表示一个 JSON 文件，其中包含我们要加载到矢量数据库中的数据。我们使用 Spring AI 的 JsonReader 加载 JSON 中的特定字段，并将这些字段拆分成小块，然后将这些小块传递给矢量存储实现。VectorStore VectorStore 计算嵌入，并将 JSON 和嵌入存储在矢量数据库中：\n@Autowired VectorStore vectorStore; void load(String sourceFile) { JsonReader jsonReader = new JsonReader(new FileSystemResource(sourceFile), \u0026#34;price\u0026#34;, \u0026#34;name\u0026#34;, \u0026#34;shortDescription\u0026#34;, \u0026#34;description\u0026#34;, \u0026#34;tags\u0026#34;); List\u0026lt;Document\u0026gt; documents = jsonReader.get(); this.vectorStore.add(documents); } 之后，当用户问题传递到 AI 模型中时，会进行相似性搜索以检索类似的文档，然后将其“塞入”提示中作为用户问题的上下文。\nString question = \u0026lt;question from user\u0026gt; List\u0026lt;Document\u0026gt; similarDocuments = store.similaritySearch(this.question); 可以将附加选项传递到 similaritySearch 方法来定义要检索的文档数量以及相似性搜索的阈值。\n元数据过滤器 # 本节介绍可用于查询结果的各种过滤器。\n过滤字符串 # 您可以将类似 SQL 的过滤表达式作为 String 传递给 similaritySearch 重载之一。\n请考虑以下示例：\n过滤器.表达式 # 您可以使用暴露流畅 API 的 FilterExpressionBuilder 创建 Filter.Expression 的实例。一个简单的示例如下：\nFilterExpressionBuilder b = new FilterExpressionBuilder(); Expression expression = this.b.eq(\u0026#34;country\u0026#34;, \u0026#34;BG\u0026#34;).build(); 您可以使用以下运算符构建复杂的表达式：\nEQUALS: \u0026#39;==\u0026#39; MINUS : \u0026#39;-\u0026#39; PLUS: \u0026#39;+\u0026#39; GT: \u0026#39;\u0026gt;\u0026#39; GE: \u0026#39;\u0026gt;=\u0026#39; LT: \u0026#39;\u0026lt;\u0026#39; LE: \u0026#39;\u0026lt;=\u0026#39; NE: \u0026#39;!=\u0026#39; 您可以使用以下运算符组合表达式：\nAND: \u0026#39;AND\u0026#39; | \u0026#39;and\u0026#39; | \u0026#39;\u0026amp;\u0026amp;\u0026#39;; OR: \u0026#39;OR\u0026#39; | \u0026#39;or\u0026#39; | \u0026#39;||\u0026#39;; 考虑以下示例：\nExpression exp = b.and(b.eq(\u0026#34;genre\u0026#34;, \u0026#34;drama\u0026#34;), b.gte(\u0026#34;year\u0026#34;, 2020)).build(); 您还可以使用以下运算符：\nIN: \u0026#39;IN\u0026#39; | \u0026#39;in\u0026#39;; NIN: \u0026#39;NIN\u0026#39; | \u0026#39;nin\u0026#39;; NOT: \u0026#39;NOT\u0026#39; | \u0026#39;not\u0026#39;; 请考虑以下示例：\nExpression exp = b.and(b.in(\u0026#34;genre\u0026#34;, \u0026#34;drama\u0026#34;, \u0026#34;documentary\u0026#34;), b.not(b.lt(\u0026#34;year\u0026#34;, 2020))).build(); 从向量存储中删除文档 # Vector Store 接口提供了多种删除文档的方法，允许您通过特定的文档 ID 或使用过滤表达式删除数据。\n按文档 ID 删除 # 删除文档的最简单方法是提供文档 ID 列表：\nvoid delete(List\u0026lt;String\u0026gt; idList); 此方法将删除所有 ID 与所提供列表中的 ID 匹配的文档。如果列表中的任何 ID 在存储中不存在，则会被忽略。\n// Create and add document Document document = new Document(\u0026#34;The World is Big\u0026#34;, Map.of(\u0026#34;country\u0026#34;, \u0026#34;Netherlands\u0026#34;)); vectorStore.add(List.of(document)); // Delete document by ID vectorStore.delete(List.of(document.getId())); 按过滤表达式删除 # 对于更复杂的删除条件，可以使用过滤表达式：\nvoid delete(Filter.Expression filterExpression); 此方法接受一个 Filter.Expression 对象，该对象定义了删除文档的条件。当你需要根据文档的元数据属性删除文档时，它尤其有用。\n// Create test documents with different metadata Document bgDocument = new Document(\u0026#34;The World is Big\u0026#34;, Map.of(\u0026#34;country\u0026#34;, \u0026#34;Bulgaria\u0026#34;)); Document nlDocument = new Document(\u0026#34;The World is Big\u0026#34;, Map.of(\u0026#34;country\u0026#34;, \u0026#34;Netherlands\u0026#34;)); // Add documents to the store vectorStore.add(List.of(bgDocument, nlDocument)); // Delete documents from Bulgaria using filter expression Filter.Expression filterExpression = new Filter.Expression( Filter.ExpressionType.EQ, new Filter.Key(\u0026#34;country\u0026#34;), new Filter.Value(\u0026#34;Bulgaria\u0026#34;) ); vectorStore.delete(filterExpression); // Verify deletion with search SearchRequest request = SearchRequest.builder() .query(\u0026#34;World\u0026#34;) .filterExpression(\u0026#34;country == \u0026#39;Bulgaria\u0026#39;\u0026#34;) .build(); List\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch(request); // results will be empty as Bulgarian document was deleted 按字符串过滤表达式删除 # 为了方便起见，您还可以使用基于字符串的过滤表达式删除文档：\nvoid delete(String filterExpression); 此方法将提供的字符串过滤器内部转换为 Filter.Expression 对象。当您的过滤条件为字符串格式时，此方法非常有用。\n// Create and add documents Document bgDocument = new Document(\u0026#34;The World is Big\u0026#34;, Map.of(\u0026#34;country\u0026#34;, \u0026#34;Bulgaria\u0026#34;)); Document nlDocument = new Document(\u0026#34;The World is Big\u0026#34;, Map.of(\u0026#34;country\u0026#34;, \u0026#34;Netherlands\u0026#34;)); vectorStore.add(List.of(bgDocument, nlDocument)); // Delete Bulgarian documents using string filter vectorStore.delete(\u0026#34;country == \u0026#39;Bulgaria\u0026#39;\u0026#34;); // Verify remaining documents SearchRequest request = SearchRequest.builder() .query(\u0026#34;World\u0026#34;) .topK(5) .build(); List\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch(request); // results will only contain the Netherlands document 调用删除 API 时的错误处理 # 所有删除方法在出现错误时都可能抛出异常：\n最佳做法是将删除操作包装在 try-catch 块中：\ntry { vectorStore.delete(\u0026#34;country == \u0026#39;Bulgaria\u0026#39;\u0026#34;); } catch (Exception e) { logger.error(\u0026#34;Invalid filter expression\u0026#34;, e); } 文档版本控制用例 # 管理文档版本时，一个常见的场景是您需要上传文档的新版本并移除旧版本。以下是使用过滤表达式处理这种情况的方法：\n// Create initial document (v1) with version metadata Document documentV1 = new Document( \u0026#34;AI and Machine Learning Best Practices\u0026#34;, Map.of( \u0026#34;docId\u0026#34;, \u0026#34;AIML-001\u0026#34;, \u0026#34;version\u0026#34;, \u0026#34;1.0\u0026#34;, \u0026#34;lastUpdated\u0026#34;, \u0026#34;2024-01-01\u0026#34; ) ); // Add v1 to the vector store vectorStore.add(List.of(documentV1)); // Create updated version (v2) of the same document Document documentV2 = new Document( \u0026#34;AI and Machine Learning Best Practices - Updated\u0026#34;, Map.of( \u0026#34;docId\u0026#34;, \u0026#34;AIML-001\u0026#34;, \u0026#34;version\u0026#34;, \u0026#34;2.0\u0026#34;, \u0026#34;lastUpdated\u0026#34;, \u0026#34;2024-02-01\u0026#34; ) ); // First, delete the old version using filter expression Filter.Expression deleteOldVersion = new Filter.Expression( Filter.ExpressionType.AND, Arrays.asList( new Filter.Expression( Filter.ExpressionType.EQ, new Filter.Key(\u0026#34;docId\u0026#34;), new Filter.Value(\u0026#34;AIML-001\u0026#34;) ), new Filter.Expression( Filter.ExpressionType.EQ, new Filter.Key(\u0026#34;version\u0026#34;), new Filter.Value(\u0026#34;1.0\u0026#34;) ) ) ); vectorStore.delete(deleteOldVersion); // Add the new version vectorStore.add(List.of(documentV2)); // Verify only v2 exists SearchRequest request = SearchRequest.builder() .query(\u0026#34;AI and Machine Learning\u0026#34;) .filterExpression(\u0026#34;docId == \u0026#39;AIML-001\u0026#39;\u0026#34;) .build(); List\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch(request); // results will contain only v2 of the document 您还可以使用字符串过滤表达式完成相同的操作：\n// Delete old version using string filter vectorStore.delete(\u0026#34;docId == \u0026#39;AIML-001\u0026#39; AND version == \u0026#39;1.0\u0026#39;\u0026#34;); // Add new version vectorStore.add(List.of(documentV2)); 删除文档时的性能考虑 # 当您确切知道要删除哪些文档时，按 ID 列表删除通常会更快。 基于过滤器的删除可能需要扫描索引来查找匹配的文档；然而，这是特定于向量存储实现的。 大型删除操作应分批进行，以避免系统不堪重负。 考虑根据文档属性删除时使用过滤表达式，而不是先收集 ID。 理解向量 # 理解向量\n"},{"id":81,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B-api/moonshot-ai-%E8%81%8A%E5%A4%A9/","title":"Moonshot AI 聊天","section":"聊天模型 API","content":" Moonshot AI 聊天 # 此功能已移至 Spring AI 社区存储库。\n请访问 [ github.com/spring-ai-community/moonshot](https:// github.com/spring-ai-community/moonshot) 获取最新版本。\n"},{"id":82,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B-api/zhipuai-%E5%B5%8C%E5%85%A5/","title":"ZhiPuAI 嵌入","section":"嵌入模型 API","content":" ZhiPuAI 嵌入 # Spring AI 支持 ZhiPuAI 的文本嵌入模型。ZhiPuAI 的文本嵌入用于衡量文本字符串的相关性。嵌入是一个浮点数向量（列表）。两个向量之间的距离衡量它们的相关性。距离越小，相关性越高；距离越大，相关性越低。\n先决条件 # 您需要使用 ZhiPuAI 创建一个 API 来访问 ZhiPu AI 语言模型。\n在[ 智普 AI 注册页面]( https://open.bigmodel.cn/login)创建账号，并在 [ API Keys 页面]( https://open.bigmodel.cn/usercenter/apikeys)生成 token。\nSpring AI 项目定义了一个名为 spring.ai.zhipu.api-key 的配置属性，您应该将其设置为从 API Keys 页面获取的 API Key 的值。\n您可以在 application.properties 文件中设置此配置属性：\nspring.ai.zhipu.api-key=\u0026lt;your-zhipu-api-key\u0026gt; 为了在处理 API 密钥等敏感信息时增强安全性，您可以使用 Spring 表达式语言 (SpEL) 来引用环境变量：\n# In application.yml spring: ai: zhipu: api-key: ${ZHIPU_API_KEY} # In your environment or .env file export ZHIPU_API_KEY=\u0026lt;your-zhipu-api-key\u0026gt; 您还可以在应用程序代码中以编程方式设置此配置：\n// Retrieve API key from a secure source or environment variable String apiKey = System.getenv(\u0026#34;ZHIPU_API_KEY\u0026#34;); 添加存储库和 BOM # Spring AI 的构件已发布在 Maven Central 和 Spring Snapshot 仓库中。请参阅 [ “构件仓库”](../../getting-started.html#artifact-repositories) 部分，将这些仓库添加到您的构建系统中。\n为了帮助管理依赖项，Spring AI 提供了 BOM（物料清单），以确保在整个项目中使用一致版本的 Spring AI。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # Spring AI 为 Azure ZhiPuAI 嵌入模型提供 Spring Boot 自动配置。要启用此功能，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-zhipuai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-zhipuai\u0026#39; } 嵌入属性 # 重试属性 # 前缀 spring.ai.retry 用作属性前缀，可让您配置 ZhiPuAI Embedding 模型的重试机制。\n连接属性 # 前缀 spring.ai.zhipuai 用作允许您连接到 ZhiPuAI 的属性前缀。\n配置属性 # 前缀 spring.ai.zhipuai.embedding 是配置 ZhiPuAI 的 EmbeddingModel 实现的属性前缀。\n运行时选项 # [ ZhiPuAiEmbeddingOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-zhipuai/src/main/java/org/springframework/ai/zhipuai/[ZhiPuAiEmbeddingOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-zhipuai/src/main/java/org/springframework/ai/zhipuai/ZhiPuAiEmbeddingOptions.java)) 提供 ZhiPuAI 配置，例如要使用的模型等。\n也可以使用 spring.ai.zhipuai.embedding.options 属性来配置默认选项。\n启动时，使用 ZhiPuAiEmbeddingModel 构造函数设置所有嵌入请求的默认选项。运行时，您可以使用 ZhiPuAiEmbeddingOptions 实例作为 EmbeddingRequest 的一部分来覆盖默认选项。\n例如，要覆盖特定请求的默认模型名称：\nEmbeddingResponse embeddingResponse = embeddingModel.call( new EmbeddingRequest(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;), ZhiPuAiEmbeddingOptions.builder() .model(\u0026#34;Different-Embedding-Model-Deployment-Name\u0026#34;) .build())); 样品控制器 # 这将创建一个 EmbeddingModel 实现，您可以将其注入到您的类中。这是一个使用 EmbeddingModel 实现的简单 @Controller 类的示例。\nspring.ai.zhipuai.api-key=YOUR_API_KEY spring.ai.zhipuai.embedding.options.model=embedding-2 @RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(\u0026#34;/ai/embedding\u0026#34;) public Map embed(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { EmbeddingResponse embeddingResponse = this.embeddingModel.embedForResponse(List.of(message)); return Map.of(\u0026#34;embedding\u0026#34;, embeddingResponse); } } 手动配置 # 如果您不使用 Spring Boot，则可以手动配置 ZhiPuAI 嵌入模型。为此，请将 spring-ai-zhipuai 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-zhipuai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-zhipuai\u0026#39; } 接下来，创建一个 ZhiPuAiEmbeddingModel 实例并使用它来计算两个输入文本之间的相似度：\nvar zhiPuAiApi = new ZhiPuAiApi(System.getenv(\u0026#34;ZHIPU_AI_API_KEY\u0026#34;)); var embeddingModel = new ZhiPuAiEmbeddingModel(api, MetadataMode.EMBED, ZhiPuAiEmbeddingOptions.builder() .model(\u0026#34;embedding-3\u0026#34;) .dimensions(1536) .build()); EmbeddingResponse embeddingResponse = this.embeddingModel .embedForResponse(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;)); ZhiPuAiEmbeddingOptions 提供了嵌入请求的配置信息。options 类提供了一个 builder() ，方便用户轻松创建选项。\n"},{"id":83,"href":"/docs/%E5%8F%82%E8%80%83/%E5%8F%AF%E8%A7%82%E5%AF%9F%E6%80%A7/","title":"可观察性","section":"参考","content":" 可观察性 # Spring AI 以 Spring 生态系统中的可观察性功能为基础，提供对 AI 相关操作的洞察。Spring AI 为其核心组件提供了指标和跟踪功能： ChatClient （包括 Advisor ）、 ChatModel 、 EmbeddingModel 、 ImageModel 和 VectorStore 。\n聊天客户端 # 当 ChatClient 的 call() 或 stream() 操作被调用时， spring.ai.chat.client 观测值会被记录下来。它们会测量执行调用所花费的时间，并传播相关的跟踪信息。\n提示内容 # ChatClient 提示内容通常很大，并且可能包含敏感信息。因此，默认情况下不会导出该内容。\nSpring AI 支持记录提示内容，以帮助调试和排除故障。\n输入数据（已弃用） # ChatClient 输入数据通常很大，并且可能包含敏感信息。因此，默认情况下不会导出这些数据。\nSpring AI 支持记录输入数据以帮助调试和故障排除。\n聊天客户顾问 # spring.ai.advisor 观测值会在 advisor 执行时进行记录。它们会测量 advisor 执行的时间（包括内部 advisor 执行的时间），并传播相关的跟踪信息。\n聊天模型 # gen_ai.client.operation 观察值在调用 ChatModel 的 call 或 stream 方法时记录。它们测量方法完成所花费的时间并传播相关的跟踪信息。\n聊天提示和完成数据 # 聊天提示和完成数据通常很大，并且可能包含敏感信息。因此，默认情况下不会导出这些数据。\nSpring AI 支持记录聊天提示和完成数据，这对于故障排除场景非常有用。当跟踪可用时，日志将包含跟踪信息，以便更好地进行关联。\n工具调用 # spring.ai.tool 观测值会在聊天模型交互的上下文中执行工具调用时记录。它们会测量完成通话所花费的时间，并传播相关的跟踪信息。\n工具调用参数和结果数据 # 默认情况下，工具调用的输入参数和结果不会导出，因为它们可能很敏感。\nSpring AI 支持将工具调用参数和结果数据导出为 span 属性。\n嵌入模型 # gen_ai.client.operation 观测值记录在嵌入模型方法调用中。它们测量方法完成所花费的时间并传播相关的跟踪信息。\n图像模型 # gen_ai.client.operation 观测值记录在图像模型方法调用中。它们测量方法完成所花费的时间并传播相关的跟踪信息。\n图像提示数据 # 图片提示数据通常很大，并且可能包含敏感信息。因此，默认情况下不会导出这些数据。\nSpring AI 支持记录图像提示数据，这对于故障排除场景非常有用。当跟踪可用时，日志将包含跟踪信息，以便更好地进行关联。\n向量存储 # Spring AI 中的所有向量存储实现都经过检测，可通过 Micrometer 提供指标和分布式跟踪数据。\ndb.vector.client.operation 观测值在与 Vector Store 交互时记录。它们测量 query 、 add 和 remove 操作所花费的时间，并传播相关的跟踪信息。\n响应数据 # 向量搜索响应数据通常很大，并且可能包含敏感信息。因此，默认情况下不会导出这些数据。\nSpring AI 支持记录向量搜索响应数据，这对于故障排除场景非常有用。当跟踪可用时，日志将包含跟踪信息，以便更好地进行关联。\n"},{"id":84,"href":"/docs/%E5%8F%82%E8%80%83/%E7%9F%A2%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%BC%80%E6%94%BE%E6%90%9C%E7%B4%A2/","title":"开放搜索","section":"矢量数据库","content":" 开放搜索 # 本节将引导您设置 OpenSearchVectorStore 来存储文档嵌入并执行相似性搜索。\n[ OpenSearch]( https://opensearch.org) 是一个开源搜索和分析引擎，最初由 Elasticsearch 分叉而来，遵循 Apache 2.0 许可证发行。它通过简化 AI 生成资产的集成和管理来增强 AI 应用程序开发。[ OpenSearch]( https://opensearch.org) 支持向量、词汇和混合搜索功能，并利用高级向量数据库功能实现低延迟查询和相似性搜索，详情请参阅[ 向量数据库页面]( https://opensearch.org/platform/search/vector-database.html) 。\n[ OpenSearch k-NN]( https://opensearch.org/docs/latest/search-plugins/knn/index/) 功能允许用户从大型数据集中查询向量嵌入。嵌入是数据对象（例如文本、图像、音频或文档）的数值表示。嵌入可以存储在索引中，并使用各种相似度函数进行查询。\n先决条件 # 正在运行的 OpenSearch 实例。有以下选项可用： 自主管理的 OpenSearch 亚马逊开放搜索服务 自主管理的 OpenSearch 亚马逊开放搜索服务 如果需要，可以使用 EmbeddingModel 的 API 密钥来生成 OpenSearchVectorStore 存储的嵌入。 自动配置 # Spring AI 为 OpenSearch 向量存储提供了 Spring Boot 自动配置功能。要启用此功能，请在项目的 Maven pom.xml 文件中添加以下依赖项：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-opensearch\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件：\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-vector-store-opensearch\u0026#39; } 对于 Amazon OpenSearch Service，请改用以下依赖项：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-opensearch\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者对于 Gradle：\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-vector-store-opensearch\u0026#39; } 请查看向量存储的[ 配置参数](#_configuration_properties)列表，以了解默认值和配置选项。\n此外，您还需要一个已配置的 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) bean。有关更多信息，请参阅 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) 部分。\n现在，您可以将 OpenSearchVectorStore 自动连接为应用程序中的向量存储：\n@Autowired VectorStore vectorStore; // ... List\u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); // Add the documents to OpenSearch vectorStore.add(documents); // Retrieve documents similar to a query List\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch(SearchRequest.builder().query(\u0026#34;Spring\u0026#34;).topK(5).build()); 配置属性 # 要连接到 OpenSearch 并使用 OpenSearchVectorStore ，您需要提供实例的访问详细信息。您可以通过 Spring Boot 的 application.yml 提供一个简单的配置：\nspring: ai: vectorstore: opensearch: uris: \u0026lt;opensearch instance URIs\u0026gt; username: \u0026lt;opensearch username\u0026gt; password: \u0026lt;opensearch password\u0026gt; index-name: spring-ai-document-index initialize-schema: true similarity-function: cosinesimil read-timeout: \u0026lt;time to wait for response\u0026gt; connect-timeout: \u0026lt;time to wait until connection established\u0026gt; path-prefix: \u0026lt;custom path prefix\u0026gt; ssl-bundle: \u0026lt;name of SSL bundle\u0026gt; aws: # Only for Amazon OpenSearch Service host: \u0026lt;aws opensearch host\u0026gt; service-name: \u0026lt;aws service name\u0026gt; access-key: \u0026lt;aws access key\u0026gt; secret-key: \u0026lt;aws secret key\u0026gt; region: \u0026lt;aws region\u0026gt; 以 spring.ai.vectorstore.opensearch.* 开头的属性用于配置 OpenSearchVectorStore ：\n可以使用以下相似度函数：\ncosinesimil - 默认值，适用于大多数用例。测量向量之间的余弦相似度。 l1 向量之间的曼哈顿距离。 l2 向量之间的欧几里得距离。 linf 向量之间的切比雪夫距离。 手动配置 # 除了使用 Spring Boot 自动配置之外，您还可以手动配置 OpenSearch 向量存储。为此，您需要将 spring-ai-opensearch-store 添加到您的项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-opensearch-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件：\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-opensearch-store\u0026#39; } 创建 OpenSearch 客户端 bean：\n@Bean public OpenSearchClient openSearchClient() { RestClient restClient = RestClient.builder( HttpHost.create(\u0026#34;http://localhost:9200\u0026#34;)) .build(); return new OpenSearchClient(new RestClientTransport( restClient, new JacksonJsonpMapper())); } 然后使用构建器模式创建 OpenSearchVectorStore bean：\n@Bean public VectorStore vectorStore(OpenSearchClient openSearchClient, EmbeddingModel embeddingModel) { return OpenSearchVectorStore.builder(openSearchClient, embeddingModel) .index(\u0026#34;custom-index\u0026#34;) // Optional: defaults to \u0026#34;spring-ai-document-index\u0026#34; .similarityFunction(\u0026#34;l2\u0026#34;) // Optional: defaults to \u0026#34;cosinesimil\u0026#34; .initializeSchema(true) // Optional: defaults to false .batchingStrategy(new TokenCountBatchingStrategy()) // Optional: defaults to TokenCountBatchingStrategy .build(); } // This can be any EmbeddingModel implementation @Bean public EmbeddingModel embeddingModel() { return new OpenAiEmbeddingModel(new OpenAiApi(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;))); } 元数据过滤 # 您还可以利用 OpenSearch 的通用、可移植元[ 数据过滤器](../vectordbs.html#metadata-filters) 。\n例如，您可以使用文本表达语言：\nvectorStore.similaritySearch( SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(\u0026#34;author in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; \u0026#39;article_type\u0026#39; == \u0026#39;blog\u0026#39;\u0026#34;).build()); 或者以编程方式使用 Filter.Expression DSL：\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(b.and( b.in(\u0026#34;author\u0026#34;, \u0026#34;john\u0026#34;, \u0026#34;jill\u0026#34;), b.eq(\u0026#34;article_type\u0026#34;, \u0026#34;blog\u0026#34;)).build()).build()); 例如，这个便携式过滤器表达式：\nauthor in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; \u0026#39;article_type\u0026#39; == \u0026#39;blog\u0026#39; 转换为专有的 OpenSearch 过滤器格式：\n(metadata.author:john OR jill) AND metadata.article_type:blog 访问 Native Client # OpenSearch Vector Store 实现通过 getNativeClient() 方法提供对底层原生 OpenSearch 客户端（ OpenSearchClient ）的访问：\nOpenSearchVectorStore vectorStore = context.getBean(OpenSearchVectorStore.class); Optional\u0026lt;OpenSearchClient\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { OpenSearchClient client = nativeClient.get(); // Use the native client for OpenSearch-specific operations } 本机客户端使您可以访问可能无法通过 VectorStore 界面公开的 OpenSearch 特定功能和操作。\n"},{"id":85,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B-api/nvidia-%E8%81%8A%E5%A4%A9/","title":"NVIDIA 聊天","section":"聊天模型 API","content":" NVIDIA 聊天 # [ NVIDIA LLM API]( https://docs.api.nvidia.com/nim/reference/llm-apis) 是一个代理 AI 推理引擎，提供来自[ 不同提供商]( https://docs.api.nvidia.com/nim/reference/llm-apis#models)的多种模型。\nSpring AI 通过重用现有的 [ OpenAI](openai-chat.html) 客户端与 NVIDIA LLM API 集成。为此，您需要将 base-url 设置为 [[integrate.api.nvidia.com](https://integrate.api.nvidia.com)](https://[integrate.api.nvidia.com](https://integrate.api.nvidia.com)) ，选择其中一个提供的 [ LLM 模型]( https://docs.api.nvidia.com/nim/reference/llm-apis#model)并获取其 api-key 。\n检查 [ NvidiaWithOpenAiChatModelIT.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/chat/proxy/[NvidiaWithOpenAiChatModelIT.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/chat/proxy/NvidiaWithOpenAiChatModelIT.java)) 测试，了解将 NVIDIA LLM API 与 Spring AI 结合使用的示例。\n先决条件 # 创建具有足够积分的 NVIDIA 帐户。 选择要使用的 LLM 模型。例如，下图中的 meta/llama-3.1-70b-instruct 。 从所选模型的页面中，您可以获取访问该模型的 api-key 。 自动配置 # Spring AI 为 OpenAI Chat Client 提供了 Spring Boot 自动配置功能。要启用此功能，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-openai\u0026#39; } 聊天属性 # 重试属性 # 前缀 spring.ai.retry 用作属性前缀，可让您配置 OpenAI 聊天模型的重试机制。\n连接属性 # 前缀 spring.ai.openai 用作允许您连接到 OpenAI 的属性前缀。\n配置属性 # 前缀 spring.ai.openai.chat 是允许您为 OpenAI 配置聊天模型实现的属性前缀。\n运行时选项 # [ OpenAiChatOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/[OpenAiChatOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/OpenAiChatOptions.java)) 提供模型配置，例如要使用的模型、温度、频率惩罚等。\n启动时，可以使用 OpenAiChatModel(api, options) 构造函数或 spring.ai.openai.chat.options.* 属性配置默认选项。\n在运行时，您可以通过向 Prompt 调用添加新的、特定于请求的选项来覆盖默认选项。例如，要覆盖特定请求的默认模型和温度：\nChatResponse response = chatModel.call( new Prompt( \u0026#34;Generate the names of 5 famous pirates.\u0026#34;, OpenAiChatOptions.builder() .model(\u0026#34;mixtral-8x7b-32768\u0026#34;) .temperature(0.4) .build() )); 函数调用 # 当选择支持它的模型时，NVIDIA LLM API 支持工具/函数调用。\n您可以将自定义 Java 函数注册到 ChatModel 中，并让提供的模型智能地选择输出包含参数的 JSON 对象，以调用一个或多个已注册的函数。这是一种将 LLM 功能与外部工具和 API 连接起来的强大技术。\n工具示例 # 下面是一个如何使用 NVIDIA LLM API 函数调用和 Spring AI 的简单示例：\nspring.ai.openai.api-key=${NVIDIA_API_KEY} spring.ai.openai.base-url=https://integrate.api.nvidia.com spring.ai.openai.chat.options.model=meta/llama-3.1-70b-instruct spring.ai.openai.chat.options.max-tokens=2048 @SpringBootApplication public class NvidiaLlmApplication { public static void main(String[] args) { SpringApplication.run(NvidiaLlmApplication.class, args); } @Bean CommandLineRunner runner(ChatClient.Builder chatClientBuilder) { return args -\u0026gt; { var chatClient = chatClientBuilder.build(); var response = chatClient.prompt() .user(\u0026#34;What is the weather in Amsterdam and Paris?\u0026#34;) .functions(\u0026#34;weatherFunction\u0026#34;) // reference by bean name. .call() .content(); System.out.println(response); }; } @Bean @Description(\u0026#34;Get the weather in location\u0026#34;) public Function\u0026lt;WeatherRequest, WeatherResponse\u0026gt; weatherFunction() { return new MockWeatherService(); } public static class MockWeatherService implements Function\u0026lt;WeatherRequest, WeatherResponse\u0026gt; { public record WeatherRequest(String location, String unit) {} public record WeatherResponse(double temp, String unit) {} @Override public WeatherResponse apply(WeatherRequest request) { double temperature = request.location().contains(\u0026#34;Amsterdam\u0026#34;) ? 20 : 25; return new WeatherResponse(temperature, request.unit); } } } 在此示例中，当模型需要天气信息时，它将自动调用 weatherFunction bean，该 bean 可以获取实时天气数据。预期响应如下：“阿姆斯特丹的天气当前为 20 摄氏度，巴黎的天气当前为 25 摄氏度。”\n阅读有关 OpenAI [ 函数调用的]( https://docs.spring.io/spring-ai/reference/api/chat/functions/openai-chat-functions.html)更多信息。\n样品控制器 # [ 创建]( https://start.spring.io/)一个新的 Spring Boot 项目并将 spring-ai-starter-model-openai 添加到您的 pom（或 gradle）依赖项中。\n在 src/main/resources 目录下添加一个 application.properties 文件，以启用和配置 OpenAi 聊天模型：\nspring.ai.openai.api-key=${NVIDIA_API_KEY} spring.ai.openai.base-url=https://integrate.api.nvidia.com spring.ai.openai.chat.options.model=meta/llama-3.1-70b-instruct # The NVIDIA LLM API doesn\u0026#39;t support embeddings, so we need to disable it. spring.ai.openai.embedding.enabled=false # The NVIDIA LLM API requires this parameter to be set explicitly or server internal error will be thrown. spring.ai.openai.chat.options.max-tokens=2048 这是一个使用聊天模型生成文本的简单 @Controller 类的示例。\n@RestController public class ChatController { private final OpenAiChatModel chatModel; @Autowired public ChatController(OpenAiChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, this.chatModel.call(message)); } @GetMapping(\u0026#34;/ai/generateStream\u0026#34;) public Flux\u0026lt;ChatResponse\u0026gt; generateStream(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { Prompt prompt = new Prompt(new UserMessage(message)); return this.chatModel.stream(prompt); } } "},{"id":86,"href":"/docs/%E5%8F%82%E8%80%83/%E7%9F%A2%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/oracle-database-23ai---ai-%E5%90%91%E9%87%8F%E6%90%9C%E7%B4%A2/","title":"Oracle Database 23ai - AI 向量搜索","section":"矢量数据库","content":" Oracle Database 23ai - AI 向量搜索 # Oracle Database 23ai（23.4+）的 [ AI 向量搜索]( https://docs.oracle.com/en/database/oracle/oracle-database/23/vecse/overview-ai-vector-search.html)功能现已作为 Spring AI VectorStore 提供，可帮助您存储文档嵌入并执行相似性搜索。当然，所有其他功能也均可使用。\n自动配置 # 首先将 Oracle Vector Store 启动程序依赖项添加到您的项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-oracle\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-vector-store-oracle\u0026#39; } 如果您需要此向量存储为您初始化模式，那么您需要在适当的构造函数中为 initializeSchema 布尔参数传递 true，或者在 application.properties 文件中设置 …​initialize-schema=true 。\nVector Store 还需要一个 EmbeddingModel 实例来计算文档的嵌入。你可以从可用的 EmbeddingModel 实现中选择一个。\n例如，要使用 [ OpenAI EmbeddingModel，](../embeddings/openai-embeddings.html) 请将以下依赖项添加到您的项目：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-openai\u0026#39; } 要连接并配置 OracleVectorStore ，您需要提供数据库的访问详细信息。您可以通过 Spring Boot 的 application.yml 提供简单的配置。\n现在您可以在应用程序中自动连接 OracleVectorStore 并使用它：\n@Autowired VectorStore vectorStore; // ... List\u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); // Add the documents to Oracle Vector Store vectorStore.add(documents); // Retrieve documents similar to a query List\u0026lt;Document\u0026gt; results = this.vectorStore.similaritySearch(SearchRequest.builder().query(\u0026#34;Spring\u0026#34;).topK(5).build()); 配置属性 # 您可以在 Spring Boot 配置中使用以下属性来自定义 OracleVectorStore 。\n元数据过滤 # 您可以利用 OracleVectorStore 的通用、可移植[ 元数据过滤器]( https://docs.spring.io/spring-ai/reference/api/vectordbs.html#_metadata_filters) 。\n例如，您可以使用文本表达语言：\nvectorStore.similaritySearch( SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(\u0026#34;author in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; article_type == \u0026#39;blog\u0026#39;\u0026#34;).build()); 或者以编程方式使用 Filter.Expression DSL：\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(b.and( b.in(\u0026#34;author\u0026#34;,\u0026#34;john\u0026#34;, \u0026#34;jill\u0026#34;), b.eq(\u0026#34;article_type\u0026#34;, \u0026#34;blog\u0026#34;)).build()).build()); 手动配置 # 除了使用 Spring Boot 自动配置之外，您还可以手动配置 OracleVectorStore 。为此，您需要将 Oracle JDBC 驱动程序和 JdbcTemplate 自动配置依赖项添加到项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-jdbc\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.oracle.database.jdbc\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;ojdbc11\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;runtime\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-oracle-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 要在应用程序中配置 OracleVectorStore ，您可以使用以下设置：\n@Bean public VectorStore vectorStore(JdbcTemplate jdbcTemplate, EmbeddingModel embeddingModel) { return OracleVectorStore.builder(jdbcTemplate, embeddingModel) .tableName(\u0026#34;my_vectors\u0026#34;) .indexType(OracleVectorStoreIndexType.IVF) .distanceType(OracleVectorStoreDistanceType.COSINE) .dimensions(1536) .searchAccuracy(95) .initializeSchema(true) .build(); } 在本地运行 Oracle Database 23ai # 然后您可以使用以下命令连接到数据库：\n访问 Native Client # Oracle Vector Store 实现通过 getNativeClient() 方法提供对底层本机 Oracle 客户端（ OracleConnection ）的访问：\nOracleVectorStore vectorStore = context.getBean(OracleVectorStore.class); Optional\u0026lt;OracleConnection\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { OracleConnection connection = nativeClient.get(); // Use the native client for Oracle-specific operations } 本机客户端使您能够访问可能无法通过 VectorStore 接口公开的 Oracle 特定功能和操作。\n"},{"id":87,"href":"/docs/%E5%8F%82%E8%80%83/%E5%BC%80%E5%8F%91%E6%97%B6%E6%9C%8D%E5%8A%A1/","title":"开发时服务","section":"参考","content":" 开发时服务 # Spring AI 提供了 Spring Boot 自动配置，用于建立与通过 Docker Compose 运行的模型服务或向量存储的连接。要启用此功能，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-spring-boot-docker-compose\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-spring-boot-docker-compose\u0026#39; } 服务连接 # spring-ai-spring-boot-docker-compose 模块中提供了以下服务连接工厂：\n"},{"id":88,"href":"/docs/%E5%8F%82%E8%80%83/%E7%9F%A2%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/pg-%E8%BD%BD%E4%BD%93/","title":"PG 载体","section":"矢量数据库","content":" PG 载体 # 本节将引导您设置 PGvector VectorStore 来存储文档嵌入并执行相似性搜索。\n[ PGvector]( https://github.com/pgvector/pgvector) 是 PostgreSQL 的一个开源扩展，支持存储和搜索机器学习生成的嵌入。它提供了多种功能，让用户能够识别精确和近似的最近邻。它旨在与其他 PostgreSQL 功能（包括索引和查询）无缝协作。\n先决条件 # 首先，您需要访问启用了 vector 、 hstore 和 uuid-ossp 扩展的 PostgreSQL 实例。\n启动时， PgVectorStore 将尝试安装所需的数据库扩展，并创建带有索引的所需 vector_store 表（如果不存在）。\n或者，您可以像这样手动执行此操作：\n接下来，如果需要， [ EmbeddingModel](../embeddings.html#available-implementations) 的 API 密钥可以生成 PgVectorStore 存储的嵌入。\n自动配置 # 然后将 PgVectorStore 启动程序依赖项添加到您的项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-pgvector\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-vector-store-pgvector\u0026#39; } 向量存储实现可以为您初始化所需的模式，但您必须通过在适当的构造函数中指定 initializeSchema 布尔值或在 application.properties 文件中设置 …​initialize-schema=true 来选择加入。\nVector Store 还需要一个 EmbeddingModel 实例来计算文档的嵌入。您可以从可用的 EmbeddingModel 实现中选择一个。\n例如，要使用 [ OpenAI EmbeddingModel](../embeddings/openai-embeddings.html) ，请将以下依赖项添加到您的项目：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-openai\u0026#39; } 要连接并配置 PgVectorStore ，您需要提供实例的访问详细信息。您可以通过 Spring Boot 的 application.yml 提供简单的配置。\n现在您可以在应用程序中自动连接 VectorStore 并使用它\n@Autowired VectorStore vectorStore; // ... List\u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); // Add the documents to PGVector vectorStore.add(documents); // Retrieve documents similar to a query List\u0026lt;Document\u0026gt; results = this.vectorStore.similaritySearch(SearchRequest.builder().query(\u0026#34;Spring\u0026#34;).topK(5).build()); 配置属性 # 您可以在 Spring Boot 配置中使用以下属性来自定义 PGVector 向量存储。\n元数据过滤 # 您可以利用 PgVector 存储的通用、可移植元[ 数据过滤器]( https://docs.spring.io/spring-ai/reference/api/vectordbs.html#_metadata_filters) 。\n例如，您可以使用文本表达语言：\nvectorStore.similaritySearch( SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(\u0026#34;author in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; article_type == \u0026#39;blog\u0026#39;\u0026#34;).build()); 或者以编程方式使用 Filter.Expression DSL：\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(b.and( b.in(\u0026#34;author\u0026#34;,\u0026#34;john\u0026#34;, \u0026#34;jill\u0026#34;), b.eq(\u0026#34;article_type\u0026#34;, \u0026#34;blog\u0026#34;)).build()).build()); 手动配置 # 除了使用 Spring Boot 自动配置之外，您还可以手动配置 PgVectorStore 。为此，您需要将 PostgreSQL 连接和 JdbcTemplate 自动配置依赖项添加到您的项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-jdbc\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.postgresql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;postgresql\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;runtime\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-pgvector-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 要在应用程序中配置 PgVector，您可以使用以下设置：\n@Bean public VectorStore vectorStore(JdbcTemplate jdbcTemplate, EmbeddingModel embeddingModel) { return PgVectorStore.builder(jdbcTemplate, embeddingModel) .dimensions(1536) // Optional: defaults to model dimensions or 1536 .distanceType(COSINE_DISTANCE) // Optional: defaults to COSINE_DISTANCE .indexType(HNSW) // Optional: defaults to HNSW .initializeSchema(true) // Optional: defaults to false .schemaName(\u0026#34;public\u0026#34;) // Optional: defaults to \u0026#34;public\u0026#34; .vectorTableName(\u0026#34;vector_store\u0026#34;) // Optional: defaults to \u0026#34;vector_store\u0026#34; .maxDocumentBatchSize(10000) // Optional: defaults to 10000 .build(); } 在本地运行 Postgres 和 PGVector DB # 您可以像这样连接到该服务器：\n访问 Native Client # PGVector Store 实现通过 getNativeClient() 方法提供对底层本机 JDBC 客户端（ JdbcTemplate ）的访问：\nPgVectorStore vectorStore = context.getBean(PgVectorStore.class); Optional\u0026lt;JdbcTemplate\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { JdbcTemplate jdbc = nativeClient.get(); // Use the native client for PostgreSQL-specific operations } 本机客户端使您可以访问可能无法通过 VectorStore 接口公开的 PostgreSQL 特定功能和操作。\n"},{"id":89,"href":"/docs/%E5%8F%82%E8%80%83/%E6%B5%8B%E8%AF%95/","title":"测试","section":"参考","content":" 测试 # "},{"id":90,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B-api/%E8%81%8A%E5%A4%A9/","title":"聊天","section":"聊天模型 API","content":" 聊天 # 使用 [ Ollama]( https://ollama.ai/) ，您可以在本地运行各种大型语言模型 (LLM) 并从中生成文本。Spring AI 通过 [[Ollama](https://ollama.ai/)](https://ollama.ai/)ChatModel API 支持 [ Ollama]( https://ollama.ai/) 聊天补全功能。\n先决条件 # 首先，您需要访问 Ollama 实例。有以下几种选择：\n在本地机器上下载并安装 Ollama 。 通过 Testcontainers 配置并运行 Ollama 。 通过 Kubernetes 服务绑定绑定到 Ollama 实例。 您可以从 [ Ollama 模型库中]( https://ollama.com/library)提取您想要在应用程序中使用的模型：\nollama pull \u0026lt;model-name\u0026gt; 您还可以从数千个免费的 [ GGUF Hugging Face Models]( https://huggingface.co/models?library=gguf\u0026sort=trending) 中挑选任意一个：\nollama pull hf.co/\u0026lt;username\u0026gt;/\u0026lt;model-repository\u0026gt; 或者，您可以启用自动下载任何所需模型的选项： [ 自动拉取模型](#auto-pulling-models) 。\n自动配置 # Spring AI 为 Ollama 聊天集成提供了 Spring Boot 自动配置。要启用此功能，请将以下依赖项添加到项目的 Maven pom.xml 或 Gradle build.gradle 构建文件中：\n基本属性 # 前缀 spring.ai.ollama 是配置与 Ollama 的连接的属性前缀。\n以下是初始化 Ollama 集成和[ 自动拉取模型的](#auto-pulling-models)属性。\n聊天属性 # 前缀 `spring.ai.ollama.chat.options``` 是配置 Ollama 聊天模型的属性前缀。它包含 Ollama 请求（高级）参数，例如 model、keep-alive和format以及 Ollama 模型options`` 属性。\n以下是 Ollama 聊天模型的高级请求参数：\n其余 options 属性基于 [ Ollama 有效参数和值]( https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values)以及 [ Ollama 类型]( https://github.com/ollama/ollama/blob/main/api/types.go) 。默认值基于 [ Ollama 类型]( https://github.com/ollama/ollama/blob/main/api/types.go)默认值 。\n运行时选项 # [ OllamaOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-ollama/src/main/java/org/springframework/ai/ollama/api/[OllamaOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-ollama/src/main/java/org/springframework/ai/ollama/api/OllamaOptions.java)) 类提供模型配置，例如要使用的模型、温度等。\n启动时，可以使用 OllamaChatModel(api, options) 构造函数或 spring.ai.ollama.chat.options.* 属性配置默认选项。\n在运行时，您可以通过向 Prompt 调用添加新的特定于请求的选项来覆盖默认选项。例如，要覆盖特定请求的默认模型和温度：\nChatResponse response = chatModel.call( new Prompt( \u0026#34;Generate the names of 5 famous pirates.\u0026#34;, OllamaOptions.builder() .model(OllamaModel.LLAMA3_1) .temperature(0.4) .build() )); 自动拉动模型 # Spring AI Ollama 可以在您的 Ollama 实例中不可用时自动拉取模型。此功能对于开发和测试以及将应用程序部署到新环境特别有用。\n拉模型的策略有三种：\nalways （在 PullModelStrategy.ALWAYS 中定义）：始终拉取模型，即使它已经可用。这有助于确保您使用的是最新版本的模型。 when_missing （在 PullModelStrategy.WHEN_MISSING 中定义）：仅当模型不可用时才提取该模型。这可能会导致使用旧版本的模型。 never （在 PullModelStrategy.NEVER 中定义）：从不自动拉动模型。 所有通过配置属性和默认选项定义的模型都可以在启动时自动拉取。您可以使用配置属性配置拉取策略、超时时间和最大重试次数：\nspring: ai: ollama: init: pull-model-strategy: always timeout: 60s max-retries: 1 您可以在启动时初始化其他模型，这对于运行时动态使用的模型很有用：\nspring: ai: ollama: init: pull-model-strategy: always chat: additional-models: - llama3.2 - qwen2.5 如果您希望仅将拉取策略应用于特定类型的模型，则可以将聊天模型从初始化任务中排除：\nspring: ai: ollama: init: pull-model-strategy: always chat: include: false 此配置将把拉取策略应用到除聊天模型之外的所有模型。\n函数调用 # 您可以使用 OllamaChatModel 注册自定义 Java 函数，并让 Ollama 模型智能地选择输出包含参数的 JSON 对象，以调用一个或多个已注册的函数。这是一种将 LLM 功能与外部工具和 API 连接起来的强大技术。了解更多关于[ 工具调用的](../tools.html)信息。\n多式联运 # 多模态性是指模型同时理解和处理来自各种来源的信息的能力，包括文本、图像、音频和其他数据格式。\nOllama 中支持多模态模型包括 [ LLaVA]( https://ollama.com/library/llava) 和 Bak[ LLaVA]( https://ollama.com/library/llava) （查看[ 完整列表]( https://ollama.com/search?c=vision) ）。更多详情，请参阅 [ LLaVA]( https://ollama.com/library/llava)：大型语言和视觉助手 。\nOllama [ 消息 API]( https://github.com/ollama/ollama/blob/main/docs/api.md#parameters-1) 提供了一个“图像”参数，用于将 base64 编码的图像列表与消息合并。\nSpring AI 的 [ Message]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/messages/[Message](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/messages/Message.java).java) 接口通过引入 [ Media]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/model/[Media](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/model/Media.java).java) 类型来促进多模态 AI 模型的发展。此类型包含消息中媒体附件的数据和详细信息，并利用 Spring 的 org.springframework.util.MimeType 和 org.springframework.core.io.Resource 来存储原始媒体数据。\n下面是从 [ OllamaChatModelMultimodalIT.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-ollama/src/test/java/org/springframework/ai/ollama/[OllamaChatModelMultimodalIT.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-ollama/src/test/java/org/springframework/ai/ollama/OllamaChatModelMultimodalIT.java)) 中摘录的简单代码示例，说明了用户文本与图像的融合。\nvar imageResource = new ClassPathResource(\u0026#34;/multimodal.test.png\u0026#34;); var userMessage = new UserMessage(\u0026#34;Explain what do you see on this picture?\u0026#34;, new Media(MimeTypeUtils.IMAGE_PNG, this.imageResource)); ChatResponse response = chatModel.call(new Prompt(this.userMessage, OllamaOptions.builder().model(OllamaModel.LLAVA)).build()); 该示例展示了一个将 multimodal.test.png 图像作为输入的模型：\n以及文本消息“解释一下你在图片上看到了什么？”，并生成如下的回应：\n结构化输出 # Ollama 提供自定义的[ 结构化输出]( https://ollama.com/blog/structured-outputs) API，确保您的模型生成严格符合您提供的 JSON Schema 响应。除了现有的与 Spring AI 模型无关的[ 结构化输出]( https://ollama.com/blog/structured-outputs)转换器之外，这些 API 还提供了增强的控制力和精度。\n配置 # Spring AI 允许您使用 OllamaOptions 构建器以编程方式配置响应格式。\n使用聊天选项生成器 # 您可以使用 OllamaOptions 构建器以编程方式设置响应格式，如下所示：\nString jsonSchema = \u0026#34;\u0026#34;\u0026#34; { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;steps\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, \u0026#34;items\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;explanation\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;output\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } }, \u0026#34;required\u0026#34;: [\u0026#34;explanation\u0026#34;, \u0026#34;output\u0026#34;], \u0026#34;additionalProperties\u0026#34;: false } }, \u0026#34;final_answer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } }, \u0026#34;required\u0026#34;: [\u0026#34;steps\u0026#34;, \u0026#34;final_answer\u0026#34;], \u0026#34;additionalProperties\u0026#34;: false } \u0026#34;\u0026#34;\u0026#34;; Prompt prompt = new Prompt(\u0026#34;how can I solve 8x + 7 = -23\u0026#34;, OllamaOptions.builder() .model(OllamaModel.LLAMA3_2.getName()) .format(new ObjectMapper().readValue(jsonSchema, Map.class)) .build()); ChatResponse response = this.ollamaChatModel.call(this.prompt); 与 BeanOutputConverter 实用程序集成 # 您可以利用现有的 [ BeanOutputConverter](../structured-output-converter.html#_bean_output_converter) 实用程序从域对象自动生成 JSON 模式，然后将结构化响应转换为特定于域的实例：\nrecord MathReasoning( @JsonProperty(required = true, value = \u0026#34;steps\u0026#34;) Steps steps, @JsonProperty(required = true, value = \u0026#34;final_answer\u0026#34;) String finalAnswer) { record Steps( @JsonProperty(required = true, value = \u0026#34;items\u0026#34;) Items[] items) { record Items( @JsonProperty(required = true, value = \u0026#34;explanation\u0026#34;) String explanation, @JsonProperty(required = true, value = \u0026#34;output\u0026#34;) String output) { } } } var outputConverter = new BeanOutputConverter\u0026lt;\u0026gt;(MathReasoning.class); Prompt prompt = new Prompt(\u0026#34;how can I solve 8x + 7 = -23\u0026#34;, OllamaOptions.builder() .model(OllamaModel.LLAMA3_2.getName()) .format(outputConverter.getJsonSchemaMap()) .build()); ChatResponse response = this.ollamaChatModel.call(this.prompt); String content = this.response.getResult().getOutput().getText(); MathReasoning mathReasoning = this.outputConverter.convert(this.content); OpenAI API 兼容性 # Ollama 与 OpenAI API 兼容，您可以使用 [ Spring AI OpenAI](openai-chat.html) 客户端与 Ollama 通信并使用工具。为此，您需要将 OpenAI 基本网址配置到您的 Ollama 实例： spring.ai.openai.chat.base-url=http://localhost:11434 ，并从提供的 Ollama 模型中选择一种： spring.ai.openai.chat.options.model=mistral 。\n检查 [ OllamaWithOpenAiChatModelIT.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/chat/proxy/[OllamaWithOpenAiChatModelIT.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/chat/proxy/OllamaWithOpenAiChatModelIT.java)) 测试，了解通过 Spring AI OpenAI 使用 Ollama 的示例。\nHuggingFace 模型 # Ollama 可以立即访问所有 [ GGUF Hugging Face]( https://huggingface.co/models?library=gguf\u0026sort=trending) Chat 模型。您可以按名称 ollama pull hf.co/\u0026lt;username\u0026gt;/\u0026lt;model-repository\u0026gt; 拉取以下任意模型，或配置自动拉取策略： [ 自动拉取模型](#auto-pulling-models) ：\nspring.ai.ollama.chat.options.model=hf.co/bartowski/gemma-2-2b-it-GGUF spring.ai.ollama.init.pull-model-strategy=always spring.ai.ollama.chat.options.model ：指定要使用的 Hugging Face GGUF 模型 。 spring.ai.ollama.init.pull-model-strategy=always ：（可选）在启动时启用自动模型拉取。对于生产环境，您应该预先下载模型以避免延迟： ollama pull hf.co/bartowski/gemma-2-2b-it-GGUF 。 样品控制器 # [ 创建]( https://start.spring.io/)一个新的 Spring Boot 项目并将 spring-ai-starter-model-ollama 添加到您的 pom（或 gradle）依赖项中。\n在 src/main/resources 目录下添加一个 application.yaml 文件，以启用和配置 Ollama 聊天模型：\nspring: ai: ollama: base-url: http://localhost:11434 chat: options: model: mistral temperature: 0.7 这将创建一个 OllamaChatModel 实现，您可以将其注入到您的类中。这是一个使用聊天模型生成文本的简单 @RestController 类的示例。\n@RestController public class ChatController { private final OllamaChatModel chatModel; @Autowired public ChatController(OllamaChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map\u0026lt;String,String\u0026gt; generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, this.chatModel.call(message)); } @GetMapping(\u0026#34;/ai/generateStream\u0026#34;) public Flux\u0026lt;ChatResponse\u0026gt; generateStream(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { Prompt prompt = new Prompt(new UserMessage(message)); return this.chatModel.stream(prompt); } } 手动配置 # 如果[ 您]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-ollama/src/main/java/org/springframework/ai/ollama/OllamaChatModel.java)不想使用 Spring Boot 自动配置，[ 您]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-ollama/src/main/java/org/springframework/ai/ollama/OllamaChatModel.java)可以在应用程序中手动配置 `OllamaChatModel``` 实现了 ChatModel 和 StreamingChatModel`` ，并使用 [ Low-level OllamaApi Client](#low-level-api) 连接到 Ollama 服务。\n要使用它，请将 spring-ai-ollama 依赖项添加到项目的 Maven pom.xml 或 Gradle build.gradle 构建文件中：\n接下来，创建一个 OllamaChatModel 实例并使用它来发送文本生成请求：\nvar ollamaApi = OllamaApi.builder().build(); var chatModel = OllamaChatModel.builder() .ollamaApi(ollamaApi) .defaultOptions( OllamaOptions.builder() .model(OllamaModel.MISTRAL) .temperature(0.9) .build()) .build(); ChatResponse response = this.chatModel.call( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); // Or with streaming responses Flux\u0026lt;ChatResponse\u0026gt; response = this.chatModel.stream( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); OllamaOptions 提供所有聊天请求的配置信息。\n低级 OllamaApi 客户端 # [ OllamaApi]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-ollama/src/main/java/org/springframework/ai/ollama/api/[OllamaApi](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-ollama/src/main/java/org/springframework/ai/ollama/api/OllamaApi.java).java) 为 Ollama Chat Completion [ API]( https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-chat-completion) 提供了一个轻量级的 Java 客户端。\n以下类图说明了 OllamaApi 聊天接口和构建块：\n下面是一个简单的代码片段，展示了如何以编程方式使用 API：\nOllamaApi ollamaApi = new OllamaApi(\u0026#34;YOUR_HOST:YOUR_PORT\u0026#34;); // Sync request var request = ChatRequest.builder(\u0026#34;orca-mini\u0026#34;) .stream(false) // not streaming .messages(List.of( Message.builder(Role.SYSTEM) .content(\u0026#34;You are a geography teacher. You are talking to a student.\u0026#34;) .build(), Message.builder(Role.USER) .content(\u0026#34;What is the capital of Bulgaria and what is the size? \u0026#34; + \u0026#34;What is the national anthem?\u0026#34;) .build())) .options(OllamaOptions.builder().temperature(0.9).build()) .build(); ChatResponse response = this.ollamaApi.chat(this.request); // Streaming request var request2 = ChatRequest.builder(\u0026#34;orca-mini\u0026#34;) .ttream(true) // streaming .messages(List.of(Message.builder(Role.USER) .content(\u0026#34;What is the capital of Bulgaria and what is the size? \u0026#34; + \u0026#34;What is the national anthem?\u0026#34;) .build())) .options(OllamaOptions.builder().temperature(0.9).build().toMap()) .build(); Flux\u0026lt;ChatResponse\u0026gt; streamingResponse = this.ollamaApi.streamingChat(this.request2); "},{"id":91,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B-api/%E5%9B%B0%E6%83%91%E8%81%8A%E5%A4%A9/","title":"困惑聊天","section":"聊天模型 API","content":" 困惑聊天 # [ Perplexity AI]( https://perplexity.ai/) 提供独特的 AI 服务，将其语言模型与实时搜索功能相结合。它提供多种模型，并支持对话式 AI 的流式响应。\nSpring AI 通过重用现有的 [ OpenAI](openai-chat.html) 客户端与 Perplexity AI 集成。首先，您需要获取 [ Perplexity API 密钥]( https://docs.perplexity.ai/guides/getting-started) ，配置基本 URL ，并选择其中一个受支持的[ 模型]( https://docs.perplexity.ai/guides/model-cards) 。\n检查 [ PerplexityWithOpenAiChatModelIT.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/chat/proxy/[PerplexityWithOpenAiChatModelIT.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/chat/proxy/PerplexityWithOpenAiChatModelIT.java)) 测试，了解将 Perplexity 与 Spring AI 结合使用的示例。\n先决条件 # 创建 API 密钥 ：访问此处创建 API 密钥。使用 Spring AI 项目中的 spring.ai.openai.api-key 属性进行配置。 设置 Perplexity Base URL ：将 spring.ai.openai.base-url 属性设置为 api.perplexity.ai 。 选择困惑度模型 ：使用 spring.ai.openai.chat.model= 属性指定模型。请参阅支持的模型 ，了解可用选项。 设置聊天完成路径 ：将 spring.ai.openai.chat.completions-path 设置为 /chat/completions 。更多详情，请参阅聊天完成 API 。 您可以在 application.properties 文件中设置这些配置属性：\nspring.ai.openai.api-key=\u0026lt;your-perplexity-api-key\u0026gt; spring.ai.openai.base-url=https://api.perplexity.ai spring.ai.openai.chat.model=llama-3.1-sonar-small-128k-online spring.ai.openai.chat.completions-path=/chat/completions 为了在处理 API 密钥等敏感信息时增强安全性，您可以使用 Spring 表达式语言 (SpEL) 来引用自定义环境变量：\n# In application.yml spring: ai: openai: api-key: ${PERPLEXITY_API_KEY} base-url: ${PERPLEXITY_BASE_URL} chat: model: ${PERPLEXITY_MODEL} completions-path: ${PERPLEXITY_COMPLETIONS_PATH} # In your environment or .env file export PERPLEXITY_API_KEY=\u0026lt;your-perplexity-api-key\u0026gt; export PERPLEXITY_BASE_URL=https://api.perplexity.ai export PERPLEXITY_MODEL=llama-3.1-sonar-small-128k-online export PERPLEXITY_COMPLETIONS_PATH=/chat/completions 您还可以在应用程序代码中以编程方式设置这些配置：\n// Retrieve configuration from secure sources or environment variables String apiKey = System.getenv(\u0026#34;PERPLEXITY_API_KEY\u0026#34;); String baseUrl = System.getenv(\u0026#34;PERPLEXITY_BASE_URL\u0026#34;); String model = System.getenv(\u0026#34;PERPLEXITY_MODEL\u0026#34;); String completionsPath = System.getenv(\u0026#34;PERPLEXITY_COMPLETIONS_PATH\u0026#34;); 添加存储库和 BOM # Spring AI 的构件已发布在 Maven Central 和 Spring Snapshot 仓库中。请参阅 [ “构件仓库”](../../getting-started.html#artifact-repositories) 部分，将这些仓库添加到您的构建系统中。\n为了帮助管理依赖项，Spring AI 提供了 BOM（物料清单），以确保在整个项目中使用一致版本的 Spring AI。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # Spring AI 为 OpenAI Chat Client 提供了 Spring Boot 自动配置功能。要启用此功能，请将以下依赖项添加到项目的 Maven pom.xml 或 Gradle build.gradle 构建文件中：\n聊天属性 # 重试属性 # 前缀 spring.ai.retry 用作属性前缀，可让您配置 OpenAI 聊天模型的重试机制。\n连接属性 # 前缀 spring.ai.openai 用作允许您连接到 OpenAI 的属性前缀。\n配置属性 # 前缀 spring.ai.openai.chat 是允许您为 OpenAI 配置聊天模型实现的属性前缀。\n运行时选项 # [ OpenAiChatOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/[OpenAiChatOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/OpenAiChatOptions.java)) 提供模型配置，例如要使用的模型、温度、频率惩罚等。\n启动时，可以使用 OpenAiChatModel(api, options) 构造函数或 spring.ai.openai.chat.options.* 属性配置默认选项。\n在运行时，您可以通过向 Prompt 调用添加新的、特定于请求的选项来覆盖默认选项。例如，要覆盖特定请求的默认模型和温度：\nChatResponse response = chatModel.call( new Prompt( \u0026#34;Generate the names of 5 famous pirates.\u0026#34;, OpenAiChatOptions.builder() .model(\u0026#34;llama-3.1-sonar-large-128k-online\u0026#34;) .temperature(0.4) .build() )); 函数调用 # 多式联运 # 样品控制器 # [ 创建]( https://start.spring.io/)一个新的 Spring Boot 项目并将 spring-ai-starter-model-openai 添加到您的 pom（或 gradle）依赖项中。\n在 src/main/resources 目录下添加一个 application.properties 文件，以启用和配置 OpenAi 聊天模型：\nspring.ai.openai.api-key=\u0026lt;PERPLEXITY_API_KEY\u0026gt; spring.ai.openai.base-url=https://api.perplexity.ai spring.ai.openai.chat.completions-path=/chat/completions spring.ai.openai.chat.options.model=llama-3.1-sonar-small-128k-online spring.ai.openai.chat.options.temperature=0.7 # The Perplexity API doesn\u0026#39;t support embeddings, so we need to disable it. spring.ai.openai.embedding.enabled=false 这将创建一个 OpenAiChatModel 实现，您可以将其注入到您的类中。这是一个使用聊天模型进行文本生成的简单 @Controller 类的示例。\n@RestController public class ChatController { private final OpenAiChatModel chatModel; @Autowired public ChatController(OpenAiChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, this.chatModel.call(message)); } @GetMapping(\u0026#34;/ai/generateStream\u0026#34;) public Flux\u0026lt;ChatResponse\u0026gt; generateStream(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { Prompt prompt = new Prompt(new UserMessage(message)); return this.chatModel.stream(prompt); } } 支持的型号 # Perplexity 支持多种针对搜索增强型对话式 AI 进行优化的模型。详情请参阅[ 支持的模型]( https://docs.perplexity.ai/guides/model-cards) 。\n参考 # 文档主页 API 参考 入门 速率限制 "},{"id":92,"href":"/docs/%E5%8F%82%E8%80%83/%E7%9F%A2%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/%E6%9D%BE%E6%9E%9C/","title":"松果","section":"矢量数据库","content":" 松果 # 本节将引导您设置 Pinecone VectorStore 来存储文档嵌入并执行相似性搜索。\n[ Pinecone]( https://www.pinecone.io/) 是一个流行的基于云的矢量数据库，它允许您高效地存储和搜索矢量。\n先决条件 # 要设置 PineconeVectorStore ，请从您的 Pinecone 帐户收集以下详细信息：\nPinecone API 密钥 松果索引名称 Pinecone 命名空间 自动配置 # Spring AI 为 Pinecone 矢量存储提供了 Spring Boot 自动配置功能。要启用此功能，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-pinecone\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-vector-store-pinecone\u0026#39; } 此外，您还需要一个已配置的 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) bean。有关更多信息，请参阅 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) 部分。\n以下是所需 bean 的示例：\n@Bean public EmbeddingModel embeddingModel() { // Can be any other EmbeddingModel implementation. return new OpenAiEmbeddingModel(new OpenAiApi(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;))); } 要连接到 Pinecone，您需要提供实例的访问详细信息。您可以通过 Spring Boot 的 application.properties 提供简单的配置，\nspring.ai.vectorstore.pinecone.apiKey=\u0026lt;your api key\u0026gt; spring.ai.vectorstore.pinecone.index-name=\u0026lt;your index name\u0026gt; # API key if needed, e.g. OpenAI spring.ai.openai.api.key=\u0026lt;api-key\u0026gt; 请查看向量存储的[ 配置参数](#_configuration_properties)列表，以了解默认值和配置选项。\n现在您可以在应用程序中自动连接 Pinecone Vector Store 并使用它\n@Autowired VectorStore vectorStore; // ... List \u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); // Add the documents vectorStore.add(documents); // Retrieve documents similar to a query List\u0026lt;Document\u0026gt; results = this.vectorStore.similaritySearch(SearchRequest.builder().query(\u0026#34;Spring\u0026#34;).topK(5).build()); 配置属性 # 您可以在 Spring Boot 配置中使用以下属性来自定义 Pinecone 矢量存储。\n元数据过滤 # 您可以利用 Pinecone 商店的通用、可移植[ 元数据过滤器]( https://docs.spring.io/spring-ai/reference/api/vectordbs.html#_metadata_filters) 。\n例如，您可以使用文本表达语言：\nvectorStore.similaritySearch( SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(\u0026#34;author in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; article_type == \u0026#39;blog\u0026#39;\u0026#34;).build()); 或者以编程方式使用 Filter.Expression DSL：\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(b.and( b.in(\u0026#34;author\u0026#34;,\u0026#34;john\u0026#34;, \u0026#34;jill\u0026#34;), b.eq(\u0026#34;article_type\u0026#34;, \u0026#34;blog\u0026#34;)).build()).build()); 手动配置 # 如果您希望手动配置 PineconeVectorStore ，则可以使用 PineconeVectorStore#Builder 进行配置。\n将这些依赖项添加到您的项目：\nOpenAI：计算嵌入所需。 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 松果 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-pinecone-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 示例代码 # 要在您的应用程序中配置 Pinecone，您可以使用以下设置：\n@Bean public VectorStore pineconeVectorStore(EmbeddingModel embeddingModel) { return PineconeVectorStore.builder(embeddingModel) .apiKey(PINECONE_API_KEY) .indexName(PINECONE_INDEX_NAME) .namespace(PINECONE_NAMESPACE) // the free tier doesn\u0026#39;t support namespaces. .contentFieldName(CUSTOM_CONTENT_FIELD_NAME) // optional field to store the original content. Defaults to `document_content` .build(); } 在您的主代码中，创建一些文档：\nList\u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); 将文档添加到 Pinecone：\nvectorStore.add(documents); 最后，检索与查询类似的文档：\nList\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch(SearchRequest.query(\u0026#34;Spring\u0026#34;).topK(5).build()); 如果一切顺利，您应该检索包含文本“Spring AI rocks!!”的文档。\n访问 Native Client # Pinecone Vector Store 实现通过 getNativeClient() 方法提供对底层本机 Pinecone 客户端（ PineconeConnection ）的访问：\nPineconeVectorStore vectorStore = context.getBean(PineconeVectorStore.class); Optional\u0026lt;PineconeConnection\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { PineconeConnection client = nativeClient.get(); // Use the native client for Pinecone-specific operations } 本机客户端允许您访问可能无法通过 VectorStore 接口公开的 Pinecone 特定功能和操作。\n"},{"id":93,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B-api/oci-%E7%94%9F%E6%88%90%E5%BC%8F-ai/","title":"OCI 生成式 AI","section":"聊天模型 API","content":" OCI 生成式 AI # "},{"id":94,"href":"/docs/%E5%8F%82%E8%80%83/%E7%9F%A2%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%8D%A1%E5%BE%B7%E5%85%B0%E7%89%B9/","title":"卡德兰特","section":"矢量数据库","content":" 卡德兰特 # 本节将引导您设置 Qdrant VectorStore 来存储文档嵌入并执行相似性搜索。\n[ Qdrant]( https://www.qdrant.tech/) 是一款开源的高性能向量搜索引擎/数据库。它采用 HNSW（分层可导航小世界）算法进行高效的 k-NN 搜索操作，并为基于元数据的查询提供高级过滤功能。\n先决条件 # Qdrant 实例：按照 Qdrant 文档中的安装说明设置 Qdrant 实例。 如果需要， EmbeddingModel 的 API 密钥可以生成 QdrantVectorStore 存储的嵌入。 自动配置 # Spring AI 为 Qdrant 矢量存储提供了 Spring Boot 自动配置功能。要启用此功能，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-qdrant\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-vector-store-qdrant\u0026#39; } 请查看向量存储的[ 配置参数](#qdrant-vectorstore-properties)列表，以了解默认值和配置选项。\n向量存储实现可以为您初始化必要的模式，但您必须通过在构建器中指定 initializeSchema 布尔值或在 application.properties 文件中设置 …​initialize-schema=true 来选择加入。\n此外，您还需要一个已配置的 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) bean。有关更多信息，请参阅 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) 部分。\n现在您可以将 QdrantVectorStore 自动连接为应用程序中的矢量存储。\n@Autowired VectorStore vectorStore; // ... List\u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); // Add the documents to Qdrant vectorStore.add(documents); // Retrieve documents similar to a query List\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch(SearchRequest.builder().query(\u0026#34;Spring\u0026#34;).topK(5).build()); 配置属性 # 要连接到 Qdrant 并使用 QdrantVectorStore ，您需要提供实例的访问详细信息。您可以通过 Spring Boot 的 application.yml 提供一个简单的配置：\nspring: ai: vectorstore: qdrant: host: \u0026lt;qdrant host\u0026gt; port: \u0026lt;qdrant grpc port\u0026gt; api-key: \u0026lt;qdrant api key\u0026gt; collection-name: \u0026lt;collection name\u0026gt; use-tls: false initialize-schema: true 以 spring.ai.vectorstore.qdrant.* 开头的属性用于配置 QdrantVectorStore ：\n手动配置 # 除了使用 Spring Boot 自动配置之外，您还可以手动配置 Qdrant 向量存储。为此，您需要将 spring-ai-qdrant-store 添加到您的项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-qdrant-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-qdrant-store\u0026#39; } 创建 Qdrant 客户端 bean：\n@Bean public QdrantClient qdrantClient() { QdrantGrpcClient.Builder grpcClientBuilder = QdrantGrpcClient.newBuilder( \u0026#34;\u0026lt;QDRANT_HOSTNAME\u0026gt;\u0026#34;, \u0026lt;QDRANT_GRPC_PORT\u0026gt;, \u0026lt;IS_TLS\u0026gt;); grpcClientBuilder.withApiKey(\u0026#34;\u0026lt;QDRANT_API_KEY\u0026gt;\u0026#34;); return new QdrantClient(grpcClientBuilder.build()); } 然后使用构建器模式创建 QdrantVectorStore bean：\n@Bean public VectorStore vectorStore(QdrantClient qdrantClient, EmbeddingModel embeddingModel) { return QdrantVectorStore.builder(qdrantClient, embeddingModel) .collectionName(\u0026#34;custom-collection\u0026#34;) // Optional: defaults to \u0026#34;vector_store\u0026#34; .initializeSchema(true) // Optional: defaults to false .batchingStrategy(new TokenCountBatchingStrategy()) // Optional: defaults to TokenCountBatchingStrategy .build(); } // This can be any EmbeddingModel implementation @Bean public EmbeddingModel embeddingModel() { return new OpenAiEmbeddingModel(new OpenAiApi(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;))); } 元数据过滤 # 您还可以利用 Qdrant 存储的通用、可移植元[ 数据过滤器](../vectordbs.html#metadata-filters) 。\n例如，您可以使用文本表达语言：\nvectorStore.similaritySearch( SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(\u0026#34;author in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; article_type == \u0026#39;blog\u0026#39;\u0026#34;).build()); 或者以编程方式使用 Filter.Expression DSL：\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(b.and( b.in(\u0026#34;author\u0026#34;, \u0026#34;john\u0026#34;, \u0026#34;jill\u0026#34;), b.eq(\u0026#34;article_type\u0026#34;, \u0026#34;blog\u0026#34;)).build()).build()); 访问 Native Client # Qdrant Vector Store 实现通过 getNativeClient() 方法提供对底层本机 Qdrant 客户端（ QdrantClient ）的访问：\nQdrantVectorStore vectorStore = context.getBean(QdrantVectorStore.class); Optional\u0026lt;QdrantClient\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { QdrantClient client = nativeClient.get(); // Use the native client for Qdrant-specific operations } 本机客户端使您可以访问可能无法通过 VectorStore 接口公开的 Qdrant 特定功能和操作。\n"},{"id":95,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B-api/openai-%E8%81%8A%E5%A4%A9/","title":"OpenAI 聊天","section":"聊天模型 API","content":" OpenAI 聊天 # Spring AI 支持 ChatGPT 背后的公司 OpenAI 的各种 AI 语言模型，该公司通过创建业界领先的文本生成模型和嵌入，在激发人们对 AI 驱动的文本生成的兴趣方面发挥了重要作用。\n先决条件 # 您需要使用 OpenAI 创建一个 API 来访问 ChatGPT 模型。\n在 [ OpenAI 注册页面]( https://platform.openai.com/signup)创建一个帐户，并在 [ API 密钥页面]( https://platform.openai.com/account/api-keys)上生成令牌。\nSpring AI 项目定义了一个名为 spring.ai.openai.api-key 的配置属性，您应该将其设置为从 openai.com 获取的 API Key 的值。\n您可以在 application.properties 文件中设置此配置属性：\nspring.ai.openai.api-key=\u0026lt;your-openai-api-key\u0026gt; 为了在处理 API 密钥等敏感信息时增强安全性，您可以使用 Spring 表达式语言 (SpEL) 引用自定义环境变量：\n# In application.yml spring: ai: openai: api-key: ${OPENAI_API_KEY} # In your environment or .env file export OPENAI_API_KEY=\u0026lt;your-openai-api-key\u0026gt; 您还可以在应用程序代码中以编程方式设置此配置：\n// Retrieve API key from a secure source or environment variable String apiKey = System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;); 添加存储库和 BOM # Spring AI 的构件已发布在 Maven Central 和 Spring Snapshot 仓库中。请参阅 [ “构件仓库”](../../getting-started.html#artifact-repositories) 部分，将这些仓库添加到您的构建系统中。\n为了帮助管理依赖项，Spring AI 提供了 BOM（物料清单），以确保在整个项目中使用一致版本的 Spring AI。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # Spring AI 为 OpenAI Chat Client 提供了 Spring Boot 自动配置功能。要启用此功能，请将以下依赖项添加到项目的 Maven pom.xml 或 Gradle build.gradle 构建文件中：\n聊天属性 # 重试属性 # 前缀 spring.ai.retry 用作属性前缀，可让您配置 OpenAI 聊天模型的重试机制。\n连接属性 # 前缀 spring.ai.openai 用作允许您连接到 OpenAI 的属性前缀。\n配置属性 # 前缀 spring.ai.openai.chat 是允许您为 OpenAI 配置聊天模型实现的属性前缀。\n运行时选项 # [ OpenAiChatOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/[OpenAiChatOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/OpenAiChatOptions.java)) 类提供模型配置，例如要使用的模型、温度、频率惩罚等。\n启动时，可以使用 OpenAiChatModel(api, options) 构造函数或 spring.ai.openai.chat.options.* 属性配置默认选项。\n在运行时，您可以通过向 Prompt 调用添加新的特定于请求的选项来覆盖默认选项。例如，要覆盖特定请求的默认模型和温度：\nChatResponse response = chatModel.call( new Prompt( \u0026#34;Generate the names of 5 famous pirates.\u0026#34;, OpenAiChatOptions.builder() .model(\u0026#34;gpt-4o\u0026#34;) .temperature(0.4) .build() )); 函数调用 # 您可以使用 OpenAiChatModel 注册自定义 Java 函数，并让 OpenAI 模型智能地选择输出包含参数的 JSON 对象，以调用一个或多个已注册的函数。这是一种将 LLM 功能与外部工具和 API 连接起来的强大技术。了解更多关于[ 工具调用的](../tools.html)信息。\n多式联运 # 多模态是指模型能够同时理解和处理来自各种来源的信息，包括文本、图像、音频和其他数据格式。OpenAI 支持文本、视觉和音频输入模态。\n想象 # 提供[ 视觉]( https://platform.openai.com/docs/guides/vision)多模态支持的 OpenAI 模型包括 gpt-4 、 gpt-4o 和 gpt-4o-mini 。有关更多信息，请参阅[ 视觉]( https://platform.openai.com/docs/guides/vision)指南。\nOpenAI [用户[ 消息](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/messages/Message.java) API]( https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages) 可以将一系列 base64 编码的图像或图像 URL 添加到[ 消息]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/messages/Message.java)中。Spring AI 的[ 消息]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/messages/Message.java)接口通过引入[ 媒体]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/model/Media.java)类型来促进多模态 AI 模型的发展。此类型包含[ 消息]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/messages/Message.java)中[ 媒体]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/model/Media.java)附件的数据和详细信息，并利用 Spring 的 org.springframework.util.MimeType 和 org.springframework.core.io.Resource 来存储原始[ 媒体]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/model/Media.java)数据。\n下面是摘录自 [ OpenAiChatModelIT.java]( https://github.com/spring-projects/spring-ai/blob/c9a3e66f90187ce7eae7eb78c462ec622685de6c/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/chat/[OpenAiChatModelIT.java](https://github.com/spring-projects/spring-ai/blob/c9a3e66f90187ce7eae7eb78c462ec622685de6c/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/chat/OpenAiChatModelIT.java#L293)#L293) 的代码示例，展示了如何使用 gpt-4o 模型将用户文本与图像融合。\nvar imageResource = new ClassPathResource(\u0026#34;/multimodal.test.png\u0026#34;); var userMessage = new UserMessage(\u0026#34;Explain what do you see on this picture?\u0026#34;, new Media(MimeTypeUtils.IMAGE_PNG, this.imageResource)); ChatResponse response = chatModel.call(new Prompt(this.userMessage, OpenAiChatOptions.builder().model(OpenAiApi.ChatModel.GPT_4_O.getValue()).build())); 或者使用 gpt-4o 模型等效的图像 URL：\nvar userMessage = new UserMessage(\u0026#34;Explain what do you see on this picture?\u0026#34;, new Media(MimeTypeUtils.IMAGE_PNG, URI.create(\u0026#34;https://docs.spring.io/spring-ai/reference/_images/multimodal.test.png\u0026#34;))); ChatResponse response = chatModel.call(new Prompt(this.userMessage, OpenAiChatOptions.builder().model(OpenAiApi.ChatModel.GPT_4_O.getValue()).build())); 该示例展示了一个将 multimodal.test.png 图像作为输入的模型：\n以及文本消息“解释一下你在图片上看到了什么？”，并生成如下的回应：\n声音的 # 提供输入[ 音频]( https://platform.openai.com/docs/guides/audio)多模态支持的 OpenAI 模型包括 gpt-4o-audio-preview 。有关更多信息，请参阅[ 音频]( https://platform.openai.com/docs/guides/audio)指南。\nOpenAI [ 用户消息 API]( https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages) 可以将一系列 base64 编码的音频文件添加到消息中。Spring AI 的 [ Message]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/messages/[Message](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/messages/Message.java).java) 接口通过引入 [ Media]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-client-chat/src/main/java/org/springframework/ai/chat/messages/[Media](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-client-chat/src/main/java/org/springframework/ai/chat/messages/Media.java).java) 类型来促进多模态 AI 模型的发展。该类型包含消息中媒体附件的数据和详细信息，并利用 Spring 的 org.springframework.util.MimeType 和 org.springframework.core.io.Resource 来存储原始媒体数据。目前，OpenAI 仅支持以下媒体类型： audio/mp3 和 audio/wav 。\n下面是从 [ OpenAiChatModelIT.java]( https://github.com/spring-projects/spring-ai/blob/c9a3e66f90187ce7eae7eb78c462ec622685de6c/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/chat/[OpenAiChatModelIT.java](https://github.com/spring-projects/spring-ai/blob/c9a3e66f90187ce7eae7eb78c462ec622685de6c/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/chat/OpenAiChatModelIT.java#L442)#L442) 中摘录的代码示例，说明了如何使用 gpt-4o-audio-preview 模型将用户文本与音频文件融合。\nvar audioResource = new ClassPathResource(\u0026#34;speech1.mp3\u0026#34;); var userMessage = new UserMessage(\u0026#34;What is this recording about?\u0026#34;, List.of(new Media(MimeTypeUtils.parseMimeType(\u0026#34;audio/mp3\u0026#34;), audioResource))); ChatResponse response = chatModel.call(new Prompt(List.of(userMessage), OpenAiChatOptions.builder().model(OpenAiApi.ChatModel.GPT_4_O_AUDIO_PREVIEW).build())); 输出音频 # 提供输入[ 音频]( https://platform.openai.com/docs/guides/audio)多模态支持的 OpenAI 模型包括 gpt-4o-audio-preview 。有关更多信息，请参阅[ 音频]( https://platform.openai.com/docs/guides/audio)指南。\nOpenAI [Assystant [ Message](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/messages/ Message.java) API]( https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages) 可以在消息中包含一系列 base64 编码的音频文件。Spring AI 的 [ Message]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/messages/[Message](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/messages/Message.java).java) 接口通过引入 [ Media]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/messages/[Media](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/messages/Media.java).java) 类型来促进多模态 AI 模型的发展。该类型包含消息中媒体附件的数据和详细信息，并利用 Spring 的 org.springframework.util.MimeType 和 org.springframework.core.io.Resource 来存储原始媒体数据。目前，OpenAI 仅支持以下音频类型： audio/mp3 和 audio/wav 。\n下面是一个代码示例，使用 gpt-4o-audio-preview 模型演示了用户文本以及音频字节数组的响应：\nvar userMessage = new UserMessage(\u0026#34;Tell me joke about Spring Framework\u0026#34;); ChatResponse response = chatModel.call(new Prompt(List.of(userMessage), OpenAiChatOptions.builder() .model(OpenAiApi.ChatModel.GPT_4_O_AUDIO_PREVIEW) .outputModalities(List.of(\u0026#34;text\u0026#34;, \u0026#34;audio\u0026#34;)) .outputAudio(new AudioParameters(Voice.ALLOY, AudioResponseFormat.WAV)) .build())); String text = response.getResult().getOutput().getContent(); // audio transcript byte[] waveAudio = response.getResult().getOutput().getMedia().get(0).getDataAsByteArray(); // audio data 您必须在 OpenAiChatOptions 中指定 audio 模式才能生成音频输出。AudioParameters AudioParameters 提供了音频输出所需的语音和音频格式。\n结构化输出 # OpenAI 提供自定义[ 结构化输出]( https://platform.openai.com/docs/guides/structured-outputs) API，确保您的模型生成严格符合您提供的 JSON Schema 响应。除了现有的与 Spring AI 模型无关的[ 结构化输出]( https://platform.openai.com/docs/guides/structured-outputs)转换器之外，这些 API 还提供了增强的控制力和精度。\n配置 # Spring AI 允许您使用 OpenAiChatOptions 构建器以编程方式或通过应用程序属性配置响应格式。\n使用聊天选项生成器 # 您可以使用 OpenAiChatOptions 构建器以编程方式设置响应格式，如下所示：\nString jsonSchema = \u0026#34;\u0026#34;\u0026#34; { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;steps\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, \u0026#34;items\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;explanation\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;output\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } }, \u0026#34;required\u0026#34;: [\u0026#34;explanation\u0026#34;, \u0026#34;output\u0026#34;], \u0026#34;additionalProperties\u0026#34;: false } }, \u0026#34;final_answer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } }, \u0026#34;required\u0026#34;: [\u0026#34;steps\u0026#34;, \u0026#34;final_answer\u0026#34;], \u0026#34;additionalProperties\u0026#34;: false } \u0026#34;\u0026#34;\u0026#34;; Prompt prompt = new Prompt(\u0026#34;how can I solve 8x + 7 = -23\u0026#34;, OpenAiChatOptions.builder() .model(ChatModel.GPT_4_O_MINI) .responseFormat(new ResponseFormat(ResponseFormat.Type.JSON_SCHEMA, this.jsonSchema)) .build()); ChatResponse response = this.openAiChatModel.call(this.prompt); 与 BeanOutputConverter 实用程序集成 # 您可以利用现有的 [ BeanOutputConverter](../structured-output-converter.html#_bean_output_converter) 实用程序从域对象自动生成 JSON 模式，然后将结构化响应转换为特定于域的实例：\n通过应用程序属性进行配置 # 或者，当使用 OpenAI 自动配置时，您可以通过以下应用程序属性配置所需的响应格式：\nspring.ai.openai.api-key=YOUR_API_KEY spring.ai.openai.chat.options.model=gpt-4o-mini spring.ai.openai.chat.options.response-format.type=JSON_SCHEMA spring.ai.openai.chat.options.response-format.name=MySchemaName spring.ai.openai.chat.options.response-format.schema={\u0026#34;type\u0026#34;:\u0026#34;object\u0026#34;,\u0026#34;properties\u0026#34;:{\u0026#34;steps\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;array\u0026#34;,\u0026#34;items\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;object\u0026#34;,\u0026#34;properties\u0026#34;:{\u0026#34;explanation\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;},\u0026#34;output\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;}},\u0026#34;required\u0026#34;:[\u0026#34;explanation\u0026#34;,\u0026#34;output\u0026#34;],\u0026#34;additionalProperties\u0026#34;:false}},\u0026#34;final_answer\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;}},\u0026#34;required\u0026#34;:[\u0026#34;steps\u0026#34;,\u0026#34;final_answer\u0026#34;],\u0026#34;additionalProperties\u0026#34;:false} spring.ai.openai.chat.options.response-format.strict=true 样品控制器 # [ 创建]( https://start.spring.io/)一个新的 Spring Boot 项目并将 spring-ai-starter-model-openai 添加到您的 pom（或 gradle）依赖项中。\n在 src/main/resources 目录下添加 application.properties 文件，用于启用和配置 OpenAi 聊天模型：\nspring.ai.openai.api-key=YOUR_API_KEY spring.ai.openai.chat.options.model=gpt-4o spring.ai.openai.chat.options.temperature=0.7 这将创建一个 OpenAiChatModel 实现，您可以将其注入到您的类中。这是一个使用聊天模型进行文本生成的简单 @RestController 类的示例。\n@RestController public class ChatController { private final OpenAiChatModel chatModel; @Autowired public ChatController(OpenAiChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map\u0026lt;String,String\u0026gt; generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, this.chatModel.call(message)); } @GetMapping(\u0026#34;/ai/generateStream\u0026#34;) public Flux\u0026lt;ChatResponse\u0026gt; generateStream(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { Prompt prompt = new Prompt(new UserMessage(message)); return this.chatModel.stream(prompt); } } 手动配置 # OpenAiChatModel 实现了 ChatModel 和 StreamingChatModel ，并使用 [ Low-level OpenAiApi Client](#low-level-api) 连接到 OpenAI 服务。\n将 spring-ai-openai 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-openai\u0026#39; } 接下来，创建一个 OpenAiChatModel 并将其用于文本生成：\nvar openAiApi = OpenAiApi.builder() .apiKey(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;)) .build(); var openAiChatOptions = OpenAiChatOptions.builder() .model(\u0026#34;gpt-3.5-turbo\u0026#34;) .temperature(0.4) .maxTokens(200) .build(); var chatModel = new OpenAiChatModel(this.openAiApi, this.openAiChatOptions); ChatResponse response = this.chatModel.call( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); // Or with streaming responses Flux\u0026lt;ChatResponse\u0026gt; response = this.chatModel.stream( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); OpenAiChatOptions 提供聊天请求的配置信息。OpenAiApi.Builder 和 OpenAiChatOptions.Builder OpenAiApi.Builder 是 API 客户端和聊天配置的流畅选项构建器。\n低级 OpenAiApi 客户端 # [ OpenAiApi]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/api/[OpenAiApi](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/api/OpenAiApi.java).java) 为 [ OpenAI Chat API]( https://platform.openai.com/docs/api-reference/chat) [ OpenAI Chat API]( https://platform.openai.com/docs/api-reference/chat) 提供了轻量级 Java 客户端。\n以下类图说明了 OpenAiApi 聊天接口和构建块：\n下面是一个简单的代码片段，展示了如何以编程方式使用 API：\nOpenAiApi openAiApi = OpenAiApi.builder() .apiKey(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;)) .build(); ChatCompletionMessage chatCompletionMessage = new ChatCompletionMessage(\u0026#34;Hello world\u0026#34;, Role.USER); // Sync request ResponseEntity\u0026lt;ChatCompletion\u0026gt; response = this.openAiApi.chatCompletionEntity( new ChatCompletionRequest(List.of(this.chatCompletionMessage), \u0026#34;gpt-3.5-turbo\u0026#34;, 0.8, false)); // Streaming request Flux\u0026lt;ChatCompletionChunk\u0026gt; streamResponse = this.openAiApi.chatCompletionStream( new ChatCompletionRequest(List.of(this.chatCompletionMessage), \u0026#34;gpt-3.5-turbo\u0026#34;, 0.8, true)); 请关注 [ OpenAiApi.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/api/[OpenAiApi.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/api/OpenAiApi.java)) 的 JavaDoc 以获取更多信息。\n低级 API 示例 # OpenAiApiIT.java 测试提供了一些如何使用轻量级库的一般示例。 OpenAiApiToolFunctionCallIT.java 测试展示了如何使用低级 API 调用工具函数。基于 OpenAI 函数调用教程。 API 密钥管理 # Spring AI 通过 ApiKey 接口及其实现提供灵活的 API 密钥管理。默认实现 SimpleApiKey 适用于大多数用例，但您也可以为更复杂的场景创建自定义实现。\n默认配置 # 默认情况下，Spring Boot 自动配置将使用 spring.ai.openai.api-key 属性创建 API 密钥 bean：\nspring.ai.openai.api-key=your-api-key-here 自定义 API 密钥配置 # 您可以使用构建器模式通过您自己的 ApiKey 实现创建 OpenAiApi 的自定义实例：\nApiKey customApiKey = new ApiKey() { @Override public String getValue() { // Custom logic to retrieve API key return \u0026#34;your-api-key-here\u0026#34;; } }; OpenAiApi openAiApi = OpenAiApi.builder() .apiKey(customApiKey) .build(); // Create a chat client with the custom OpenAiApi instance OpenAiChatClient chatClient = new OpenAiChatClient(openAiApi); 当您需要执行以下操作时，这很有用：\n从安全密钥库中检索 API 密钥 动态轮换 API 密钥 实现自定义 API 密钥选择逻辑 "},{"id":96,"href":"/docs/%E5%8F%82%E8%80%83/%E7%9F%A2%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/redis/","title":"Redis","section":"矢量数据库","content":" Redis # 本节将引导您设置 RedisVectorStore 来存储文档嵌入并执行相似性搜索。\n[ Redis]( https://redis.io) 是一个开源（BSD 许可）的内存数据结构存储，可用作数据库、缓存、消息代理和流引擎。[ Redis]( https://redis.io) 提供多种数据结构，例如字符串、哈希、列表、集合、支持范围查询的有序集合、位图、超日志、地理空间索引和流。\n[ Redis Search 和 Query]( https://redis.io/docs/interact/search-and-query/) 扩展了 Redis OSS 的核心功能，并允许您使用 Redis 作为矢量数据库：\n将向量和相关元数据存储在哈希或 JSON 文档中 检索向量 执行向量搜索 先决条件 # 自动配置 # Spring AI 为 Redis 矢量存储提供了 Spring Boot 自动配置功能。要启用此功能，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-redis\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-vector-store-redis\u0026#39; } 向量存储实现可以为您初始化必要的模式，但您必须通过在适当的构造函数中指定 initializeSchema 布尔值或在 application.properties 文件中设置 …​initialize-schema=true 来选择加入。\n请查看向量存储的[ 配置参数](#redisvector-properties)列表，以了解默认值和配置选项。\n此外，您还需要一个已配置的 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) bean。有关更多信息，请参阅 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) 部分。\n现在，您可以将 RedisVectorStore 自动连接为应用程序中的矢量存储。\n@Autowired VectorStore vectorStore; // ... List \u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); // Add the documents to Redis vectorStore.add(documents); // Retrieve documents similar to a query List\u0026lt;Document\u0026gt; results = this.vectorStore.similaritySearch(SearchRequest.builder().query(\u0026#34;Spring\u0026#34;).topK(5).build()); 配置属性 # 要连接到 Redis 并使用 RedisVectorStore ，您需要提供实例的访问详细信息。您可以通过 Spring Boot 的 application.yml 提供简单的配置，\nspring: data: redis: url: \u0026lt;redis instance url\u0026gt; ai: vectorstore: redis: initialize-schema: true index-name: custom-index prefix: custom-prefix 对于 redis 连接配置，也可以通过 Spring Boot 的 application.properties 提供一个简单的配置。\nspring.data.redis.host=localhost spring.data.redis.port=6379 spring.data.redis.username=default spring.data.redis.password= 以 spring.ai.vectorstore.redis.* 开头的属性用于配置 RedisVectorStore ：\n元数据过滤 # 您还可以利用 Redis 的通用、可移植元[ 数据过滤器](../vectordbs.html#metadata-filters) 。\n例如，您可以使用文本表达语言：\nvectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(\u0026#34;country in [\u0026#39;UK\u0026#39;, \u0026#39;NL\u0026#39;] \u0026amp;\u0026amp; year \u0026gt;= 2020\u0026#34;).build()); 或者以编程方式使用 Filter.Expression DSL：\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(b.and( b.in(\u0026#34;country\u0026#34;, \u0026#34;UK\u0026#34;, \u0026#34;NL\u0026#34;), b.gte(\u0026#34;year\u0026#34;, 2020)).build()).build()); 例如，这个便携式过滤器表达式：\ncountry in [\u0026#39;UK\u0026#39;, \u0026#39;NL\u0026#39;] \u0026amp;\u0026amp; year \u0026gt;= 2020 转换为专有的 Redis 过滤器格式：\n@country:{UK | NL} @year:[2020 inf] 手动配置 # 除了使用 Spring Boot 自动配置之外，您还可以手动配置 Redis 向量存储。为此，您需要将 spring-ai-redis-store 添加到您的项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-redis-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-redis-store\u0026#39; } 创建一个 JedisPooled bean：\n@Bean public JedisPooled jedisPooled() { return new JedisPooled(\u0026#34;\u0026lt;host\u0026gt;\u0026#34;, 6379); } 然后使用构建器模式创建 RedisVectorStore bean：\n@Bean public VectorStore vectorStore(JedisPooled jedisPooled, EmbeddingModel embeddingModel) { return RedisVectorStore.builder(jedisPooled, embeddingModel) .indexName(\u0026#34;custom-index\u0026#34;) // Optional: defaults to \u0026#34;spring-ai-index\u0026#34; .prefix(\u0026#34;custom-prefix\u0026#34;) // Optional: defaults to \u0026#34;embedding:\u0026#34; .metadataFields( // Optional: define metadata fields for filtering MetadataField.tag(\u0026#34;country\u0026#34;), MetadataField.numeric(\u0026#34;year\u0026#34;)) .initializeSchema(true) // Optional: defaults to false .batchingStrategy(new TokenCountBatchingStrategy()) // Optional: defaults to TokenCountBatchingStrategy .build(); } // This can be any EmbeddingModel implementation @Bean public EmbeddingModel embeddingModel() { return new OpenAiEmbeddingModel(new OpenAiApi(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;))); } 访问 Native Client # Redis Vector Store 实现通过 getNativeClient() 方法提供对底层原生 Redis 客户端（ JedisPooled ）的访问：\nRedisVectorStore vectorStore = context.getBean(RedisVectorStore.class); Optional\u0026lt;JedisPooled\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { JedisPooled jedis = nativeClient.get(); // Use the native client for Redis-specific operations } 本机客户端使您可以访问可能无法通过 VectorStore 接口公开的 Redis 特定功能和操作。\n"},{"id":97,"href":"/docs/%E5%8F%82%E8%80%83/%E7%9F%A2%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/sap-hana-%E4%BA%91/","title":"SAP HANA 云","section":"矢量数据库","content":" SAP HANA 云 # 先决条件 # 您需要一个 SAP HANA Cloud 矢量引擎帐户 - 请参阅 SAP HANA Cloud 矢量引擎 - 提供试用帐户指南来创建试用帐户。 如果需要， EmbeddingModel 的 API 密钥可以生成向量存储所存储的嵌入。 自动配置 # Spring AI 没有为 SAP Hana 向量存储提供专用模块。用户需要使用 Spring AI 中 SAP Hana 向量存储的标准向量存储模块 spring-ai-hanadb-store 在应用程序中自行配置。\n请查看矢量存储的 [ HanaCloudVectorStore 属性](#hanacloudvectorstore-properties)列表，以了解默认值和配置选项。\n此外，您还需要一个已配置的 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) bean。有关更多信息，请参阅 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) 部分。\nHanaCloudVectorStore 属性 # 您可以在 Spring Boot 配置中使用以下属性来自定义 SAP Hana 矢量存储。它使用 spring.datasource. 属性来配置 Hana 数据源和 spring.ai.vectorstore.hanadb. 属性来配置 Hana 矢量存储。\n构建示例 RAG 应用程序 # 展示如何设置使用 SAP Hana Cloud 作为矢量数据库的项目并利用 OpenAI 实现 RAG 模式\n在 SAP Hana DB 中创建表 CRICKET_WORLD_CUP ：\n在你的 pom.xml 中添加以下依赖项\n您可以将属性 spring-ai-version 设置为 \u0026lt;spring-ai-version\u0026gt;1.0.0-SNAPSHOT\u0026lt;/spring-ai-version\u0026gt; ：\n\u0026lt;dependencyManagement\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-bom\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${spring-ai-version}\u0026lt;/version\u0026gt; \u0026lt;type\u0026gt;pom\u0026lt;/type\u0026gt; \u0026lt;scope\u0026gt;import\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/dependencyManagement\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-pdf-document-reader\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-hana\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lombok\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.18.30\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; 在 application.properties 文件中添加以下属性： 创建一个名为 CricketWorldCup 的 Entity 类，该类从 HanaVectorEntity 扩展而来： # package com.interviewpedia.spring.ai.hana; import jakarta.persistence.Column; import jakarta.persistence.Entity; import jakarta.persistence.Table; import lombok.Data; import lombok.NoArgsConstructor; import lombok.extern.jackson.Jacksonized; import org.springframework.ai.vectorstore.hanadb.HanaVectorEntity; @Entity @Table(name = \u0026#34;CRICKET_WORLD_CUP\u0026#34;) @Data @Jacksonized @NoArgsConstructor public class CricketWorldCup extends HanaVectorEntity { @Column(name = \u0026#34;content\u0026#34;) private String content; } 创建一个名为 CricketWorldCupRepository 的 Repository ，该存储库实现 HanaVectorRepository 接口： package com.interviewpedia.spring.ai.hana; import jakarta.persistence.EntityManager; import jakarta.persistence.PersistenceContext; import jakarta.transaction.Transactional; import org.springframework.ai.vectorstore.hanadb.HanaVectorRepository; import org.springframework.stereotype.Repository; import java.util.List; @Repository public class CricketWorldCupRepository implements HanaVectorRepository\u0026lt;CricketWorldCup\u0026gt; { @PersistenceContext private EntityManager entityManager; @Override @Transactional public void save(String tableName, String id, String embedding, String content) { String sql = String.format(\u0026#34;\u0026#34;\u0026#34; INSERT INTO %s (_ID, EMBEDDING, CONTENT) VALUES(:_id, TO_REAL_VECTOR(:embedding), :content) \u0026#34;\u0026#34;\u0026#34;, tableName); this.entityManager.createNativeQuery(sql) .setParameter(\u0026#34;_id\u0026#34;, id) .setParameter(\u0026#34;embedding\u0026#34;, embedding) .setParameter(\u0026#34;content\u0026#34;, content) .executeUpdate(); } @Override @Transactional public int deleteEmbeddingsById(String tableName, List\u0026lt;String\u0026gt; idList) { String sql = String.format(\u0026#34;\u0026#34;\u0026#34; DELETE FROM %s WHERE _ID IN (:ids) \u0026#34;\u0026#34;\u0026#34;, tableName); return this.entityManager.createNativeQuery(sql) .setParameter(\u0026#34;ids\u0026#34;, idList) .executeUpdate(); } @Override @Transactional public int deleteAllEmbeddings(String tableName) { String sql = String.format(\u0026#34;\u0026#34;\u0026#34; DELETE FROM %s \u0026#34;\u0026#34;\u0026#34;, tableName); return this.entityManager.createNativeQuery(sql).executeUpdate(); } @Override public List\u0026lt;CricketWorldCup\u0026gt; cosineSimilaritySearch(String tableName, int topK, String queryEmbedding) { String sql = String.format(\u0026#34;\u0026#34;\u0026#34; SELECT TOP :topK * FROM %s ORDER BY COSINE_SIMILARITY(EMBEDDING, TO_REAL_VECTOR(:queryEmbedding)) DESC \u0026#34;\u0026#34;\u0026#34;, tableName); return this.entityManager.createNativeQuery(sql, CricketWorldCup.class) .setParameter(\u0026#34;topK\u0026#34;, topK) .setParameter(\u0026#34;queryEmbedding\u0026#34;, queryEmbedding) .getResultList(); } } 现在，创建一个 REST 控制器类 CricketWorldCupHanaController ，并将 ChatModel 和 VectorStore 自动连接为依赖项 在此控制器类中，创建以下 REST 端点： /ai/hana-vector-store/cricket-world-cup/purge-embeddings - 清除向量存储区中的所有嵌入 /ai/hana-vector-store/cricket-world-cup/upload - 上传 Cricket_World_Cup.pdf，以便将其数据作为嵌入存储在 SAP Hana Cloud Vector DB 中 /ai/hana-vector-store/cricket-world-cup - 使用 SAP Hana DB 中的 Cosine_ Similarity 实现 RAG /ai/hana-vector-store/cricket-world-cup/purge-embeddings - 清除向量存储区中的所有嵌入 /ai/hana-vector-store/cricket-world-cup/upload - 上传 Cricket_World_Cup.pdf，以便将其数据作为嵌入存储在 SAP Hana Cloud Vector DB 中 /ai/hana-vector-store/cricket-world-cup - 使用 SAP Hana DB 中的 Cosine_ Similarity 实现 RAG package com.interviewpedia.spring.ai.hana; import lombok.extern.slf4j.Slf4j; import org.springframework.ai.chat.model.ChatModel; import org.springframework.ai.chat.messages.UserMessage; import org.springframework.ai.chat.prompt.Prompt; import org.springframework.ai.chat.prompt.SystemPromptTemplate; import org.springframework.ai.document.Document; import org.springframework.ai.reader.pdf.PagePdfDocumentReader; import org.springframework.ai.transformer.splitter.TokenTextSplitter; import org.springframework.ai.vectorstore.hanadb.HanaCloudVectorStore; import org.springframework.ai.vectorstore.VectorStore; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.core.io.Resource; import org.springframework.http.ResponseEntity; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.PostMapping; import org.springframework.web.bind.annotation.RequestParam; import org.springframework.web.bind.annotation.RestController; import org.springframework.web.multipart.MultipartFile; import java.io.IOException; import java.util.List; import java.util.Map; import java.util.function.Function; import java.util.function.Supplier; import java.util.stream.Collectors; @RestController @Slf4j public class CricketWorldCupHanaController { private final VectorStore hanaCloudVectorStore; private final ChatModel chatModel; @Autowired public CricketWorldCupHanaController(ChatModel chatModel, VectorStore hanaCloudVectorStore) { this.chatModel = chatModel; this.hanaCloudVectorStore = hanaCloudVectorStore; } @PostMapping(\u0026#34;/ai/hana-vector-store/cricket-world-cup/purge-embeddings\u0026#34;) public ResponseEntity\u0026lt;String\u0026gt; purgeEmbeddings() { int deleteCount = ((HanaCloudVectorStore) this.hanaCloudVectorStore).purgeEmbeddings(); log.info(\u0026#34;{} embeddings purged from CRICKET_WORLD_CUP table in Hana DB\u0026#34;, deleteCount); return ResponseEntity.ok().body(String.format(\u0026#34;%d embeddings purged from CRICKET_WORLD_CUP table in Hana DB\u0026#34;, deleteCount)); } @PostMapping(\u0026#34;/ai/hana-vector-store/cricket-world-cup/upload\u0026#34;) public ResponseEntity\u0026lt;String\u0026gt; handleFileUpload(@RequestParam(\u0026#34;pdf\u0026#34;) MultipartFile file) throws IOException { Resource pdf = file.getResource(); Supplier\u0026lt;List\u0026lt;Document\u0026gt;\u0026gt; reader = new PagePdfDocumentReader(pdf); Function\u0026lt;List\u0026lt;Document\u0026gt;, List\u0026lt;Document\u0026gt;\u0026gt; splitter = new TokenTextSplitter(); List\u0026lt;Document\u0026gt; documents = splitter.apply(reader.get()); log.info(\u0026#34;{} documents created from pdf file: {}\u0026#34;, documents.size(), pdf.getFilename()); this.hanaCloudVectorStore.accept(documents); return ResponseEntity.ok().body(String.format(\u0026#34;%d documents created from pdf file: %s\u0026#34;, documents.size(), pdf.getFilename())); } @GetMapping(\u0026#34;/ai/hana-vector-store/cricket-world-cup\u0026#34;) public Map\u0026lt;String, String\u0026gt; hanaVectorStoreSearch(@RequestParam(value = \u0026#34;message\u0026#34;) String message) { var documents = this.hanaCloudVectorStore.similaritySearch(message); var inlined = documents.stream().map(Document::getText).collect(Collectors.joining(System.lineSeparator())); var similarDocsMessage = new SystemPromptTemplate(\u0026#34;Based on the following: {documents}\u0026#34;) .createMessage(Map.of(\u0026#34;documents\u0026#34;, inlined)); var userMessage = new UserMessage(message); Prompt prompt = new Prompt(List.of(similarDocsMessage, userMessage)); String generation = this.chatModel.call(prompt).getResult().getOutput().getContent(); log.info(\u0026#34;Generation: {}\u0026#34;, generation); return Map.of(\u0026#34;generation\u0026#34;, generation); } } 由于 HanaDB 向量存储支持不提供自动配置模块，因此您还需要在应用程序中提供向量存储 bean，如下所示作为示例。\n@Bean public VectorStore hanaCloudVectorStore(CricketWorldCupRepository cricketWorldCupRepository, EmbeddingModel embeddingModel) { return HanaCloudVectorStore.builder(cricketWorldCupRepository, embeddingModel) .tableName(\u0026#34;CRICKET_WORLD_CUP\u0026#34;) .topK(1) .build(); } 使用来自维基百科的 contextual PDF 文件 前往[ 维基百科]( https://en.wikipedia.org/wiki/Cricket_World_Cup)并[ 下载]( https://en.wikipedia.org/w/index.php?title=Special:DownloadAsPdf\u0026page=Cricket_World_Cup\u0026action=show-download-screen) Cricket World Cup 页面的 PDF 文件。\n使用我们在上一步中创建的文件上传 REST 端点上传此 ​​PDF 文件。\n"},{"id":98,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B-api/%E5%8D%83%E5%B8%86%E8%81%8A%E5%A4%A9/","title":"千帆聊天","section":"聊天模型 API","content":" 千帆聊天 # 此功能已移至 Spring AI 社区存储库。\n请访问 [ github.com/spring-ai-community/qianfan](https:// github.com/spring-ai-community/qianfan) 获取最新版本。\n"},{"id":99,"href":"/docs/%E5%8F%82%E8%80%83/%E7%9F%A2%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/typesense/","title":"Typesense","section":"矢量数据库","content":" Typesense # 本节将引导您设置 TypesenseVectorStore 来存储文档嵌入并执行相似性搜索。\n[ Typesense]( https://typesense.org) 是一款开源的拼写错误容忍搜索引擎，它针对 50 毫秒内的即时搜索进行了优化，同时提供了直观的开发者体验。它提供向量搜索功能，允许您在常规搜索数据的同时存储和查询高维向量。\n先决条件 # 正在运行的 Typesense 实例。以下选项可用： Typesense Cloud （推荐） Docker 镜像 typesense/typesense:latest Typesense Cloud （推荐） Docker 镜像 typesense/typesense:latest 如果需要， EmbeddingModel 的 API 密钥可以生成 TypesenseVectorStore 存储的嵌入。 自动配置 # Spring AI 为 Typesense 矢量存储提供了 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-typesense\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-vector-store-typesense\u0026#39; } 请查看向量存储的[ 配置参数](#_configuration_properties)列表，以了解默认值和配置选项。\n向量存储实现可以为您初始化必要的模式，但您必须通过在 application.properties 文件中设置 …​initialize-schema=true 来选择加入。\n此外，您还需要一个已配置的 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) bean。有关更多信息，请参阅 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) 部分。\n现在，您可以将 TypesenseVectorStore 自动连接为应用程序中的向量存储：\n@Autowired VectorStore vectorStore; // ... List\u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); // Add the documents to Typesense vectorStore.add(documents); // Retrieve documents similar to a query List\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch(SearchRequest.builder().query(\u0026#34;Spring\u0026#34;).topK(5).build()); 配置属性 # 要连接到 Typesense 并使用 TypesenseVectorStore ，您需要提供实例的访问详细信息。可以通过 Spring Boot 的 application.yml 提供简单的配置：\nspring: ai: vectorstore: typesense: initialize-schema: true collection-name: vector_store embedding-dimension: 1536 client: protocol: http host: localhost port: 8108 api-key: xyz 以 spring.ai.vectorstore.typesense.* 开头的属性用于配置 TypesenseVectorStore ：\n手动配置 # 除了使用 Spring Boot 自动配置之外，您还可以手动配置 Typesense 向量存储。为此，您需要将 spring-ai-typesense-store 添加到您的项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-typesense-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-typesense-store\u0026#39; } 创建 Typesense Client bean：\n@Bean public Client typesenseClient() { List\u0026lt;Node\u0026gt; nodes = new ArrayList\u0026lt;\u0026gt;(); nodes.add(new Node(\u0026#34;http\u0026#34;, \u0026#34;localhost\u0026#34;, \u0026#34;8108\u0026#34;)); Configuration configuration = new Configuration(nodes, Duration.ofSeconds(5), \u0026#34;xyz\u0026#34;); return new Client(configuration); } 然后使用构建器模式创建 TypesenseVectorStore bean：\n@Bean public VectorStore vectorStore(Client client, EmbeddingModel embeddingModel) { return TypesenseVectorStore.builder(client, embeddingModel) .collectionName(\u0026#34;custom_vectors\u0026#34;) // Optional: defaults to \u0026#34;vector_store\u0026#34; .embeddingDimension(1536) // Optional: defaults to 1536 .initializeSchema(true) // Optional: defaults to false .batchingStrategy(new TokenCountBatchingStrategy()) // Optional: defaults to TokenCountBatchingStrategy .build(); } // This can be any EmbeddingModel implementation @Bean public EmbeddingModel embeddingModel() { return new OpenAiEmbeddingModel(new OpenAiApi(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;))); } 元数据过滤 # 您也可以利用 Typesense 存储的通用便携式[ 元数据过滤器](../vectordbs.html#metadata-filters) 。\n例如，您可以使用文本表达语言：\nvectorStore.similaritySearch( SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(\u0026#34;country in [\u0026#39;UK\u0026#39;, \u0026#39;NL\u0026#39;] \u0026amp;\u0026amp; year \u0026gt;= 2020\u0026#34;).build()); 或者以编程方式使用 Filter.Expression DSL：\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(b.and( b.in(\u0026#34;country\u0026#34;, \u0026#34;UK\u0026#34;, \u0026#34;NL\u0026#34;), b.gte(\u0026#34;year\u0026#34;, 2020)).build()).build()); 例如这个便携式过滤器表达式：\ncountry in [\u0026#39;UK\u0026#39;, \u0026#39;NL\u0026#39;] \u0026amp;\u0026amp; year \u0026gt;= 2020 转换为专有的 Typesense 过滤器格式：\ncountry: [\u0026#39;UK\u0026#39;, \u0026#39;NL\u0026#39;] \u0026amp;\u0026amp; year: \u0026gt;=2020 访问 Native Client # Typesense Vector Store 实现通过 getNativeClient() 方法提供对底层本机 Typesense 客户端（ Client ）的访问：\nTypesenseVectorStore vectorStore = context.getBean(TypesenseVectorStore.class); Optional\u0026lt;Client\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { Client client = nativeClient.get(); // Use the native client for Typesense-specific operations } 本机客户端使您可以访问可能无法通过 VectorStore 界面公开的 Typesense 特定功能和操作。\n"},{"id":100,"href":"/docs/%E5%8F%82%E8%80%83/%E6%A8%A1%E5%9E%8B/%E8%81%8A%E5%A4%A9%E6%A8%A1%E5%9E%8B-api/%E6%99%BA%E6%B5%A6-ai-%E8%81%8A%E5%A4%A9/","title":"智浦 AI 聊天","section":"聊天模型 API","content":" 智浦 AI 聊天 # Spring AI 支持 ZhiPu AI 的各种 AI 语言模型。您可以与 ZhiPu AI 语言模型进行交互，并基于 ZhiPuAI 模型创建多语言对话助手。\n先决条件 # 您需要使用 ZhiPuAI 创建一个 API 来访问 ZhiPu AI 语言模型。\n在[ 智普 AI 注册页面]( https://open.bigmodel.cn/login)创建账号，并在 [ API Keys 页面]( https://open.bigmodel.cn/usercenter/apikeys)生成 token。\nSpring AI 项目定义了一个名为 spring.ai.zhipuai.api-key 的配置属性，您应该将其设置为从 API Keys 页面获取的 API Key 的值。\n您可以在 application.properties 文件中设置此配置属性：\nspring.ai.zhipuai.api-key=\u0026lt;your-zhipuai-api-key\u0026gt; 为了在处理 API 密钥等敏感信息时增强安全性，您可以使用 Spring 表达式语言 (SpEL) 引用自定义环境变量：\n# In application.yml spring: ai: zhipuai: api-key: ${ZHIPUAI_API_KEY} # In your environment or .env file export ZHIPUAI_API_KEY=\u0026lt;your-zhipuai-api-key\u0026gt; 您还可以在应用程序代码中以编程方式设置此配置：\n// Retrieve API key from a secure source or environment variable String apiKey = System.getenv(\u0026#34;ZHIPUAI_API_KEY\u0026#34;); 添加存储库和 BOM # Spring AI 的构件已发布在 Maven Central 和 Spring Snapshot 仓库中。请参阅 [ “构件仓库”](../../getting-started.html#artifact-repositories) 部分，将这些仓库添加到您的构建系统中。\n为了帮助管理依赖项，Spring AI 提供了 BOM（物料清单），以确保在整个项目中使用一致版本的 Spring AI。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # Spring AI 为 ZhiPuAI 聊天客户端提供了 Spring Boot 自动配置功能。要启用此功能，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-zhipuai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-zhipuai\u0026#39; } 聊天属性 # 重试属性 # 前缀 spring.ai.retry 用作属性前缀，可让您配置 ZhiPu AI 聊天模型的重试机制。\n连接属性 # 前缀 spring.ai.zhiPu 用作允许您连接到 ZhiPuAI 的属性前缀。\n配置属性 # 前缀 spring.ai.zhipuai.chat 是允许您配置 ZhiPuAI 的聊天模型实现的属性前缀。\n运行时选项 # [ ZhiPuAiChatOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-zhipuai/src/main/java/org/springframework/ai/zhipuai/[ZhiPuAiChatOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-zhipuai/src/main/java/org/springframework/ai/zhipuai/ZhiPuAiChatOptions.java)) 提供模型配置，例如要使用的模型、温度、频率惩罚等。\n启动时，可以使用 ZhiPuAiChatModel(api, options) 构造函数或 spring.ai.zhipuai.chat.options.* 属性配置默认选项。\n在运行时，您可以通过向 Prompt 调用添加新的、特定于请求的选项来覆盖默认选项。例如，要覆盖特定请求的默认模型和温度：\nChatResponse response = chatModel.call( new Prompt( \u0026#34;Generate the names of 5 famous pirates.\u0026#34;, ZhiPuAiChatOptions.builder() .model(ZhiPuAiApi.ChatModel.GLM_3_Turbo.getValue()) .temperature(0.5) .build() )); 样品控制器 # [ 创建]( https://start.spring.io/)一个新的 Spring Boot 项目并将 spring-ai-starter-model-zhipuai 添加到您的 pom（或 gradle）依赖项中。\n在 src/main/resources 目录下添加一个 application.properties 文件，以启用和配置 ZhiPuAi 聊天模型：\nspring.ai.zhipuai.api-key=YOUR_API_KEY spring.ai.zhipuai.chat.options.model=glm-4-air spring.ai.zhipuai.chat.options.temperature=0.7 这将创建一个 ZhiPuAiChatModel 实现，您可以将其注入到您的类中。这是一个使用聊天模型生成文本的简单 @Controller 类的示例。\n@RestController public class ChatController { private final ZhiPuAiChatModel chatModel; @Autowired public ChatController(ZhiPuAiChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, this.chatModel.call(message)); } @GetMapping(\u0026#34;/ai/generateStream\u0026#34;) public Flux\u0026lt;ChatResponse\u0026gt; generateStream(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { var prompt = new Prompt(new UserMessage(message)); return this.chatModel.stream(prompt); } } 手动配置 # ZhiPuAiChatModel 实现了 ChatModel 和 StreamingChatModel ，并使用 [ Low-level ZhiPuAiApi Client](#low-level-api) 连接到 ZhiPuAI 服务。\n将 spring-ai-zhipuai 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-zhipuai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-zhipuai\u0026#39; } 接下来，创建一个 ZhiPuAiChatModel 并使用它来生成文本：\nvar zhiPuAiApi = new ZhiPuAiApi(System.getenv(\u0026#34;ZHIPU_AI_API_KEY\u0026#34;)); var chatModel = new ZhiPuAiChatModel(this.zhiPuAiApi, ZhiPuAiChatOptions.builder() .model(ZhiPuAiApi.ChatModel.GLM_3_Turbo.getValue()) .temperature(0.4) .maxTokens(200) .build()); ChatResponse response = this.chatModel.call( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); // Or with streaming responses Flux\u0026lt;ChatResponse\u0026gt; streamResponse = this.chatModel.stream( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); ZhiPuAiChatOptions 提供聊天请求的配置信息。ZhiPuAiChatOptions.Builder 是一个流畅的选项构建 ZhiPuAiChatOptions.Builder 。\n底层 ZhiPuAiApi 客户端 # [ ZhiPuAiApi]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-zhipuai/src/main/java/org/springframework/ai/zhipuai/api/[ZhiPuAiApi](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-zhipuai/src/main/java/org/springframework/ai/zhipuai/api/ZhiPuAiApi.java).java) 为 [ ZhiPu AI API]( https://open.bigmodel.cn/dev/api) 提供了轻量级的 Java 客户端。\n以下是以编程方式使用 API 的简单代码片段：\nZhiPuAiApi zhiPuAiApi = new ZhiPuAiApi(System.getenv(\u0026#34;ZHIPU_AI_API_KEY\u0026#34;)); ChatCompletionMessage chatCompletionMessage = new ChatCompletionMessage(\u0026#34;Hello world\u0026#34;, Role.USER); // Sync request ResponseEntity\u0026lt;ChatCompletion\u0026gt; response = this.zhiPuAiApi.chatCompletionEntity( new ChatCompletionRequest(List.of(this.chatCompletionMessage), ZhiPuAiApi.ChatModel.GLM_3_Turbo.getValue(), 0.7, false)); // Streaming request Flux\u0026lt;ChatCompletionChunk\u0026gt; streamResponse = this.zhiPuAiApi.chatCompletionStream( new ChatCompletionRequest(List.of(this.chatCompletionMessage), ZhiPuAiApi.ChatModel.GLM_3_Turbo.getValue(), 0.7, true)); 请关注 [ ZhiPuAiApi.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-zhipuai/src/main/java/org/springframework/ai/zhipuai/api/[ZhiPuAiApi.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-zhipuai/src/main/java/org/springframework/ai/zhipuai/api/ZhiPuAiApi.java)) 的 JavaDoc 以获取更多信息。\nZhiPuAiApi 示例 # ZhiPuAiApiIT.java 测试提供了一些如何使用轻量级库的一般示例。 "},{"id":101,"href":"/docs/%E5%8F%82%E8%80%83/%E7%9F%A2%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%A8%81%E7%BB%B4%E7%89%B9/","title":"威维特","section":"矢量数据库","content":" 威维特 # 本节将引导您设置 Weaviate VectorStore 来存储文档嵌入并执行相似性搜索。\n[ Weaviate]( https://weaviate.io/) 是一个开源向量数据库，它允许您存储来自您常用的 ML 模型的数据对象和向量嵌入，并无缝扩展到数十亿个数据对象。它提供了用于存储文档嵌入、内容和元数据的工具，并支持搜索这些嵌入，包括元数据过滤。\n先决条件 # 正在运行的 Weaviate 实例。以下选项可用： Weaviate 云服务 （需要创建帐户和 API 密钥） Docker 容器 Weaviate 云服务 （需要创建帐户和 API 密钥） Docker 容器 如果需要，可以使用 EmbeddingModel 的 API 密钥来生成 WeaviateVectorStore 存储的嵌入。 依赖项 # 将 Weaviate Vector Store 依赖项添加到您的项目：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-weaviate-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-weaviate-store\u0026#39; } 配置 # 要连接到 Weaviate 并使用 WeaviateVectorStore ，您需要提供实例的访问详细信息。可以通过 Spring Boot 的 application.properties 提供配置：\nspring.ai.vectorstore.weaviate.host=\u0026lt;host_of_your_weaviate_instance\u0026gt; spring.ai.vectorstore.weaviate.scheme=\u0026lt;http_or_https\u0026gt; spring.ai.vectorstore.weaviate.api-key=\u0026lt;your_api_key\u0026gt; # API key if needed, e.g. OpenAI spring.ai.openai.api-key=\u0026lt;api-key\u0026gt; 如果您希望使用环境变量来存储 API 密钥等敏感信息，则您有多种选择：\n选项 1：使用 Spring 表达语言（SpEL） # 您可以使用自定义环境变量名称并在应用程序配置中引用它们：\n# In application.yml spring: ai: vectorstore: weaviate: host: ${WEAVIATE_HOST} scheme: ${WEAVIATE_SCHEME} api-key: ${WEAVIATE_API_KEY} openai: api-key: ${OPENAI_API_KEY} # In your environment or .env file export WEAVIATE_HOST=\u0026lt;host_of_your_weaviate_instance\u0026gt; export WEAVIATE_SCHEME=\u0026lt;http_or_https\u0026gt; export WEAVIATE_API_KEY=\u0026lt;your_api_key\u0026gt; export OPENAI_API_KEY=\u0026lt;api-key\u0026gt; 选项 2：以编程方式访问环境变量 # 或者，您可以在 Java 代码中访问环境变量：\nString weaviateApiKey = System.getenv(\u0026#34;WEAVIATE_API_KEY\u0026#34;); String openAiApiKey = System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;); 自动配置 # Spring AI 为 Weaviate 矢量存储提供了 Spring Boot 自动配置功能。要启用此功能，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-weaviate\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者到你的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-vector-store-weaviate\u0026#39; } 请查看向量存储的[ 配置参数](#_weaviatevectorstore_properties)列表，以了解默认值和配置选项。\n此外，您还需要一个已配置的 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) bean。有关更多信息，请参阅 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) 部分。\n以下是所需 bean 的示例：\n@Bean public EmbeddingModel embeddingModel() { // Retrieve API key from a secure source or environment variable String apiKey = System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;); // Can be any other EmbeddingModel implementation return new OpenAiEmbeddingModel(OpenAiApi.builder().apiKey(apiKey).build()); } 现在您可以将 WeaviateVectorStore 自动连接为应用程序中的矢量存储。\n手动配置 # 除了使用 Spring Boot 自动配置之外，您还可以使用构建器模式手动配置 WeaviateVectorStore ：\n@Bean public WeaviateClient weaviateClient() { return new WeaviateClient(new Config(\u0026#34;http\u0026#34;, \u0026#34;localhost:8080\u0026#34;)); } @Bean public VectorStore vectorStore(WeaviateClient weaviateClient, EmbeddingModel embeddingModel) { return WeaviateVectorStore.builder(weaviateClient, embeddingModel) .objectClass(\u0026#34;CustomClass\u0026#34;) // Optional: defaults to \u0026#34;SpringAiWeaviate\u0026#34; .consistencyLevel(ConsistentLevel.QUORUM) // Optional: defaults to ConsistentLevel.ONE .filterMetadataFields(List.of( // Optional: fields that can be used in filters MetadataField.text(\u0026#34;country\u0026#34;), MetadataField.number(\u0026#34;year\u0026#34;))) .build(); } 元数据过滤 # 您还可以利用 Wea​​viate 商店的通用、可移植元[ 数据过滤器](../vectordbs.html#metadata-filters) 。\n例如，您可以使用文本表达语言：\nvectorStore.similaritySearch( SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(\u0026#34;country in [\u0026#39;UK\u0026#39;, \u0026#39;NL\u0026#39;] \u0026amp;\u0026amp; year \u0026gt;= 2020\u0026#34;).build()); 或者以编程方式使用 Filter.Expression DSL：\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(b.and( b.in(\u0026#34;country\u0026#34;, \u0026#34;UK\u0026#34;, \u0026#34;NL\u0026#34;), b.gte(\u0026#34;year\u0026#34;, 2020)).build()).build()); 例如，这个便携式过滤器表达式：\ncountry in [\u0026#39;UK\u0026#39;, \u0026#39;NL\u0026#39;] \u0026amp;\u0026amp; year \u0026gt;= 2020 转换为专有的 Weaviate GraphQL 过滤器格式：\noperator: And operands: [{ operator: Or operands: [{ path: [\u0026#34;meta_country\u0026#34;] operator: Equal valueText: \u0026#34;UK\u0026#34; }, { path: [\u0026#34;meta_country\u0026#34;] operator: Equal valueText: \u0026#34;NL\u0026#34; }] }, { path: [\u0026#34;meta_year\u0026#34;] operator: GreaterThanEqual valueNumber: 2020 }] 在 Docker 中运行 Weaviate # 要快速开始使用本地 Weaviate 实例，您可以在 Docker 中运行它：\ndocker run -it --rm --name weaviate \\ -e AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED=true \\ -e PERSISTENCE_DATA_PATH=/var/lib/weaviate \\ -e QUERY_DEFAULTS_LIMIT=25 \\ -e DEFAULT_VECTORIZER_MODULE=none \\ -e CLUSTER_HOSTNAME=node1 \\ -p 8080:8080 \\ semitechnologies/weaviate:1.22.4 这将启动一个可通过 [ localhost:8080](http:// localhost:8080) 访问的 Weaviate 实例。\nWeaviateVectorStore 属性 # 您可以在 Spring Boot 配置中使用以下属性来自定义 Weaviate 矢量存储。\n访问 Native Client # Weaviate Vector Store 实现通过 getNativeClient() 方法提供对底层原生 Weaviate 客户端（ WeaviateClient ）的访问：\nWeaviateVectorStore vectorStore = context.getBean(WeaviateVectorStore.class); Optional\u0026lt;WeaviateClient\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { WeaviateClient client = nativeClient.get(); // Use the native client for Weaviate-specific operations } 本机客户端使您可以访问可能无法通过 VectorStore 界面公开的 Weaviate 特定功能和操作。\n"}]