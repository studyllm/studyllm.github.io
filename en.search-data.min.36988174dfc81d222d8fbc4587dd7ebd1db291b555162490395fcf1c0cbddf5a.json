[{"id":0,"href":"/docs/overview-%E6%A6%82%E8%BF%B0/ai-concepts-ai-%E6%A6%82%E5%BF%B5/","title":"AI 概念","section":"介绍","content":" AI 概念 # 本节介绍 Spring AI 使用的核心概念。我们建议仔细阅读它，以了解 Spring AI 是如何实现的。\n模型 # AI 模型是旨在处理和生成信息的算法，通常模仿人类的认知功能。通过从大型数据集中学习模式和见解，这些模型可以进行预测、文本、图像或其他输出，从而增强跨行业的各种应用程序。 有许多不同类型的 AI 模型，每种模型都适用于特定的使用案例。虽然 ChatGPT 及其生成式 AI 功能通过文本输入和输出吸引了用户，但许多模型和公司都提供了不同的输入和输出。在 ChatGPT 之前，许多人对 Midjourney 和 Stable Diffusion 等文本到图像生成模型着迷。 下表根据模型的输入和输出类型对多个模型进行分类： Spring AI 目前支持将输入和输出处理为语言、图像和音频的模型。上表中的最后一行接受文本作为输入并输出数字，通常称为嵌入文本，表示 AI 模型中使用的内部数据结构。Spring AI 支持嵌入以支持更高级的用例。 像 GPT 这样的模型的不同之处在于它们的预训练性质，如 GPT 中的“P”所示——聊天生成预训练转换器。此预训练功能将 AI 转换为通用的开发人员工具，不需要广泛的机器学习或模型训练背景。\n提示 # 提示是基于语言的输入的基础，这些输入可指导 AI 模型生成特定输出。对于熟悉 ChatGPT 的人来说，提示可能看起来只是在发送到 API 的对话框中输入的文本。然而，它包含的远不止于此。在许多 AI 模型中，提示的文本不仅仅是一个简单的字符串。 ChatGPT 的 API 在一个提示中有多个文本输入，每个文本输入都分配了一个角色。例如，有 system 角色，它告诉模型如何行为并设置交互的上下文。还有 user role，通常是来自用户的 Importing。 制作有效的提示既是一门艺术，也是一门科学。ChatGPT 专为人类对话而设计。这与使用 SQL 之类的东西来 “ask a question” 完全不同。一个人必须与 AI 模型进行交流，类似于与另一个人交谈。 正是这种交互方式的重要性，以至于“Prompt Engineering”一词已经成为一门独立的学科。有一系列新兴的技术可以提高提示的有效性。投入时间制作提示可以大大提高结果输出。 分享提示已成为一种公共实践，并且正在积极地进行关于这一主题的学术研究。例如，创建有效的提示（例如，与 SQL 形成对比）是多么违反直觉， [ 最近的一篇研究论文]( https://arxiv.org/abs/2205.11916)发现，您可以使用的最有效的提示之一以短语“深呼吸并逐步完成此工作”开头。这应该可以告诉你为什么语言如此重要。我们还不完全了解如何最有效地利用这项技术的先前迭代，例如 ChatGPT 3.5，更不用说正在开发的新版本了。\n提示模板 # 创建有效的提示包括建立请求的上下文，并将请求的各个部分替换为特定于用户输入的值。 此过程使用传统的基于文本的模板引擎进行提示创建和管理。Spring AI 为此使用了 OSS 库 [ StringTemplate]( https://www.stringtemplate.org/)。 例如，考虑简单的提示模板：\nTell me a {adjective} joke about {content}. 在 Spring AI 中，提示模板可以比作 Spring MVC 架构中的 “视图”。提供模型对象（通常是 java.util.Map）来填充模板中的占位符。“rendered” 字符串成为提供给 AI 模型的提示的内容。 发送到模型的提示的特定数据格式存在相当大的变化。提示最初从简单字符串开始，现在已经发展到包含多条消息，其中每条消息中的每个字符串代表模型的不同角色。\n嵌入 # 嵌入是文本、图像或视频的数字表示形式，用于捕获输入之间的关系。 嵌入的工作原理是将文本、图像和视频转换为浮点数数组（称为向量）。这些矢量旨在捕获文本、图像和视频的含义。嵌入数组的长度称为向量的维数。 通过计算两段文本的向量表示之间的数值距离，应用程序可以确定用于生成嵌入向量的对象之间的相似性。 作为探索 AI 的 Java 开发人员，没有必要理解复杂的数学理论或这些向量表示背后的具体实现。对它们在 AI 系统中的角色和功能有基本的了解就足够了，尤其是在您将 AI 功能集成到应用程序中时。 嵌入在 Retrieval Augmented Generation （RAG） 模式等实际应用中尤其相关。它们能够将数据表示为语义空间中的点，这类似于欧几里得几何的二维空间，但维度更高。这意味着就像欧几里得几何中平面上的点可以根据其坐标来接近或远一样，在语义空间中，点的接近反映了含义的相似性。在这个多维空间中，关于相似主题的句子被放置在更近的位置，就像图表上彼此靠近的点一样。这种接近有助于文本分类、语义搜索甚至产品推荐等任务，因为它允许 AI 根据相关概念在这个扩展的语义环境中的 “位置” 来识别和分组。 您可以将此语义空间视为一个向量。\n令 牌 # 代币是 AI 模型工作原理的构建块。在输入时，模型将单词转换为标记。在输出时，他们将标记转换回单词。 在英语中，一个标记大约相当于一个单词的 75%。作为参考，莎士比亚全集总计约 900,000 字，可翻译成大约 120 万个代币。 也许更重要的是代币 = 货币。在托管 AI 模型的上下文中，您的费用由使用的令牌数量决定。输入和输出都会影响总令牌计数。 此外，模型还受令牌限制的约束，这些限制限制了在单个 API 调用中处理的文本量。此阈值通常称为 “上下文窗口”。模型不会处理任何超过此限制的文本。 例如，ChatGPT3 有 4K 代币限制，而 GPT4 提供不同的选项，例如 8K、16K 和 32K。Anthropic 的 Claude AI 模型具有 100K 代币限制，而 Meta 最近的研究产生了 1M 代币限制模型。 要使用 GPT4 总结莎士比亚的汇编作品，您需要设计软件工程策略来切碎数据并在模型的上下文窗口限制内呈现数据。Spring AI 项目可帮助您完成此任务。\n结构化输出 # AI 模型的输出传统上以 java.lang.String 的形式到达，即使您要求以 JSON 格式回复也是如此。它可能是正确的 JSON，但不是 JSON 数据结构。它只是一个字符串。此外，在提示中请求 “for JSON” 并不是 100% 准确的。 这种复杂性导致了一个专业领域的出现，该领域涉及创建提示以产生预期的输出，然后将生成的简单字符串转换为可用于应用程序集成的数据结构。 [ 结构化输出转换](api/structured-output-converter.html#_structuredoutputconverter)采用精心设计的提示，通常需要与模型进行多次交互才能获得所需的格式。\n将您的数据和 API 引入 AI 模型 # 如何为 AI 模型配备尚未训练的信息？ 请注意，GPT 3.5/4.0 数据集仅延长至 2021 年 9 月。因此，该模型表示它不知道需要该日期之后知识的问题的答案。一个有趣的琐事是，这个数据集大约有 650GB。 有三种技术可用于自定义 AI 模型以合并您的数据：\n微调 ：这种传统的机器学习技术涉及定制模型和更改其内部权重。然而，对于机器学习专家来说，这是一个具有挑战性的过程，并且由于 GPT 等模型的大小，它非常耗费资源。此外，某些型号可能不提供此选项。 Prompt Stuffing：一种更实用的替代方案涉及将数据嵌入到提供给模型的提示中。给定模型的 token 限制，需要技术在模型的上下文窗口中呈现相关数据。这种方法俗称 “填充提示”。Spring AI 库可帮助您实现基于“填充提示”技术（也称为检索增强生成 （RAG））的解决方案。 工具调用 ：该技术允许注册将大型语言模型连接到外部系统 API 的工具（用户定义的服务）。Spring AI 大大简化了您需要编写以支持工具调用的代码。 检索增强一代 # 一种称为检索增强生成 （RAG） 的技术已经出现，用于解决将相关数据纳入提示以实现准确 AI 模型响应的挑战。 该方法涉及批处理风格的编程模型，其中作业从您的文档中读取非结构化数据，对其进行转换，然后将其写入矢量数据库。概括地说，这是一个 ETL （提取、转换和加载） 管道。向量数据库用于 RAG 技术的检索部分。 作为将非结构化数据加载到矢量数据库的一部分，最重要的转换之一是将原始文档拆分为更小的部分。将原始文档拆分为较小部分的过程有两个重要步骤： RAG 的下一阶段是处理用户输入。当 AI 模型要回答用户的问题时，该问题和所有“相似”文档片段都会被放入发送到 AI 模型的提示中。这就是使用向量数据库的原因。它非常擅长寻找相似的内容。 ETL 管道提供了有关编排从数据源提取数据并将其存储在结构化向量存储中的流程的更多信息，从而确保数据在传递给 AI 模型时处于最佳检索格式。 ChatClient - RAG 介绍了如何使用 QuestionAnswerAdvisor 在应用程序中启用 RAG 功能。 工具调用 # 大型语言模型 （LLM） 在训练后被冻结，导致知识过时，并且无法访问或修改外部数据。 [ Tool Calling](api/tools.html) 机制解决了这些缺点。它允许您将自己的服务注册为工具，以将大型语言模型连接到外部系统的 API。这些系统可以为 LLM 提供实时数据并代表他们执行数据处理作。 Spring AI 大大简化了您需要编写以支持工具调用的代码。它为您处理工具调用对话。您可以将工具作为 @Tool 注释的方法提供，并在提示选项中提供它以使其可供模型使用。此外，您还可以在单个提示中定义和引用多个工具。 有关如何将此功能与不同 AI 模型一起使用的更多信息，请遵循[ 工具调用](api/tools.html)文档。\n评估 AI 响应 # 根据用户请求有效评估 AI 系统的输出对于确保最终应用程序的准确性和有用性非常重要。为此，几种新兴技术允许使用预训练模型本身。 此评估过程包括分析生成的响应是否与用户的意图和查询的上下文一致。相关性、连贯性和事实正确性等指标用于衡量 AI 生成的响应的质量。 一种方法涉及将用户的请求和 AI 模型的响应呈现给模型，查询响应是否与提供的数据一致。 此外，利用向量数据库中存储的信息作为补充数据可以增强评估过程，有助于确定响应相关性。 Spring AI 项目提供了一个 Evaluator API，它目前可以访问评估模型响应的基本策略。有关详细信息，请遵循[ 评估测试](api/testing.html)文档。\n"},{"id":1,"href":"/docs/vector-databases/azure-ai-service/","title":"Azure AI 服务","section":"矢量数据库","content":" Azure AI 服务 # 本部分将指导你设置 AzureVectorStore 以存储文档嵌入并使用 Azure AI 搜索服务执行相似性搜索。 [ Azure AI 搜索]( https://azure.microsoft.com/en-us/products/ai-services/ai-search/)是一种多功能的云托管云信息检索系统，是 Microsoft 更大的 AI 平台的一部分。除其他功能外，它还允许用户使用基于向量的存储和检索来查询信息。\n先决条件 # 配置 # 启动时，AzureVectorStore 可以尝试在 AI 搜索服务实例中创建新索引，前提是您通过在构造函数中将相关的 initialize-schema 布尔属性设置为 true，或者如果使用 Spring Boot，则设置 \u0026hellip;initialize-schema=true 在 application.properties 文件中。 或者，您可以手动创建索引。 若要设置 AzureVectorStore，需要从上述先决条件中检索到的设置以及索引名称：\nAzure AI 搜索终结点 Azure AI 搜索密钥 （可选）Azure OpenAI API 终结点 （可选）Azure OpenAI API 密钥 您可以将这些值作为作系统环境变量提供。 export AZURE_AI_SEARCH_API_KEY=\u0026lt;My AI Search API Key\u0026gt; export AZURE_AI_SEARCH_ENDPOINT=\u0026lt;My AI Search Index\u0026gt; export OPENAI_API_KEY=\u0026lt;My Azure AI API Key\u0026gt; (Optional) 依赖 # 将这些依赖项添加到您的项目中：\n1. 选择 Embeddings 接口实现。您可以选择： # 2. Azure （AI Search） 矢量存储 # \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-azure-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 配置属性 # 您可以在 Spring Boot 配置中使用以下属性来自定义 Azure 矢量存储。\n示例代码 # 若要在应用程序中配置 Azure SearchIndexClient，可以使用以下代码：\n@Bean public SearchIndexClient searchIndexClient() { return new SearchIndexClientBuilder().endpoint(System.getenv(\u0026#34;AZURE_AI_SEARCH_ENDPOINT\u0026#34;)) .credential(new AzureKeyCredential(System.getenv(\u0026#34;AZURE_AI_SEARCH_API_KEY\u0026#34;))) .buildClient(); } 要创建向量存储，可以使用以下代码，方法是注入在上面示例中创建的 SearchIndexClient Bean 以及 Spring AI 库提供的 EmbeddingModel，该库实现了所需的 Embeddings 接口。\n@Bean public VectorStore vectorStore(SearchIndexClient searchIndexClient, EmbeddingModel embeddingModel) { return AzureVectorStore.builder(searchIndexClient, embeddingModel) .initializeSchema(true) // Define the metadata fields to be used // in the similarity search filters. .filterMetadataFields(List.of(MetadataField.text(\u0026#34;country\u0026#34;), MetadataField.int64(\u0026#34;year\u0026#34;), MetadataField.date(\u0026#34;activationDate\u0026#34;))) .defaultTopK(5) .defaultSimilarityThreshold(0.7) .indexName(\u0026#34;spring-ai-document-index\u0026#34;) .build(); } 在您的主代码中，创建一些文档：\nList\u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;country\u0026#34;, \u0026#34;BG\u0026#34;, \u0026#34;year\u0026#34;, 2020)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;country\u0026#34;, \u0026#34;NL\u0026#34;, \u0026#34;year\u0026#34;, 2023))); 将文档添加到您的 vector 存储中：\nvectorStore.add(documents); 最后，检索类似于查询的文档：\nList\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch( SearchRequest.builder() .query(\u0026#34;Spring\u0026#34;) .topK(5).build()); 如果一切顺利，您应该检索包含文本 “Spring AI rocks！！” 的文档。\n元数据筛选 # 还可以将通用的可移植[ 元数据筛选器]( https://docs.spring.io/spring-ai/reference/api/vectordbs.html#_metadata_filters)与 AzureVectorStore 结合使用。 例如，您可以使用文本表达式语言：\nvectorStore.similaritySearch( SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(\u0026#34;country in [\u0026#39;UK\u0026#39;, \u0026#39;NL\u0026#39;] \u0026amp;\u0026amp; year \u0026gt;= 2020\u0026#34;).build()); 或者以编程方式使用表达式 DSL：\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch( SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(b.and( b.in(\u0026#34;country\u0026#34;, \u0026#34;UK\u0026#34;, \u0026#34;NL\u0026#34;), b.gte(\u0026#34;year\u0026#34;, 2020)).build()).build()); 可移植筛选器表达式会自动转换为专有的 Azure 搜索 [ OData 筛选器]( https://learn.microsoft.com/en-us/azure/search/search-query-odata-filter) 。例如，以下可移植筛选条件表达式：\ncountry in [\u0026#39;UK\u0026#39;, \u0026#39;NL\u0026#39;] \u0026amp;\u0026amp; year \u0026gt;= 2020 转换为以下 Azure OData [ 筛选器表达式]( https://learn.microsoft.com/en-us/azure/search/search-query-odata-filter) ：\n$filter search.in(meta_country, \u0026#39;UK,NL\u0026#39;, \u0026#39;,\u0026#39;) and meta_year ge 2020 访问 Native Client # Azure 矢量存储实现通过 getNativeClient（） 方法提供对基础本机 Azure 搜索客户端 （SearchClient） 的访问：\nAzureVectorStore vectorStore = context.getBean(AzureVectorStore.class); Optional\u0026lt;SearchClient\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { SearchClient client = nativeClient.get(); // Use the native client for Azure Search-specific operations } 本机客户端允许你访问特定于 Azure 搜索的功能和作，这些功能和作可能不会通过 VectorStore 接口公开。\n"},{"id":2,"href":"/docs/models/audio-models/transcription-api/azure-openai/","title":"Azure OpenAI 听录","section":"转录 API","content":" Azure OpenAI 听录 # Spring AI 支持 [ Azure Whisper 模型]( https://learn.microsoft.com/en-us/azure/ai-services/openai/whisper-quickstart?tabs=command-line%2Cpython-new\u0026pivots=rest-api) 。\n先决条件 # 从 [ Azure 门户]( https://portal.azure.com)上的 Azure OpenAI 服务部分获取 Azure OpenAI 终结点和 API 密钥 。Spring AI 定义了一个名为 spring.ai.azure.openai.api-key config property 的配置属性，您应该将其设置为从 Azure 获取的 API Key 的值。还有一个名为 spring.ai.azure.openai.endpoint 的配置属性，您应该将其设置为在 Azure 中预置模型时获取的终端节点 URL。导出环境变量是设置该配置属性的一种方法：\n自动配置 # Spring AI 为 Azure OpenAI 听录生成客户端提供 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-azure-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-azure-openai\u0026#39; } 转录属性 # 前缀 spring.ai.openai.audio.transcription 用作属性前缀，允许您为 OpenAI 图像模型配置重试机制。\n运行时选项 # 该 AzureOpenAiAudioTranscriptionOptions 类提供了进行转录时要使用的选项。启动时，将使用指定的选项， spring.ai.azure.openai.audio.transcription 但您可以在运行时覆盖这些选项。 例如：\nAzureOpenAiAudioTranscriptionOptions.TranscriptResponseFormat responseFormat = AzureOpenAiAudioTranscriptionOptions.TranscriptResponseFormat.VTT; AzureOpenAiAudioTranscriptionOptions transcriptionOptions = AzureOpenAiAudioTranscriptionOptions.builder() .language(\u0026#34;en\u0026#34;) .prompt(\u0026#34;Ask not this, but ask that\u0026#34;) .temperature(0f) .responseFormat(this.responseFormat) .build(); AudioTranscriptionPrompt transcriptionRequest = new AudioTranscriptionPrompt(audioFile, this.transcriptionOptions); AudioTranscriptionResponse response = azureOpenAiTranscriptionModel.call(this.transcriptionRequest); 手动配置 # 将 spring-ai-openai 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-azure-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-azure-openai\u0026#39; } 接下来，创建一个 AzureOpenAiAudioTranscriptionModel\nvar openAIClient = new OpenAIClientBuilder() .credential(new AzureKeyCredential(System.getenv(\u0026#34;AZURE_OPENAI_API_KEY\u0026#34;))) .endpoint(System.getenv(\u0026#34;AZURE_OPENAI_ENDPOINT\u0026#34;)) .buildClient(); var azureOpenAiAudioTranscriptionModel = new AzureOpenAiAudioTranscriptionModel(this.openAIClient, null); var transcriptionOptions = AzureOpenAiAudioTranscriptionOptions.builder() .responseFormat(TranscriptResponseFormat.TEXT) .temperature(0f) .build(); var audioFile = new FileSystemResource(\u0026#34;/path/to/your/resource/speech/jfk.flac\u0026#34;); AudioTranscriptionPrompt transcriptionRequest = new AudioTranscriptionPrompt(this.audioFile, this.transcriptionOptions); AudioTranscriptionResponse response = this.azureOpenAiAudioTranscriptionModel.call(this.transcriptionRequest); "},{"id":3,"href":"/docs/models/image-models/azure-openai/","title":"Azure OpenAI 映像生成","section":"图像模型 API","content":" Azure OpenAI 映像生成 # Spring AI 支持 DALL-E，这是 Azure OpenAI 中的图像生成模型。\n先决条件 # 从 [ Azure 门户]( https://portal.azure.com)上的 Azure OpenAI 服务部分获取 Azure OpenAI 终结点和 API 密钥 。 Spring AI 定义了两个配置属性： 您可以在 application.properties 文件中设置这些配置属性：\nspring.ai.azure.openai.api-key=\u0026lt;your-azure-openai-api-key\u0026gt; spring.ai.azure.openai.endpoint=\u0026lt;your-azure-openai-endpoint\u0026gt; 为了在处理 API 密钥等敏感信息时增强安全性，您可以使用 Spring 表达式语言 （SpEL） 来引用自定义环境变量：\n# In application.yml spring: ai: azure: openai: api-key: ${AZURE_OPENAI_API_KEY} endpoint: ${AZURE_OPENAI_ENDPOINT} # In your environment or .env file export AZURE_OPENAI_API_KEY=\u0026lt;your-azure-openai-api-key\u0026gt; export AZURE_OPENAI_ENDPOINT=\u0026lt;your-azure-openai-endpoint\u0026gt; 您还可以在应用程序代码中以编程方式设置这些配置：\n// Retrieve API key and endpoint from secure sources or environment variables String apiKey = System.getenv(\u0026#34;AZURE_OPENAI_API_KEY\u0026#34;); String endpoint = System.getenv(\u0026#34;AZURE_OPENAI_ENDPOINT\u0026#34;); 部署名称 # 要使用运行 Azure AI 应用程序，请通过 [Azure AI 门户]（[ oai.azure.com/portal](https:// oai.azure.com/portal)） 创建 Azure AI 部署。 在 Azure 中，每个客户端都必须指定一个部署名称才能连接到 Azure OpenAI 服务。 必须了解 Deployment Name 与您选择部署的模型不同 例如，可以将名为“MyImgAiDeployment”的部署配置为使用 Dalle3 模型或 Dalle2 模型。 现在，为了简单起见，您可以使用以下设置创建部署： 部署名称：MyImgAiDeployment 模型： Dalle3 此 Azure 配置将与 Spring Boot Azure AI Starter 及其自动配置功能的默认配置保持一致。 如果你使用不同的 Deployment Name，请相应地更新 configuration 属性：\nspring.ai.azure.openai.image.options.deployment-name=\u0026lt;my deployment name\u0026gt; Azure OpenAI 和 OpenAI 的不同部署结构会导致 Azure OpenAI 客户端库中有一个名为 deploymentOrModelName 的属性。这是因为在 OpenAI 中没有部署名称 ，只有模型名称 。\n添加存储库和 BOM # Spring AI 工件发布在 Maven Central 和 Spring Snapshot 存储库中。请参阅 [ Artifact Repositories](../../getting-started.html#artifact-repositories) 部分，将这些存储库添加到您的构建系统中。 为了帮助进行[ 依赖项管理](../../getting-started.html#dependency-management)，Spring AI 提供了一个 BOM（物料清单），以确保在整个项目中使用一致的 Spring AI 版本。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # Spring AI 为 Azure OpenAI 聊天客户端提供 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-azure-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-azure-openai\u0026#39; } 图像生成属性 # 前缀 spring.ai.openai.image 是属性前缀，允许您为 OpenAI 配置 ImageModel 实现。\n连接属性 # 前缀 spring.ai.openai 用作属性前缀，用于连接到 Azure OpenAI。\n运行时选项 # [ OpenAiImageOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/[OpenAiImageOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/OpenAiImageOptions.java)) 提供模型配置，例如要使用的模型、质量、大小等。 启动时，可以使用 AzureOpenAiImageModel(OpenAiImageApi openAiImageApi) constructor 和 withDefaultOptions(OpenAiImageOptions defaultOptions) method 配置默认选项。或者，使用前面描述的属性 spring.ai.azure.openai.image.options.* 。 在运行时，您可以通过向 ImagePrompt 调用添加新的、特定于请求的选项来覆盖默认选项。例如，要覆盖 OpenAI 特定选项（例如质量和要创建的图像数量），请使用以下代码示例：\nImageResponse response = azureOpenaiImageModel.call( new ImagePrompt(\u0026#34;A light cream colored mini golden doodle\u0026#34;, OpenAiImageOptions.builder() .quality(\u0026#34;hd\u0026#34;) .N(4) .height(1024) .width(1024).build()) ); "},{"id":4,"href":"/docs/models/embedding-models/amazon-bedrock/cohere/","title":"Cohere 嵌入","section":"亚马逊基岩版","content":" Cohere 嵌入 # 提供 Bedrock Cohere Embedding 模型。将生成式 AI 功能集成到基本应用程序和工作流中，以改善业务成果。 [ AWS Bedrock Cohere 模型页面]( https://aws.amazon.com/bedrock/cohere-command-embed/)和 [ Amazon Bedrock 用户指南]( https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html)包含有关如何使用 AWS 托管模型的详细信息。\n先决条件 # 请参阅 [ Amazon Bedrock 上的 Spring AI 文档](../bedrock.html)以设置 API 访问。\n添加存储库和 BOM # Spring AI 工件发布在 Maven Central 和 Spring Snapshot 存储库中。请参阅 [ Artifact Repositories](../../getting-started.html#artifact-repositories) 部分，将这些存储库添加到您的构建系统中。 为了帮助进行[ 依赖项管理](../../getting-started.html#dependency-management)，Spring AI 提供了一个 BOM（物料清单），以确保在整个项目中使用一致的 Spring AI 版本。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # 将 spring-ai-starter-model-bedrock 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-bedrock\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-bedrock\u0026#39; } 启用 Cohere 嵌入支持 # 默认情况下，Cohere 嵌入模型处于禁用状态。要启用它，请在应用程序配置中将 spring.ai.model.embedding 属性设置为 bedrock-cohere：\nspring.ai.model.embedding=bedrock-cohere 或者，你可以使用 Spring 表达式语言 （SpEL） 来引用环境变量：\n# In application.yml spring: ai: model: embedding: ${AI_MODEL_EMBEDDING} # In your environment or .env file export AI_MODEL_EMBEDDING=bedrock-cohere 您还可以在启动应用程序时使用 Java 系统属性设置此属性：\njava -Dspring.ai.model.embedding=bedrock-cohere -jar your-application.jar 嵌入属性 # 前缀 spring.ai.bedrock.aws 是用于配置与 AWS Bedrock 的连接的属性前缀。 前缀 spring.ai.bedrock.cohere.embedding （在 BedrockCohereEmbeddingProperties 中定义）是为 Cohere 配置嵌入模型实现的属性前缀。 查看 [ CohereEmbeddingModel]( https://github.com/spring-projects/spring-ai/blob/056b95a00efa5b014a1f488329fbd07a46c02378/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/cohere/api/CohereEmbeddingBedrockApi.java#L150) 以获取其他模型 ID。支持的值为：cohere.embed-multilingual-v3 和 cohere.embed-english-v3。模型 ID 值也可以在 [ AWS Bedrock 文档中找到基本模型 ID]( https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids-arns.html)。\n运行时选项 # [ BedrockCohereEmbeddingOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/cohere/[BedrockCohereEmbeddingOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/cohere/BedrockCohereEmbeddingOptions.java)) 提供模型配置，例如 input-type 或 truncate。 启动时，可以使用 BedrockCohereEmbeddingModel(api, options) constructor 或 spring.ai.bedrock.cohere.embedding.options.* properties 配置默认选项。 在运行时，您可以通过向 EmbeddingRequest 调用添加新的特定于请求的选项来覆盖默认选项。例如，要覆盖特定请求的默认输入类型：\nEmbeddingResponse embeddingResponse = embeddingModel.call( new EmbeddingRequest(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;), BedrockCohereEmbeddingOptions.builder() .withInputType(InputType.SEARCH_DOCUMENT) .build())); 样品控制器 # [ 创建一个新]( https://start.spring.io/)的 Spring Boot 项目，并将 添加到您的 spring-ai-starter-model-bedrock pom（或 gradle）依赖项中。 在 src/main/resources 目录下添加一个 application.properties 文件，以启用和配置 Cohere 嵌入模型：\nspring.ai.bedrock.aws.region=eu-central-1 spring.ai.bedrock.aws.access-key=${AWS_ACCESS_KEY_ID} spring.ai.bedrock.aws.secret-key=${AWS_SECRET_ACCESS_KEY} spring.ai.model.embedding=bedrock-cohere spring.ai.bedrock.cohere.embedding.options.input-type=search-document 这将创建一个 BedrockCohereEmbeddingModel 实现，您可以将其注入到您的类中。下面是一个简单的 @Controller 类示例，该类使用 chat 模型生成文本。\n@RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(\u0026#34;/ai/embedding\u0026#34;) public Map embed(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { EmbeddingResponse embeddingResponse = this.embeddingModel.embedForResponse(List.of(message)); return Map.of(\u0026#34;embedding\u0026#34;, embeddingResponse); } } 手动配置 # BedrockCohereEmbeddingModel 实现 EmbeddingModel，并使用[ 低级 CohereEmbeddingBedrockApi 客户端](#low-level-api)连接到 Bedrock Cohere 服务。 将 spring-ai-bedrock 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-bedrock\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-bedrock\u0026#39; } 接下来，创建一个 [ BedrockCohereEmbeddingModel]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/cohere/[BedrockCohereEmbeddingModel](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/cohere/BedrockCohereEmbeddingModel.java).java) 并将其用于文本嵌入：\nvar cohereEmbeddingApi =new CohereEmbeddingBedrockApi( CohereEmbeddingModel.COHERE_EMBED_MULTILINGUAL_V1.id(), EnvironmentVariableCredentialsProvider.create(), Region.US_EAST_1.id(), new ObjectMapper()); var embeddingModel = new BedrockCohereEmbeddingModel(this.cohereEmbeddingApi); EmbeddingResponse embeddingResponse = this.embeddingModel .embedForResponse(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;)); 低级 CohereEmbeddingBedrockApi 客户端 # [ CohereEmbeddingBedrockApi]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/cohere/api/[CohereEmbeddingBedrockApi](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/cohere/api/CohereEmbeddingBedrockApi.java).java) 提供的是基于 AWS Bedrock [ Cohere Command 模型的]( https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-cohere-command.html)轻量级 Java 客户端。 以下类图说明了 CohereEmbeddingBedrockApi 接口和构建块： CohereEmbeddingBedrockApi 支持 cohere.embed-english-v3 和 cohere.embed-multilingual-v3 模型进行单嵌入和批量嵌入计算。 以下是如何以编程方式使用 api 的简单代码段：\nCohereEmbeddingBedrockApi api = new CohereEmbeddingBedrockApi( CohereEmbeddingModel.COHERE_EMBED_MULTILINGUAL_V1.id(), EnvironmentVariableCredentialsProvider.create(), Region.US_EAST_1.id(), new ObjectMapper()); CohereEmbeddingRequest request = new CohereEmbeddingRequest( List.of(\u0026#34;I like to eat apples\u0026#34;, \u0026#34;I like to eat oranges\u0026#34;), CohereEmbeddingRequest.InputType.search_document, CohereEmbeddingRequest.Truncate.NONE); CohereEmbeddingResponse response = this.api.embedding(this.request); "},{"id":5,"href":"/docs/retrieval-augmented-generation-rag/etl-pipeline/","title":"ETL 管道","section":"检索增强一代","content":" ETL 管道 # 提取、转换和加载 （ETL） 框架是检索增强生成 （RAG） 用例中数据处理的主干。 ETL 管道编排从原始数据源到结构化向量存储的流程，确保数据处于最佳格式，以便 AI 模型进行检索。 RAG 用例是文本，通过从数据主体中检索相关信息来提高生成输出的质量和相关性，从而增强生成模型的功能。\nAPI 概述 # ETL 管道创建、转换和存储 Document 实例。 Document 类包含文本、元数据和可选的其他媒体类型，如图像、音频和视频。 ETL 管道有三个主要组件：\n实现 Supplier\u0026lt;List\u0026gt; 的 DocumentReader DocumentTransformer 实现 Function\u0026lt;List, List\u0026gt; 实现 Consumer\u0026lt;List\u0026gt; 的 DocumentWriter Document 类内容是在 DocumentReader 的帮助下从 PDF、文本文件和其他文档类型创建的。 要构建简单的 ETL 管道，您可以将每种类型的实例链接在一起。 假设我们有这三种 ETL 类型的以下实例 PagePdfDocumentReader 的实现 TokenTextSplitter，DocumentTransformer 的实现 VectorStore 是 DocumentWriter 的实现 要执行将数据基本加载到 Vector Database 中以用于 Retrieval Augmented Generation 模式的作，请使用以下 Java 函数样式语法代码。 vectorStore.accept(tokenTextSplitter.apply(pdfReader.get())); 或者，您可以使用对域更自然地表达的方法名称\nvectorStore.write(tokenTextSplitter.split(pdfReader.read())); ETL 接口 # ETL 管道由以下接口和实现组成。详细的 [ ETL 类图](#etl-class-diagram)显示在 [ ETL 类图](#etl-class-diagram)部分中。\n文档阅读器 # 提供来自不同来源的文档源。\npublic interface DocumentReader extends Supplier\u0026lt;List\u0026lt;Document\u0026gt;\u0026gt; { default List\u0026lt;Document\u0026gt; read() { return get(); } } 文档 Transformer # 在处理工作流中转换一批文档。\npublic interface DocumentTransformer extends Function\u0026lt;List\u0026lt;Document\u0026gt;, List\u0026lt;Document\u0026gt;\u0026gt; { default List\u0026lt;Document\u0026gt; transform(List\u0026lt;Document\u0026gt; transform) { return apply(transform); } } 文档编写器 # 管理 ETL 流程的最后阶段，准备要存储的文档。\npublic interface DocumentWriter extends Consumer\u0026lt;List\u0026lt;Document\u0026gt;\u0026gt; { default void write(List\u0026lt;Document\u0026gt; documents) { accept(documents); } } ETL 类图 # 下面的类图说明了 ETL 接口和实现。 文档读者 # JSON 格式 # JsonReader 处理 JSON 文档，将它们转换为 Document 对象列表。\n例 # @Component class MyJsonReader { private final Resource resource; MyJsonReader(@Value(\u0026#34;classpath:bikes.json\u0026#34;) Resource resource) { this.resource = resource; } List\u0026lt;Document\u0026gt; loadJsonAsDocuments() { JsonReader jsonReader = new JsonReader(this.resource, \u0026#34;description\u0026#34;, \u0026#34;content\u0026#34;); return jsonReader.get(); } } 构造函数选项 # JsonReader 提供了几个构造函数选项：\n参数 # resource：指向 JSON 文件的 Spring Resource 对象。 jsonKeysToUse：JSON 中的键数组，应用作生成的 Document 对象中的文本内容。 jsonMetadataGenerator：一个可选的 JsonMetadataGenerator，用于为每个 Document 创建元数据。 行为 # JsonReader 按如下方式处理 JSON 内容：\n它可以处理 JSON 数组和单个 JSON 对象。 对于每个 JSON 对象（在数组或单个对象中）： 它根据指定的 jsonKeysToUse 提取内容。 如果未指定键，则使用整个 JSON 对象作为内容。 它使用提供的 JsonMetadataGenerator （如果未提供，则为空） 生成元数据。 它会创建一个 Document 包含提取的内容和元数据的对象。 使用 JSON 指针 # JsonReader 现在支持使用 JSON 指针检索 JSON 文档的特定部分。此功能允许您轻松地从复杂的 JSON 结构中提取嵌套数据。\nget（String pointer） 方法 # public List\u0026lt;Document\u0026gt; get(String pointer) 此方法允许您使用 JSON 指针检索 JSON 文档的特定部分。\n参数 # pointer：一个 JSON 指针字符串（如 RFC 6901 中所定义），用于在 JSON 结构中查找所需的元素。 返回值 # 返回一个 List，其中包含从指针定位的 JSON 元素解析的文档。 行为 # 该方法使用提供的 JSON 指针导航到 JSON 结构中的特定位置。 如果指针有效并指向现有元素： 对于 JSON 对象：它返回一个包含单个 Document 的列表。 对于 JSON 数组：它返回一个 Documents 列表，数组中的每个元素对应一个 Documents。 如果指针无效或指向不存在的元素，则会引发 IllegalArgumentException。 例 # JsonReader jsonReader = new JsonReader(resource, \u0026#34;description\u0026#34;); List\u0026lt;Document\u0026gt; documents = this.jsonReader.get(\u0026#34;/store/books/0\u0026#34;); 示例 JSON 结构 # [ { \u0026#34;id\u0026#34;: 1, \u0026#34;brand\u0026#34;: \u0026#34;Trek\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;A high-performance mountain bike for trail riding.\u0026#34; }, { \u0026#34;id\u0026#34;: 2, \u0026#34;brand\u0026#34;: \u0026#34;Cannondale\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;An aerodynamic road bike for racing enthusiasts.\u0026#34; } ] 在此示例中，如果 JsonReader 配置了 “description” 作为 jsonKeysToUse，它将创建 Document 对象，其中内容是数组中每辆自行车的 “description” 字段的值。\n笔记 # JsonReader 使用 Jackson 进行 JSON 解析。 它可以通过使用数组的流式处理来高效地处理大型 JSON 文件。 如果在 jsonKeysToUse 中指定了多个键，则内容将是这些键的值的串联。 读取器非常灵活，可以通过自定义 jsonKeysToUse 和 JsonMetadataGenerator 来适应各种 JSON 结构。 发短信 # TextReader 处理纯文本文档，并将其转换为 Document 对象列表。\n例 # @Component class MyTextReader { private final Resource resource; MyTextReader(@Value(\u0026#34;classpath:text-source.txt\u0026#34;) Resource resource) { this.resource = resource; } List\u0026lt;Document\u0026gt; loadText() { TextReader textReader = new TextReader(this.resource); textReader.getCustomMetadata().put(\u0026#34;filename\u0026#34;, \u0026#34;text-source.txt\u0026#34;); return textReader.read(); } } 构造函数选项 # TextReader 提供了两个构造函数选项：\n参数 # resourceUrl：一个字符串，表示要读取的资源的 URL。 resource：指向文本文件的 Spring Resource 对象。 配置 # setCharset（Charset charset）： 设置用于读取文本文件的字符集。默认值为 UTF-8。 getCustomMetadata（）：返回一个可变映射，您可以在其中为文档添加自定义元数据。 行为 # TextReader 按如下方式处理文本内容：\n它将文本文件的全部内容读取到单个 Document 对象中。 文件的内容将成为 Document 的内容。 元数据会自动添加到文档中 ： charset：用于读取文件的字符集（默认值：“UTF-8”）。 source：源文本文件的文件名。 通过 getCustomMetadata（） 添加的任何自定义元数据都包含在 Document 中。 笔记 # TextReader 将整个文件内容读入内存，因此它可能不适合处理非常大的文件。 如果需要将文本拆分成更小的块，可以在阅读文档后使用像 TokenTextSplitter 这样的文本拆分器： List\u0026lt;Document\u0026gt; documents = textReader.get(); List\u0026lt;Document\u0026gt; splitDocuments = new TokenTextSplitter().apply(this.documents); 读取器使用 Spring 的 Resource 抽象，允许它从各种源（Classpath、文件系统、URL 等）读取数据。 自定义元数据可以使用 getCustomMetadata（） 方法添加到读者创建的所有文档中。 HTML （JSoup） # Jsoup``Document``Reader 处理 HTML 文档，使用 JSoup 库将它们转换为 Document 对象列表。\n例 # @Component class MyHtmlReader { private final Resource resource; MyHtmlReader(@Value(\u0026#34;classpath:/my-page.html\u0026#34;) Resource resource) { this.resource = resource; } List\u0026lt;Document\u0026gt; loadHtml() { JsoupDocumentReaderConfig config = JsoupDocumentReaderConfig.builder() .selector(\u0026#34;article p\u0026#34;) // Extract paragraphs within \u0026lt;article\u0026gt; tags .charset(\u0026#34;ISO-8859-1\u0026#34;) // Use ISO-8859-1 encoding .includeLinkUrls(true) // Include link URLs in metadata .metadataTags(List.of(\u0026#34;author\u0026#34;, \u0026#34;date\u0026#34;)) // Extract author and date meta tags .additionalMetadata(\u0026#34;source\u0026#34;, \u0026#34;my-page.html\u0026#34;) // Add custom metadata .build(); JsoupDocumentReader reader = new JsoupDocumentReader(this.resource, config); return reader.get(); } } ```JsoupDocumentReaderConfig` 允许你自定义 JsoupDocumentReader`` 的行为：\ncharset：指定 HTML 文档的字符编码（默认为 “UTF-8”）。 selector：一个 JSoup CSS 选择器，用于指定要从中提取文本的元素（默认为 “body”）。 separator：用于连接多个选定元素中的文本的字符串（默认为 “\\n”）。 allElements：如果为 true，则从 元素中提取所有文本，忽略选择器 （默认为 false）。 groupByElement：如果为 true，则为选择器匹配的每个元素创建一个单独的 Document（默认为 false）。 includeLinkUrls：如果为 true，则提取绝对链接 URL 并将其添加到元数据中（默认为 false）。 metadataTags：要从中提取内容的 标签名称列表（默认为 [“description”， “keywords”]）。 additionalMetadata：允许您将自定义元数据添加到所有创建的 Document 对象。 示例文档：my-page.html # \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;My Web Page\u0026lt;/title\u0026gt; \u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;A sample web page for Spring AI\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;keywords\u0026#34; content=\u0026#34;spring, ai, html, example\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;author\u0026#34; content=\u0026#34;John Doe\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;date\u0026#34; content=\u0026#34;2024-01-15\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;style.css\u0026#34;\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;header\u0026gt; \u0026lt;h1\u0026gt;Welcome to My Page\u0026lt;/h1\u0026gt; \u0026lt;/header\u0026gt; \u0026lt;nav\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;/\u0026#34;\u0026gt;Home\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;/about\u0026#34;\u0026gt;About\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/nav\u0026gt; \u0026lt;article\u0026gt; \u0026lt;h2\u0026gt;Main Content\u0026lt;/h2\u0026gt; \u0026lt;p\u0026gt;This is the main content of my web page.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;It contains multiple paragraphs.\u0026lt;/p\u0026gt; \u0026lt;a href=\u0026#34;https://www.example.com\u0026#34;\u0026gt;External Link\u0026lt;/a\u0026gt; \u0026lt;/article\u0026gt; \u0026lt;footer\u0026gt; \u0026lt;p\u0026gt;\u0026amp;copy; 2024 John Doe\u0026lt;/p\u0026gt; \u0026lt;/footer\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 行为： Jsoup``Document``Reader 处理 HTML 内容并根据配置创建 Document 对象：\n选择器确定哪些元素用于文本提取。 如果 allElements 为 true，则 中的所有文本都将提取到单个 Document 中。 如果 groupByElement 为 true，则与选择器匹配的每个元素都会创建一个单独的 Document。 如果 allElements 和 groupByElement 都不为 true，则与选择器匹配的所有元素中的文本将使用分隔符联接。 文档标题、来自指定 标记的内容以及（可选）链接 URL 将添加到文档元数据中。 用于解析相对链接的基 URI 将从 URL 资源中提取。 Reader 会保留所选元素的文本内容，但会删除其中的任何 HTML 标签。 Markdown``Document``Reader 处理 Markdown 文档，将它们转换为 Document 对象列表。 例 # @Component class MyMarkdownReader { private final Resource resource; MyMarkdownReader(@Value(\u0026#34;classpath:code.md\u0026#34;) Resource resource) { this.resource = resource; } List\u0026lt;Document\u0026gt; loadMarkdown() { MarkdownDocumentReaderConfig config = MarkdownDocumentReaderConfig.builder() .withHorizontalRuleCreateDocument(true) .withIncludeCodeBlock(false) .withIncludeBlockquote(false) .withAdditionalMetadata(\u0026#34;filename\u0026#34;, \u0026#34;code.md\u0026#34;) .build(); MarkdownDocumentReader reader = new MarkdownDocumentReader(this.resource, config); return reader.get(); } } MarkdownDocumentReaderConfig 允许你自定义 MarkdownDocumentReader 的行为：\nhorizontalRuleCreateDocument：当设置为 true 时，Markdown 中的水平线将创建新的 Document 对象。 includeCodeBlock：当设置为 true 时，代码块将与周围的文本包含在同一个 Document 中。当 false 时，代码块将创建单独的 Document 对象。 includeBlockquote：当设置为 true 时，blockquotes 将与周围文本包含在同一个 Document 中。当 false 时，块引用会创建单独的 Document 对象。 additionalMetadata：允许您将自定义元数据添加到所有创建的 Document 对象。 示例文档：code.md # This is a Java sample application: ```java package com.example.demo; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; @SpringBootApplication public class DemoApplication { public static void main(String[] args) { SpringApplication.run(DemoApplication.class, args); } } Markdown also provides the possibility to use inline code formatting throughout the entire sentence.\nAnother possibility is to set block code without specific highlighting:\n./mvnw spring-javaformat:apply 行为：MarkdownDocumentReader 处理 Markdown 内容并根据配置创建 Document 对象： - 标题将成为 Document 对象中的元数据。 - 段落成为 Document 对象的内容。 - 代码块可以分隔到它们自己的 Document 对象中，也可以包含在周围的文本中。 - 块引用可以分隔到它们自己的 Document 对象中，也可以包含在周围的文本中。 - 水平线可用于将内容拆分为单独的 Document 对象。 Reader 在 Document 对象的内容中保留内联代码、列表和文本样式等格式。 ## PDF 页面 ``PagePdfDocumentReader`` 使用 Apache PdfBox 库来解析 PDF 文档 使用 Maven 或 Gradle 将依赖项添加到您的项目中。 ```xml \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-pdf-document-reader\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-pdf-document-reader\u0026#39; } 例 # @Component public class MyPagePdfDocumentReader { List\u0026lt;Document\u0026gt; getDocsFromPdf() { PagePdfDocumentReader pdfReader = new PagePdfDocumentReader(\u0026#34;classpath:/sample1.pdf\u0026#34;, PdfDocumentReaderConfig.builder() .withPageTopMargin(0) .withPageExtractedTextFormatter(ExtractedTextFormatter.builder() .withNumberOfTopTextLinesToDelete(0) .build()) .withPagesPerDocument(1) .build()); return pdfReader.read(); } } PDF 段落 # ParagraphPdfDocumentReader 使用 PDF 目录（例如 TOC）信息将输入 PDF 拆分为文本段落，并为每个段落输出一个文档 。注意：并非所有 PDF 文档都包含 PDF 目录。\n依赖 # 使用 Maven 或 Gradle 将依赖项添加到您的项目中。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-pdf-document-reader\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-pdf-document-reader\u0026#39; } 例 # @Component public class MyPagePdfDocumentReader { List\u0026lt;Document\u0026gt; getDocsFromPdfWithCatalog() { ParagraphPdfDocumentReader pdfReader = new ParagraphPdfDocumentReader(\u0026#34;classpath:/sample1.pdf\u0026#34;, PdfDocumentReaderConfig.builder() .withPageTopMargin(0) .withPageExtractedTextFormatter(ExtractedTextFormatter.builder() .withNumberOfTopTextLinesToDelete(0) .build()) .withPagesPerDocument(1) .build()); return pdfReader.read(); } } 蒂卡 （DOCX， PPTX， HTML\u0026hellip; # TikaDocumentReader 使用 Apache Tika 从各种文档格式中提取文本，例如 PDF、DOC/DOCX、PPT/PPTX 和 HTML。有关受支持格式的完整列表，请参阅 [ Tika 文档]( https://tika.apache.org/3.1.0/formats.html) 。\n依赖 # \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-tika-document-reader\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-tika-document-reader\u0026#39; } 例 # @Component class MyTikaDocumentReader { private final Resource resource; MyTikaDocumentReader(@Value(\u0026#34;classpath:/word-sample.docx\u0026#34;) Resource resource) { this.resource = resource; } List\u0026lt;Document\u0026gt; loadText() { TikaDocumentReader tikaDocumentReader = new TikaDocumentReader(this.resource); return tikaDocumentReader.read(); } } 变形金刚 # TextSplitter 文本拆分器 # TextSplitter 是一个抽象基类，可帮助划分文档以适应 AI 模型的上下文窗口。 `TokenTextSplitter``` 是 TextSplitter`` 的一种实现，它使用 CL100K_BASE 编码根据标记计数将文本拆分为块。\n用法 # @Component class MyTokenTextSplitter { public List\u0026lt;Document\u0026gt; splitDocuments(List\u0026lt;Document\u0026gt; documents) { TokenTextSplitter splitter = new TokenTextSplitter(); return splitter.apply(documents); } public List\u0026lt;Document\u0026gt; splitCustomized(List\u0026lt;Document\u0026gt; documents) { TokenTextSplitter splitter = new TokenTextSplitter(1000, 400, 10, 5000, true); return splitter.apply(documents); } } 构造函数选项 # TokenTextSplitter 提供了两个构造函数选项：\n参数 # defaultChunkSize：标记中每个文本块的目标大小（默认值：800）。 minChunkSizeChars：每个文本块的最小大小（以字符为单位）（默认值：350）。 minChunkLengthToEmbed：要包含的块的最小长度（默认值：5）。 maxNumChunks：要从文本生成的最大块数（默认值：10000）。 keepSeparator：是否在块中保留分隔符（如换行符）（默认值：true）。 行为 # TokenTextSplitter 按如下方式处理文本内容：\n例 # Document doc1 = new Document(\u0026#34;This is a long piece of text that needs to be split into smaller chunks for processing.\u0026#34;, Map.of(\u0026#34;source\u0026#34;, \u0026#34;example.txt\u0026#34;)); Document doc2 = new Document(\u0026#34;Another document with content that will be split based on token count.\u0026#34;, Map.of(\u0026#34;source\u0026#34;, \u0026#34;example2.txt\u0026#34;)); TokenTextSplitter splitter = new TokenTextSplitter(); List\u0026lt;Document\u0026gt; splitDocuments = this.splitter.apply(List.of(this.doc1, this.doc2)); for (Document doc : splitDocuments) { System.out.println(\u0026#34;Chunk: \u0026#34; + doc.getContent()); System.out.println(\u0026#34;Metadata: \u0026#34; + doc.getMetadata()); } 笔记 # TokenTextSplitter 使用 jtokkit 库中的 CL100K_BASE 编码，该库与较新的 OpenAI 模型兼容。 拆分器尝试在可能的情况下通过在句子边界处断开来创建语义上有意义的块。 原始文档中的元数据将被保留并复制到从该文档派生的所有块中。 如果 copyContentFormatter 设置为 true（默认行为），则原始文档中的内容格式化程序（如果已设置）也会复制到派生的块中。 此拆分器对于为具有标记限制的大型语言模型准备文本特别有用，可确保每个块都在模型的处理容量范围内。 ContentFormatTransformer 格式转换器 # 确保所有文档中的内容格式一致。\n关键字元数据扩充器 # KeywordMetadataEnricher 是一个 DocumentTransformer，它使用生成式 AI 模型从文档内容中提取关键字并将其添加为元数据。\n用法 # @Component class MyKeywordEnricher { private final ChatModel chatModel; MyKeywordEnricher(ChatModel chatModel) { this.chatModel = chatModel; } List\u0026lt;Document\u0026gt; enrichDocuments(List\u0026lt;Document\u0026gt; documents) { KeywordMetadataEnricher enricher = new KeywordMetadataEnricher(this.chatModel, 5); return enricher.apply(documents); } } 构造 函数 # KeywordMetadataEnricher 构造函数采用两个参数：\n行为 # KeywordMetadataEnricher 按如下方式处理文档：\n定制 # 可以通过修改类中的 KEYWORDS_TEMPLATE 常量来自定义关键字提取提示。默认模板为：\n\\{context_str}. Give %s unique keywords for this document. Format as comma separated. Keywords: 其中 {context_str} 替换为文档内容，%s 替换为指定的关键字 count。\n例 # ChatModel chatModel = // initialize your chat model KeywordMetadataEnricher enricher = new KeywordMetadataEnricher(chatModel, 5); Document doc = new Document(\u0026#34;This is a document about artificial intelligence and its applications in modern technology.\u0026#34;); List\u0026lt;Document\u0026gt; enrichedDocs = enricher.apply(List.of(this.doc)); Document enrichedDoc = this.enrichedDocs.get(0); String keywords = (String) this.enrichedDoc.getMetadata().get(\u0026#34;excerpt_keywords\u0026#34;); System.out.println(\u0026#34;Extracted keywords: \u0026#34; + keywords); 笔记 # KeywordMetadataEnricher 需要一个正常运行的 ChatModel 来生成关键字。 关键字计数必须为 1 或更大。 扩充器将 “excerpt_keywords” 元数据字段添加到每个已处理的文档。 生成的关键字以逗号分隔的字符串形式返回。 此扩充器对于提高文档的可搜索性以及为文档生成标记或类别特别有用。 摘要元数据 Enricher # SummaryMetadataEnricher 是一个 DocumentTransformer，它使用生成式 AI 模型为文档创建摘要并将其添加为元数据。它可以为当前文档以及相邻文档（上一个和下一个）生成摘要。\n用法 # @Configuration class EnricherConfig { @Bean public SummaryMetadataEnricher summaryMetadata(OpenAiChatModel aiClient) { return new SummaryMetadataEnricher(aiClient, List.of(SummaryType.PREVIOUS, SummaryType.CURRENT, SummaryType.NEXT)); } } @Component class MySummaryEnricher { private final SummaryMetadataEnricher enricher; MySummaryEnricher(SummaryMetadataEnricher enricher) { this.enricher = enricher; } List\u0026lt;Document\u0026gt; enrichDocuments(List\u0026lt;Document\u0026gt; documents) { return this.enricher.apply(documents); } } 构造 函数 # SummaryMetadataEnricher 提供两个构造函数：\n参数 # chatModel：用于生成摘要的 AI 模型。 summaryTypes：一个 SummaryType 枚举值列表，指示要生成的摘要（PREVIOUS、CURRENT、NEXT）。 summaryTemplate：用于生成摘要的自定义模板（可选）。 metadataMode：指定在生成摘要时如何处理文档元数据（可选）。 行为 # SummaryMetadataEnricher 按如下方式处理文档：\n\u0026#34;\u0026#34;\u0026#34; Here is the content of the section: {context_str} Summarize the key topics and entities of the section. Summary: \u0026#34;\u0026#34;\u0026#34; ChatModel chatModel = // initialize your chat model SummaryMetadataEnricher enricher = new SummaryMetadataEnricher(chatModel, List.of(SummaryType.PREVIOUS, SummaryType.CURRENT, SummaryType.NEXT)); Document doc1 = new Document(\u0026#34;Content of document 1\u0026#34;); Document doc2 = new Document(\u0026#34;Content of document 2\u0026#34;); List\u0026lt;Document\u0026gt; enrichedDocs = enricher.apply(List.of(this.doc1, this.doc2)); // Check the metadata of the enriched documents for (Document doc : enrichedDocs) { System.out.println(\u0026#34;Current summary: \u0026#34; + doc.getMetadata().get(\u0026#34;section_summary\u0026#34;)); System.out.println(\u0026#34;Previous summary: \u0026#34; + doc.getMetadata().get(\u0026#34;prev_section_summary\u0026#34;)); System.out.println(\u0026#34;Next summary: \u0026#34; + doc.getMetadata().get(\u0026#34;next_section_summary\u0026#34;)); } @Component class MyDocumentWriter { public void writeDocuments(List\u0026lt;Document\u0026gt; documents) { FileDocumentWriter writer = new FileDocumentWriter(\u0026#34;output.txt\u0026#34;, true, MetadataMode.ALL, false); writer.accept(documents); } } ### Doc: [index], pages:[start_page_number,end_page_number] 这些在编写文档标记时使用。\n例 # List\u0026lt;Document\u0026gt; documents = // initialize your documents FileDocumentWriter writer = new FileDocumentWriter(\u0026#34;output.txt\u0026#34;, true, MetadataMode.ALL, true); writer.accept(documents); 这会使用所有可用的元数据将所有文档写入“output.txt”，包括文档标记，并附加到文件（如果已存在）。\n笔记 # 编写器使用 FileWriter，因此它使用作系统的默认字符编码写入文本文件。 如果在写入过程中发生错误，则会引发 RuntimeException，并将原始异常作为其原因。 metadataMode 参数允许控制如何将现有元数据合并到写入的内容中。 此编写器对于调试或创建文档集合的可读输出特别有用。 矢量存储 # 提供与各种矢量存储的集成。有关完整列表，请参阅 [ Vector DB 文档](vectordbs.html) 。\n"},{"id":6,"href":"/docs/model-context-protocol-mcp/mcp-client-boot-starters/","title":"MCP 客户端启动启动器","section":"模型上下文协议 （MCP）","content":" MCP 客户端启动启动器 # Spring AI MCP（模型上下文协议）客户端引导启动器为 Spring Boot 应用程序中的 MCP 客户端功能提供自动配置。它支持具有各种传输选项的同步和异步客户端实现。 MCP Client Boot Starter 提供：\n首先 # 标准 MCP 客户端 # \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-mcp-client\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 标准启动器通过 STDIO（进程内）和/或 SSE（远程）传输同时连接到一个或多个 MCP 服务器。SSE 连接使用基于 HttpClient 的传输实现。与 MCP 服务器的每次连接都会创建一个新的 MCP 客户端实例。您可以选择 SYNC 或 ASYNC MCP 客户端（注意：您不能混合使用 sync 和 async 客户端）。对于生产部署，我们建议将基于 WebFlux 的 SSE 连接与 spring-ai-starter-mcp-client-webflux .\nWebFlux 客户端 # WebFlux 启动器提供与标准启动器类似的功能，但使用基于 WebFlux 的 SSE 传输实现。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-mcp-client-webflux\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 配置属性 # 通用属性 # 公共属性以 spring.ai.mcp.client 为前缀：\n标准传输属性 # 标准 I/O 传输的属性以 spring.ai.mcp.client.stdio 为前缀： 配置示例：\nspring: ai: mcp: client: stdio: root-change-notification: true connections: server1: command: /path/to/server args: - --port=8080 - --mode=production env: API_KEY: your-api-key DEBUG: \u0026#34;true\u0026#34; 或者，您也可以使用 [ Claude Desktop 格式]( https://modelcontextprotocol.io/quickstart/user)的外部 JSON 文件配置 stdio 连接：\nspring: ai: mcp: client: stdio: servers-configuration: classpath:mcp-servers.json Claude Desktop 格式如下所示：\n{ \u0026#34;mcpServers\u0026#34;: { \u0026#34;filesystem\u0026#34;: { \u0026#34;command\u0026#34;: \u0026#34;npx\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;-y\u0026#34;, \u0026#34;@modelcontextprotocol/server-filesystem\u0026#34;, \u0026#34;/Users/username/Desktop\u0026#34;, \u0026#34;/Users/username/Downloads\u0026#34; ] } } } 目前，Claude Desktop 格式仅支持 STDIO 连接类型。\nSSE 传输属性 # 服务器发送事件 （SSE） 传输的属性以 spring.ai.mcp.client.sse 为前缀： 配置示例：\nspring: ai: mcp: client: sse: connections: server1: url: http://localhost:8080 server2: url: http://otherserver:8081 sse-endpoint: /custom-sse 特征 # 同步/异步客户端类型 # Starter 支持两种类型的客户端：\nSynchronous - 默认客户端类型，适用于具有阻塞作的传统请求-响应模式 异步 - 适用于具有非阻塞作的反应式应用程序，使用 spring.ai.mcp.client.type=ASYNC 客户端定制 # 自动配置通过回调接口提供广泛的客户端规范自定义功能。这些定制器允许您配置 MCP 客户端行为的各个方面，从请求超时到事件处理和消息处理。\n自定义类型 # 以下自定义选项可用：\n请求配置 - 设置自定义请求超时 自定义采样处理程序 - 服务器通过客户端从 LLM 请求 LLM 采样（ 完成或生成 ）的标准化方式。此流程允许客户端保持对模型访问、选择和权限的控制，同时使服务器能够利用 AI 功能，而无需服务器 API 密钥。 文件系统（根）访问 - 客户端向服务器公开文件系统根的标准化方式。根定义了服务器在文件系统中可以运行的位置的边界，使它们能够了解它们可以访问哪些目录和文件。服务器可以从支持客户端请求根列表，并在该列表更改时接收通知。 Event Handlers - 当某个服务器事件发生时通知客户端的处理程序： 工具更改通知 - 当可用服务器工具列表发生更改时 资源更改通知 - 当可用服务器资源列表发生更改时。 提示更改通知 - 当可用服务器列表提示更改时。 日志记录处理程序 - 服务器向客户端发送结构化日志消息的标准化方式。客户端可以通过设置最小日志级别来控制日志记录详细程度 您可以为同步客户端实现 McpSyncClientCustomizer，也可以为异步客户端实现 McpAsyncClientCustomizer，具体取决于应用程序的需要。 serverConfigurationName 参数是定制器所应用到的服务器配置的名称，也是为其创建 MCP 客户端的服务器配置的名称。 MCP 客户端自动配置会自动检测并应用在应用程序上下文中找到的任何定制器。 运输支持 # 自动配置支持多种传输类型：\n标准 I/O （Stdio）（由 spring-ai-starter-mcp-client 激活） SSE HTTP（由 spring-ai-starter-mcp-client 激活） SSE WebFlux（由 spring-ai-starter-mcp-client-webflux 激活） 与 Spring AI 集成 # 启动器可以配置与 Spring AI 的工具执行框架集成的工具回调，从而允许将 MCP 工具用作 AI 交互的一部分。默认情况下，此集成处于启用状态，可以通过设置 spring.ai.mcp.client.toolcallback.enabled=false 属性来禁用。\n使用示例 # 将适当的 starter 依赖项添加到您的项目中，并在 application.properties 或 application.yml 中配置客户端：\nspring: ai: mcp: client: enabled: true name: my-mcp-client version: 1.0.0 request-timeout: 30s type: SYNC # or ASYNC for reactive applications sse: connections: server1: url: http://localhost:8080 server2: url: http://otherserver:8081 stdio: root-change-notification: false connections: server1: command: /path/to/server args: - --port=8080 - --mode=production env: API_KEY: your-api-key DEBUG: \u0026#34;true\u0026#34; MCP 客户端 bean 将自动配置并可供注入：\n@Autowired private List\u0026lt;McpSyncClient\u0026gt; mcpSyncClients; // For sync client // OR @Autowired private List\u0026lt;McpAsyncClient\u0026gt; mcpAsyncClients; // For async client 启用工具回调（默认行为）后，所有 MCP 客户端的已注册 MCP 工具将作为 ToolCallbackProvider 实例提供：\n@Autowired private SyncMcpToolCallbackProvider toolCallbackProvider; ToolCallback[] toolCallbacks = toolCallbackProvider.getToolCallbacks(); 示例应用 # Brave Web Search Chatbot - 使用模型上下文协议与 Web 搜索服务器交互的聊天机器人。 默认 MCP 客户端启动器 - 使用默认 spring-ai-starter-mcp-client MCP 客户端引导启动器的简单示例。 WebFlux MCP Client Starter - 使用 spring-ai-starter-mcp-client-webflux MCP Client Boot Starter 的简单示例。 其他资源 # Spring AI 文档 Model 上下文协议规范 Spring Boot 自动配置 "},{"id":7,"href":"/docs/models/audio-models/text-to-speech-tts-api/openai/","title":"OpenAI 文本转语音 （TTS）","section":"文本转语音 （TTS） API","content":" OpenAI 文本转语音 （TTS） # 介绍 # 音频 API 提供基于 OpenAI 的 TTS（文本到语音转换）模型的语音终端节点，使用户能够：\n讲述一篇书面博客文章。 生成多种语言的语音音频。 使用流提供实时音频输出。 先决条件 # 自动配置 # Spring AI 为 OpenAI 文本转语音客户端提供 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或复制到您的 Gradle build.gradle 构建文件：\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-openai\u0026#39; } 语音属性 # 连接属性 # 前缀 spring.ai.openai 用作允许您连接到 OpenAI 的属性前缀。\n配置属性 # 前缀 spring.ai.openai.audio.speech 用作属性前缀，允许您配置 OpenAI Text-to-Speech 客户端。\n运行时选项 # OpenAiAudioSpeechOptions 类提供了在发出文本转语音请求时要使用的选项。在启动时，将使用 spring.ai.openai.audio.speech 指定的选项，但你可以在运行时覆盖这些选项。 例如：\nOpenAiAudioSpeechOptions speechOptions = OpenAiAudioSpeechOptions.builder() .model(\u0026#34;tts-1\u0026#34;) .voice(OpenAiAudioApi.SpeechRequest.Voice.ALLOY) .responseFormat(OpenAiAudioApi.SpeechRequest.AudioResponseFormat.MP3) .speed(1.0f) .build(); SpeechPrompt speechPrompt = new SpeechPrompt(\u0026#34;Hello, this is a text-to-speech example.\u0026#34;, speechOptions); SpeechResponse response = openAiAudioSpeechModel.call(speechPrompt); 手动配置 # 将 spring-ai-openai 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或复制到您的 Gradle build.gradle 构建文件：\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-openai\u0026#39; } 接下来，创建一个 OpenAiAudioSpeechModel：\nvar openAiAudioApi = new OpenAiAudioApi() .apiKey(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;)) .build(); var openAiAudioSpeechModel = new OpenAiAudioSpeechModel(openAiAudioApi); var speechOptions = OpenAiAudioSpeechOptions.builder() .responseFormat(OpenAiAudioApi.SpeechRequest.AudioResponseFormat.MP3) .speed(1.0f) .model(OpenAiAudioApi.TtsModel.TTS_1.value) .build(); var speechPrompt = new SpeechPrompt(\u0026#34;Hello, this is a text-to-speech example.\u0026#34;, speechOptions); SpeechResponse response = openAiAudioSpeechModel.call(speechPrompt); // Accessing metadata (rate limit info) OpenAiAudioSpeechResponseMetadata metadata = response.getMetadata(); byte[] responseAsBytes = response.getResult().getOutput(); 流式传输实时音频 # 语音 API 支持使用块传输编码进行实时音频流式处理。这意味着可以在生成完整文件并使其可访问之前播放音频。\nvar openAiAudioApi = new OpenAiAudioApi() .apiKey(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;)) .build(); var openAiAudioSpeechModel = new OpenAiAudioSpeechModel(openAiAudioApi); OpenAiAudioSpeechOptions speechOptions = OpenAiAudioSpeechOptions.builder() .voice(OpenAiAudioApi.SpeechRequest.Voice.ALLOY) .speed(1.0f) .responseFormat(OpenAiAudioApi.SpeechRequest.AudioResponseFormat.MP3) .model(OpenAiAudioApi.TtsModel.TTS_1.value) .build(); SpeechPrompt speechPrompt = new SpeechPrompt(\u0026#34;Today is a wonderful day to build something people love!\u0026#34;, speechOptions); Flux\u0026lt;SpeechResponse\u0026gt; responseStream = openAiAudioSpeechModel.stream(speechPrompt); 示例代码 # OpenAiSpeechModelIT.java 测试提供了一些有关如何使用该库的一般示例。 "},{"id":8,"href":"/docs/models/chat-models/google-vertexai/vertexai-gemini/","title":"VertexAI Gemini 聊天","section":"谷歌 VertexAI API","content":" VertexAI Gemini 聊天 # [ Vertex AI Gemini API]( https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/overview) 允许开发人员使用 Gemini 模型构建生成式 AI 应用程序。[ Vertex AI Gemini API]( https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/overview) 支持将多模态提示作为输入和输出文本或代码。多模态模型是一种能够处理来自多种模态的信息（包括图像、视频和文本）的模型。例如，您可以向模型发送一盘饼干的照片，并要求它为您提供这些饼干的配方。 Gemini 是由 Google DeepMind 开发的一系列生成式 AI 模型，专为多模式用例而设计。Gemini API 允许您访问 [ Gemini 2.0 Flash]( https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-0-flash) 和 [ Gemini 2.0 Flash]( https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-0-flash)-Lite。有关 Vertex AI Gemini API 模型的规格，请参阅[ 模型信息]( https://cloud.google.com/vertex-ai/generative-ai/docs/models#gemini-models) 。 Gemini API 参考\n先决条件 # 安装适合您作系统的 gcloud CLI。 通过运行以下命令进行身份验证。将 PROJECT_ID 替换为您的 Google Cloud 项目 ID，将 ACCOUNT 替换为您的 Google Cloud 用户名。 gcloud config set project \u0026lt;PROJECT_ID\u0026gt; \u0026amp;\u0026amp; gcloud auth application-default login \u0026lt;ACCOUNT\u0026gt; 自动配置 # Spring AI 为 VertexAI Gemini Chat 客户端提供 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 或 Gradle build.gradle 构建文件中：\n聊天属性 # 前缀 spring.ai.vertex.ai.gemini 用作属性前缀，可让您连接到 VertexAI。 前缀 spring.ai.vertex.ai.gemini.chat 是属性前缀，允许您为 VertexAI Gemini Chat 配置聊天模型实施。\n运行时选项 # [ VertexAiGeminiChatOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-vertex-ai-gemini/src/main/java/org/springframework/ai/vertexai/gemini/[VertexAiGeminiChatOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-vertex-ai-gemini/src/main/java/org/springframework/ai/vertexai/gemini/VertexAiGeminiChatOptions.java)) 提供模型配置，例如温度、topK 等。 启动时，可以使用 VertexAiGeminiChatModel(api, options) constructor 或 spring.ai.vertex.ai.chat.options.* properties 配置默认选项。 在运行时，您可以通过向 Prompt 调用添加新的、特定于请求的选项来覆盖默认选项。例如，要覆盖特定请求的默认温度：\nChatResponse response = chatModel.call( new Prompt( \u0026#34;Generate the names of 5 famous pirates.\u0026#34;, VertexAiGeminiChatOptions.builder() .temperature(0.4) .build() )); 工具调用 # Vertex AI Gemini 模型支持工具调用（在 Google Gemini 上下文中称为函数调用 ）功能，允许模型在对话期间使用工具。以下是如何定义和使用基于 @Tool 的工具的示例：\npublic class WeatherService { @Tool(description = \u0026#34;Get the weather in location\u0026#34;) public String weatherByLocation(@ToolParam(description= \u0026#34;City or state name\u0026#34;) String location) { ... } } String response = ChatClient.create(this.chatModel) .prompt(\u0026#34;What\u0026#39;s the weather like in Boston?\u0026#34;) .tools(new WeatherService()) .call() .content(); 您也可以将 java.util.function bean 用作工具：\n@Bean @Description(\u0026#34;Get the weather in location. Return temperature in 36°F or 36°C format.\u0026#34;) public Function\u0026lt;Request, Response\u0026gt; weatherFunction() { return new MockWeatherService(); } String response = ChatClient.create(this.chatModel) .prompt(\u0026#34;What\u0026#39;s the weather like in Boston?\u0026#34;) .tools(\u0026#34;weatherFunction\u0026#34;) .inputType(Request.class) .call() .content(); 在[ 工具](../tools.html)文档中查找更多信息。\n模 态 # 多模态是指模型同时理解和处理来自各种（输入）源的信息的能力，包括文本 、pdf、 图像 、 音频和其他数据格式。\n图像、音频、视频 # Google 的 Gemini AI 模型通过理解和集成文本、代码、音频、图像和视频来支持此功能。有关更多详细信息，请参阅博客文章 [ Gemini 简介]( https://blog.google/technology/ai/google-gemini-ai/#introducing-gemini) 。 Spring AI 的 Message 接口通过引入 Media 类型来支持多模态 AI 模型。此类型包含有关消息中媒体附件的数据和信息，使用 Spring org.springframework.util.MimeType 和 java.lang.Object 作为原始媒体数据。 下面是一个从 [ VertexAiGeminiChatModelIT#multiModalityTest（）]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-vertex-ai-gemini/src/test/java/org/springframework/ai/vertexai/gemini/VertexAiGeminiChatModelIT.java) 中提取的简单代码示例，演示了用户文本与图像的组合。\nbyte[] data = new ClassPathResource(\u0026#34;/vertex-test.png\u0026#34;).getContentAsByteArray(); var userMessage = new UserMessage(\u0026#34;Explain what do you see on this picture?\u0026#34;, List.of(new Media(MimeTypeUtils.IMAGE_PNG, this.data))); ChatResponse response = chatModel.call(new Prompt(List.of(this.userMessage))); PDF 格式 # 最新的 Vertex Gemini 提供对 PDF 输入类型的支持..使用 application/pdf 媒体类型将 PDF 文件附加到消息中：\nvar pdfData = new ClassPathResource(\u0026#34;/spring-ai-reference-overview.pdf\u0026#34;); var userMessage = new UserMessage( \u0026#34;You are a very professional document summarization specialist. Please summarize the given document.\u0026#34;, List.of(new Media(new MimeType(\u0026#34;application\u0026#34;, \u0026#34;pdf\u0026#34;), pdfData))); var response = this.chatModel.call(new Prompt(List.of(userMessage))); 样品控制器 # [ 创建一个新]( https://start.spring.io/)的 Spring Boot 项目，并将 添加到您的 spring-ai-starter-model-vertex-ai-gemini pom（或 gradle）依赖项中。 在 src/main/resources 目录下添加一个 application.properties 文件，以启用和配置 VertexAi 聊天模型：\nspring.ai.vertex.ai.gemini.project-id=PROJECT_ID spring.ai.vertex.ai.gemini.location=LOCATION spring.ai.vertex.ai.gemini.chat.options.model=gemini-2.0-flash spring.ai.vertex.ai.gemini.chat.options.temperature=0.5 这将创建一个 VertexAiGeminiChatModel 实现，您可以将其注入到您的类中。下面是一个简单的 @Controller 类示例，该类使用 chat 模型生成文本。\n@RestController public class ChatController { private final VertexAiGeminiChatModel chatModel; @Autowired public ChatController(VertexAiGeminiChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, this.chatModel.call(message)); } @GetMapping(\u0026#34;/ai/generateStream\u0026#34;) public Flux\u0026lt;ChatResponse\u0026gt; generateStream(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { Prompt prompt = new Prompt(new UserMessage(message)); return this.chatModel.stream(prompt); } } 手动配置 # VertexAiGeminiChatModel 实现了 ChatModel，并使用 VertexAI 连接了 Vertex AI Gemini 服务。 将 spring-ai-vertex-ai-gemini 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-vertex-ai-gemini\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-vertex-ai-gemini\u0026#39; } 接下来，创建一个 VertexAiGeminiChatModel 并将其用于文本生成：\nVertexAI vertexApi = new VertexAI(projectId, location); var chatModel = new VertexAiGeminiChatModel(this.vertexApi, VertexAiGeminiChatOptions.builder() .model(ChatModel.GEMINI_2_0_FLASH) .temperature(0.4) .build()); ChatResponse response = this.chatModel.call( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); VertexAiGeminiChatOptions 提供聊天请求的配置信息。这是 VertexAiGeminiChatOptions.Builder fluent 选项构建器。\n低级 Java 客户端 # 以下类图说明了 Vertex AI Gemini 原生 Java API： "},{"id":9,"href":"/docs/models/embedding-models/amazon-bedrock/","title":"亚马逊基岩版","section":"嵌入模型 API","content":" 亚马逊基岩版 # [ Amazon Bedrock]( https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html) 是一项托管服务，提供来自各种 AI 提供商的基础模型，可通过统一的 API 使用。 Spring AI 通过实现 Spring EmbeddingModel 接口来支持通过 Amazon Bedrock 提供的 [ Embedding AI 模型]( https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids-arns.html) 。 此外，Spring AI 为所有客户端提供 Spring 自动配置和引导启动器，从而可以轻松引导和配置 Bedrock 模型。\n开始 # 有几个步骤可以开始\n将 Spring Boot starter for Bedrock 添加到您的项目中。 获取 AWS 凭证：如果您还没有配置 AWS 账户和 AWS CLI，此视频指南可以帮助您配置它：AWS CLI 和 SDK 设置在不到 4 分钟的时间内，只需不到 4 分钟）。 您应该能够获取您的访问密钥和安全密钥。 启用要使用的模型：转到 Amazon Bedrock，然后从左侧的 Model Access（模型访问 ）菜单中，配置对要使用的模型的访问。 项目依赖关系 # 然后将 Spring Boot Starter 依赖项添加到项目的 Maven pom.xml 构建文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-bedrock\u0026lt;/artifactId\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-bedrock\u0026#39; } 连接到 AWS Bedrock # 使用 BedrockAwsConnectionProperties 配置 AWS 凭证和区域：\nspring.ai.bedrock.aws.region=us-east-1 spring.ai.bedrock.aws.access-key=YOUR_ACCESS_KEY spring.ai.bedrock.aws.secret-key=YOUR_SECRET_KEY spring.ai.bedrock.aws.timeout=10m region 属性是必需的。 AWS 凭证按以下顺序解析： AWS 区域按以下顺序解析： 除了标准的 Spring-AI Bedrock 凭证和区域属性配置之外，Spring-AI 还提供对自定义 AwsCredentialsProvider 和 AwsRegionProvider bean 的支持。\n启用选定的 Bedrock 模型 # 以下是支持的 \u0026lsquo;\u0026lsquo;s： 例如，要启用 Bedrock Cohere 嵌入模型，您需要将 spring.ai.bedrock.cohere.embedding.enabled=true . 接下来，您可以使用属性 spring.ai.bedrock.\u0026lt;model\u0026gt;.embedding.* 按提供的方式配置每个模型。 有关更多信息，请参阅以下文档，了解每个受支持的型号。\nSpring AI Bedrock Cohere 嵌入 ： spring.ai.bedrock.cohere.embedding.enabled=true Spring AI Bedrock Titan 嵌入 ： spring.ai.bedrock.titan.embedding.enabled=true "},{"id":10,"href":"/docs/overview-%E6%A6%82%E8%BF%B0/","title":"介绍","section":"Docs","content":" 介绍 # Spring AI 项目旨在简化包含人工智能功能的应用程序的开发，而不会产生不必要的复杂性。 该项目从著名的 Python 项目（如 LangChain 和 LlamaIndex）中汲取灵感，但 Spring AI 并不是这些项目的直接移植。该项目的成立理念是，下一波生成式 AI 应用程序将不仅适用于 Python 开发人员，而且将在许多编程语言中无处不在。 Spring AI 提供了抽象，作为开发 AI 应用程序的基础。这些抽象具有多种实现，支持以最少的代码更改轻松交换组件。 Spring AI 提供以下功能：\n跨 AI 提供商的可移植 API 支持，用于聊天、文本到图像和嵌入模型。支持同步和流式处理 API 选项。此外，还可以访问特定于模型的特征。 支持所有主要的 AI 模型提供商 ，例如 Anthropic、OpenAI、Microsoft、Amazon、Google 和 Ollama。支持的模型类型包括： 聊天完成 嵌入 文本到图像 音频转录 文本到语音 适度 结构化输出 - AI 模型输出到 POJO 的映射。 支持所有主要的矢量数据库提供商 ，例如 Apache Cassandra、Azure Cosmos DB、Azure Vector Search、Chroma、Elasticsearch、GemFire、MariaDB、Milvus、MongoDB Atlas、Neo4j、OpenSearch、Oracle、PostgreSQL/PGVector、PineCone、Qdrant、Redis、SAP Hana、Typesense 和 Weaviate。 跨 Vector Store 提供商的可移植 API，包括新颖的类似 SQL 的元数据过滤器 API。 工具/函数调用 - 允许模型请求执行客户端工具和函数，从而根据需要访问必要的实时信息并采取措施。 可观察性 - 提供对 AI 相关作的见解。 数据工程的文档摄取 ETL 框架 。 AI 模型评估 - 帮助评估生成的内容并防止幻觉响应的实用程序。 AI 模型和向量存储的 Spring Boot 自动配置和启动器。 ChatClient API - 用于与 AI 聊天模型通信的 Fluent API，惯用性类似于 WebClient 和 RestClient API。 Advisors API - 封装重复的生成式 AI 模式，转换发送到和传出语言模型 （LLM） 的数据，并提供跨各种模型和用例的可移植性。 支持 Chat Conversation Memory and Retrieval Augmented Generation （RAG）。 此功能集允许您实施常见使用案例，例如“就您的文档进行问答”或“与您的文档聊天”。 [ 概念部分](concepts.html)提供了 AI 概念及其在 Spring AI 中的表示的高级概述。 [ 入门](getting-started.html)部分介绍如何创建您的第一个 AI 应用程序。后续部分将采用以代码为中心的方法深入研究每个组件和常见使用案例。 "},{"id":11,"href":"/docs/upgrade-notes-%E5%8D%87%E7%BA%A7%E8%AF%B4%E6%98%8E/migrating-functioncallback-to-toolcallback-api/","title":"从 FunctionCallback 迁移到 ToolCallback API","section":"升级说明","content":" 从 FunctionCallback 迁移到 ToolCallback API # 本指南可帮助您从已弃用的 FunctionCallback API 迁移到 Spring AI 中的新 ToolCallback API。有关新 API 的更多信息，请查看 [ Tools Calling](tools.html) 文档。\n变更概述 # 这些更改是改进和扩展 Spring AI 中的工具调用功能的更广泛努力的一部分。除其他外，新 API 从 “functions” 改为 “tools” 术语，以更好地与行业惯例保持一致。这涉及多项 API 更改，同时通过已弃用的方法保持向后兼容性。\n主要变化 # 迁移示例 # 1. 基本函数回调 # 以前：\nFunctionCallback.builder() .function(\u0026#34;getCurrentWeather\u0026#34;, new MockWeatherService()) .description(\u0026#34;Get the weather in location\u0026#34;) .inputType(MockWeatherService.Request.class) .build() 后：\nFunctionToolCallback.builder(\u0026#34;getCurrentWeather\u0026#34;, new MockWeatherService()) .description(\u0026#34;Get the weather in location\u0026#34;) .inputType(MockWeatherService.Request.class) .build() 2. ChatClient 使用情况 # 以前：\nString response = ChatClient.create(chatModel) .prompt() .user(\u0026#34;What\u0026#39;s the weather like in San Francisco?\u0026#34;) .functions(FunctionCallback.builder() .function(\u0026#34;getCurrentWeather\u0026#34;, new MockWeatherService()) .description(\u0026#34;Get the weather in location\u0026#34;) .inputType(MockWeatherService.Request.class) .build()) .call() .content(); 后：\nString response = ChatClient.create(chatModel) .prompt() .user(\u0026#34;What\u0026#39;s the weather like in San Francisco?\u0026#34;) .tools(FunctionToolCallback.builder(\u0026#34;getCurrentWeather\u0026#34;, new MockWeatherService()) .description(\u0026#34;Get the weather in location\u0026#34;) .inputType(MockWeatherService.Request.class) .build()) .call() .content(); 3. 基于方法的函数回调 # 以前：\nFunctionCallback.builder() .method(\u0026#34;getWeatherInLocation\u0026#34;, String.class, Unit.class) .description(\u0026#34;Get the weather in location\u0026#34;) .targetClass(TestFunctionClass.class) .build() 后：\nvar toolMethod = ReflectionUtils.findMethod(TestFunctionClass.class, \u0026#34;getWeatherInLocation\u0026#34;); MethodToolCallback.builder() .toolDefinition(ToolDefinition.builder(toolMethod) .description(\u0026#34;Get the weather in location\u0026#34;) .build()) .toolMethod(toolMethod) .build() 或者使用声明式方法：\nclass WeatherTools { @Tool(description = \u0026#34;Get the weather in location\u0026#34;) public void getWeatherInLocation(String location, Unit unit) { // ... } } 您可以使用相同的 ChatClient#tools（） API 来注册基于方法的工具回调：\nString response = ChatClient.create(chatModel) .prompt() .user(\u0026#34;What\u0026#39;s the weather like in San Francisco?\u0026#34;) .tools(MethodToolCallback.builder() .toolDefinition(ToolDefinition.builder(toolMethod) .description(\u0026#34;Get the weather in location\u0026#34;) .build()) .toolMethod(toolMethod) .build()) .call() .content(); 或者使用声明式方法：\nString response = ChatClient.create(chatModel) .prompt() .user(\u0026#34;What\u0026#39;s the weather like in San Francisco?\u0026#34;) .tools(new WeatherTools()) .call() .content(); 4. 选项配置 # 以前：\nFunctionCallingOptions.builder() .model(modelName) .function(\u0026#34;weatherFunction\u0026#34;) .build() 后：\nToolCallingChatOptions.builder() .model(modelName) .toolNames(\u0026#34;weatherFunction\u0026#34;) .build() 5. ChatClient Builder 中的默认函数 # 以前：\nChatClient.builder(chatModel) .defaultFunctions(FunctionCallback.builder() .function(\u0026#34;getCurrentWeather\u0026#34;, new MockWeatherService()) .description(\u0026#34;Get the weather in location\u0026#34;) .inputType(MockWeatherService.Request.class) .build()) .build() 后：\nChatClient.builder(chatModel) .defaultTools(FunctionToolCallback.builder(\u0026#34;getCurrentWeather\u0026#34;, new MockWeatherService()) .description(\u0026#34;Get the weather in location\u0026#34;) .inputType(MockWeatherService.Request.class) .build()) .build() 6. Spring Bean 配置 # 以前：\n@Bean public FunctionCallback weatherFunctionInfo() { return FunctionCallback.builder() .function(\u0026#34;WeatherInfo\u0026#34;, new MockWeatherService()) .description(\u0026#34;Get the current weather\u0026#34;) .inputType(MockWeatherService.Request.class) .build(); } 后：\n@Bean public ToolCallback weatherFunctionInfo() { return FunctionToolCallback.builder(\u0026#34;WeatherInfo\u0026#34;, new MockWeatherService()) .description(\u0026#34;Get the current weather\u0026#34;) .inputType(MockWeatherService.Request.class) .build(); } 重大更改 # 已弃用的方法 # 以下方法已弃用，并将在未来发行版中删除： 请改用他们的工具 。\n带有 @Tool 的声明性规范 # 现在，您可以使用方法级注释 （@Tool） 向 Spring AI 注册工具：\nclass Home { @Tool(description = \u0026#34;Turn light On or Off in a room.\u0026#34;) void turnLight(String roomName, boolean on) { // ... logger.info(\u0026#34;Turn light in room: {} to: {}\u0026#34;, roomName, on); } } String response = ChatClient.create(this.chatModel).prompt() .user(\u0026#34;Turn the light in the living room On.\u0026#34;) .tools(new Home()) .call() .content(); 其他说明 # 时间线 # 已弃用的方法将在当前里程碑版本中保留以实现向后兼容性，但将在下一个里程碑版本中删除。建议尽快迁移到新的 API。\n"},{"id":12,"href":"/docs/models/chat-models/","title":"聊天模型 API","section":"None","content":" 聊天模型 API # 聊天模型 API 使开发人员能够将 AI 驱动的聊天完成功能集成到其应用程序中。它利用预先训练的语言模型，例如 GPT （Generative Pre-trained Transformer），以自然语言生成对用户输入的类似人类的响应。 API 的工作原理通常是向 AI 模型发送提示或部分对话，然后 AI 模型根据其训练数据和对自然语言模式的理解生成对话的完成或延续。然后，完成的响应将返回给应用程序，应用程序可以将其呈现给用户或将其用于进一步处理。 Spring AI Chat 模型 API 旨在成为一个简单且可移植的接口，用于与各种 [ AI 模型](../concepts.html#_models)交互，允许开发人员以最少的代码更改在不同模型之间切换。这种设计与 Spring 的模块化和可互换性理念一致。 此外，在用于输入封装的 Prompt 和用于输出处理的 ChatResponse 等配套类的帮助下，聊天模型 API 统一了与 AI 模型的通信。它管理请求准备和响应解析的复杂性，提供直接且简化的 API 交互。 您可以在 [ 可用实施](#_available_implementations) 部分找到有关[ 可用实施](#_available_implementations)的更多信息，并在 [ 聊天模型比较](chat/comparison.html) 部分找到详细比较。\nAPI 概述 # 本节提供了 Spring AI Chat 模型 API 接口和相关类的指南。\n聊天模型 # 以下是 [ ChatModel]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-client-chat/src/main/java/org/springframework/ai/chat//model/[ChatModel](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-client-chat/src/main/java/org/springframework/ai/chat//model/ChatModel.java).java) 接口定义：\npublic interface ChatModel extends Model\u0026lt;Prompt, ChatResponse\u0026gt; { default String call(String message) {...} @Override ChatResponse call(Prompt prompt); } 带有 String 参数的 call（） 方法简化了初始使用，避免了更复杂的 Prompt 和 ChatResponse 类的复杂性。在实际应用程序中，更常见的是使用 call（） 方法，该方法采用 Prompt 实例并返回 ChatResponse。\nStreamingChatModel （流式聊天模型） # 下面是 [ StreamingChatModel]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/model/[StreamingChatModel](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/model/StreamingChatModel.java).java) 接口定义：\npublic interface StreamingChatModel extends StreamingModel\u0026lt;Prompt, ChatResponse\u0026gt; { default Flux\u0026lt;String\u0026gt; stream(String message) {...} @Override Flux\u0026lt;ChatResponse\u0026gt; stream(Prompt prompt); } stream（） 方法采用类似于 ChatModel 的 String 或 Prompt 参数，但它使用反应式 Flux API 对响应进行流式处理。\n提示 # [[Prompt](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-client-chat/src/main/java/org/springframework/ai/chat/prompt/Prompt.java)](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-client-chat/src/main/java/org/springframework/ai/chat/prompt/[Prompt](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-client-chat/src/main/java/org/springframework/ai/chat/prompt/Prompt.java).java) 是一个 ModelRequest，它封装了 [ Message]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/messages/[Message](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/messages/Message.java).java) 对象列表和可选的模型请求选项。下面的清单显示了 [[Prompt](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-client-chat/src/main/java/org/springframework/ai/chat/prompt/Prompt.java)](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-client-chat/src/main/java/org/springframework/ai/chat/prompt/[Prompt](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-client-chat/src/main/java/org/springframework/ai/chat/prompt/Prompt.java).java) 类的截断版本，不包括构造函数和其他实用程序方法：\npublic class Prompt implements ModelRequest\u0026lt;List\u0026lt;Message\u0026gt;\u0026gt; { private final List\u0026lt;Message\u0026gt; messages; private ChatOptions modelOptions; @Override public ChatOptions getOptions() {...} @Override public List\u0026lt;Message\u0026gt; getInstructions() {...} // constructors and utility methods omitted } 消息 # Message 接口封装了 Prompt 文本内容、元数据属性的集合和称为 MessageType 的分类。 接口定义如下：\npublic interface Content { String getText(); Map\u0026lt;String, Object\u0026gt; getMetadata(); } public interface Message extends Content { MessageType getMessageType(); } 多模式消息类型还实现了 ```MediaContent` 接口，该接口提供了 Media`` 内容对象列表。\npublic interface MediaContent extends Content { Collection\u0026lt;Media\u0026gt; getMedia(); } Message 接口具有各种实现，这些实现对应于 AI 模型可以处理的消息类别： 聊天完成终端节点，根据对话角色区分消息类别，由 MessageType 有效映射。 例如，OpenAI 可以识别不同对话角色的消息类别，例如系统 、 用户 、 功能或助手 。 虽然术语 MessageType 可能意味着特定的消息格式，但在此上下文中，它有效地指定了消息在对话中扮演的角色。 对于不使用特定角色的 AI 模型，`UserMessage``` 实现充当标准类别，通常表示用户生成的查询或说明。要了解实际应用以及 Prompt和Message之间的关系，尤其是在这些角色或消息类别的上下文中，请参阅Prompt``s 部分中的详细说明。\n聊天选项 # 表示可以传递给 AI 模型的选项。ChatOptions 类是 ModelOptions 的子类，用于定义可传递给 AI 模型的几个可移植选项。ChatOptions 类定义如下：\npublic interface ChatOptions extends ModelOptions { String getModel(); Float getFrequencyPenalty(); Integer getMaxTokens(); Float getPresencePenalty(); List\u0026lt;String\u0026gt; getStopSequences(); Float getTemperature(); Integer getTopK(); Float getTopP(); ChatOptions copy(); } 此外，每个特定于模型的 ChatModel/StreamingChatModel 实现都可以有自己的选项，这些选项可以传递给 AI 模型。例如，OpenAI 聊天完成模型有自己的选项，如 logitBias、seed 和 user。 这是一项强大的功能，允许开发人员在启动应用程序时使用特定于模型的选项，然后在运行时使用 Prompt 请求覆盖这些选项。 Spring AI 提供了一个复杂的系统来配置和使用聊天模型。它允许在启动时设置默认配置，同时还提供了根据每个请求覆盖这些设置的灵活性。这种方法使开发人员能够轻松使用不同的 AI 模型并根据需要调整参数，所有这些都在 Spring AI 框架提供的一致界面中完成。 以程图说明了 Spring AI 如何处理聊天模型的配置和执行，并结合了启动和运行时选项： 启动和运行时选项的分离允许全局配置和特定于请求的调整。\n聊天响应 # ChatResponse 类的结构如下：\npublic class ChatResponse implements ModelResponse\u0026lt;Generation\u0026gt; { private final ChatResponseMetadata chatResponseMetadata; private final List\u0026lt;Generation\u0026gt; generations; @Override public ChatResponseMetadata getMetadata() {...} @Override public List\u0026lt;Generation\u0026gt; getResults() {...} // other methods omitted } [ ChatResponse]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/model/[ChatResponse](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/model/ChatResponse.java).java) 类保存 AI 模型的输出，每个 Generation 实例都包含单个提示可能产生的多个输出之一。 ChatResponse 类还携带有关 AI 模型响应的 ChatResponseMetadata 元数据。\n代 # 最后，[ Generation]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/model/[Generation](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/model/Generation.java).java) 类从 ModelResult 扩展来表示模型输出（助手消息）和相关元数据：\npublic class Generation implements ModelResult\u0026lt;AssistantMessage\u0026gt; { private final AssistantMessage assistantMessage; private ChatGenerationMetadata chatGenerationMetadata; @Override public AssistantMessage getOutput() {...} @Override public ChatGenerationMetadata getMetadata() {...} // other methods omitted } 可用的实现 # 此图说明了统一接口 ChatModel 和 StreamingChatModel，用于与来自不同提供商的各种 AI 聊天模型进行交互，从而允许在不同的 AI 服务之间轻松集成和切换，同时为客户端应用程序保持一致的 API。 OpenAI 聊天完成 （流媒体、多模式和函数调用支持） Microsoft Azure Open AI Chat Completion（支持直播和函数调用） Ollama 聊天完成 （流媒体、多模态和函数调用支持） Hugging Face Chat 完成 （不支持流式传输） Google Vertex AI Gemini 聊天完成功能（支持流式、多模态和函数调用） 亚马逊基岩版 Mistral AI 聊天完成 （支持流媒体和函数调用） Anthropic Chat Completion （流媒体和函数调用支持） 聊天模型 API # Spring AI Chat 模型 API 构建在 Spring AI 通用模型 API 之上，提供特定于 Chat 的抽象和实现。这允许在不同 AI 服务之间轻松集成和切换，同时为客户端应用程序保持一致的 API。下面的类图说明了 Spring AI Chat 模型 API 的主要类和接口。 "},{"id":13,"href":"/docs/models/chat-models/chat-models-comparison/","title":"聊天模型比较","section":"聊天模型 API","content":" 聊天模型比较 # 下表比较了 Spring AI 支持的各种聊天模型，详细介绍了它们的功能：\n多模态 ：模型可以处理的输入类型（例如，文本、图像、音频、视频）。 Tools/Function Calling：模型是否支持函数调用或工具使用。 Streaming：如果模型提供 Streaming 响应。 Retry：支持重试机制。 可观测性 ：用于监视和调试的功能。 内置 JSON： 对 JSON 输出的原生支持。 本地部署：模型是否可以在本地运行。 OpenAI API 兼容性：如果模型与 OpenAI 的 API 兼容。 "},{"id":14,"href":"/docs/models/audio-models/transcription-api/","title":"转录 API","section":"None","content":" 转录 API # Spring AI 为 OpenAI 的 Transcription API 提供支持。当实施其他 Transcription 提供程序时，将提取一个通用的 AudioTranscriptionModel 接口。\n"},{"id":15,"href":"/docs/models/moderation-models/openai/","title":"适度","section":"None","content":" 适度 # 介绍 # Spring AI 支持 OpenAI 的审核模型，该模型允许您检测文本中可能有害或敏感的内容。请按照[ 本指南]( https://platform.openai.com/docs/guides/moderation)了解有关 OpenAI 审核模型的更多信息。\n先决条件 # 自动配置 # Spring AI 为 OpenAI 审核模型提供 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或复制到您的 Gradle build.gradle 构建文件：\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-openai\u0026#39; } 审核属性 # 连接属性 # 前缀 spring.ai.openai 用作允许您连接到 OpenAI 的属性前缀。\n配置属性 # 前缀 spring.ai.openai.moderation 用作配置 OpenAI 审核模型的属性前缀。\n运行时选项 # OpenAiModerationOptions 类提供了在发出审核请求时要使用的选项。在启动时，将使用 spring.ai.openai.moderation 指定的选项，但您可以在运行时覆盖这些选项。 例如：\nOpenAiModerationOptions moderationOptions = OpenAiModerationOptions.builder() .model(\u0026#34;text-moderation-latest\u0026#34;) .build(); ModerationPrompt moderationPrompt = new ModerationPrompt(\u0026#34;Text to be moderated\u0026#34;, this.moderationOptions); ModerationResponse response = openAiModerationModel.call(this.moderationPrompt); // Access the moderation results Moderation moderation = moderationResponse.getResult().getOutput(); // Print general information System.out.println(\u0026#34;Moderation ID: \u0026#34; + moderation.getId()); System.out.println(\u0026#34;Model used: \u0026#34; + moderation.getModel()); // Access the moderation results (there\u0026#39;s usually only one, but it\u0026#39;s a list) for (ModerationResult result : moderation.getResults()) { System.out.println(\u0026#34;\\nModeration Result:\u0026#34;); System.out.println(\u0026#34;Flagged: \u0026#34; + result.isFlagged()); // Access categories Categories categories = this.result.getCategories(); System.out.println(\u0026#34;\\nCategories:\u0026#34;); System.out.println(\u0026#34;Sexual: \u0026#34; + categories.isSexual()); System.out.println(\u0026#34;Hate: \u0026#34; + categories.isHate()); System.out.println(\u0026#34;Harassment: \u0026#34; + categories.isHarassment()); System.out.println(\u0026#34;Self-Harm: \u0026#34; + categories.isSelfHarm()); System.out.println(\u0026#34;Sexual/Minors: \u0026#34; + categories.isSexualMinors()); System.out.println(\u0026#34;Hate/Threatening: \u0026#34; + categories.isHateThreatening()); System.out.println(\u0026#34;Violence/Graphic: \u0026#34; + categories.isViolenceGraphic()); System.out.println(\u0026#34;Self-Harm/Intent: \u0026#34; + categories.isSelfHarmIntent()); System.out.println(\u0026#34;Self-Harm/Instructions: \u0026#34; + categories.isSelfHarmInstructions()); System.out.println(\u0026#34;Harassment/Threatening: \u0026#34; + categories.isHarassmentThreatening()); System.out.println(\u0026#34;Violence: \u0026#34; + categories.isViolence()); // Access category scores CategoryScores scores = this.result.getCategoryScores(); System.out.println(\u0026#34;\\nCategory Scores:\u0026#34;); System.out.println(\u0026#34;Sexual: \u0026#34; + scores.getSexual()); System.out.println(\u0026#34;Hate: \u0026#34; + scores.getHate()); System.out.println(\u0026#34;Harassment: \u0026#34; + scores.getHarassment()); System.out.println(\u0026#34;Self-Harm: \u0026#34; + scores.getSelfHarm()); System.out.println(\u0026#34;Sexual/Minors: \u0026#34; + scores.getSexualMinors()); System.out.println(\u0026#34;Hate/Threatening: \u0026#34; + scores.getHateThreatening()); System.out.println(\u0026#34;Violence/Graphic: \u0026#34; + scores.getViolenceGraphic()); System.out.println(\u0026#34;Self-Harm/Intent: \u0026#34; + scores.getSelfHarmIntent()); System.out.println(\u0026#34;Self-Harm/Instructions: \u0026#34; + scores.getSelfHarmInstructions()); System.out.println(\u0026#34;Harassment/Threatening: \u0026#34; + scores.getHarassmentThreatening()); System.out.println(\u0026#34;Violence: \u0026#34; + scores.getViolence()); } 手动配置 # 将 spring-ai-openai 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或复制到您的 Gradle build.gradle 构建文件：\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-openai\u0026#39; } 接下来，创建一个 OpenAiModerationModel：\nOpenAiModerationApi openAiModerationApi = new OpenAiModerationApi(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;)); OpenAiModerationModel openAiModerationModel = new OpenAiModerationModel(this.openAiModerationApi); OpenAiModerationOptions moderationOptions = OpenAiModerationOptions.builder() .model(\u0026#34;text-moderation-latest\u0026#34;) .build(); ModerationPrompt moderationPrompt = new ModerationPrompt(\u0026#34;Text to be moderated\u0026#34;, this.moderationOptions); ModerationResponse response = this.openAiModerationModel.call(this.moderationPrompt); 示例代码 # OpenAiModerationModelIT 测试提供了一些如何使用该库的一般示例。您可以参考此测试以获取更详细的使用示例。\n"},{"id":16,"href":"/docs/chat-client-api/advisors/","title":"顾问 API","section":"聊天客户端 API","content":" 顾问 API # Spring AI Advisors API 提供了一种灵活而强大的方法来拦截、修改和增强 Spring 应用程序中的 AI 驱动的交互。通过利用 Advisors API，开发人员可以创建更复杂、可重用和可维护的 AI 组件。 主要优势包括封装重复的生成式 AI 模式、转换发送到大型语言模型 （LLM） 和从大型语言模型 （LLM） 发送的数据，以及提供跨各种模型和用例的可移植性。 您可以使用 [ ChatClient API](chatclient.html#_advisor_configuration_in_chatclient) 配置现有顾问，如以下示例所示： 建议在构建时使用 builder 的 defaultAdvisors（） 方法注册 advisor。 顾问还参与可观测性堆栈，因此您可以查看与其执行相关的指标和跟踪。\n核心组件 # 该 API 由用于非流式处理场景的 CallAroundAdvisor 和 CallAroundAdvisorChain 以及用于流式处理场景的 StreamAroundAdvisor 和 StreamAroundAdvisorChain 组成。它还包括 AdvisedRequest 来表示未密封的 Prompt 请求，AdvisedResponse 用于聊天完成响应。两者都持有一个 advise-context 来在整个 advisor 链中共享状态。 nextAroundCall（） 和 nextAroundStream（） 是关键的 advisor 方法，通常执行一些作，例如检查未密封的 Prompt 数据、自定义和扩充 Prompt 数据、调用 advisor 链中的下一个实体、选择性地阻止请求、检查聊天完成响应以及引发异常以指示处理错误。 此外，getOrder（） 方法确定链中的 advisor 顺序，而 getName（） 提供唯一的 advisor 名称。 由 Spring AI 框架创建的 Advisor 链允许按顺序调用多个 advisor，这些 advisor 按其 getOrder（） 值排序。首先执行较低的值。自动添加的最后一个 advisor 将请求发送到 LLM。 以程图说明了顾问链和聊天模型之间的交互： 顾问订单 # 链中 advisor 的执行顺序由 getOrder（） 方法决定。需要了解的要点：\n首先执行订单值较低的顾问。 advisor 链以堆栈的形式运行： 链中的第一个 advisor 是第一个处理请求的人。 它也是最后一个处理响应的服务器。 要控制执行顺序： 将 order close 设置为 Ordered.HIGHEST_PRECEDENCE 以确保 advisor 在链中首先执行（首先用于请求处理，最后用于响应处理）。 将 order close 设置为 Ordered.LOWEST_PRECEDENCE 以确保 advisor 在链中最后执行（last 用于请求处理，first 用于响应处理）。 较高的值被解释为较低的优先级。 如果多个 advisor 具有相同的订单价值，则不能保证他们的执行顺序。 提醒一下，以下是 Spring Ordered 接口的语义： public interface Ordered { /** * Constant for the highest precedence value. * @see java.lang.Integer#MIN_VALUE */ int HIGHEST_PRECEDENCE = Integer.MIN_VALUE; /** * Constant for the lowest precedence value. * @see java.lang.Integer#MAX_VALUE */ int LOWEST_PRECEDENCE = Integer.MAX_VALUE; /** * Get the order value of this object. * \u0026lt;p\u0026gt;Higher values are interpreted as lower priority. As a consequence, * the object with the lowest value has the highest priority (somewhat * analogous to Servlet {@code load-on-startup} values). * \u0026lt;p\u0026gt;Same order values will result in arbitrary sort positions for the * affected objects. * @return the order value * @see #HIGHEST_PRECEDENCE * @see #LOWEST_PRECEDENCE */ int getOrder(); } API 概述 # 主要的 Advisor 界面位于包 org.springframework.ai.chat.client.advisor.api 中。以下是您在创建自己的 advisor 时会遇到的关键界面：\npublic interface Advisor extends Ordered { String getName(); } 同步和反应式 Advisor 的两个子接口是\npublic interface CallAroundAdvisor extends Advisor { /** * Around advice that wraps the ChatModel#call(Prompt) method. * @param advisedRequest the advised request * @param chain the advisor chain * @return the response */ AdvisedResponse aroundCall(AdvisedRequest advisedRequest, CallAroundAdvisorChain chain); } 和\npublic interface StreamAroundAdvisor extends Advisor { /** * Around advice that wraps the invocation of the advised request. * @param advisedRequest the advised request * @param chain the chain of advisors to execute * @return the result of the advised request */ Flux\u0026lt;AdvisedResponse\u0026gt; aroundStream(AdvisedRequest advisedRequest, StreamAroundAdvisorChain chain); } 要继续 Advice 链，请在 Advice 实现中使用 CallAroundAdvisorChain 和 StreamAroundAdvisorChain： 这些接口包括\npublic interface CallAroundAdvisorChain { AdvisedResponse nextAroundCall(AdvisedRequest advisedRequest); } 和\npublic interface StreamAroundAdvisorChain { Flux\u0026lt;AdvisedResponse\u0026gt; nextAroundStream(AdvisedRequest advisedRequest); } 实施 Advisor # 要创建 advisor，请实现 CallAroundAdvisor 或 StreamAroundAdvisor （或两者）。要实现的关键方法是 nextAroundCall（）（ 用于非流式处理）或 nextAroundStream（）（ 用于流式处理顾问）。\n例子 # 我们将提供一些动手实践示例来说明如何实施 advisor 来观察和增强用例。\n日志记录顾问 # 我们可以实现一个简单的日志记录 advisor，在调用链中的下一个 advisor 之前记录 AdvisedRequest，并在调用链中的下一个 advisor 之后记录 AdvisedResponse。请注意，advisor 仅观察请求和响应，不会修改它们。此实现支持非流式处理和流式处理方案。\npublic class SimpleLoggerAdvisor implements CallAroundAdvisor, StreamAroundAdvisor { private static final Logger logger = LoggerFactory.getLogger(SimpleLoggerAdvisor.class); @Override public String getName() { (1) return this.getClass().getSimpleName(); } @Override public int getOrder() { (2) return 0; } @Override public AdvisedResponse aroundCall(AdvisedRequest advisedRequest, CallAroundAdvisorChain chain) { logger.debug(\u0026#34;BEFORE: {}\u0026#34;, advisedRequest); AdvisedResponse advisedResponse = chain.nextAroundCall(advisedRequest); logger.debug(\u0026#34;AFTER: {}\u0026#34;, advisedResponse); return advisedResponse; } @Override public Flux\u0026lt;AdvisedResponse\u0026gt; aroundStream(AdvisedRequest advisedRequest, StreamAroundAdvisorChain chain) { logger.debug(\u0026#34;BEFORE: {}\u0026#34;, advisedRequest); Flux\u0026lt;AdvisedResponse\u0026gt; advisedResponses = chain.nextAroundStream(advisedRequest); return new MessageAggregator().aggregateAdvisedResponse(advisedResponses, advisedResponse -\u0026gt; logger.debug(\u0026#34;AFTER: {}\u0026#34;, advisedResponse)); (3) } } 重读 （Re2） 顾问 # “[ Re-Reading Improves Reasoning in Large Language Models]( https://arxiv.org/pdf/2309.06275)” 一文介绍了一种称为 Re-Reading （Re2） 的技术，该技术可以提高大型语言模型的推理能力。Re2 技术需要像这样扩充输入提示： 实现将 Re2 技术应用于用户输入查询的 advisor 可以像这样完成：\npublic class ReReadingAdvisor implements CallAroundAdvisor, StreamAroundAdvisor { private AdvisedRequest before(AdvisedRequest advisedRequest) { (1) Map\u0026lt;String, Object\u0026gt; advisedUserParams = new HashMap\u0026lt;\u0026gt;(advisedRequest.userParams()); advisedUserParams.put(\u0026#34;re2_input_query\u0026#34;, advisedRequest.userText()); return AdvisedRequest.from(advisedRequest) .userText(\u0026#34;\u0026#34;\u0026#34; {re2_input_query} Read the question again: {re2_input_query} \u0026#34;\u0026#34;\u0026#34;) .userParams(advisedUserParams) .build(); } @Override public AdvisedResponse aroundCall(AdvisedRequest advisedRequest, CallAroundAdvisorChain chain) { (2) return chain.nextAroundCall(this.before(advisedRequest)); } @Override public Flux\u0026lt;AdvisedResponse\u0026gt; aroundStream(AdvisedRequest advisedRequest, StreamAroundAdvisorChain chain) { (3) return chain.nextAroundStream(this.before(advisedRequest)); } @Override public int getOrder() { (4) return 0; } @Override public String getName() { (5) return this.getClass().getSimpleName(); } } Spring AI 内置顾问程序 # Spring AI 框架提供了几个内置的顾问程序来增强您的 AI 交互。以下是可用顾问的概述：\n聊天记忆顾问 # 这些顾问在聊天内存存储中管理对话历史记录：\n问题解答顾问 # 内容安全顾问 # 流式处理与非流式处理 # 非流式处理顾问处理完整的请求和响应。 Streaming advisor 使用反应式编程概念（例如，用于响应的 Flux）将请求和响应作为连续流处理。 @Override public Flux\u0026lt;AdvisedResponse\u0026gt; aroundStream(AdvisedRequest advisedRequest, StreamAroundAdvisorChain chain) { return Mono.just(advisedRequest) .publishOn(Schedulers.boundedElastic()) .map(request -\u0026gt; { // This can be executed by blocking and non-blocking Threads. // Advisor before next section }) .flatMapMany(request -\u0026gt; chain.nextAroundStream(request)) .map(response -\u0026gt; { // Advisor after next section }); } 最佳实践 # 向后兼容性 # 重大 API 更改 # Spring AI Advisor 链从 1.0 M2 版本到 1.0 M3 发生了重大变化。以下是主要修改：\nAdvisor 接口 # 在 1.0 M2 中，有单独的 RequestAdvisor 和 ResponseAdvisor 接口。 RequestAdvisor 在 ChatModel.call 和 ChatModel.stream 方法之前调用。 ResponseAdvisor 是在这些方法之后调用的。 在 1.0 M3 中，这些接口已被替换为： StreamResponseMode（以前是 ResponseAdvisor 的一部分）已被删除。 上下文映射处理 # 在 1.0 m2 中： 上下文映射是一个单独的方法参数。 地图是可变的，并沿着链传递。 在 1.0 m3 中： 上下文映射现在是 AdvisedRequest 和 AdvisedResponse 记录的一部分。 map 是不可变的。 要更新上下文，请使用 updateContext 方法，该方法使用更新的内容创建一个新的不可修改的映射。 在 1.0 M3 中更新上下文的示例： @Override public AdvisedResponse aroundCall(AdvisedRequest advisedRequest, CallAroundAdvisorChain chain) { this.advisedRequest = advisedRequest.updateContext(context -\u0026gt; { context.put(\u0026#34;aroundCallBefore\u0026#34; + getName(), \u0026#34;AROUND_CALL_BEFORE \u0026#34; + getName()); // Add multiple key-value pairs context.put(\u0026#34;lastBefore\u0026#34;, getName()); // Add a single key-value pair return context; }); // Method implementation continues... } "},{"id":17,"href":"/docs/models/embedding-models/azure-openai/","title":"Azure OpenAI 嵌入","section":"嵌入模型 API","content":" Azure OpenAI 嵌入 # Azure 的 OpenAI 扩展了 OpenAI 功能，为各种任务提供安全的文本生成和嵌入计算模型： Azure OpenAI 嵌入依赖于余弦相似性来计算文档和查询之间的相似性。\n先决条件 # Azure OpenAI 客户端提供三个连接选项：使用 Azure API 密钥或使用 OpenAI API 密钥，或使用 Microsoft Entra ID。\nAzure API 密钥和端点 # 从 [ Azure 门户]( https://portal.azure.com)上的 Azure OpenAI 服务部分获取 Azure OpenAI 终结点和 API 密钥 。 Spring AI 定义了两个配置属性： 您可以在 application.properties 或 application.yml 文件中设置这些配置属性：\nspring.ai.azure.openai.api-key=\u0026lt;your-azure-api-key\u0026gt; spring.ai.azure.openai.endpoint=\u0026lt;your-azure-endpoint-url\u0026gt; 如果您更喜欢对 API 密钥等敏感信息使用环境变量，则可以在配置中使用 Spring 表达式语言 （SpEL）：\n# In application.yml spring: ai: azure: openai: api-key: ${AZURE_OPENAI_API_KEY} endpoint: ${AZURE_OPENAI_ENDPOINT} # In your environment or .env file export AZURE_OPENAI_API_KEY=\u0026lt;your-azure-openai-api-key\u0026gt; export AZURE_OPENAI_ENDPOINT=\u0026lt;your-azure-endpoint-url\u0026gt; OpenAI 密钥 # 要使用 OpenAI 服务（而不是 Azure）进行身份验证，请提供 OpenAI API 密钥。这会自动将终端节点设置为 [ api.openai.com/v1](https:// api.openai.com/v1)。 使用此方法时，请将 spring.ai.azure.openai.chat.options.deployment-name property 设置为您要使用的 [ OpenAI 模型]( https://platform.openai.com/docs/models)的名称。 在您的应用程序配置中：\nspring.ai.azure.openai.openai-api-key=\u0026lt;your-azure-openai-key\u0026gt; spring.ai.azure.openai.chat.options.deployment-name=\u0026lt;openai-model-name\u0026gt; 在 SPEL 中使用环境变量：\n# In application.yml spring: ai: azure: openai: openai-api-key: ${AZURE_OPENAI_API_KEY} chat: options: deployment-name: ${OPENAI_MODEL_NAME} # In your environment or .env file export AZURE_OPENAI_API_KEY=\u0026lt;your-openai-key\u0026gt; export OPENAI_MODEL_NAME=\u0026lt;openai-model-name\u0026gt; Microsoft 输入 ID # 对于使用 Microsoft Entra ID（以前称为 Azure Active Directory）的无密钥身份验证， 请仅设置 configuration 属性， spring.ai.azure.openai.endpoint 而不设置上述 api-key 属性。 仅查找 endpoint 属性，您的应用程序将评估用于检索凭证的几个不同选项，并且将使用令牌凭证创建一个 OpenAIClient 实例。\n添加存储库和 BOM # Spring AI 工件发布在 Maven Central 和 Spring Snapshot 存储库中。请参阅 [ Artifact Repositories](../../getting-started.html#artifact-repositories) 部分，将这些存储库添加到您的构建系统中。 为了帮助进行[ 依赖项管理](../../getting-started.html#dependency-management)，Spring AI 提供了一个 BOM（物料清单），以确保在整个项目中使用一致的 Spring AI 版本。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # Spring AI 为 Azure OpenAI 嵌入模型提供 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-azure-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-azure-openai\u0026#39; } 嵌入属性 # 前缀 spring.ai.azure.openai 是用于配置与 Azure OpenAI 的连接的属性前缀。 前缀 spring.ai.azure.openai.embedding 是配置 Azure OpenAI 的 EmbeddingModel 实现的属性前缀\n运行时选项 # AzureOpenAiEmbeddingOptions 提供嵌入请求的配置信息。AzureOpenAiEmbeddingOptions 提供了一个用于创建选项的生成器。 开始时，使用 AzureOpenAiEmbeddingModel 构造函数设置用于所有嵌入请求的默认选项。在运行时，可以通过将 AzureOpenAiEmbeddingOptions 实例与 You 一起传递给 EmbeddingRequest 请求来替代默认选项。 例如，要覆盖特定请求的默认模型名称：\nEmbeddingResponse embeddingResponse = embeddingModel.call( new EmbeddingRequest(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;), AzureOpenAiEmbeddingOptions.builder() .model(\u0026#34;Different-Embedding-Model-Deployment-Name\u0026#34;) .build())); 示例代码 # 这将创建一个 EmbeddingModel 实现，您可以将其注入到您的类中。下面是一个使用 EmbeddingModel 实现的简单 @Controller 类的示例。\nspring.ai.azure.openai.api-key=YOUR_API_KEY spring.ai.azure.openai.endpoint=YOUR_ENDPOINT spring.ai.azure.openai.embedding.options.model=text-embedding-ada-002 @RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(\u0026#34;/ai/embedding\u0026#34;) public Map embed(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { EmbeddingResponse embeddingResponse = this.embeddingModel.embedForResponse(List.of(message)); return Map.of(\u0026#34;embedding\u0026#34;, embeddingResponse); } } 手动配置 # 如果不想使用 Spring Boot 自动配置，可以在应用程序中手动配置 AzureOpenAiEmbeddingModel。为此，请将 spring-ai-azure-openai 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-azure-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-azure-openai\u0026#39; } 接下来，创建一个 AzureOpenAiEmbeddingModel 实例，并使用它来计算两个输入文本之间的相似性：\nvar openAIClient = OpenAIClientBuilder() .credential(new AzureKeyCredential(System.getenv(\u0026#34;AZURE_OPENAI_API_KEY\u0026#34;))) .endpoint(System.getenv(\u0026#34;AZURE_OPENAI_ENDPOINT\u0026#34;)) .buildClient(); var embeddingModel = new AzureOpenAiEmbeddingModel(this.openAIClient) .withDefaultOptions(AzureOpenAiEmbeddingOptions.builder() .model(\u0026#34;text-embedding-ada-002\u0026#34;) .user(\u0026#34;user-6\u0026#34;) .build()); EmbeddingResponse embeddingResponse = this.embeddingModel .embedForResponse(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;)); "},{"id":18,"href":"/docs/model-context-protocol-mcp/mcp-server-boot-starters/","title":"MCP 服务器启动启动器","section":"模型上下文协议 （MCP）","content":" MCP 服务器启动启动器 # Spring AI MCP（模型上下文协议）服务器启动器提供了在 Spring Boot 应用程序中设置 MCP 服务器的自动配置。它支持将 MCP 服务器功能与 Spring Boot 的自动配置系统无缝集成。 MCP Server Boot Starter 提供：\n首先 # 根据您的运输要求选择以下启动器之一：\n标准 MCP 服务器 # 完整的 MCP 服务器功能支持 STDIO 服务器传输。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-mcp-server-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 适用于命令行和桌面工具 不需要额外的 Web 依赖项 启动器激活 McpServerAutoConfiguration 自动配置，负责： 配置基本 Server 组件 处理工具、资源和提示规范 管理 Server 功能和更改通知 提供同步和异步服务器实现 WebMVC 服务器传输 # 完整的 MCP 服务器功能支持基于 Spring MVC 的 SSE（服务器发送事件）服务器传输和可选的 STDIO 传输。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-mcp-server-webmvc\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 启动器激活 McpWebMvcServerAutoConfiguration 和 McpServerAutoConfiguration 自动配置以提供：\n使用 Spring MVC 的基于 HTTP 的传输 （ WebMvcSseServerTransportProvider ） 自动配置的 SSE 终端节点 可选的 STDIO 传输（通过设置 spring.ai.mcp.server.stdio=true 启用 ） 包括 spring-boot-starter-web 和 mcp-spring-webmvc 依赖项 WebFlux 服务器传输 # 完整的 MCP 服务器功能支持基于 Spring WebFlux 的 SSE（服务器发送事件）服务器传输和可选的 STDIO 传输。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-mcp-server-webflux\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 启动器激活 McpWebFluxServerAutoConfiguration 和 McpServerAutoConfiguration 自动配置以提供：\n使用 Spring WebFlux 的反应式传输 （） WebFluxSseServerTransportProvider 自动配置的反应式 SSE 终端节点 可选的 STDIO 传输（通过设置 spring.ai.mcp.server.stdio=true 启用 ） 包括 spring-boot-starter-webflux 和 mcp-spring-webflux 依赖项 配置属性 # 所有属性都以 spring.ai.mcp.server 为前缀：\nSync/Async 服务器类型 # 同步服务器 - 使用 McpSyncServer 实现的默认服务器类型。它专为应用程序中的简单请求-响应模式而设计。要启用此服务器类型， spring.ai.mcp.server.type=SYNC 请在您的配置中设置。激活后，它会自动处理同步工具规格的配置。 异步服务器 - 异步服务器实现使用 McpAsyncServer，并针对非阻塞作进行了优化。要启用此服务器类型，请使用 spring.ai.mcp.server.type=ASYNC 配置您的应用程序。此服务器类型会自动设置具有内置 Project Reactor 支持的异步工具规范。 服务器功能 # MCP 服务器支持四种主要功能类型，可以单独启用或禁用：\n工具 - 启用 spring.ai.mcp.server.capabilities.tool=true|false /禁用工具功能 资源 - 启用 spring.ai.mcp.server.capabilities.resource=true|false /禁用资源功能 提示 - 启用 spring.ai.mcp.server.capabilities.prompt=true|false /禁用提示功能 完成 - 启用 spring.ai.mcp.server.capabilities.completion=true|false /禁用完成功能 默认情况下，所有功能均处于启用状态。禁用功能将阻止服务器注册和向客户端公开相应的功能。 交通方式 # MCP 服务器支持三种传输机制，每种机制都有其专用的启动器：\n标准输入/输出 （STDIO） - spring-ai-starter-mcp-server Spring MVC（服务器发送的事件）- spring-ai-starter-mcp-server-webmvc Spring WebFlux（反应式 SSE）- spring-ai-starter-mcp-server-webflux 特性和功能 # MCP Server Boot Starter 允许服务器向客户端公开工具、资源和提示。它根据服务器类型自动将注册为 Spring bean 的自定义功能处理程序转换为同步/异步规范：\n工具 # 允许服务器公开可由语言模型调用的工具。MCP Server Boot Starter 提供：\n更改通知支持 Spring AI 工具会根据服务器类型自动转换为同步/异步规范 通过 Spring bean 自动指定工具： @Bean public ToolCallbackProvider myTools(...) { List\u0026lt;ToolCallback\u0026gt; tools = ... return ToolCallbackProvider.from(tools); } 或使用低级 API：\n@Bean public List\u0026lt;McpServerFeatures.SyncToolSpecification\u0026gt; myTools(...) { List\u0026lt;McpServerFeatures.SyncToolSpecification\u0026gt; tools = ... return tools; } 自动配置将自动检测并注册来自以下来源的所有工具回调： * 单个 ToolCallback bean * ToolCallback bean 列表 * ToolCallbackProvider bean 按名称对工具进行重复数据删除，并使用每个工具名称的第一个匹配项。\n工具上下文支持 # 支持 [ ToolContext](../tools.html#_tool_context)，允许将上下文信息传递给工具调用。它在交换密钥下包含一个 McpSyncServerExchange 实例，可通过 McpToolUtils.getMcpExchange(toolContext) .请参阅[ 此示例演示]( https://github.com/spring-projects/spring-ai-examples/blob/3fab8483b8deddc241b1e16b8b049616604b7767/model-context-protocol/sampling/mcp-weather-webmvc-server/src/main/java/org/springframework/ai/mcp/sample/server/WeatherService.java#L59-L126) exchange.loggingNotification(…​) exchange.createMessage（\u0026hellip;）。\n资源管理 # 为服务器提供一种标准化的方法，以便向客户端公开资源。\n静态和动态资源规范 可选更改通知 支持资源模板 同步/异步资源规格之间的自动转换 通过 Spring bean 自动指定资源： @Bean public List\u0026lt;McpServerFeatures.SyncResourceSpecification\u0026gt; myResources(...) { var systemInfoResource = new McpSchema.Resource(...); var resourceSpecification = new McpServerFeatures.SyncResourceSpecification(systemInfoResource, (exchange, request) -\u0026gt; { try { var systemInfo = Map.of(...); String jsonContent = new ObjectMapper().writeValueAsString(systemInfo); return new McpSchema.ReadResourceResult( List.of(new McpSchema.TextResourceContents(request.uri(), \u0026#34;application/json\u0026#34;, jsonContent))); } catch (Exception e) { throw new RuntimeException(\u0026#34;Failed to generate system info\u0026#34;, e); } }); return List.of(resourceSpecification); } 及时管理 # 为服务器提供一种标准化的方法，以便向客户端公开提示模板。\n更改通知支持 模板版本控制 sync/async prompt 规范之间的自动转换 通过 Spring bean 自动提示指定： @Bean public List\u0026lt;McpServerFeatures.SyncPromptSpecification\u0026gt; myPrompts() { var prompt = new McpSchema.Prompt(\u0026#34;greeting\u0026#34;, \u0026#34;A friendly greeting prompt\u0026#34;, List.of(new McpSchema.PromptArgument(\u0026#34;name\u0026#34;, \u0026#34;The name to greet\u0026#34;, true))); var promptSpecification = new McpServerFeatures.SyncPromptSpecification(prompt, (exchange, getPromptRequest) -\u0026gt; { String nameArgument = (String) getPromptRequest.arguments().get(\u0026#34;name\u0026#34;); if (nameArgument == null) { nameArgument = \u0026#34;friend\u0026#34;; } var userMessage = new PromptMessage(Role.USER, new TextContent(\u0026#34;Hello \u0026#34; + nameArgument + \u0026#34;! How can I assist you today?\u0026#34;)); return new GetPromptResult(\u0026#34;A personalized greeting message\u0026#34;, List.of(userMessage)); }); return List.of(promptSpecification); } 完成管理 # 为服务器提供一种标准化的方法，以便向客户端公开完成功能。\n支持 sync 和 async completion 规范 通过 Spring bean 自动注册： @Bean public List\u0026lt;McpServerFeatures.SyncCompletionSpecification\u0026gt; myCompletions() { var completion = new McpServerFeatures.SyncCompletionSpecification( \u0026#34;code-completion\u0026#34;, \u0026#34;Provides code completion suggestions\u0026#34;, (exchange, request) -\u0026gt; { // Implementation that returns completion suggestions return new McpSchema.CompletionResult(List.of( new McpSchema.Completion(\u0026#34;suggestion1\u0026#34;, \u0026#34;First suggestion\u0026#34;), new McpSchema.Completion(\u0026#34;suggestion2\u0026#34;, \u0026#34;Second suggestion\u0026#34;) )); } ); return List.of(completion); } 根更改使用者 # 当根更改时，支持 listChanged 的客户端会发送 Root Change 通知。\n支持监控根更改 自动转换为反应式应用程序的异步使用者 通过 Spring bean 进行可选注册 @Bean public BiConsumer\u0026lt;McpSyncServerExchange, List\u0026lt;McpSchema.Root\u0026gt;\u0026gt; rootsChangeHandler() { return (exchange, roots) -\u0026gt; { logger.info(\u0026#34;Registering root resources: {}\u0026#34;, roots); }; } 使用示例 # 标准 STDIO 服务器配置 # # Using spring-ai-starter-mcp-server spring: ai: mcp: server: name: stdio-mcp-server version: 1.0.0 type: SYNC WebMVC 服务器配置 # # Using spring-ai-starter-mcp-server-webmvc spring: ai: mcp: server: name: webmvc-mcp-server version: 1.0.0 type: SYNC instructions: \u0026#34;This server provides weather information tools and resources\u0026#34; sse-message-endpoint: /mcp/messages capabilities: tool: true resource: true prompt: true completion: true WebFlux 服务器配置 # # Using spring-ai-starter-mcp-server-webflux spring: ai: mcp: server: name: webflux-mcp-server version: 1.0.0 type: ASYNC # Recommended for reactive applications instructions: \u0026#34;This reactive server provides weather information tools and resources\u0026#34; sse-message-endpoint: /mcp/messages capabilities: tool: true resource: true prompt: true completion: true 使用 MCP 服务器创建 Spring Boot 应用程序 # @Service public class WeatherService { @Tool(description = \u0026#34;Get weather information by city name\u0026#34;) public String getWeather(String cityName) { // Implementation } } @SpringBootApplication public class McpServerApplication { private static final Logger logger = LoggerFactory.getLogger(McpServerApplication.class); public static void main(String[] args) { SpringApplication.run(McpServerApplication.class, args); } @Bean public ToolCallbackProvider weatherTools(WeatherService weatherService) { return MethodToolCallbackProvider.builder().toolObjects(weatherService).build(); } } 自动配置会自动将工具回调注册为 MCP 工具。你可以让多个 bean 生成 ToolCallbacks。自动配置将合并它们。\n示例应用 # 天气服务器 （WebFlux） - 具有 WebFlux 传输的 Spring AI MCP 服务器启动器。 天气服务器 （STDIO） - 具有 STDIO 传输的 Spring AI MCP 服务器启动启动器。 天气服务器手动配置 - Spring AI MCP 服务器启动启动器，它不使用自动配置，而是使用 Java SDK 手动配置服务器。 其他资源 # Spring AI 文档 Model 上下文协议规范 Spring Boot 自动配置 "},{"id":19,"href":"/docs/vector-databases/azure-cosmos-db/","title":"None","section":"矢量数据库","content":" None # 本部分将指导你完成设置 CosmosDBVectorStore 以存储文档嵌入并执行相似性搜索。\n什么是 Azure Cosmos DB？ # [ Azure Cosmos DB]( https://azure.microsoft.com/en-us/services/cosmos-db/) 是 Microsoft 的全球分布式云原生数据库服务，专为任务关键型应用程序而设计。它提供高可用性、低延迟以及水平扩展以满足现代应用程序需求的能力。它是从头开始构建的，其核心是全球分发、精细的多租户和水平可扩展性。它是 Azure 中的一项基础服务，被全球范围内的大多数 Microsoft 关键任务应用程序使用，包括 Teams、Skype、Xbox Live、Office 365、Bing、Azure Active Directory、Azure 门户、Microsoft Store 等。它还被数以千计的外部客户使用，包括 OpenAI for ChatGPT 和其他需要弹性扩展、交钥匙全球分发以及全球低延迟和高可用性的任务关键型 AI 应用程序。\n什么是 DiskANN？ # DiskANN（基于磁盘的近似最近邻搜索）是 Azure Cosmos DB 中用于增强矢量搜索性能的创新技术。它通过对 Cosmos DB 中存储的嵌入进行索引，实现对高维数据的高效且可缩放的相似性搜索。 DiskANN 具有以下优势：\n效率 ：与传统方法相比，通过利用基于磁盘的结构，DiskANN 显著缩短了查找最近邻所需的时间。 可扩展性 ：它可以处理超过内存容量的大型数据集，使其适用于各种应用程序，包括机器学习和 AI 驱动的解决方案。 低延迟 ：DiskANN 最大限度地减少了搜索作期间的延迟，确保应用程序即使在数据量很大的情况下也能快速检索结果。 在 Spring AI for Azure Cosmos DB 的上下文中，向量搜索将创建并利用 DiskANN 索引来确保相似性查询的最佳性能。 使用自动配置设置 Azure Cosmos DB 矢量存储 # 以下代码演示如何使用自动配置设置 CosmosDBVectorStore：\npackage com.example.demo; import io.micrometer.observation.ObservationRegistry; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import org.springframework.ai.document.Document; import org.springframework.ai.vectorstore.SearchRequest; import org.springframework.ai.vectorstore.VectorStore; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.autoconfigure.EnableAutoConfiguration; import org.springframework.boot.CommandLineRunner; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Lazy; import java.util.List; import java.util.Map; import java.util.UUID; import static org.assertj.core.api.Assertions.assertThat; @SpringBootApplication @EnableAutoConfiguration public class DemoApplication implements CommandLineRunner { private static final Logger log = LoggerFactory.getLogger(DemoApplication.class); @Lazy @Autowired private VectorStore vectorStore; public static void main(String[] args) { SpringApplication.run(DemoApplication.class, args); } @Override public void run(String... args) throws Exception { Document document1 = new Document(UUID.randomUUID().toString(), \u0026#34;Sample content1\u0026#34;, Map.of(\u0026#34;key1\u0026#34;, \u0026#34;value1\u0026#34;)); Document document2 = new Document(UUID.randomUUID().toString(), \u0026#34;Sample content2\u0026#34;, Map.of(\u0026#34;key2\u0026#34;, \u0026#34;value2\u0026#34;)); this.vectorStore.add(List.of(document1, document2)); List\u0026lt;Document\u0026gt; results = this.vectorStore.similaritySearch(SearchRequest.builder().query(\u0026#34;Sample content\u0026#34;).topK(1).build()); log.info(\u0026#34;Search results: {}\u0026#34;, results); // Remove the documents from the vector store this.vectorStore.delete(List.of(document1.getId(), document2.getId())); } @Bean public ObservationRegistry observationRegistry() { return ObservationRegistry.create(); } } 自动配置 # 将以下依赖项添加到您的 Maven 项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-azure-cosmos-db\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 配置属性 # 以下配置属性可用于 Cosmos DB 矢量存储：\n使用过滤器进行复杂搜索 # 可以使用 Cosmos DB 矢量存储中的筛选器执行更复杂的搜索。下面是演示如何在搜索查询中使用筛选器的示例。\nMap\u0026lt;String, Object\u0026gt; metadata1 = new HashMap\u0026lt;\u0026gt;(); metadata1.put(\u0026#34;country\u0026#34;, \u0026#34;UK\u0026#34;); metadata1.put(\u0026#34;year\u0026#34;, 2021); metadata1.put(\u0026#34;city\u0026#34;, \u0026#34;London\u0026#34;); Map\u0026lt;String, Object\u0026gt; metadata2 = new HashMap\u0026lt;\u0026gt;(); metadata2.put(\u0026#34;country\u0026#34;, \u0026#34;NL\u0026#34;); metadata2.put(\u0026#34;year\u0026#34;, 2022); metadata2.put(\u0026#34;city\u0026#34;, \u0026#34;Amsterdam\u0026#34;); Document document1 = new Document(\u0026#34;1\u0026#34;, \u0026#34;A document about the UK\u0026#34;, this.metadata1); Document document2 = new Document(\u0026#34;2\u0026#34;, \u0026#34;A document about the Netherlands\u0026#34;, this.metadata2); vectorStore.add(List.of(document1, document2)); FilterExpressionBuilder builder = new FilterExpressionBuilder(); List\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch(SearchRequest.builder().query(\u0026#34;The World\u0026#34;) .topK(10) .filterExpression((this.builder.in(\u0026#34;country\u0026#34;, \u0026#34;UK\u0026#34;, \u0026#34;NL\u0026#34;)).build()).build()); 在不进行自动配置的情况下设置 Azure Cosmos DB 矢量存储 # 以下代码演示了如何在不依赖自动配置的情况下设置 CosmosDBVectorStore。[默认 Azure 凭据]建议对 Azure Cosmos DB 进行身份验证时 [ learn.microsoft.com/azure/developer/java/sdk/authentication/credential-chains#defaultazurecredential-overview](https:// learn.microsoft.com/azure/developer/java/sdk/authentication/credential-chains#defaultazurecredential-overview)。\n@Bean public VectorStore vectorStore(ObservationRegistry observationRegistry) { // Create the Cosmos DB client CosmosAsyncClient cosmosClient = new CosmosClientBuilder() .endpoint(System.getenv(\u0026#34;COSMOSDB_AI_ENDPOINT\u0026#34;)) .credential(new DefaultAzureCredentialBuilder().build()) .userAgentSuffix(\u0026#34;SpringAI-CDBNoSQL-VectorStore\u0026#34;) .gatewayMode() .buildAsyncClient(); // Create and configure the vector store return CosmosDBVectorStore.builder(cosmosClient, embeddingModel) .databaseName(\u0026#34;test-database\u0026#34;) .containerName(\u0026#34;test-container\u0026#34;) // Configure metadata fields for filtering .metadataFields(List.of(\u0026#34;country\u0026#34;, \u0026#34;year\u0026#34;, \u0026#34;city\u0026#34;)) // Set the partition key path (optional) .partitionKeyPath(\u0026#34;/id\u0026#34;) // Configure performance settings .vectorStoreThroughput(1000) .vectorDimensions(1536) // Match your embedding model\u0026#39;s dimensions // Add custom batching strategy (optional) .batchingStrategy(new TokenCountBatchingStrategy()) // Add observation registry for metrics .observationRegistry(observationRegistry) .build(); } @Bean public EmbeddingModel embeddingModel() { return new TransformersEmbeddingModel(); } 此配置显示所有可用的生成器选项：\ndatabaseName：Cosmos DB 数据库的名称 containerName：数据库中容器的名称 partitionKeyPath：分区键的路径（例如，“/id”） metadataFields：将用于筛选的元数据字段列表 vectorStoreThroughput：向量存储容器的吞吐量 （RU/s） vectorDimensions：向量的维度数（应与嵌入模型匹配） batchingStrategy：批处理文档作的策略（可选） 手动依赖项设置 # 在 Maven 项目中添加以下依赖项：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-azure-cosmos-db-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 访问 Native Client # Azure Cosmos DB 矢量存储实现通过 getNativeClient（） 方法提供对基础本机 Azure Cosmos DB 客户端 （CosmosClient） 的访问：\nCosmosDBVectorStore vectorStore = context.getBean(CosmosDBVectorStore.class); Optional\u0026lt;CosmosClient\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { CosmosClient client = nativeClient.get(); // Use the native client for Azure Cosmos DB-specific operations } 本机客户端允许你访问特定于 Azure Cosmos DB 的功能和作，这些功能和作可能不会通过 VectorStore 接口公开。\n"},{"id":20,"href":"/docs/models/image-models/openai/","title":"OpenAI 图像生成","section":"图像模型 API","content":" OpenAI 图像生成 # Spring AI 支持 OpenAI 的图像生成模型 DALL-E。\n先决条件 # 您需要使用 OpenAI 创建 API 密钥才能访问 ChatGPT 模型。 在 [ OpenAI 注册页面]( https://platform.openai.com/signup)创建一个帐户，并在 [ API 密钥页面上]( https://platform.openai.com/account/api-keys)生成令牌。 Spring AI 项目定义了一个名为 spring.ai.openai.api-key 的配置属性，您应该将其设置为从 openai.com 获取的 API 密钥的值。 您可以在 application.properties 文件中设置此配置属性：\nspring.ai.openai.api-key=\u0026lt;your-openai-api-key\u0026gt; 为了在处理 API 密钥等敏感信息时增强安全性，您可以使用 Spring 表达式语言 （SpEL） 来引用自定义环境变量：\n# In application.yml spring: ai: openai: api-key: ${OPENAI_API_KEY} # In your environment or .env file export OPENAI_API_KEY=\u0026lt;your-openai-api-key\u0026gt; 您还可以在应用程序代码中以编程方式设置此配置：\n// Retrieve API key from a secure source or environment variable String apiKey = System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;); 自动配置 # Spring AI 为 OpenAI Image Generation Client 提供 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-openai\u0026#39; } 图像生成属性 # 连接属性 # 前缀 spring.ai.openai 用作允许您连接到 OpenAI 的属性前缀。\n重试属性 # 前缀 spring.ai.retry 用作属性前缀，允许您为 OpenAI Image 客户端配置重试机制。\n配置属性 # 前缀 spring.ai.openai.image 是属性前缀，允许您为 OpenAI 配置 ImageModel 实现。\n运行时选项 # [ OpenAiImageOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/[OpenAiImageOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/OpenAiImageOptions.java)) 提供模型配置，例如要使用的模型、质量、大小等。 启动时，可以使用 OpenAiImageModel(OpenAiImageApi openAiImageApi) constructor 和 withDefaultOptions(OpenAiImageOptions defaultOptions) method 配置默认选项。或者，使用前面描述的属性 spring.ai.openai.image.options.* 。 在运行时，您可以通过向 ImagePrompt 调用添加新的、特定于请求的选项来覆盖默认选项。例如，要覆盖 OpenAI 特定选项（例如质量和要创建的图像数量），请使用以下代码示例：\nImageResponse response = openaiImageModel.call( new ImagePrompt(\u0026#34;A light cream colored mini golden doodle\u0026#34;, OpenAiImageOptions.builder() .quality(\u0026#34;hd\u0026#34;) .N(4) .height(1024) .width(1024).build()) ); "},{"id":21,"href":"/docs/models/embedding-models/amazon-bedrock/titan/","title":"Titan 嵌入","section":"亚马逊基岩版","content":" Titan 嵌入 # 提供 Bedrock Titan Embedding 模型。 [ Amazon Titan]( https://aws.amazon.com/bedrock/titan/) 基础模型 （FM） 通过完全托管的 API 为客户提供了广泛的高性能图像、多模态嵌入和文本模型选择。[ Amazon Titan]( https://aws.amazon.com/bedrock/titan/) 模型由 AWS 创建，并在大型数据集上进行预训练，使其成为强大的通用模型，旨在支持各种使用案例，同时还支持负责任地使用 AI。按原样使用它们，或使用您自己的数据私下自定义它们。 [ AWS Bedrock Titan 模型页面]( https://aws.amazon.com/bedrock/titan/)和 [ Amazon Bedrock 用户指南]( https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html)包含有关如何使用 AWS 托管模型的详细信息。\n先决条件 # 请参阅 [ Amazon Bedrock 上的 Spring AI 文档](../bedrock.html)以设置 API 访问。\n添加存储库和 BOM # Spring AI 工件发布在 Maven Central 和 Spring Snapshot 存储库中。请参阅 [ Artifact Repositories](../../getting-started.html#artifact-repositories) 部分，将这些存储库添加到您的构建系统中。 为了帮助进行[ 依赖项管理](../../getting-started.html#dependency-management)，Spring AI 提供了一个 BOM（物料清单），以确保在整个项目中使用一致的 Spring AI 版本。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # 将 spring-ai-starter-model-bedrock 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-bedrock\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-bedrock\u0026#39; } 启用 Titan 嵌入支持 # 默认情况下，Titan 嵌入模型处于禁用状态。要启用它，请在应用程序配置中将 spring.ai.model.embedding 属性设置为 bedrock-titan：\nspring.ai.model.embedding=bedrock-titan 或者，你可以使用 Spring 表达式语言 （SpEL） 来引用环境变量：\n# In application.yml spring: ai: model: embedding: ${AI_MODEL_EMBEDDING} # In your environment or .env file export AI_MODEL_EMBEDDING=bedrock-titan 您还可以在启动应用程序时使用 Java 系统属性设置此属性：\njava -Dspring.ai.model.embedding=bedrock-titan -jar your-application.jar 嵌入属性 # 前缀 spring.ai.bedrock.aws 是用于配置与 AWS Bedrock 的连接的属性前缀。 前缀 spring.ai.bedrock.titan.embedding （在 BedrockTitanEmbeddingProperties 中定义）是为 Titan 配置嵌入模型实现的属性前缀。 支持的值为：amazon.titan-embed-image-v1、amazon.titan-embed-text-v1 和 amazon.titan-embed-text-v2：0。模型 ID 值也可以在 [ AWS Bedrock 文档中找到基本模型 ID]( https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids-arns.html)。\n运行时选项 # [ BedrockTitanEmbeddingOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/titan/[BedrockTitanEmbeddingOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/titan/BedrockTitanEmbeddingOptions.java)) 提供 model 配置，例如 input-type。启动时，可以使用 BedrockTitanEmbeddingModel(api).withInputType(type) method 或 spring.ai.bedrock.titan.embedding.input-type properties 配置默认选项。 在运行时，您可以通过向 EmbeddingRequest 调用添加新的、特定于请求的选项来覆盖默认选项。例如，要覆盖特定请求的默认温度：\nEmbeddingResponse embeddingResponse = embeddingModel.call( new EmbeddingRequest(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;), BedrockTitanEmbeddingOptions.builder() .withInputType(InputType.TEXT) .build())); 样品控制器 # [ 创建一个新]( https://start.spring.io/)的 Spring Boot 项目，并将 添加到您的 spring-ai-starter-model-bedrock pom（或 gradle）依赖项中。 在 src/main/resources 目录下添加一个 application.properties 文件，以启用和配置 Titan Embedding 模型：\nspring.ai.bedrock.aws.region=eu-central-1 spring.ai.bedrock.aws.access-key=${AWS_ACCESS_KEY_ID} spring.ai.bedrock.aws.secret-key=${AWS_SECRET_ACCESS_KEY} spring.ai.model.embedding=bedrock-titan 这将创建一个 EmbeddingController 实现，您可以将其注入到您的类中。下面是一个简单的 @Controller 类示例，该类使用 chat 模型生成文本。\n@RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(\u0026#34;/ai/embedding\u0026#34;) public Map embed(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { EmbeddingResponse embeddingResponse = this.embeddingModel.embedForResponse(List.of(message)); return Map.of(\u0026#34;embedding\u0026#34;, embeddingResponse); } } 手动配置 # BedrockTitanEmbeddingModel 实现 EmbeddingModel，并使用[ 低级 TitanEmbeddingBedrockApi 客户端](#low-level-api)连接到 Bedrock Titan 服务。 将 spring-ai-bedrock 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-bedrock\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-bedrock\u0026#39; } 接下来，创建一个 [ BedrockTitanEmbeddingModel]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/titan/[BedrockTitanEmbeddingModel](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/titan/BedrockTitanEmbeddingModel.java).java) 并将其用于文本嵌入：\nvar titanEmbeddingApi = new TitanEmbeddingBedrockApi( TitanEmbeddingModel.TITAN_EMBED_IMAGE_V1.id(), Region.US_EAST_1.id()); var embeddingModel = new BedrockTitanEmbeddingModel(this.titanEmbeddingApi); EmbeddingResponse embeddingResponse = this.embeddingModel .embedForResponse(List.of(\u0026#34;Hello World\u0026#34;)); // NOTE titan does not support batch embedding. 低级 TitanEmbeddingBedrockApi 客户端 # [ TitanEmbeddingBedrockApi]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/titan/api/[TitanEmbeddingBedrockApi](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/titan/api/TitanEmbeddingBedrockApi.java).java) 提供的是基于 AWS Bedrock [ Titan Embedding 模型的]( https://docs.aws.amazon.com/bedrock/latest/userguide/titan-multiemb-models.html)轻量级 Java 客户端。 下面的类图说明了 TitanEmbeddingBedrockApi 接口和构建块： TitanEmbeddingBedrockApi 支持使用 amazon.titan-embed-image-v1 和 amazon.titan-embed-image-v1 模型进行单次和批量嵌入计算。 以下是如何以编程方式使用 api 的简单代码段：\nTitanEmbeddingBedrockApi titanEmbedApi = new TitanEmbeddingBedrockApi( TitanEmbeddingModel.TITAN_EMBED_TEXT_V1.id(), Region.US_EAST_1.id()); TitanEmbeddingRequest request = TitanEmbeddingRequest.builder() .withInputText(\u0026#34;I like to eat apples.\u0026#34;) .build(); TitanEmbeddingResponse response = this.titanEmbedApi.embedding(this.request); 要嵌入图像，您需要将其转换为 base64 格式：\nTitanEmbeddingBedrockApi titanEmbedApi = new TitanEmbeddingBedrockApi( TitanEmbeddingModel.TITAN_EMBED_IMAGE_V1.id(), Region.US_EAST_1.id()); byte[] image = new DefaultResourceLoader() .getResource(\u0026#34;classpath:/spring_framework.png\u0026#34;) .getContentAsByteArray(); TitanEmbeddingRequest request = TitanEmbeddingRequest.builder() .withInputImage(Base64.getEncoder().encodeToString(this.image)) .build(); TitanEmbeddingResponse response = this.titanEmbedApi.embedding(this.request); "},{"id":22,"href":"/docs/models/chat-models/amazon-bedrock-converse/","title":"基岩版 Converse API","section":"聊天模型 API","content":" 基岩版 Converse API # [ Amazon Bedrock Converse API]( https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html) 为对话式 AI 模型提供了一个统一的接口，具有增强功能，包括函数/工具调用、多模态输入和流式响应。 Bedrock Converse API 具有以下高级功能：\n先决条件 # 请参阅 [ Amazon Bedrock 入门]( https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html)以设置 API 访问\n获取 AWS 凭证：如果您还没有配置 AWS 账户和 AWS CLI，此视频指南可以帮助您配置它：AWS CLI 和 SDK 设置在不到 4 分钟的时间内，只需不到 4 分钟）。 您应该能够获取您的访问密钥和安全密钥。 启用要使用的模型：转到 Amazon Bedrock，然后从左侧的 Model Access（模型访问 ）菜单中，配置对要使用的模型的访问。 自动配置 # 将 spring-ai-starter-model-bedrock-converse 依赖项添加到项目的 Maven pom.xml 或 Gradle build.gradle 构建文件中：\n聊天属性 # 前缀 spring.ai.bedrock.aws 是用于配置与 AWS Bedrock 的连接的属性前缀。 prefix spring.ai.bedrock.converse.chat 是为 Converse API 配置聊天模型实现的属性 prefix。\n运行时选项 # 使用便携式 ChatOptions 或 ToolCallingChatOptions 便携式构建器创建模型配置，例如 temperature、maxToken、topP 等。 启动时，可以使用 BedrockConverseProxyChatModel(api, options) constructor 或 spring.ai.bedrock.converse.chat.options.* properties 配置默认选项。 在运行时，您可以通过向 Prompt 调用添加新的、特定于请求的选项来覆盖默认选项：\nvar options = ToolCallingChatOptions.builder() .model(\u0026#34;anthropic.claude-3-5-sonnet-20240620-v1:0\u0026#34;) .temperature(0.6) .maxTokens(300) .toolCallbacks(List.of(FunctionToolCallback.builder(\u0026#34;getCurrentWeather\u0026#34;, new WeatherService()) .description(\u0026#34;Get the weather in location. Return temperature in 36°F or 36°C format. Use multi-turn if needed.\u0026#34;) .inputType(WeatherService.Request.class) .build())) .build(); String response = ChatClient.create(this.chatModel) .prompt(\u0026#34;What is current weather in Amsterdam?\u0026#34;) .options(options) .call() .content(); 工具调用 # Bedrock Converse API 支持工具调用功能，允许模型在对话期间使用工具。以下是如何定义和使用基于 @Tool 的工具的示例：\npublic class WeatherService { @Tool(description = \u0026#34;Get the weather in location\u0026#34;) public String weatherByLocation(@ToolParam(description= \u0026#34;City or state name\u0026#34;) String location) { ... } } String response = ChatClient.create(this.chatModel) .prompt(\u0026#34;What\u0026#39;s the weather like in Boston?\u0026#34;) .tools(new WeatherService()) .call() .content(); 您也可以将 java.util.function bean 用作工具：\n@Bean @Description(\u0026#34;Get the weather in location. Return temperature in 36°F or 36°C format.\u0026#34;) public Function\u0026lt;Request, Response\u0026gt; weatherFunction() { return new MockWeatherService(); } String response = ChatClient.create(this.chatModel) .prompt(\u0026#34;What\u0026#39;s the weather like in Boston?\u0026#34;) .tools(\u0026#34;weatherFunction\u0026#34;) .inputType(Request.class) .call() .content(); 在[ 工具](../tools.html)文档中查找更多信息。\n模 态 # 多模态是指模型同时理解和处理来自各种来源的信息的能力，包括文本、图像、视频、pdf、doc、html、md 和更多数据格式。 Bedrock Converse API 支持多模态输入，包括文本和图像输入，并且可以根据组合输入生成文本响应。 您需要一个支持多模态输入的模型，例如 Anthropic Claude 或 Amazon Nova 模型。\n图像 # 对于支持视觉多模态的[ 模型]( https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference-supported-models-features.html) ，例如 Amazon Nova、Anthropic Claude、Llama 3.2，Amazon 的 Bedrock Converse API 允许您在负载中包含多个图像。这些[ 模型]( https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference-supported-models-features.html)可以分析传递的图像并回答问题、对图像进行分类，以及根据提供的说明对图像进行汇总。 目前，Bedrock Converse 支持 image/jpeg、image/png、image/gif 和 image/webp mime 类型的 base64 编码图像。 Spring AI 的 Message 接口通过引入 Media 类型来支持多模态 AI 模型。它包含有关消息中媒体附件的数据和信息，使用 Spring org.springframework.util.MimeType 和 java.lang.Object 作为原始媒体数据。 下面是一个简单的代码示例，演示了用户文本与图像的组合。\nString response = ChatClient.create(chatModel) .prompt() .user(u -\u0026gt; u.text(\u0026#34;Explain what do you see on this picture?\u0026#34;) .media(Media.Format.IMAGE_PNG, new ClassPathResource(\u0026#34;/test.png\u0026#34;))) .call() .content(); logger.info(response); 它将 test.png 图像作为输入： 以及文本消息“Explain what do you see on this picture？”，并生成如下响应：\n视频 # [ Amazon Nova 模型]( https://docs.aws.amazon.com/nova/latest/userguide/modalities-video.html)允许您在负载中包含单个视频，该视频可以采用 base64 格式或通过 Amazon S3 URI 提供。 目前，Bedrock Nova 支持 video/x-matros、video/quicktime、video/mp4、video/video/webm、video/x-flv、video/mpeg、video/x-ms-wmv 和 image/3gpp mime 类型的图像。 Spring AI 的 Message 接口通过引入 Media' 类型来支持多模态 AI 模型。它包含有关消息中媒体附件的数据和信息，使用 Spring org.springframework.util.MimeType 和 java.lang.Object 作为原始媒体数据。 下面是一个简单的代码示例，演示了用户文本与视频的组合。\nString response = ChatClient.create(chatModel) .prompt() .user(u -\u0026gt; u.text(\u0026#34;Explain what do you see in this video?\u0026#34;) .media(Media.Format.VIDEO_MP4, new ClassPathResource(\u0026#34;/test.video.mp4\u0026#34;))) .call() .content(); logger.info(response); 它将 test.video.mp4 图像作为输入： 以及文本消息“Explain what do you see in this video？”，并生成如下响应：\n文件 # 对于某些模型，Bedrock 允许您通过 Converse API 文档支持将文档包含在有效负载中，该支持可以以字节为单位提供。文档支持有两种不同的变体，如下所述：\n文本文档类型 （txt、csv、html、md 等），其中重点是文本理解。这些用例包括根据文档的文本元素进行回答。 媒体文档类型 （pdf、docx、xlsx），其中重点是基于视觉的理解来回答问题。这些使用案例包括根据图表、图形等回答问题。 目前，Anthropic [ PDF 支持（测试版）]( https://docs.anthropic.com/en/docs/build-with-claude/pdf-support) 和 Amazon Bedrock Nova 模型支持文档多模态。 下面是一个简单的代码示例，演示了用户文本与媒体文档的组合。 String response = ChatClient.create(chatModel) .prompt() .user(u -\u0026gt; u.text( \u0026#34;You are a very professional document summarization specialist. Please summarize the given document.\u0026#34;) .media(Media.Format.DOC_PDF, new ClassPathResource(\u0026#34;/spring-ai-reference-overview.pdf\u0026#34;))) .call() .content(); logger.info(response); 它将 spring-ai-reference-overview.pdf 文档作为输入： 伴随着短信“您是一位非常专业的文档摘要专家。请总结给定的文档“，并生成如下响应：\n样品控制器 # 创建一个新的 Spring Boot 项目并将其添加到 spring-ai-starter-model-bedrock-converse 您的依赖项中。 在 src/main/resources 下添加 application.properties 文件：\nspring.ai.bedrock.aws.region=eu-central-1 spring.ai.bedrock.aws.timeout=10m spring.ai.bedrock.aws.access-key=${AWS_ACCESS_KEY_ID} spring.ai.bedrock.aws.secret-key=${AWS_SECRET_ACCESS_KEY} # session token is only required for temporary credentials spring.ai.bedrock.aws.session-token=${AWS_SESSION_TOKEN} spring.ai.bedrock.converse.chat.options.temperature=0.8 spring.ai.bedrock.converse.chat.options.top-k=15 下面是一个使用 chat 模型的示例控制器：\n@RestController public class ChatController { private final ChatClient chatClient; @Autowired public ChatController(ChatClient.Builder builder) { this.chatClient = builder.build(); } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, this.chatClient.prompt(message).call().content()); } @GetMapping(\u0026#34;/ai/generateStream\u0026#34;) public Flux\u0026lt;ChatResponse\u0026gt; generateStream(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return this.chatClient.prompt(message).stream().content(); } } "},{"id":23,"href":"/docs/models/embedding-models/","title":"嵌入模型 API","section":"None","content":" 嵌入模型 API # 嵌入是文本、图像或视频的数字表示形式，用于捕获输入之间的关系。 嵌入的工作原理是将文本、图像和视频转换为浮点数数组（称为向量）。这些矢量旨在捕获文本、图像和视频的含义。嵌入数组的长度称为向量的维数。 通过计算两段文本的向量表示之间的数值距离，应用程序可以确定用于生成嵌入向量的对象之间的相似性。 EmbeddingModel 接口旨在与 AI 和机器学习中的嵌入模型直接集成。它的主要功能是将文本转换为数字向量，通常称为嵌入向量。这些嵌入对于各种任务（如语义分析和文本分类）至关重要。 EmbeddingModel 接口的设计围绕两个主要目标：\nAPI 概述 # 嵌入模型 API 构建在通用 [ Spring AI 模型 API]( https://github.com/spring-projects/spring-ai/tree/main/spring-ai-model/src/main/java/org/springframework/ai/model) 之上，后者是 Spring AI 库的一部分。因此，EmbeddingModel 接口扩展了 Model 接口，该接口提供了一组用于与 AI 模型交互的标准方法。EmbeddingRequest 和 EmbeddingResponse 类扩展自 ModelRequest 和 ModelResponse 分别用于封装嵌入模型的输入和输出。 嵌入 API 反过来由更高级别的组件用于为特定嵌入模型（如 OpenAI、Titan、Azure OpenAI、Ollie 等）实施嵌入模型。 下图说明了嵌入 API 及其与 Spring AI 模型 API 和嵌入模型的关系： 本节提供了 EmbeddingModel 接口和关联类的指南。\npublic interface EmbeddingModel extends Model\u0026lt;EmbeddingRequest, EmbeddingResponse\u0026gt; { @Override EmbeddingResponse call(EmbeddingRequest request); /** * Embeds the given document\u0026#39;s content into a vector. * @param document the document to embed. * @return the embedded vector. */ float[] embed(Document document); /** * Embeds the given text into a vector. * @param text the text to embed. * @return the embedded vector. */ default float[] embed(String text) { Assert.notNull(text, \u0026#34;Text must not be null\u0026#34;); return this.embed(List.of(text)).iterator().next(); } /** * Embeds a batch of texts into vectors. * @param texts list of texts to embed. * @return list of list of embedded vectors. */ default List\u0026lt;float[]\u0026gt; embed(List\u0026lt;String\u0026gt; texts) { Assert.notNull(texts, \u0026#34;Texts must not be null\u0026#34;); return this.call(new EmbeddingRequest(texts, EmbeddingOptions.EMPTY)) .getResults() .stream() .map(Embedding::getOutput) .toList(); } /** * Embeds a batch of texts into vectors and returns the {@link EmbeddingResponse}. * @param texts list of texts to embed. * @return the embedding response. */ default EmbeddingResponse embedForResponse(List\u0026lt;String\u0026gt; texts) { Assert.notNull(texts, \u0026#34;Texts must not be null\u0026#34;); return this.call(new EmbeddingRequest(texts, EmbeddingOptions.EMPTY)); } /** * @return the number of dimensions of the embedded vectors. It is generative * specific. */ default int dimensions() { return embed(\u0026#34;Test String\u0026#34;).size(); } } 嵌入方法提供了各种选项，用于将文本转换为嵌入、容纳单个字符串、结构化 Document 对象或批量文本。 提供了多种用于嵌入文本的快捷方法，包括 embed（String text） 方法，该方法采用单个字符串并返回相应的嵌入向量。所有快捷方式都是围绕 call 方法实现的，这是调用嵌入模型的主要方法。 通常，嵌入向量会返回一个浮点数列表，以数字向量格式表示嵌入向量。 embedForResponse 方法提供更全面的输出，可能包括有关嵌入的其他信息。 dimensions 方法是开发人员快速确定嵌入向量大小的便捷工具，这对于理解嵌入空间和后续处理步骤非常重要。\n嵌入请求 # EmbeddingRequest 是一个 ModelRequest，它接受文本对象列表和可选的嵌入请求选项。下面的清单显示了 EmbeddingRequest 类的截断版本，不包括构造函数和其他 Util 方法：\npublic class EmbeddingRequest implements ModelRequest\u0026lt;List\u0026lt;String\u0026gt;\u0026gt; { private final List\u0026lt;String\u0026gt; inputs; private final EmbeddingOptions options; // other methods omitted } EmbeddingResponse 类的结构如下：\npublic class EmbeddingResponse implements ModelResponse\u0026lt;Embedding\u0026gt; { private List\u0026lt;Embedding\u0026gt; embeddings; private EmbeddingResponseMetadata metadata = new EmbeddingResponseMetadata(); // other methods omitted } ```EmbeddingResponse` 类保存 AI 模型的输出，每个 Embedding实例都包含来自单个文本输入的结果向量数据。EmbeddingResponse类还携带有关 AI 模型响应的EmbeddingResponse``Metadata 元数据。\n嵌入 # Embedding 表示单个嵌入向量。\npublic class Embedding implements ModelResult\u0026lt;float[]\u0026gt; { private float[] embedding; private Integer index; private EmbeddingResultMetadata metadata; // other methods omitted } 可用的实现 # 在内部，各种 EmbeddingModel 实现使用不同的低级库和 API 来执行嵌入任务。以下是 EmbeddingModel 实现的一些可用实现：\nSpring AI OpenAI 嵌入 Spring AI Azure OpenAI 嵌入 Spring AI Ollama 嵌入 Spring AI Transformers （ONNX） 嵌入 Spring AI PostgresML 嵌入 Spring AI 基岩 Cohere 嵌入 Spring AI Bedrock Titan 嵌入 Spring AI VertexAI 嵌入 Spring AI Mistral AI 嵌入 Spring AI Oracle Cloud Infrastructure GenAI 嵌入 "},{"id":24,"href":"/docs/getting-started-%E5%BC%80%E5%A7%8B/","title":"开始","section":"Docs","content":" 开始 # 本节提供了如何开始使用 Spring AI 的起点。 您应该根据需要按照以下每个部分中的步骤进行作。\nSpring 初始化 # 前往 [ start.spring.io](https:// start.spring.io/) 并选择要在新应用程序中使用的 AI 模型和矢量存储。\n构件存储库 # 里程碑 - 使用 Maven Central # 从 1.0.0-M6 开始，Maven Central 中提供了版本。无需更改您的构建文件。\n快照 - 添加快照存储库 # 要使用 Snapshot（以及 1.0.0-M6 之前的里程碑）版本，您需要在构建文件中添加以下快照存储库。 将以下存储库定义添加到您的 Maven 或 Gradle 构建文件中： 注意： 将 Maven 与 Spring AI 快照一起使用时，请注意您的 Maven 镜像配置。如果您在 settings.xml 中配置了镜像，如下所示：\n\u0026lt;mirror\u0026gt; \u0026lt;id\u0026gt;my-mirror\u0026lt;/id\u0026gt; \u0026lt;mirrorOf\u0026gt;*\u0026lt;/mirrorOf\u0026gt; \u0026lt;url\u0026gt;https://my-company-repository.com/maven\u0026lt;/url\u0026gt; \u0026lt;/mirror\u0026gt; 通配符 * 会将所有存储库请求重定向到您的镜像，从而阻止访问 Spring 快照存储库。要解决此问题，请修改 mirrorOf 配置以排除 Spring 存储库：\n\u0026lt;mirror\u0026gt; \u0026lt;id\u0026gt;my-mirror\u0026lt;/id\u0026gt; \u0026lt;mirrorOf\u0026gt;*,!spring-snapshots,!central-portal-snapshots\u0026lt;/mirrorOf\u0026gt; \u0026lt;url\u0026gt;https://my-company-repository.com/maven\u0026lt;/url\u0026gt; \u0026lt;/mirror\u0026gt; 此配置允许 Maven 直接访问 Spring 快照存储库，同时仍将镜像用于其他依赖项。\n依赖关系管理 # Spring AI 物料清单 （BOM） 声明了给定版本的 Spring AI 使用的所有依赖项的推荐版本。这是一个仅限 BOM 的版本，它只包含依赖项 Management，不包含插件声明或对 Spring 或 Spring Boot 的直接引用。您可以使用 Spring Boot 父 POM，或使用 Spring Boot 中的 BOM （spring-boot-dependencies） 来管理 Spring Boot 版本。 将 BOM 添加到您的项目中：\n为特定组件添加依赖项 # 文档中的以下每个部分都显示了您需要添加到项目构建系统中的依赖项。\n聊天模型 嵌入模型 图像生成模型 转录模型 文本到语音转换 （TTS） 模型 矢量数据库 Spring AI 示例 # 有关与 Spring AI 相关的更多资源和示例，请参阅[ 此页面]( https://github.com/danvega/awesome-spring-ai) 。\n"},{"id":25,"href":"/docs/models/audio-models/text-to-speech-tts-api/","title":"文本转语音 （TTS） API","section":"None","content":" 文本转语音 （TTS） API # Spring AI 提供了对 OpenAI 的语音 API 的支持。当实现语音的其他提供程序时，将提取通用的 SpeechModel 和 StreamingSpeechModel 接口。\n"},{"id":26,"href":"/docs/models/moderation-models/mistral-ai/","title":"适度","section":"None","content":" 适度 # 介绍 # Spring AI 支持由 Mistral AI 引入并由 Mistral 审核模型提供支持的新审核服务。它支持沿多个策略维度检测有害文本内容。点击此[ 链接]( https://docs.mistral.ai/capabilities/guardrailing/)了解有关 Mistral AI 审核模型的更多信息。\n先决条件 # 自动配置 # Spring AI 为 Mistral AI 审核模型提供了 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-mistral-ai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或复制到您的 Gradle build.gradle 构建文件：\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-mistral-ai\u0026#39; } 审核属性 # 连接属性 # 前缀 spring.ai.mistralai 用作属性前缀，可让您连接到 Mistral AI。\n配置属性 # 前缀 spring.ai.mistralai.moderation 用作配置 Mistral AI 审核模型的属性前缀。\n运行时选项 # MistralAiModerationOptions 类提供了在发出审核请求时要使用的选项。在启动时，使用 spring.ai.mistralai.moderation 指定的选项，但你可以在运行时覆盖这些选项。 例如：\nMistralAiModerationOptions moderationOptions = MistralAiModerationOptions.builder() .model(\u0026#34;mistral-moderation-latest\u0026#34;) .build(); ModerationPrompt moderationPrompt = new ModerationPrompt(\u0026#34;Text to be moderated\u0026#34;, this.moderationOptions); ModerationResponse response = mistralAiModerationModel.call(this.moderationPrompt); // Access the moderation results Moderation moderation = moderationResponse.getResult().getOutput(); // Print general information System.out.println(\u0026#34;Moderation ID: \u0026#34; + moderation.getId()); System.out.println(\u0026#34;Model used: \u0026#34; + moderation.getModel()); // Access the moderation results (there\u0026#39;s usually only one, but it\u0026#39;s a list) for (ModerationResult result : moderation.getResults()) { System.out.println(\u0026#34;\\nModeration Result:\u0026#34;); System.out.println(\u0026#34;Flagged: \u0026#34; + result.isFlagged()); // Access categories Categories categories = this.result.getCategories(); System.out.println(\u0026#34;\\nCategories:\u0026#34;); System.out.println(\u0026#34;Law: \u0026#34; + categories.isLaw()); System.out.println(\u0026#34;Financial: \u0026#34; + categories.isFinancial()); System.out.println(\u0026#34;PII: \u0026#34; + categories.isPii()); System.out.println(\u0026#34;Sexual: \u0026#34; + categories.isSexual()); System.out.println(\u0026#34;Hate: \u0026#34; + categories.isHate()); System.out.println(\u0026#34;Harassment: \u0026#34; + categories.isHarassment()); System.out.println(\u0026#34;Self-Harm: \u0026#34; + categories.isSelfHarm()); System.out.println(\u0026#34;Sexual/Minors: \u0026#34; + categories.isSexualMinors()); System.out.println(\u0026#34;Hate/Threatening: \u0026#34; + categories.isHateThreatening()); System.out.println(\u0026#34;Violence/Graphic: \u0026#34; + categories.isViolenceGraphic()); System.out.println(\u0026#34;Self-Harm/Intent: \u0026#34; + categories.isSelfHarmIntent()); System.out.println(\u0026#34;Self-Harm/Instructions: \u0026#34; + categories.isSelfHarmInstructions()); System.out.println(\u0026#34;Harassment/Threatening: \u0026#34; + categories.isHarassmentThreatening()); System.out.println(\u0026#34;Violence: \u0026#34; + categories.isViolence()); // Access category scores CategoryScores scores = this.result.getCategoryScores(); System.out.println(\u0026#34;\\nCategory Scores:\u0026#34;); System.out.println(\u0026#34;Law: \u0026#34; + scores.getLaw()); System.out.println(\u0026#34;Financial: \u0026#34; + scores.getFinancial()); System.out.println(\u0026#34;PII: \u0026#34; + scores.getPii()); System.out.println(\u0026#34;Sexual: \u0026#34; + scores.getSexual()); System.out.println(\u0026#34;Hate: \u0026#34; + scores.getHate()); System.out.println(\u0026#34;Harassment: \u0026#34; + scores.getHarassment()); System.out.println(\u0026#34;Self-Harm: \u0026#34; + scores.getSelfHarm()); System.out.println(\u0026#34;Sexual/Minors: \u0026#34; + scores.getSexualMinors()); System.out.println(\u0026#34;Hate/Threatening: \u0026#34; + scores.getHateThreatening()); System.out.println(\u0026#34;Violence/Graphic: \u0026#34; + scores.getViolenceGraphic()); System.out.println(\u0026#34;Self-Harm/Intent: \u0026#34; + scores.getSelfHarmIntent()); System.out.println(\u0026#34;Self-Harm/Instructions: \u0026#34; + scores.getSelfHarmInstructions()); System.out.println(\u0026#34;Harassment/Threatening: \u0026#34; + scores.getHarassmentThreatening()); System.out.println(\u0026#34;Violence: \u0026#34; + scores.getViolence()); } 手动配置 # 将 spring-ai-mistral-ai 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-mistral-ai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或复制到您的 Gradle build.gradle 构建文件：\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-mistral-ai\u0026#39; } 接下来，创建一个 MistralAiModerationModel：\nMistralAiModerationApi mistralAiModerationApi = new MistralAiModerationApi(System.getenv(\u0026#34;MISTRAL_AI_API_KEY\u0026#34;)); MistralAiModerationModel mistralAiModerationModel = new MistralAiModerationModel(this.mistralAiModerationApi); MistralAiModerationOptions moderationOptions = MistralAiModerationOptions.builder() .model(\u0026#34;mistral-moderation-latest\u0026#34;) .build(); ModerationPrompt moderationPrompt = new ModerationPrompt(\u0026#34;Text to be moderated\u0026#34;, this.moderationOptions); ModerationResponse response = this.mistralAiModerationModel.call(this.moderationPrompt); 示例代码 # MistralAiModerationModelIT 测试提供了一些有关如何使用该库的一般示例。您可以参考此测试以获取更详细的使用示例。\n"},{"id":27,"href":"/docs/models/chat-models/anthropic-3/","title":"Anthropic 3 聊天","section":"聊天模型 API","content":" Anthropic 3 聊天 # [ Anthropic Claude]( https://www.anthropic.com/) 是一系列基础 AI 模型，可用于各种应用程序。对于开发人员和企业，您可以利用 API 访问并直接在 [ Anthropic 的 AI 基础设施]( https://www.anthropic.com/api)之上进行构建。 Spring AI 支持 Anthropic [ Messaging API]( https://docs.anthropic.com/claude/reference/messages_post) 进行同步和流式文本生成。\n先决条件 # 您需要在 Anthropic 门户上创建一个 API 密钥。 在 [ Anthropic API 控制面板]( https://console.anthropic.com/dashboard)上创建一个账户，并在 [ Get API Keys]( https://console.anthropic.com/settings/keys) 页面上生成 API 密钥。 Spring AI 项目定义了一个名为 spring.ai.anthropic.api-key 的配置属性，您应该将其设置为从 anthropic.com 获取的 API 密钥的值。 您可以在 application.properties 文件中设置此配置属性：\nspring.ai.anthropic.api-key=\u0026lt;your-anthropic-api-key\u0026gt; 为了在处理 API 密钥等敏感信息时增强安全性，您可以使用 Spring 表达式语言 （SpEL） 来引用自定义环境变量：\n# In application.yml spring: ai: anthropic: api-key: ${ANTHROPIC_API_KEY} # In your environment or .env file export ANTHROPIC_API_KEY=\u0026lt;your-anthropic-api-key\u0026gt; 您还可以在应用程序代码中以编程方式设置此配置：\n// Retrieve API key from a secure source or environment variable String apiKey = System.getenv(\u0026#34;ANTHROPIC_API_KEY\u0026#34;); 添加存储库和 BOM # Spring AI 工件发布在 Maven Central 和 Spring Snapshot 存储库中。请参阅 [ Artifact Repositories](../../getting-started.html#artifact-repositories) 部分，将这些存储库添加到您的构建系统中。 为了帮助进行[ 依赖项管理](../../getting-started.html#dependency-management)，Spring AI 提供了一个 BOM（物料清单），以确保在整个项目中使用一致的 Spring AI 版本。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # Spring AI 为 Anthropic Chat Client 提供 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 或 Gradle build.gradle 文件中：\n聊天属性 # 重试属性 # 前缀 spring.ai.retry 用作属性前缀，允许您为 Anthropic 聊天模型配置重试机制。\n连接属性 # 前缀 spring.ai.anthropic 用作允许您连接到 Anthropic 的属性前缀。\n配置属性 # 前缀 spring.ai.anthropic.chat 是属性前缀，它允许你为 Anthropic 配置聊天模型实现。\n运行时选项 # [ AnthropicChatOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-anthropic/src/main/java/org/springframework/ai/anthropic/[AnthropicChatOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-anthropic/src/main/java/org/springframework/ai/anthropic/AnthropicChatOptions.java)) 提供模型配置，例如要使用的模型、温度、最大令牌计数等。 启动时，可以使用 AnthropicChatModel(api, options) constructor 或 spring.ai.anthropic.chat.options.* properties 配置默认选项。 在运行时，您可以通过向 Prompt 调用添加新的、特定于请求的选项来覆盖默认选项。例如，要覆盖特定请求的默认模型和温度：\nChatResponse response = chatModel.call( new Prompt( \u0026#34;Generate the names of 5 famous pirates.\u0026#34;, AnthropicChatOptions.builder() .model(\u0026#34;claude-3-7-sonnet-latest\u0026#34;) .temperature(0.4) .build() )); 工具/函数调用 # 您可以使用 AnthropicChatModel 注册自定义 Java 工具，并让 Anthropic Claude 模型智能地选择输出包含参数的 JSON 对象，以调用一个或多个已注册的函数。这是一种将 LLM 功能与外部工具和 API 连接起来的强大技术。阅读有关[ 工具调用](../tools.html)的更多信息。\n模 态 # 多模态是指模型同时理解和处理来自各种来源的信息的能力，包括文本、pdf、图像、数据格式。\n图像 # 目前，Anthropic Claude 3 支持图像的 base64 源类型，以及 image/jpeg、image/png、image/gif 和 image/webp 媒体类型。有关更多信息，请查看 [ Vision 指南]( https://docs.anthropic.com/claude/docs/vision) 。Anthropic Claude 3.5 Sonnet 还支持 application/pdf 文件的 pdf 源类型。 Spring AI 的 Message 接口通过引入 Media 类型来支持多模态 AI 模型。此类型包含有关消息中媒体附件的数据和信息，使用 Spring org.springframework.util.MimeType 和 java.lang.Object 作为原始媒体数据。 下面是从 [ AnthropicChatModelIT.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-anthropic/src/test/java/org/springframework/ai/anthropic/[AnthropicChatModelIT.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-anthropic/src/test/java/org/springframework/ai/anthropic/AnthropicChatModelIT.java)) 中提取的简单代码示例，演示了用户文本与图像的组合。\nvar imageData = new ClassPathResource(\u0026#34;/multimodal.test.png\u0026#34;); var userMessage = new UserMessage(\u0026#34;Explain what do you see on this picture?\u0026#34;, List.of(new Media(MimeTypeUtils.IMAGE_PNG, this.imageData))); ChatResponse response = chatModel.call(new Prompt(List.of(this.userMessage))); logger.info(response.getResult().getOutput().getContent()); 它将 multimodal.test.png 图像作为输入： 以及文本消息“Explain what do you see on this picture？”，并生成如下响应：\nPDF 格式 # 从 Sonnet 3.5 开始，提供 [ PDF 支持（测试版）。]( https://docs.anthropic.com/en/docs/build-with-claude/pdf-support) 使用 application/pdf 媒体类型将 PDF 文件附加到消息中：\nvar pdfData = new ClassPathResource(\u0026#34;/spring-ai-reference-overview.pdf\u0026#34;); var userMessage = new UserMessage( \u0026#34;You are a very professional document summarization specialist. Please summarize the given document.\u0026#34;, List.of(new Media(new MimeType(\u0026#34;application\u0026#34;, \u0026#34;pdf\u0026#34;), pdfData))); var response = this.chatModel.call(new Prompt(List.of(userMessage))); 样品控制器 # [ 创建一个新]( https://start.spring.io/)的 Spring Boot 项目，并将 添加到您的 spring-ai-starter-model-anthropic pom（或 gradle）依赖项中。 在 src/main/resources 目录下添加 application.properties 文件，以启用和配置 Anthropic 聊天模型：\nspring.ai.anthropic.api-key=YOUR_API_KEY spring.ai.anthropic.chat.options.model=claude-3-5-sonnet-latest spring.ai.anthropic.chat.options.temperature=0.7 spring.ai.anthropic.chat.options.max-tokens=450 这将创建一个 AnthropicChatModel 实现，您可以将其注入到您的类中。下面是一个简单的 @Controller 类示例，该类使用 chat 模型生成文本。\n@RestController public class ChatController { private final AnthropicChatModel chatModel; @Autowired public ChatController(AnthropicChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, this.chatModel.call(message)); } @GetMapping(\u0026#34;/ai/generateStream\u0026#34;) public Flux\u0026lt;ChatResponse\u0026gt; generateStream(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { Prompt prompt = new Prompt(new UserMessage(message)); return this.chatModel.stream(prompt); } } 手动配置 # AnthropicChatModel 实现 ChatModel 和 StreamingChatModel，并使用[ 低级 AnthropicApi 客户端](#low-level-api)连接到 Anthropic 服务。 将 spring-ai-anthropic 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-anthropic\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-anthropic\u0026#39; } 接下来，创建一个 AnthropicChatModel 并将其用于文本生成：\nvar anthropicApi = new AnthropicApi(System.getenv(\u0026#34;ANTHROPIC_API_KEY\u0026#34;)); var chatModel = new AnthropicChatModel(this.anthropicApi, AnthropicChatOptions.builder() .model(\u0026#34;claude-3-opus-20240229\u0026#34;) .temperature(0.4) .maxTokens(200) .build()); ChatResponse response = this.chatModel.call( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); // Or with streaming responses Flux\u0026lt;ChatResponse\u0026gt; response = this.chatModel.stream( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); AnthropicChatOptions 提供聊天请求的配置信息。AnthropicChatOptions.Builder 是 Fluent 选项生成器。\n低级 AnthropicApi 客户端 # [ AnthropicApi]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-anthropic/src/main/java/org/springframework/ai/anthropic/api/[AnthropicApi](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-anthropic/src/main/java/org/springframework/ai/anthropic/api/AnthropicApi.java).java) 为 [ Anthropic Message API]( https://docs.anthropic.com/claude/reference/messages_post) 提供了轻量级的 Java 客户端。 以下类图说明了 AnthropicApi 聊天界面和构建块： 以下是如何以编程方式使用 api 的简单代码段：\nAnthropicApi anthropicApi = new AnthropicApi(System.getenv(\u0026#34;ANTHROPIC_API_KEY\u0026#34;)); AnthropicMessage chatCompletionMessage = new AnthropicMessage( List.of(new ContentBlock(\u0026#34;Tell me a Joke?\u0026#34;)), Role.USER); // Sync request ResponseEntity\u0026lt;ChatCompletionResponse\u0026gt; response = this.anthropicApi .chatCompletionEntity(new ChatCompletionRequest(AnthropicApi.ChatModel.CLAUDE_3_OPUS.getValue(), List.of(this.chatCompletionMessage), null, 100, 0.8, false)); // Streaming request Flux\u0026lt;StreamResponse\u0026gt; response = this.anthropicApi .chatCompletionStream(new ChatCompletionRequest(AnthropicApi.ChatModel.CLAUDE_3_OPUS.getValue(), List.of(this.chatCompletionMessage), null, 100, 0.8, true)); 有关详细信息，请遵循 [ AnthropicApi.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-anthropic/src/main/java/org/springframework/ai/anthropic/api/[AnthropicApi.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-anthropic/src/main/java/org/springframework/ai/anthropic/api/AnthropicApi.java)) 的 JavaDoc。\n低级 API 示例 # AnthropicApiIT.java 测试提供了一些如何使用轻量级库的一般示例。 "},{"id":28,"href":"/docs/vector-databases/apache-cassandra-vector-store/","title":"Apache Cassandra 矢量存储","section":"矢量数据库","content":" Apache Cassandra 矢量存储 # 本节将指导您设置 CassandraVectorStore 以存储文档嵌入并执行相似性搜索。\n什么是 Apache Cassandra？ # [ Apache Cassandra®]( https://cassandra.apache.org) 是真正的开源分布式数据库，以线性可扩展性、经过验证的容错能力和低延迟而闻名，使其成为任务关键型事务数据的完美平台。 其矢量相似性搜索 （VSS） 基于 JVector 库，可确保一流的性能和相关性。 Apache Cassandra 中的向量搜索非常简单：\nSELECT content FROM table ORDER BY content_vector ANN OF query_embedding; 可以[ 在此处]( https://cassandra.apache.org/doc/latest/cassandra/getting-started/vector-search-quickstart.html)阅读有关此内容的更多文档。 这个 Spring AI Vector Store 旨在适用于全新的 RAG 应用程序，并能够在现有数据和表之上进行改造。 该存储还可用于现有数据库中的非 RAG 用例，例如语义搜索、地理位置邻近搜索等。 存储将根据其配置根据需要自动创建或增强架构。如果您不需要修改架构，请使用 initializeSchema 配置存储。 使用 spring-boot-autoconfigure initializeSchema 时，根据 Spring Boot 标准默认为 false，您必须通过设置来选择模式创建/修改 ......initialize-schema=true 在 application.properties 文件中。\n什么是 JVector？ # [ JVector]( https://github.com/jbellis/jvector) 是一个纯 Java 嵌入式矢量搜索引擎。 它从其他 HNSW 向量相似性搜索实现中脱颖而出，具有以下特点：\n算法快速。JVector 使用受 DiskANN 和相关研究启发的最先进的图形算法，这些算法提供高召回率和低延迟。 实施速度快。JVector 使用 Panama SIMD API 来加速索引构建和查询。 内存高效。JVector 使用乘积量化来压缩向量，以便它们在搜索期间可以保留在内存中。 磁盘感知。JVector 的磁盘布局旨在在查询时执行最低的必要 IOPS。 并发的。索引构建以线性方式扩展到至少 32 个线程。线程翻倍，构建时间减半。 增量。在构建索引时查询索引。添加向量和在搜索结果中找到它之间没有延迟。 易于嵌入。API 旨在让人们在生产环境中使用它时轻松嵌入。 先决条件 # 依赖 # 将这些依赖项添加到您的项目中：\n仅适用于 Cassandra Vector Store： \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-cassandra-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者，对于 RAG 应用程序中需要的一切（使用默认的 ONNX 嵌入模型）： \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-cassandra\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 配置属性 # 您可以在 Spring Boot 配置中使用以下属性来自定义 Apache Cassandra 矢量存储。\n用法 # 基本用法 # 创建一个 CassandraVectorStore 实例作为 Spring Bean：\n@Bean public VectorStore vectorStore(CqlSession session, EmbeddingModel embeddingModel) { return CassandraVectorStore.builder(embeddingModel) .session(session) .keyspace(\u0026#34;my_keyspace\u0026#34;) .table(\u0026#34;my_vectors\u0026#34;) .build(); } 拥有向量存储实例后，您可以添加文档并执行搜索：\n// Add documents vectorStore.add(List.of( new Document(\u0026#34;1\u0026#34;, \u0026#34;content1\u0026#34;, Map.of(\u0026#34;key1\u0026#34;, \u0026#34;value1\u0026#34;)), new Document(\u0026#34;2\u0026#34;, \u0026#34;content2\u0026#34;, Map.of(\u0026#34;key2\u0026#34;, \u0026#34;value2\u0026#34;)) )); // Search with filters List\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch( SearchRequest.query(\u0026#34;search text\u0026#34;) .withTopK(5) .withSimilarityThreshold(0.7f) .withFilterExpression(\u0026#34;metadata.key1 == \u0026#39;value1\u0026#39;\u0026#34;) ); 高级配置 # 对于更复杂的用例，您可以在 Spring Bean 中配置其他设置：\n@Bean public VectorStore vectorStore(CqlSession session, EmbeddingModel embeddingModel) { return CassandraVectorStore.builder(embeddingModel) .session(session) .keyspace(\u0026#34;my_keyspace\u0026#34;) .table(\u0026#34;my_vectors\u0026#34;) // Configure primary keys .partitionKeys(List.of( new SchemaColumn(\u0026#34;id\u0026#34;, DataTypes.TEXT), new SchemaColumn(\u0026#34;category\u0026#34;, DataTypes.TEXT) )) .clusteringKeys(List.of( new SchemaColumn(\u0026#34;timestamp\u0026#34;, DataTypes.TIMESTAMP) )) // Add metadata columns with optional indexing .addMetadataColumns( new SchemaColumn(\u0026#34;category\u0026#34;, DataTypes.TEXT, SchemaColumnTags.INDEXED), new SchemaColumn(\u0026#34;score\u0026#34;, DataTypes.DOUBLE) ) // Customize column names .contentColumnName(\u0026#34;text\u0026#34;) .embeddingColumnName(\u0026#34;vector\u0026#34;) // Performance tuning .fixedThreadPoolExecutorSize(32) // Schema management .initializeSchema(true) // Custom batching strategy .batchingStrategy(new TokenCountBatchingStrategy()) .build(); } 连接配置 # 有两种方法可以配置与 Cassandra 的连接：\n使用注入的 CqlSession（推荐）： @Bean public VectorStore vectorStore(CqlSession session, EmbeddingModel embeddingModel) { return CassandraVectorStore.builder(embeddingModel) .session(session) .keyspace(\u0026#34;my_keyspace\u0026#34;) .table(\u0026#34;my_vectors\u0026#34;) .build(); } 直接在生成器中使用连接详细信息： @Bean public VectorStore vectorStore(EmbeddingModel embeddingModel) { return CassandraVectorStore.builder(embeddingModel) .contactPoint(new InetSocketAddress(\u0026#34;localhost\u0026#34;, 9042)) .localDatacenter(\u0026#34;datacenter1\u0026#34;) .keyspace(\u0026#34;my_keyspace\u0026#34;) .build(); } 元数据筛选 # 您可以将通用的可移植元数据筛选器与 CassandraVectorStore 结合使用。要使元数据列可搜索，它们必须是主键或 SAI 索引。要使非主键列编制索引，请使用 SchemaColumnTags.INDEXED 配置 metadata 列。 例如，您可以使用文本表达式语言：\nvectorStore.similaritySearch( SearchRequest.builder().query(\u0026#34;The World\u0026#34;) .topK(5) .filterExpression(\u0026#34;country in [\u0026#39;UK\u0026#39;, \u0026#39;NL\u0026#39;] \u0026amp;\u0026amp; year \u0026gt;= 2020\u0026#34;).build()); 或者以编程方式使用表达式 DSL：\nFilter.Expression f = new FilterExpressionBuilder() .and( f.in(\u0026#34;country\u0026#34;, \u0026#34;UK\u0026#34;, \u0026#34;NL\u0026#34;), f.gte(\u0026#34;year\u0026#34;, 2020) ).build(); vectorStore.similaritySearch( SearchRequest.builder().query(\u0026#34;The World\u0026#34;) .topK(5) .filterExpression(f).build()); 可移植的筛选表达式会自动转换为 [ CQL 查询]( https://cassandra.apache.org/doc/latest/cassandra/developing/cql/index.html) 。\n高级示例：Vector Store on top of Wikipedia Dataset # 以下示例演示如何在现有架构上使用存储。在这里，我们使用 [ github.com/datastax-labs/colbert-wikipedia-data](https:// github.com/datastax-labs/colbert-wikipedia-data) 项目中的架构，该项目附带了为您准备好的完整 wikipedia 数据集。 首先，在 Cassandra 数据库中创建架构：\nwget https://s.apache.org/colbert-wikipedia-schema-cql -O colbert-wikipedia-schema.cql cqlsh -f colbert-wikipedia-schema.cql 然后使用生成器模式配置 store：\n@Bean public VectorStore vectorStore(CqlSession session, EmbeddingModel embeddingModel) { List\u0026lt;SchemaColumn\u0026gt; partitionColumns = List.of( new SchemaColumn(\u0026#34;wiki\u0026#34;, DataTypes.TEXT), new SchemaColumn(\u0026#34;language\u0026#34;, DataTypes.TEXT), new SchemaColumn(\u0026#34;title\u0026#34;, DataTypes.TEXT) ); List\u0026lt;SchemaColumn\u0026gt; clusteringColumns = List.of( new SchemaColumn(\u0026#34;chunk_no\u0026#34;, DataTypes.INT), new SchemaColumn(\u0026#34;bert_embedding_no\u0026#34;, DataTypes.INT) ); List\u0026lt;SchemaColumn\u0026gt; extraColumns = List.of( new SchemaColumn(\u0026#34;revision\u0026#34;, DataTypes.INT), new SchemaColumn(\u0026#34;id\u0026#34;, DataTypes.INT) ); return CassandraVectorStore.builder() .session(session) .embeddingModel(embeddingModel) .keyspace(\u0026#34;wikidata\u0026#34;) .table(\u0026#34;articles\u0026#34;) .partitionKeys(partitionColumns) .clusteringKeys(clusteringColumns) .contentColumnName(\u0026#34;body\u0026#34;) .embeddingColumnName(\u0026#34;all_minilm_l6_v2_embedding\u0026#34;) .indexName(\u0026#34;all_minilm_l6_v2_ann\u0026#34;) .initializeSchema(false) .addMetadataColumns(extraColumns) .primaryKeyTranslator((List\u0026lt;Object\u0026gt; primaryKeys) -\u0026gt; { if (primaryKeys.isEmpty()) { return \u0026#34;test§¶0\u0026#34;; } return String.format(\u0026#34;%s§¶%s\u0026#34;, primaryKeys.get(2), primaryKeys.get(3)); }) .documentIdTranslator((id) -\u0026gt; { String[] parts = id.split(\u0026#34;§¶\u0026#34;); String title = parts[0]; int chunk_no = parts.length \u0026gt; 1 ? Integer.parseInt(parts[1]) : 0; return List.of(\u0026#34;simplewiki\u0026#34;, \u0026#34;en\u0026#34;, title, chunk_no, 0); }) .build(); } @Bean public EmbeddingModel embeddingModel() { // default is ONNX all-MiniLM-L6-v2 which is what we want return new TransformersEmbeddingModel(); } 加载完整的 Wikipedia 数据集 # 要加载完整的 wikipedia 数据集：\n访问 Native Client # Cassandra Vector Store 实现通过 getNativeClient（） 方法提供对底层本机 Cassandra 客户端 （CqlSession） 的访问：\nCassandraVectorStore vectorStore = context.getBean(CassandraVectorStore.class); Optional\u0026lt;CqlSession\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { CqlSession session = nativeClient.get(); // Use the native client for Cassandra-specific operations } 本机客户端允许您访问特定于 Cassandra 的功能和作，这些功能和作可能不会通过 VectorStore 接口公开。\n"},{"id":29,"href":"/docs/model-context-protocol-mcp/mcp-utilities/","title":"MCP 实用程序","section":"模型上下文协议 （MCP）","content":" MCP 实用程序 # MCP 实用程序为将模型上下文协议与 Spring AI 应用程序集成提供了基础支持。这些实用程序实现了 Spring AI 的工具系统和 MCP 服务器之间的无缝通信，支持同步和异步作。它们通常用于编程 MCP 客户端和服务器配置和交互。要获得更简化的配置，请考虑使用 boot starters。\nToolCallback 实用程序 # 工具回调适配器 # 使 MCP 工具适应 Spring AI 的工具接口，同时支持同步和异步执行。\n工具回调提供程序 # 从 MCP 客户端发现并提供 MCP 工具。\nToolCallbacks 到 ToolSpecifications # 将 Spring AI 工具回调转换为 MCP 工具规范：\nMCP 客户端到 ToolCallbacks # 从 MCP 客户端获取工具回调\n本机映像支持 # McpHints 类为 MCP 架构类提供 GraalVM 原生图像提示。在构建本机映像时，此类会自动为 MCP 架构类注册所有必要的反射提示。\n"},{"id":30,"href":"/docs/models/embedding-models/mistral-ai/","title":"Mistral AI 嵌入","section":"嵌入模型 API","content":" Mistral AI 嵌入 # Spring AI 支持 Mistral AI 的文本嵌入模型。嵌入是文本的矢量表示形式，它通过段落在高维向量空间中的位置来捕获段落的语义含义。Mistral AI Embeddings API 为文本提供尖端、最先进的嵌入，可用于许多 NLP 任务。\n先决条件 # 您需要使用 MistralAI 创建一个 API 才能访问 MistralAI 嵌入模型。 在 [ MistralAI 注册页面]( https://auth.mistral.ai/ui/registration)创建一个帐户，并在 [ API 密钥页面上]( https://console.mistral.ai/api-keys/)生成令牌。 Spring AI 项目定义了一个名为 spring.ai.mistralai.api-key 的配置属性，您应该将其设置为从 console.mistral.ai 获取的 API 密钥的值。 您可以在 application.properties 文件中设置此配置属性：\nspring.ai.mistralai.api-key=\u0026lt;your-mistralai-api-key\u0026gt; 为了在处理 API 密钥等敏感信息时增强安全性，您可以使用 Spring 表达式语言 （SpEL） 来引用环境变量：\n# In application.yml spring: ai: mistralai: api-key: ${MISTRALAI_API_KEY} # In your environment or .env file export MISTRALAI_API_KEY=\u0026lt;your-mistralai-api-key\u0026gt; 您还可以在应用程序代码中以编程方式设置此配置：\n// Retrieve API key from a secure source or environment variable String apiKey = System.getenv(\u0026#34;MISTRALAI_API_KEY\u0026#34;); 添加存储库和 BOM # Spring AI 工件发布在 Maven Central 和 Spring Snapshot 存储库中。请参阅 [ Artifact Repositories](../../getting-started.html#artifact-repositories) 部分，将这些存储库添加到您的构建系统中。 为了帮助进行[ 依赖项管理](../../getting-started.html#dependency-management)，Spring AI 提供了一个 BOM（物料清单），以确保在整个项目中使用一致的 Spring AI 版本。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # Spring AI 为 MistralAI 嵌入模型提供 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-mistral-ai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-mistral-ai\u0026#39; } 嵌入属性 # 重试属性 # 前缀 spring.ai.retry 用作属性前缀，允许您为 Mistral AI Embedding 模型配置重试机制。\n连接属性 # 前缀 spring.ai.mistralai 用作属性前缀，可让您连接到 MistralAI。\n配置属性 # 前缀 spring.ai.mistralai.embedding 是属性前缀，用于配置 MistralAI 的 EmbeddingModel 实现。\n运行时选项 # [ MistralAiEmbeddingOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-mistral-ai/src/main/java/org/springframework/ai/mistralai/[MistralAiEmbeddingOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-mistral-ai/src/main/java/org/springframework/ai/mistralai/MistralAiEmbeddingOptions.java)) 提供 MistralAI 配置，例如要使用的模型等。 也可以使用 spring.ai.mistralai.embedding.options properties 配置默认选项。 在开始时，使用 MistralAiEmbeddingModel 构造函数设置用于所有嵌入请求的默认选项。在运行时，您可以使用 MistralAiEmbeddingOptions 实例作为 EmbeddingRequest 的一部分来覆盖默认选项。 例如，要覆盖特定请求的默认模型名称：\nEmbeddingResponse embeddingResponse = embeddingModel.call( new EmbeddingRequest(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;), MistralAiEmbeddingOptions.builder() .withModel(\u0026#34;Different-Embedding-Model-Deployment-Name\u0026#34;) .build())); 样品控制器 # 这将创建一个 EmbeddingModel 实现，您可以将其注入到您的类中。下面是一个使用 EmbeddingModel 实现的简单 @Controller 类的示例。\nspring.ai.mistralai.api-key=YOUR_API_KEY spring.ai.mistralai.embedding.options.model=mistral-embed @RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(\u0026#34;/ai/embedding\u0026#34;) public Map embed(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { var embeddingResponse = this.embeddingModel.embedForResponse(List.of(message)); return Map.of(\u0026#34;embedding\u0026#34;, embeddingResponse); } } 手动配置 # 如果您不使用 Spring Boot，则可以手动配置 OpenAI 嵌入模型。为此，将 spring-ai-mistral-ai 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-mistral-ai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-mistral-ai\u0026#39; } 接下来，创建一个 MistralAiEmbeddingModel 实例，并使用它来计算两个输入文本之间的相似性：\nvar mistralAiApi = new MistralAiApi(System.getenv(\u0026#34;MISTRAL_AI_API_KEY\u0026#34;)); var embeddingModel = new MistralAiEmbeddingModel(this.mistralAiApi, MistralAiEmbeddingOptions.builder() .withModel(\u0026#34;mistral-embed\u0026#34;) .withEncodingFormat(\u0026#34;float\u0026#34;) .build()); EmbeddingResponse embeddingResponse = this.embeddingModel .embedForResponse(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;)); MistralAiEmbeddingOptions 提供嵌入请求的配置信息。options 类提供了一个 builder（） 来轻松创建选项。\n"},{"id":31,"href":"/docs/upgrade-notes-%E5%8D%87%E7%BA%A7%E8%AF%B4%E6%98%8E/","title":"升级说明","section":"Docs","content":" 升级说明 # 升级到 1.0.0-SNAPSHOT # 概述 # 1.0.0-SNAPSHOT 版本包括对构件 ID、软件包名称和模块结构的重大更改。本节提供了特定于使用 SNAPSHOT 版本的指导。\n添加 Snapshot 存储库 # 要使用 1.0.0-SNAPSHOT 版本，您需要将快照存储库添加到构建文件中。有关详细说明，请参阅入门指南中的[ 快照 - 添加快照存储库](getting-started.html#snapshots-add-snapshot-repositories)部分。\n更新依赖项管理 # 在构建配置中将 Spring AI BOM 版本更新为 1.0.0-SNAPSHOT。有关配置依赖项管理的详细说明，请参阅 Getting Started guide 中的 [ Dependency Management](getting-started.html#dependency-management) 部分。\n工件 ID、软件包和模块更改 # 1.0.0-SNAPSHOT 包括对构件 ID、软件包名称和模块结构的更改。 有关详细信息，请参阅：- [ 常见 Artifact ID 更改](#common-artifact-id-changes)\n[ 常见软件包更改](#common-package-changes) [ 通用模块结构](#common-module-structure) 升级到 1.0.0-RC1 # 您可以使用 OpenRewrite 配方自动执行升级到 1.0.0-RC1 的过程。此配方有助于为此版本应用许多必要的代码更改。在 [ Arconia Spring AI Migrations]( https://github.com/arconia-io/arconia-migrations/blob/main/docs/spring-ai.md) 中查找配方和使用说明。\n重大更改 # 聊天客户端和顾问 # 影响最终用户代码的主要更改包括：\n在 VectorStoreChatMemoryAdvisor 中： 常量 CHAT_MEMORY_RETRIEVE_SIZE_KEY 已重命名为 TOP_K。 常量 DEFAULT_CHAT_MEMORY_RESPONSE_SIZE （值：100）已重命名为 DEFAULT_TOP_K，新的默认值为 20。 该常量 CHAT_MEMORY_CONVERSATION_ID_KEY 已重命名为 CONVERSATION_ID 并从 AbstractChatMemoryAdvisor 移动到 ChatMemory 接口。更新导入以使用 org.springframework.ai.chat.memory.ChatMemory.CONVERSATION_ID . Advisor 中的自包含模板 # 执行提示增强的内置 advisor 已更新为使用自包含模板。目标是让每个 advisor 都能够执行模板作，而不会影响其他 advisor 的模板和提示决策，也不受其影响。 如果您为以下顾问提供自定义模板，则需要更新它们以确保包含所有预期的占位符。\nQuestionAnswerAdvisor 需要一个具有以下占位符的模板（查看更多详细信息 ）： 用于接收用户问题的查询占位符。 一个 question_answer_context 占位符，用于接收检索到的上下文。 PromptChatMemoryAdvisor 需要具有以下占位符的模板（查看更多详细信息 ）： 用于接收原始系统消息的 Instructions 占位符。 一个 Memory 占位符，用于接收检索到的对话内存。 VectorStoreChatMemoryAdvisor 需要具有以下占位符的模板（查看更多详细信息 ）： 用于接收原始系统消息的 Instructions 占位符。 用于接收检索到的对话内存的 long_term_memory 占位符。 可观察性 # 重构了内容观察以使用日志记录而不是跟踪 （ca843e8） 将内容观察过滤器替换为日志记录处理程序 重命名了配置属性以更好地反映其用途： 新增 TracingAwareLoggingObservationHandler 跟踪感知日志记录 替换为 micrometer-tracing-bridge-otel 千分尺跟踪 删除了基于事件的跟踪，以支持直接日志记录 删除了对 OTel SDK 的直接依赖 在观察属性（在 ChatClientBuilderProperties、ChatObservationProperties 和 ImageObservationProperties 中）中将 includePrompt 重命名为 logPrompt。 聊天内存存储库模块和自动配置重命名 # 我们通过在整个代码库中添加存储库后缀，对聊天内存组件的命名模式进行了标准化。为清楚起见，此更改会影响 Cassandra、JDBC 和 Neo4j 实现，从而影响工件 ID、Java 包名称和类名称。\n工件 ID # 所有与内存相关的工件现在都遵循一致的模式：\nspring-ai-autoconfigure-model-chat-memory- spring-ai-autoconfigure-model-chat-memory-repository- → spring-ai-starter-model-chat-memory- spring-ai-starter-model-chat-memory-repository- → Java 软件包 # 包路径现在包括 .repository。 段 示例： org.springframework.ai.chat.memory.jdbc → org.springframework.ai.chat.memory.repository.jdbc 配置类 # 主要的自动配置类现在使用 Repository 后缀 示例： JdbcChatMemoryAutoConfiguration → JdbcChatMemoryRepositoryAutoConfiguration 性能 # 配置属性已从 spring.ai.chat.memory.…​ 重命名为 spring.ai.chat.memory.repository.…​ 需要迁移： 更新您的 Maven/Gradle 依赖项以使用新的工件 ID。 更新使用旧软件包或类名称的任何导入、类引用或配置。 消息聚合器重构 # 变化 # MessageAggregator 类已从 org.springframework.ai.chat.model spring-ai-client-chat 模块中的包移动到 spring-ai-model 模块（相同的包名称） aggregateChatClientResponse 方法已从 MessageAggregator 中删除，并移动到 org.springframework.ai.chat.client 包中的新类 ChatClientMessageAggregator 迁移指南 # 如果你直接使用 MessageAggregator 中的 aggregateChatClientResponse 方法，则需要改用新的 ChatClientMessageAggregator 类：\n// Before new MessageAggregator().aggregateChatClientResponse(chatClientResponses, aggregationHandler); // After new ChatClientMessageAggregator().aggregateChatClientResponse(chatClientResponses, aggregationHandler); 不要忘记添加适当的导入：\nimport org.springframework.ai.chat.client.ChatClientMessageAggregator; 沃森 # Watson AI 模型已被删除，因为它基于较旧的文本生成，该文本生成被认为已过时，因为有新的聊天生成模型可用。希望 Watson 能在未来版本的 Spring AI 中重新出现\nMoonShot 和 QianFan # Moonshot 和 Qianfan 已被删除，因为它们无法从中国境外访问。这些已移至 Spring AI Community 存储库。\n删除了 Vector Store # 删除了 HanaDB 矢量存储自动配置 （f3b4624） 内存管理 # 删除了 CassandraChatMemory 实现 （11e3c8f） 简化了聊天内存顾问层次结构并删除了已弃用的 API （848a3fd） 删除了 JdbcChatMemory 中的弃用 （356a68f） 为清楚起见，重构了聊天内存存储库工件 （2d517ee） 为清楚起见，重构了聊天内存存储库自动配置和 Spring Boot 启动器 （f6dba1b） 消息和模板 API # 删除了已弃用的 UserMessage 构造函数 （06edee4） 删除了已弃用的 PromptTemplate 构造函数 （722c77e） 从媒体中删除了已弃用的方法 （228ef10） 重构了 StTemplateRenderer：将 supportStFunctions 重命名为 validateStFunctions （0e15197） 删除了移动 TemplateRender 接口后留下的 TemplateRender 接口 （52675d8） 其他客户端 API 更改 # 删除了 ChatClient 和 Advisor 中的弃用 （4fe74d8） 删除了 OllamaApi 和 AnthropicApi 的弃用 （46be898） 包结构更改 # 删除了 spring-ai-model 中的包间依赖关系循环 （ebfa5b9） 将 MessageAggregator 移动到 spring-ai-model 模块 （54e5c07） 依赖 # 删除了 spring-ai-openai 中未使用的 json-path 依赖项 （9de13d1） 行为更改 # 添加了具有干净自动配置的 Azure OpenAI 的 Entra ID 身份管理 （3dc86d3） 常规清理 # 删除了所有代码弃用 （76bee8c） 和 （b6ce7f3） 升级到 1.0.0-M8 # 您可以使用 OpenRewrite 配方自动执行到 1.0.0-M8 的升级过程。此配方有助于为此版本应用许多必要的代码更改。在 [ Arconia Spring AI Migrations]( https://github.com/arconia-io/arconia-migrations/blob/main/docs/spring-ai.md) 中查找配方和使用说明。\n重大更改 # 从 Spring AI 1.0 M7 升级到 1.0 M8 时，以前注册工具回调的用户会遇到重大更改，这些更改会导致工具调用功能以静默方式失败。这特别影响使用已弃用的 tools（） 方法的代码。\n例 # 下面是一个在 M7 中工作但在 M8 中不再按预期运行的代码示例：\n// This worked in M7 but silently fails in M8 ChatClient chatClient = new OpenAiChatClient(api) .tools(List.of( new Tool(\u0026#34;get_current_weather\u0026#34;, \u0026#34;Get the current weather in a given location\u0026#34;, new ToolSpecification.ToolParameter(\u0026#34;location\u0026#34;, \u0026#34;The city and state, e.g. San Francisco, CA\u0026#34;, true)) )) .toolCallbacks(List.of( new ToolCallback(\u0026#34;get_current_weather\u0026#34;, (toolName, params) -\u0026gt; { // Weather retrieval logic return Map.of(\u0026#34;temperature\u0026#34;, 72, \u0026#34;unit\u0026#34;, \u0026#34;fahrenheit\u0026#34;, \u0026#34;description\u0026#34;, \u0026#34;Sunny\u0026#34;); }) )); 溶液 # 解决方案是使用 toolSpecifications（） 方法，而不是已弃用的 tools（） 方法：\n// This works in M8 ChatClient chatClient = new OpenAiChatClient(api) .toolSpecifications(List.of( new Tool(\u0026#34;get_current_weather\u0026#34;, \u0026#34;Get the current weather in a given location\u0026#34;, new ToolSpecification.ToolParameter(\u0026#34;location\u0026#34;, \u0026#34;The city and state, e.g. San Francisco, CA\u0026#34;, true)) )) .toolCallbacks(List.of( new ToolCallback(\u0026#34;get_current_weather\u0026#34;, (toolName, params) -\u0026gt; { // Weather retrieval logic return Map.of(\u0026#34;temperature\u0026#34;, 72, \u0026#34;unit\u0026#34;, \u0026#34;fahrenheit\u0026#34;, \u0026#34;description\u0026#34;, \u0026#34;Sunny\u0026#34;); }) )); 已删除的实现和 API # 内存管理 # 删除了 CassandraChatMemory 实现 （11e3c8f） 简化了聊天内存顾问层次结构并删除了已弃用的 API （848a3fd） 删除了 JdbcChatMemory 中的弃用 （356a68f） 为清楚起见，重构了聊天内存存储库工件 （2d517ee） 为清楚起见，重构了聊天内存存储库自动配置和 Spring Boot 启动器 （f6dba1b） 客户端 API # 删除了 ChatClient 和 Advisor 中的弃用 （4fe74d8） 对 chatclient 工具调用的重大更改 （5b7849d） 删除了 OllamaApi 和 AnthropicApi 的弃用 （46be898） 消息和模板 API # 删除了已弃用的 UserMessage 构造函数 （06edee4） 删除了已弃用的 PromptTemplate 构造函数 （722c77e） 从媒体中删除了已弃用的方法 （228ef10） 重构了 StTemplateRenderer：将 supportStFunctions 重命名为 validateStFunctions （0e15197） 删除了移动 TemplateRender 接口后留下的 TemplateRender 接口 （52675d8） 模型实现 # 删除了 Watson 文本生成模型 （9e71b16） 删除了千帆代码 （bfcaad7） 删除了 HanaDB 矢量存储自动配置 （f3b4624） 从 OpenAiApi 中删除了 deepseek 选项 （59b36d1） 包结构更改 # 删除了 spring-ai-model 中的包间依赖关系循环 （ebfa5b9） 将 MessageAggregator 移动到 spring-ai-model 模块 （54e5c07） 依赖 # 删除了 spring-ai-openai 中未使用的 json-path 依赖项 （9de13d1） 行为更改 # 可观察性 # 重构了内容观察以使用日志记录而不是跟踪 （ca843e8） 将内容观察过滤器替换为日志记录处理程序 重命名了配置属性以更好地反映其用途： 新增 TracingAwareLoggingObservationHandler 跟踪感知日志记录 替换为 micrometer-tracing-bridge-otel 千分尺跟踪 删除了基于事件的跟踪，以支持直接日志记录 删除了对 OTel SDK 的直接依赖 在观察属性（在 ChatClientBuilderProperties、ChatObservationProperties 和 ImageObservationProperties 中）中将 includePrompt 重命名为 logPrompt。 添加了具有干净自动配置的 Azure OpenAI 的 Entra ID 身份管理 （3dc86d3） 常规清理 # 删除了 1.0.0-M8 （76bee8c） 中的所有弃用 常规弃用清理 （b6ce7f3） 升级到 1.0.0-M7 # 变更概述 # Spring AI 1.0.0-M7 是 RC1 和 GA 版本之前的最后一个里程碑版本。它引入了对工件 ID、软件包名称和模块结构的几项重要更改，这些更改将在最终版本中保留。\n工件 ID、软件包和模块更改 # 1.0.0-M7 包含与 1.0.0-SNAPSHOT 相同的结构更改。 有关详细信息，请参阅：- [ 常见 Artifact ID 更改](#common-artifact-id-changes)\n[ 常见软件包更改](#common-package-changes) [ 通用模块结构](#common-module-structure) MCP Java SDK 升级到 0.9.0 # Spring AI 1.0.0-M7 现在使用 MCP Java SDK 版本 0.9.0，该版本与以前版本相比有重大变化。如果您在应用程序中使用 MCP，则需要更新代码以适应这些更改。 主要更改包括：\n接口重命名 # DefaultMcpSession → McpClientSession 或 McpServerSession 所有 *注册类 → *规范类 服务器创建更改 # 使用 McpServerTransportProvider 而不是 ServerMcpTransport // Before ServerMcpTransport transport = new WebFluxSseServerTransport(objectMapper, \u0026#34;/mcp/message\u0026#34;); var server = McpServer.sync(transport) .serverInfo(\u0026#34;my-server\u0026#34;, \u0026#34;1.0.0\u0026#34;) .build(); // After McpServerTransportProvider transportProvider = new WebFluxSseServerTransportProvider(objectMapper, \u0026#34;/mcp/message\u0026#34;); var server = McpServer.sync(transportProvider) .serverInfo(\u0026#34;my-server\u0026#34;, \u0026#34;1.0.0\u0026#34;) .build(); 处理程序签名更改 # 现在，所有处理程序都接收 exchange 参数作为其第一个参数：\n// Before .tool(calculatorTool, args -\u0026gt; new CallToolResult(\u0026#34;Result: \u0026#34; + calculate(args))) // After .tool(calculatorTool, (exchange, args) -\u0026gt; new CallToolResult(\u0026#34;Result: \u0026#34; + calculate(args))) 通过 Exchange 进行客户端交互 # 以前在服务器上可用的方法现在通过 exchange 对象访问：\n// Before ClientCapabilities capabilities = server.getClientCapabilities(); CreateMessageResult result = server.createMessage(new CreateMessageRequest(...)); // After ClientCapabilities capabilities = exchange.getClientCapabilities(); CreateMessageResult result = exchange.createMessage(new CreateMessageRequest(...)); 根更改处理程序 # // Before .rootsChangeConsumers(List.of( roots -\u0026gt; System.out.println(\u0026#34;Roots changed: \u0026#34; + roots) )) // After .rootsChangeHandlers(List.of( (exchange, roots) -\u0026gt; System.out.println(\u0026#34;Roots changed: \u0026#34; + roots) )) 有关迁移 MCP 代码的完整指南，请参阅 [ MCP 迁移指南]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-docs/src/main/antora/modules/ROOT/pages/mcp-migration.adoc) 。\n启用/禁用模型自动配置 # 之前用于启用/禁用模型自动配置的配置属性已被删除： 默认情况下，如果在 Classpath 上找到模型提供程序（例如 OpenAI、Ollama），则会启用其对相关模型类型（聊天、嵌入等）的相应自动配置。如果存在同一模型类型的多个提供程序（例如，同时 spring-ai-openai-spring-boot-starter 和 spring-ai-ollama-spring-boot-starter ），则可以使用以下属性来选择应处于活动状态的提供程序的自动配置，从而有效地禁用该特定模型类型的其他提供程序。 要完全禁用特定模型类型的自动配置，即使只有一个 provider 存在，也将相应的属性设置为与 Classpath 上的任何 provider 都不匹配的值（例如，none 或 disabled）。 你可以参考 [[SpringAIModels](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/model/SpringAIModels.java)](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/model/[SpringAIModels](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/model/SpringAIModels.java).java) 枚举，以获取已知的提供者值的列表。\n使用 AI 自动升级 # 您可以使用 Claude Code CLI 工具在提供的提示下自动升级到 1.0.0-M7：\n跨版本的常见更改 # 工件 ID 更改 # Spring AI 启动器工件的命名模式已更改。您需要根据以下模式更新依赖项：\n模型起动器： spring-ai-{model}-spring-boot-starter → spring-ai-starter-model-{model} Vector Store 起始剂： spring-ai-{store}-store-spring-boot-starter → spring-ai-starter-vector-store-{store} MCP 启动器： spring-ai-mcp-{type}-spring-boot-starter → spring-ai-starter-mcp-{type} 例子 # 对 Spring AI 自动配置工件的更改 # Spring AI 自动配置已从单个整体工件更改为每个模型、矢量存储和其他组件的单个自动配置工件。进行此更改是为了最大程度地减少不同版本的依赖库冲突（例如 Google Protocol Buffers、Google RPC 等）的影响。通过将自动配置分离到特定于组件的构件中，您可以避免引入不必要的依赖项，并降低应用程序中版本冲突的风险。 原始的整体式构件不再可用：\n\u0026lt;!-- NO LONGER AVAILABLE --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-spring-boot-autoconfigure\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${project.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 相反，每个组件现在都有自己的 autoconfiguration artifact ，遵循以下模式：\n模型自动配置： spring-ai-autoconfigure-model-{model} Vector Store 自动配置： spring-ai-autoconfigure-vector-store-{store} MCP 自动配置： spring-ai-autoconfigure-mcp-{type} 新的 Autoconfiguration 工件示例 # 程序包名称更改 # 您的 IDE 应有助于重构到新的包位置。\nKeywordMetadataEnricher 和 SummaryMetadataEnricher 已从 org.springframework.ai.transformer 移动到 org.springframework.ai.chat.transformer 。 Content、MediaContent 和 Media 已从 org.springframework.ai.model 移动到 org.springframework.ai.content . 模块结构 # 该项目的模块和工件结构发生了重大变化。以前，spring-ai-core 包含所有中央接口，但现在它已被拆分为专门的域模块，以减少应用程序中不必要的依赖关系。 Base 模块，不依赖于其他 Spring AI 模块。包含： - 核心域模型（Document、TextSplitter） - JSON 实用程序和资源处理 - 结构化日志记录和可观测性支持\nspring-ai-模型 # 提供 AI 功能抽象：- ChatModel、EmbeddingModel 和 ImageModel 等接口\n消息类型和提示模板 函数调用框架（ToolDefinition、ToolCallback）- 内容过滤和观察支持 统一的向量数据库抽象： - 用于相似性搜索的 VectorStore 接口 - 使用类似 SQL 的表达式进行高级筛选 - 用于内存中使用的 SimpleVectorStore - 嵌入的批处理支持 高级对话式 AI API： - ChatClient 接口 - 通过 ChatMemory 实现对话持久性 使用 OutputConverter 进行响应转换 基于 advisor 的拦截 同步和反应式流式处理支持 桥接 RAG 的矢量存储聊天： - QuestionAnswerAdvisor：将上下文注入提示 - VectorStoreChatMemoryAdvisor：存储/检索对话历史记录 ChatMemory 的 Apache Cassandra 持久化： - CassandraChatMemory 实现 - 使用 Cassandra 的 QueryBuilder 进行类型安全的 CQL ==== spring-ai-model-chat-memory-neo4j 用于聊天对话的 Neo4j 图形数据库持久性。 弹簧-ai-rag # 用于 Retrieval Augmented Generation 的综合框架： - RAG 管道的模块化架构 - RetrievalAugmentationAdvisor 作为主要入口点 - 具有可组合组件的函数式编程原则\n依赖关系结构 # 依赖项层次结构可以概括为：\nspring-ai-commons（基础） spring-ai-model （取决于共享资源） spring-ai-vector-store 和 spring-ai-client-chat（都取决于模型） spring-ai-advisors-vector-store 和 spring-ai-rag（依赖于 client-chat 和 vector-store） spring-ai-model-chat-memory-* 模块（依赖于 client-chat） ToolContext 更改 # ToolContext 类已得到增强，可同时支持显式和隐式工具解析。工具现在可以是： 从 1.0.0-M7 开始，仅当在提示中明确请求或显式包含在调用中时，工具才会包含在对模型的调用中。 此外，ToolContext 类现在已被标记为 final，不能再扩展。它从来都不应该被子类化。您可以在实例化 ToolContext 时以 Map\u0026lt;String、Object\u0026gt; 的形式添加所需的所有上下文数据。有关更多信息，请查看 [documentation]（[ docs.spring.io/spring-ai/reference/api/tools.html#_tool_context](https:// docs.spring.io/spring-ai/reference/api/tools.html#_tool_context)）。\n升级到 1.0.0-M6 # 对使用接口和 DefaultUsage 实现的更改 # Usage 接口及其默认实现 DefaultUsage 发生了以下变化：\n所需作 # 将对 getGenerationTokens（） 的所有调用替换为 getCompletionTokens（） 更新 DefaultUsage 构造函数调用： JSON Ser/Deser 更改 # 虽然 M6 保持了 generationTokens 字段的 JSON 反序列化的向后兼容性，但此字段将在 M7 中删除。任何使用旧字段名称的持久 JSON 文档都应更新为使用 completionTokens。 新 JSON 格式的示例：\n{ \u0026#34;promptTokens\u0026#34;: 100, \u0026#34;completionTokens\u0026#34;: 50, \u0026#34;totalTokens\u0026#34;: 150 } 更改了 FunctionCallingOptions 在工具调用中的用法 # 每个 ChatModel 实例在构造时都接受一个可选的 ChatOptions 或 FunctionCallingOptions 实例，该实例可用于配置用于调用模型的默认工具。 1.0.0-M6 之前：\n通过默认 FunctionCallingOptions 实例的 functions（） 方法传递的任何工具都包含在从该 ChatModel 实例对模型的每次调用中，可能会被运行时选项覆盖。 通过默认 FunctionCallingOptions 实例的 functionCallbacks（） 方法传递的任何工具都仅用于运行时动态解析（请参阅工具解析 ），但除非明确请求，否则永远不会包含在对模型的任何调用中。 从 1.0.0-M6 开始： 通过 functions（） 方法或默认 FunctionCallingOptions 的 functionCallbacks（） 传递的任何工具 实例现在以相同的方式处理：它包含在从该 ChatModel 实例对模型的每次调用中，可能会被运行时选项覆盖。因此，在对模型的调用中包含工具的方式是一致的，并防止由于 functionCallbacks（） 和所有其他选项之间的行为差异而导致任何混淆。 如果要使工具可用于运行时动态解析，并且仅在明确请求时将其包含在对模型的聊天请求中，则可以使用[ 工具解析](api/tools.html#_tool_resolution)中描述的策略之一。 删除已弃用的 Amazon Bedrock 聊天模型 # 从 1.0.0-M6 开始，Spring AI 过渡到使用 Amazon Bedrock 的 Converse API 来实现 Spring AI 中的所有聊天对话。除 Cohere 和 Titan 的嵌入模型外，所有 Amazon Bedrock Chat 模型都将被删除。\n使用 Spring Boot 3.4.2 进行依赖项管理的更改 # Spring AI 更新以使用 Spring Boot 3.4.2 进行依赖项管理。你可以[ 参考这里]( https://github.com/spring-projects/spring-boot/blob/v3.4.2/spring-boot-project/spring-boot-dependencies/build.gradle) ，了解 Spring Boot 3.4.2 管理的依赖项\n所需作 # 如果要升级到 Spring Boot 3.4.2，请务必参考此文档，了解配置 REST 客户端所需的更改。值得注意的是，如果你在 Classpath 上没有 HTTP 客户端库，这可能会导致使用 JdkClientHttpRequestFactory，而 SimpleClientHttpRequestFactory 以前本来会使用的。要切换到使用 SimpleClientHttpRequestFactory ，您需要设置 spring.http.client.factory=simple 。 如果您使用的是不同版本的 Spring Boot（例如 Spring Boot 3.3.x）并且需要特定版本的依赖项，则可以在构建配置中覆盖它。 Vector Store API 更改 # 在版本 1.0.0-M6 中，VectorStore 接口中的 delete 方法已修改为 void 作，而不是返回 Optional\u0026lt;Boolean\u0026gt;。如果您的代码之前检查了 delete 作的返回值，则需要删除此检查。现在，如果删除失败，该作将引发异常，从而提供更直接的错误处理。\n1.0.0-M6 之前： # Optional\u0026lt;Boolean\u0026gt; result = vectorStore.delete(ids); if (result.isPresent() \u0026amp;\u0026amp; result.get()) { // handle successful deletion } 在 1.0.0-M6 及更高版本中： # vectorStore.delete(ids); // deletion successful if no exception is thrown \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-vertex-ai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-vertex-ai-palm2\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-vertex-ai-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-vertex-ai-palm2-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 主分支将移至版本 0.8.0-SNAPSHOT。它会不稳定一两周。如果您不想处于最前沿，请使用 0.7.1-SNAPSHOT。 您可以像以前一样访问 0.7.1-SNAPSHOT 构件，并且仍然可以访问 0.7.1-SNAPSHOT 文档 。\n0.7.1-SNAPSHOT 依赖项 # 开放人工智能 "},{"id":32,"href":"/docs/models/image-models/","title":"图像模型 API","section":"None","content":" 图像模型 API # Spring Image Model API 旨在成为一个简单且可移植的接口，用于与专门用于图像生成的各种 [ AI 模型](../concepts.html#_models)进行交互，使开发人员能够以最少的代码更改在不同的图像相关模型之间切换。这种设计符合 Spring 的模块化和可互换性理念，确保开发人员能够快速使其应用程序适应与图像处理相关的不同 AI 功能。 此外，通过支持用于输入封装的 ImagePrompt 和用于输出处理的 ImageResponse 等配套类，图像模型 API 统一了与专用于图像生成的 AI 模型的通信。它管理请求准备和响应解析的复杂性，为映像生成功能提供直接和简化的 API 交互。 Spring Image Model API 构建在 Spring AI 通用模型 API 之上，提供特定于图像的抽象和实现。\nAPI 概述 # 本节提供了 Spring Image Model API 接口和相关类的指南。\n图像模型 # 下面是 [ ImageModel]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/image/[ImageModel](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/image/ImageModel.java).java) 接口定义：\n@FunctionalInterface public interface ImageModel extends Model\u0026lt;ImagePrompt, ImageResponse\u0026gt; { ImageResponse call(ImagePrompt request); } ImagePrompt 图像提示 # [[ImagePrompt](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/image/ImagePrompt.java)](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/image/[ImagePrompt](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/image/ImagePrompt.java).java) 是一个 ModelRequest，它封装了 [ ImageMessage]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/image/[ImageMessage](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/image/ImageMessage.java).java) 对象列表和可选的模型请求选项。下面的清单显示了 [[ImagePrompt](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/image/ImagePrompt.java)](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/image/[ImagePrompt](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/image/ImagePrompt.java).java) 类的截断版本，不包括构造函数和其他实用程序方法：\npublic class ImagePrompt implements ModelRequest\u0026lt;List\u0026lt;ImageMessage\u0026gt;\u0026gt; { private final List\u0026lt;ImageMessage\u0026gt; messages; private ImageOptions imageModelOptions; @Override public List\u0026lt;ImageMessage\u0026gt; getInstructions() {...} @Override public ImageOptions getOptions() {...} // constructors and utility methods omitted } 图片消息 # ImageMessage 类封装要使用的文本以及文本在影响生成的图像时应具有的权重。对于支持权重的模型，权重可以是正数或负数。\npublic class ImageMessage { private String text; private Float weight; public String getText() {...} public Float getWeight() {...} // constructors and utility methods omitted } 图像选项 # 表示可以传递给 Image generation model 的选项。ImageOptions 接口扩展了 ModelOptions 接口，用于定义一些可以传递给 AI 模型的可移植选项。 ImageOptions 接口定义如下：\npublic interface ImageOptions extends ModelOptions { Integer getN(); String getModel(); Integer getWidth(); Integer getHeight(); String getResponseFormat(); // openai - url or base64 : stability ai byte[] or base64 } 此外，每个特定于模型的 ImageModel 实现都可以有自己的选项，这些选项可以传递给 AI 模型。例如，OpenAI 图像生成模型有自己的选项，如质量 、 样式等。 这是一项强大的功能，允许开发人员在启动应用程序时使用特定于模型的选项，然后在运行时使用 ImagePrompt 覆盖它们。\n图像响应 # ImageResponse 类的结构如下：\npublic class ImageResponse implements ModelResponse\u0026lt;ImageGeneration\u0026gt; { private final ImageResponseMetadata imageResponseMetadata; private final List\u0026lt;ImageGeneration\u0026gt; imageGenerations; @Override public ImageGeneration getResult() { // get the first result } @Override public List\u0026lt;ImageGeneration\u0026gt; getResults() {...} @Override public ImageResponseMetadata getMetadata() {...} // other methods omitted } [ ImageResponse]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/image/[ImageResponse](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/image/ImageResponse.java).java) 类保存 AI 模型的输出，每个 ImageGeneration 实例都包含单个提示可能产生的多个输出之一。 ImageResponse 类还携带一个 ImageResponseMetadata 对象，其中包含有关 AI 模型响应的元数据。\n图像生成 # 最后，[ ImageGeneration]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/image/[ImageGeneration](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/image/ImageGeneration.java).java) 类从 ModelResult 扩展来表示有关此结果的输出响应和相关元数据：\npublic class ImageGeneration implements ModelResult\u0026lt;Image\u0026gt; { private ImageGenerationMetadata imageGenerationMetadata; private Image image; @Override public Image getOutput() {...} @Override public ImageGenerationMetadata getMetadata() {...} // other methods omitted } 可用的实现 # ImageModel 实现适用于以下 Model 提供程序：\nOpenAI 图像生成 Azure OpenAI 映像生成 QianFan 图像生成 稳定性 AI 图像生成 智普 AI 图像生成 API 文档 # 您可以[ 在此处]( https://docs.spring.io/spring-ai/docs/current-SNAPSHOT/)找到 Javadoc。\n反馈和贡献 # 该项目的 [ GitHub 讨论]( https://github.com/spring-projects/spring-ai/discussions)是发送反馈的好地方。\n"},{"id":33,"href":"/docs/models/image-models/stability/","title":"稳定性 AI 图像生成","section":"图像模型 API","content":" 稳定性 AI 图像生成 # Spring AI 支持 Stability AI 的文本[ 到图像生成模型]( https://platform.stability.ai/docs/api-reference#tag/v1generation) 。\n先决条件 # 您需要使用 Stability AI 创建 API 密钥才能访问他们的 AI 模型。按照他们的[ 入门文档]( https://platform.stability.ai/docs/getting-started/authentication)获取您的 API 密钥。 Spring AI 项目定义了一个名为 spring.ai.stabilityai.api-key 的配置属性，您应该将其设置为从 Stability AI 获取的 API 密钥的值。 您可以在 application.properties 文件中设置此配置属性：\nspring.ai.stabilityai.api-key=\u0026lt;your-stabilityai-api-key\u0026gt; 为了在处理 API 密钥等敏感信息时增强安全性，您可以使用 Spring 表达式语言 （SpEL） 来引用自定义环境变量：\n# In application.yml spring: ai: stabilityai: api-key: ${STABILITYAI_API_KEY} # In your environment or .env file export STABILITYAI_API_KEY=\u0026lt;your-stabilityai-api-key\u0026gt; 您还可以在应用程序代码中以编程方式设置此配置：\n// Retrieve API key from a secure source or environment variable String apiKey = System.getenv(\u0026#34;STABILITYAI_API_KEY\u0026#34;); 自动配置 # Spring AI 为 Stability AI Image Generation Client 提供 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-stability-ai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-stability-ai\u0026#39; } 图像生成属性 # 前缀 spring.ai.stabilityai 用作允许您连接到 Stability AI 的属性前缀。 前缀 spring.ai.stabilityai.image 是属性前缀，它允许你为 Stability AI 配置 ImageModel 实现。\n运行时选项 # [ StabilityAiImageOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-stabilityai/src/main/java/org/springframework/ai/stabilityai/api/[StabilityAiImageOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-stabilityai/src/main/java/org/springframework/ai/stabilityai/api/StabilityAiImageOptions.java)) 提供模型配置，例如要使用的模型、样式、大小等。 启动时，可以使用 StabilityAiImageModel(StabilityAiApi stabilityAiApi, StabilityAiImageOptions options) 构造函数配置默认选项。或者，使用前面描述的属性 spring.ai.openai.image.options.* 。 在运行时，您可以通过向 ImagePrompt 调用添加新的、特定于请求的选项来覆盖默认选项。例如，要覆盖 Stability AI 特定的选项（如质量和要创建的图像数量），请使用以下代码示例：\nImageResponse response = stabilityaiImageModel.call( new ImagePrompt(\u0026#34;A light cream colored mini golden doodle\u0026#34;, StabilityAiImageOptions.builder() .stylePreset(\u0026#34;cinematic\u0026#34;) .N(4) .height(1024) .width(1024).build()) ); "},{"id":34,"href":"/docs/models/chat-models/azure-openai/","title":"Azure OpenAI 聊天","section":"聊天模型 API","content":" Azure OpenAI 聊天 # Azure 的 OpenAI 产品由 ChatGPT 提供支持，超越了传统的 OpenAI 功能，提供具有增强功能的 AI 驱动的文本生成[ 。]( https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/announcing-new-ai-safety-amp-responsible-ai-features-in-azure/ba-p/3983686)Azure 提供额外的 AI 安全和负责任的 AI 功能，如其最近的更新中所述 [ 。]( https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/announcing-new-ai-safety-amp-responsible-ai-features-in-azure/ba-p/3983686) Azure 为 Java 开发人员提供了利用 AI 的全部潜力的机会，方法是将 AI 与一系列 Azure 服务集成，其中包括 AI 相关资源，例如 Azure 上的 Vector Stores。\n先决条件 # Azure OpenAI 客户端提供三个连接选项：使用 Azure API 密钥或使用 OpenAI API 密钥，或使用 Microsoft Entra ID。\nAzure API 密钥和端点 # 要使用 API 密钥访问模型，请从 Azure 门户上的 Azure OpenAI 服务部分获取 Azure OpenAI 终端节点和 api-key。 Spring AI 定义了两个配置属性： 您可以在 application.properties 或 application.yml 文件中设置这些配置属性：\nspring.ai.azure.openai.api-key=\u0026lt;your-azure-api-key\u0026gt; spring.ai.azure.openai.endpoint=\u0026lt;your-azure-endpoint-url\u0026gt; 为了在处理 API 密钥等敏感信息时增强安全性，您可以使用 Spring 表达式语言 （SpEL） 来引用自定义环境变量：\n# In application.yml spring: ai: azure: openai: api-key: ${AZURE_OPENAI_API_KEY} endpoint: ${AZURE_OPENAI_ENDPOINT} # In your environment or .env file export AZURE_OPENAI_API_KEY=\u0026lt;your-azure-openai-api-key\u0026gt; export AZURE_OPENAI_ENDPOINT=\u0026lt;your-azure-openai-endpoint-url\u0026gt; OpenAI 密钥 # 要使用 OpenAI 服务（而不是 Azure）进行身份验证，请提供 OpenAI API 密钥。这会自动将终端节点设置为 [ api.openai.com/v1](https:// api.openai.com/v1)。 使用此方法时，请将 spring.ai.azure.openai.chat.options.deployment-name property 设置为您要使用的 [ OpenAI 模型]( https://platform.openai.com/docs/models)的名称。 在您的应用程序配置中：\nspring.ai.azure.openai.openai-api-key=\u0026lt;your-azure-openai-key\u0026gt; spring.ai.azure.openai.chat.options.deployment-name=\u0026lt;openai-model-name\u0026gt; 在 SPEL 中使用环境变量：\n# In application.yml spring: ai: azure: openai: openai-api-key: ${AZURE_OPENAI_API_KEY} chat: options: deployment-name: ${AZURE_OPENAI_MODEL_NAME} # In your environment or .env file export AZURE_OPENAI_API_KEY=\u0026lt;your-openai-key\u0026gt; export AZURE_OPENAI_MODEL_NAME=\u0026lt;openai-model-name\u0026gt; Microsoft 输入 ID # 对于使用 Microsoft Entra ID（以前称为 Azure Active Directory）的无密钥身份验证， 请仅设置 configuration 属性， spring.ai.azure.openai.endpoint 而不设置上述 api-key 属性。 仅查找 endpoint 属性，您的应用程序将评估用于检索凭证的几个不同选项，并且将使用令牌凭证创建一个 OpenAIClient 实例。\n部署名称 # 要使用 Azure AI 应用程序，您需要通过 [ Azure AI 门户]( https://oai.azure.com/portal)创建 Azure AI 部署。在 Azure 中，每个客户端都必须指定一个部署名称才能连接到 Azure OpenAI 服务。请务必注意，Deployment Name （部署名称 ） 与您选择部署的模型不同。例如，可以将名为“MyAiDeployment”的部署配置为使用 GPT 3.5 Turbo 模型或 GPT 4.0 模型。 要开始使用，请按照以下步骤创建具有默认设置的部署： 此 Azure 配置与 Spring Boot Azure AI Starter 及其自动配置功能的默认配置一致。如果你使用不同的 Deployment Name，请确保相应地更新 configuration 属性：\nspring.ai.azure.openai.chat.options.deployment-name=\u0026lt;my deployment name\u0026gt; Azure OpenAI 和 OpenAI 的不同部署结构会导致 Azure OpenAI 客户端库中有一个名为 deploymentOrModelName 的属性。这是因为在 OpenAI 中没有部署名称 ，只有模型名称 。\n访问 OpenAI 模型 # 您可以将客户端配置为直接使用 OpenAI，而不是 Azure OpenAI 部署的模型。为此，您需要设置 而不是 spring.ai.azure.openai.openai-api-key=\u0026lt;Your OpenAI Key\u0026gt; spring.ai.azure.openai.api-key=\u0026lt;Your Azure OpenAi Key\u0026gt; .\n添加存储库和 BOM # Spring AI 工件发布在 Maven Central 和 Spring Snapshot 存储库中。请参阅 [ Artifact Repositories](../../getting-started.html#artifact-repositories) 部分，将这些存储库添加到您的构建系统中。 为了帮助进行[ 依赖项管理](../../getting-started.html#dependency-management)，Spring AI 提供了一个 BOM（物料清单），以确保在整个项目中使用一致的 Spring AI 版本。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # Spring AI 为 Azure OpenAI 聊天客户端提供 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 或 Gradle build.gradle 构建文件中： Azure OpenAI 聊天客户端是使用 Azure SDK 提供的 [ OpenAIClientBuilder]( https://github.com/Azure/azure-sdk-for-java/blob/main/sdk/openai/azure-ai-openai/src/main/java/com/azure/ai/openai/[OpenAIClientBuilder](https://github.com/Azure/azure-sdk-for-java/blob/main/sdk/openai/azure-ai-openai/src/main/java/com/azure/ai/openai/OpenAIClientBuilder.java).java) 创建的。Spring AI 允许通过提供 Azure[ OpenAIClientBuilder]( https://github.com/Azure/azure-sdk-for-java/blob/main/sdk/openai/azure-ai-openai/src/main/java/com/azure/ai/openai/[OpenAIClientBuilder](https://github.com/Azure/azure-sdk-for-java/blob/main/sdk/openai/azure-ai-openai/src/main/java/com/azure/ai/openai/OpenAIClientBuilder.java).java)Customizer bean 来自定义构建器。 例如，可以使用定制器来更改默认响应超时：\n@Configuration public class AzureOpenAiConfig { @Bean public AzureOpenAIClientBuilderCustomizer responseTimeoutCustomizer() { return openAiClientBuilder -\u0026gt; { HttpClientOptions clientOptions = new HttpClientOptions() .setResponseTimeout(Duration.ofMinutes(5)); openAiClientBuilder.httpClient(HttpClient.createDefault(clientOptions)); }; } } 聊天属性 # 前缀 spring.ai.azure.openai 是用于配置与 Azure OpenAI 的连接的属性前缀。 前缀 spring.ai.azure.openai.chat 是为 Azure OpenAI 配置 ChatModel 实现的属性前缀。\n运行时选项 # [ AzureOpenAiChatOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-azure-openai/src/main/java/org/springframework/ai/azure/openai/[AzureOpenAiChatOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-azure-openai/src/main/java/org/springframework/ai/azure/openai/AzureOpenAiChatOptions.java)) 提供模型配置，例如要使用的模型、温度、频率损失等。 启动时，可以使用 AzureOpenAiChatModel(api, options) constructor 或 spring.ai.azure.openai.chat.options.* properties 配置默认选项。 在运行时，您可以通过向 Prompt 调用添加新的、特定于请求的选项来覆盖默认选项。例如，要覆盖特定请求的默认模型和温度：\nChatResponse response = chatModel.call( new Prompt( \u0026#34;Generate the names of 5 famous pirates.\u0026#34;, AzureOpenAiChatOptions.builder() .deploymentName(\u0026#34;gpt-4o\u0026#34;) .temperature(0.4) .build() )); 函数调用 # 可以使用 AzureOpenAiChatModel 注册自定义 Java 函数，并让模型智能地选择输出包含参数的 JSON 对象，以调用一个或多个已注册的函数。这是一种将 LLM 功能与外部工具和 API 连接起来的强大技术。阅读有关[ 工具调用](../tools.html)的更多信息。\n模 态 # 多模态是指模型同时理解和处理来自各种来源的信息（包括文本、图像、音频和其他数据格式）的能力。目前，Azure OpenAI gpt-4o 模型提供多模式支持。 Azure OpenAI 可以将 base64 编码的图像列表或图像 URL 与消息合并。Spring AI 的 [ Message]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/messages/[Message](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/messages/Message.java).java) 接口通过引入 [ Media]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/model/[Media](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/model/Media.java).java) 类型来促进多模态 AI 模型。此类型包含有关消息中媒体附件的数据和详细信息，将 Spring org.springframework.util.MimeType 和 java.lang.Object 用于原始媒体数据。 下面是从 [ OpenAiChatModelIT.java]( https://github.com/spring-projects/spring-ai/blob/c9a3e66f90187ce7eae7eb78c462ec622685de6c/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/chat/[OpenAiChatModelIT.java](https://github.com/spring-projects/spring-ai/blob/c9a3e66f90187ce7eae7eb78c462ec622685de6c/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/chat/OpenAiChatModelIT.java#L293)#L293) 摘录的代码示例，说明了使用 GPT_4_O 模型将用户文本与图像融合在一起。\nURL url = new URL(\u0026#34;https://docs.spring.io/spring-ai/reference/_images/multimodal.test.png\u0026#34;); String response = ChatClient.create(chatModel).prompt() .options(AzureOpenAiChatOptions.builder().deploymentName(\u0026#34;gpt-4o\u0026#34;).build()) .user(u -\u0026gt; u.text(\u0026#34;Explain what do you see on this picture?\u0026#34;).media(MimeTypeUtils.IMAGE_PNG, this.url)) .call() .content(); 它将 multimodal.test.png 图像作为输入： 以及文本消息 “Explain what do you see on this picture？”，并生成如下响应： 您还可以传入 Classpath 资源而不是 URL，如下例所示\nResource resource = new ClassPathResource(\u0026#34;multimodality/multimodal.test.png\u0026#34;); String response = ChatClient.create(chatModel).prompt() .options(AzureOpenAiChatOptions.builder() .deploymentName(\u0026#34;gpt-4o\u0026#34;).build()) .user(u -\u0026gt; u.text(\u0026#34;Explain what do you see on this picture?\u0026#34;) .media(MimeTypeUtils.IMAGE_PNG, this.resource)) .call() .content(); 样品控制器 # [ 创建一个新]( https://start.spring.io/)的 Spring Boot 项目，并将 添加到您的 spring-ai-starter-model-azure-openai pom（或 gradle）依赖项中。 在 src/main/resources 目录下添加一个 application.properties 文件，以启用和配置 OpenAi 聊天模型：\nspring.ai.azure.openai.api-key=YOUR_API_KEY spring.ai.azure.openai.endpoint=YOUR_ENDPOINT spring.ai.azure.openai.chat.options.deployment-name=gpt-4o spring.ai.azure.openai.chat.options.temperature=0.7 这将创建一个 AzureOpenAiChatModel 实现，你可以将其注入到你的类中。下面是一个简单的 @Controller 类示例，该类使用 chat 模型生成文本。\n@RestController public class ChatController { private final AzureOpenAiChatModel chatModel; @Autowired public ChatController(AzureOpenAiChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, this.chatModel.call(message)); } @GetMapping(\u0026#34;/ai/generateStream\u0026#34;) public Flux\u0026lt;ChatResponse\u0026gt; generateStream(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { Prompt prompt = new Prompt(new UserMessage(message)); return this.chatModel.stream(prompt); } } 手动配置 # AzureOpenAiChatModel 实现 ChatModel 和 StreamingChatModel，并使用 [ Azure OpenAI Java 客户端]( https://learn.microsoft.com/en-us/java/api/overview/azure/ai-openai-readme?view=azure-java-preview) 。 要启用它，请将 spring-ai-azure-openai 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-azure-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-azure-openai\u0026#39; } var openAIClientBuilder = new OpenAIClientBuilder() .credential(new AzureKeyCredential(System.getenv(\u0026#34;AZURE_OPENAI_API_KEY\u0026#34;))) .endpoint(System.getenv(\u0026#34;AZURE_OPENAI_ENDPOINT\u0026#34;)); var openAIChatOptions = AzureOpenAiChatOptions.builder() .deploymentName(\u0026#34;gpt-4o\u0026#34;) .temperature(0.4) .maxTokens(200) .build(); var chatModel = AzureOpenAiChatModel.builder() .openAIClientBuilder(openAIClientBuilder) .defaultOptions(openAIChatOptions) .build(); ChatResponse response = chatModel.call( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); // Or with streaming responses Flux\u0026lt;ChatResponse\u0026gt; streamingResponses = chatModel.stream( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); "},{"id":35,"href":"/docs/models/embedding-models/minimax/","title":"MiniMax 聊天","section":"嵌入模型 API","content":" MiniMax 聊天 # Spring AI 支持 MiniMax 的各种 AI 语言模型。您可以与 MiniMax 语言模型交互，并基于 MiniMax 模型创建多语言对话助手。\n先决条件 # 您需要使用 MiniMax 创建 API 才能访问 MiniMax 语言模型。 在 [ MiniMax 注册页面]( https://www.minimaxi.com/login)创建一个帐户，并在 [ API 密钥页面上]( https://www.minimaxi.com/user-center/basic-information/interface-key)生成令牌。 Spring AI 项目定义了一个名为 spring.ai.minimax.api-key 的配置属性，您应该将其设置为从“API 密钥”页面获取的 API 密钥的值。 您可以在 application.properties 文件中设置此配置属性：\nspring.ai.minimax.api-key=\u0026lt;your-minimax-api-key\u0026gt; 为了在处理 API 密钥等敏感信息时增强安全性，您可以使用 Spring 表达式语言 （SpEL） 来引用环境变量：\n# In application.yml spring: ai: minimax: api-key: ${MINIMAX_API_KEY} # In your environment or .env file export MINIMAX_API_KEY=\u0026lt;your-minimax-api-key\u0026gt; 您还可以在应用程序代码中以编程方式设置此配置：\n// Retrieve API key from a secure source or environment variable String apiKey = System.getenv(\u0026#34;MINIMAX_API_KEY\u0026#34;); 添加存储库和 BOM # Spring AI 工件发布在 Maven Central 和 Spring Snapshot 存储库中。请参阅 [ Artifact Repositories](../../getting-started.html#artifact-repositories) 部分，将这些存储库添加到您的构建系统中。 为了帮助进行[ 依赖项管理](../../getting-started.html#dependency-management)，Spring AI 提供了一个 BOM（物料清单），以确保在整个项目中使用一致的 Spring AI 版本。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # Spring AI 为 Azure MiniMax 嵌入模型提供 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-minimax\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-minimax\u0026#39; } 嵌入属性 # 重试属性 # 前缀 spring.ai.retry 用作属性前缀，它允许你为 MiniMax Embedding 模型配置重试机制。\n连接属性 # 前缀 spring.ai.minimax 用作属性前缀，可让您连接到 MiniMax。\n配置属性 # 前缀 spring.ai.minimax.embedding 是为 MiniMax 配置 EmbeddingModel 实现的属性前缀。\n运行时选项 # [ MiniMaxEmbeddingOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-minimax/src/main/java/org/springframework/ai/minimax/[MiniMaxEmbeddingOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-minimax/src/main/java/org/springframework/ai/minimax/MiniMaxEmbeddingOptions.java)) 提供了 MiniMax 配置，例如要使用的模型等。 也可以使用 spring.ai.minimax.embedding.options properties 配置默认选项。 在启动时，使用 MiniMaxEmbeddingModel 构造函数设置用于所有嵌入请求的默认选项。在运行时，您可以使用 MiniMaxEmbeddingOptions 实例作为 EmbeddingRequest 的一部分来覆盖默认选项。 例如，要覆盖特定请求的默认模型名称：\nEmbeddingResponse embeddingResponse = embeddingModel.call( new EmbeddingRequest(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;), MiniMaxEmbeddingOptions.builder() .model(\u0026#34;Different-Embedding-Model-Deployment-Name\u0026#34;) .build())); 样品控制器 # 这将创建一个 EmbeddingModel 实现，您可以将其注入到您的类中。下面是一个使用 EmbeddingC 实现的简单 @Controller 类的示例。\nspring.ai.minimax.api-key=YOUR_API_KEY spring.ai.minimax.embedding.options.model=embo-01 @RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(\u0026#34;/ai/embedding\u0026#34;) public Map embed(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { EmbeddingResponse embeddingResponse = this.embeddingModel.embedForResponse(List.of(message)); return Map.of(\u0026#34;embedding\u0026#34;, embeddingResponse); } } 手动配置 # 如果您不使用 Spring Boot，则可以手动配置 MiniMax Embedding Model。为此，将 spring-ai-minimax 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-minimax\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-minimax\u0026#39; } 接下来，创建一个 MiniMaxEmbeddingModel 实例，并使用它来计算两个输入文本之间的相似性：\nvar miniMaxApi = new MiniMaxApi(System.getenv(\u0026#34;MINIMAX_API_KEY\u0026#34;)); var embeddingModel = new MiniMaxEmbeddingModel(minimaxApi, MetadataMode.EMBED, MiniMaxEmbeddingOptions.builder().model(\u0026#34;embo-01\u0026#34;).build()); EmbeddingResponse embeddingResponse = this.embeddingModel .embedForResponse(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;)); MiniMaxEmbeddingOptions 提供嵌入请求的配置信息。options 类提供了一个 builder（） 来轻松创建选项。\n"},{"id":36,"href":"/docs/models/image-models/zhipuai/","title":"智普 AI 图像生成","section":"图像模型 API","content":" 智普 AI 图像生成 # Spring AI 支持 CogView，这是 ZhiPuAI 的图像生成模型。\n先决条件 # 您需要使用 ZhiPuAI 创建 API 才能访问 ZhiPu AI 语言模型。 在 [ 智普 AI 注册页面]( https://open.bigmodel.cn/login)创建账号，并在 [ API Keys 页面生成 Token]( https://open.bigmodel.cn/usercenter/apikeys)。 Spring AI 项目定义了一个名为 spring.ai.zhipuai.api-key 的配置属性，您应该将其设置为从 API 密钥页面获取的 API 密钥的值。 您可以在 application.properties 文件中设置此配置属性：\nspring.ai.zhipuai.api-key=\u0026lt;your-zhipuai-api-key\u0026gt; 为了在处理 API 密钥等敏感信息时增强安全性，您可以使用 Spring 表达式语言 （SpEL） 来引用自定义环境变量：\n# In application.yml spring: ai: zhipuai: api-key: ${ZHIPUAI_API_KEY} # In your environment or .env file export ZHIPUAI_API_KEY=\u0026lt;your-zhipuai-api-key\u0026gt; 您还可以在应用程序代码中以编程方式设置此配置：\n// Retrieve API key from a secure source or environment variable String apiKey = System.getenv(\u0026#34;ZHIPUAI_API_KEY\u0026#34;); 添加存储库和 BOM # Spring AI 工件发布在 Maven Central 和 Spring Snapshot 存储库中。请参阅 [ Artifact Repositories](../../getting-started.html#artifact-repositories) 部分，将这些存储库添加到您的构建系统中。 为了帮助进行[ 依赖项管理](../../getting-started.html#dependency-management)，Spring AI 提供了一个 BOM（物料清单），以确保在整个项目中使用一致的 Spring AI 版本。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # Spring AI 为 ZhiPuAI Chat 客户端提供了 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-zhipuai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-zhipuai\u0026#39; } 图像生成属性 # 前缀 spring.ai.zhipuai.image 是属性前缀，允许您为 ZhiPuAI 配置 ImageModel 实现。\n连接属性 # 前缀 spring.ai.zhipuai 用作允许您连接到 ZhiPuAI 的属性前缀。\n配置属性 # 重试属性 # 前缀 spring.ai.retry 用作属性前缀，允许您为 ZhiPuAI Image 客户端配置重试机制。\n运行时选项 # [ ZhiPuAiImageOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-zhipuai/src/main/java/org/springframework/ai/zhipuai/[ZhiPuAiImageOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-zhipuai/src/main/java/org/springframework/ai/zhipuai/ZhiPuAiImageOptions.java)) 提供模型配置，例如要使用的模型、质量、大小等。 启动时，可以使用 ZhiPuAiImageModel(ZhiPuAiImageApi zhiPuAiImageApi) constructor 和 withDefaultOptions(ZhiPuAiImageOptions defaultOptions) method 配置默认选项。或者，使用前面描述的属性 spring.ai.zhipuai.image.options.* 。 在运行时，您可以通过向 ImagePrompt 调用添加新的、特定于请求的选项来覆盖默认选项。例如，要覆盖 ZhiPuAI 特定的选项，例如质量和要创建的图像数量，请使用以下代码示例：\nImageResponse response = zhiPuAiImageModel.call( new ImagePrompt(\u0026#34;A light cream colored mini golden doodle\u0026#34;, ZhiPuAiImageOptions.builder() .quality(\u0026#34;hd\u0026#34;) .N(4) .height(1024) .width(1024).build()) ); "},{"id":37,"href":"/docs/chat-client-api/","title":"聊天客户端 API","section":"Docs","content":" 聊天客户端 API # ChatClient 提供用于与 AI 模型通信的 Fluent API。它支持同步和流式编程模型。 Fluent API 具有构建 [[Prompt](prompt.html#_prompt)](prompt.html#_prompt) 的组成部分的方法，这些部分作为输入传递给 AI 模型。[[Prompt](prompt.html#_prompt)](prompt.html#_prompt) 包含指导 AI 模型的输出和行为的说明文本。从 API 的角度来看，提示由一组消息组成。 AI 模型处理两种主要类型的消息：用户消息（来自用户的直接输入）和系统消息（由系统生成以指导对话）。 这些消息通常包含占位符，这些占位符在运行时根据用户输入进行替换，以自定义 AI 模型对用户输入的响应。 还有一些可以指定的 Prompt 选项，例如要使用的 AI 模型的名称以及控制生成输出的随机性或创造性的温度设置。\n创建 ChatClient # ChatClient 是使用 ChatClient.Builder 对象创建的。您可以为任何 [ ChatModel](chatmodel.html) Spring Boot 自动配置获取自动配置的 ChatClient.Builder 实例，或者以编程方式创建一个实例。\n使用自动配置的 ChatClient.Builder # 在最简单的用例中， Spring AI 提供 Spring Boot 自动配置，创建一个原型 ChatClient.Builder bean 供你注入到你的类中。下面是检索对简单用户请求的 String 响应的简单示例。\n@RestController class MyController { private final ChatClient chatClient; public MyController(ChatClient.Builder chatClientBuilder) { this.chatClient = chatClientBuilder.build(); } @GetMapping(\u0026#34;/ai\u0026#34;) String generation(String userInput) { return this.chatClient.prompt() .user(userInput) .call() .content(); } } 在这个简单的示例中，用户输入设置用户消息的内容。call（） 方法向 AI 模型发送请求，content（） 方法将 AI 模型的响应作为 String 返回。\n使用多个聊天模型 # 在以下几种情况下，您可能需要在单个应用程序中使用多个聊天模型：\n对不同类型的任务使用不同的模型（例如，用于复杂推理的强大模型和用于简单任务的更快、更便宜的模型） 当一个模型服务不可用时实现回退机制 A/B 测试不同的型号或配置 为用户提供基于其偏好的模型选择 组合专用模型（一个用于代码生成，另一个用于创意内容等） 默认情况下，Spring AI 会自动配置单个 ChatClient.Builder Bean。但是，您可能需要在应用程序中使用多个聊天模型。以下是处理此情况的方法： 在所有情况下，您都需要通过设置属性 spring.ai.chat.client.enabled=false 来禁用 ChatClient.Builder 自动配置。 这允许您手动创建多个 ChatClient 实例。 具有单个模型类型的多个 ChatClient # 本节介绍了一个常见的使用案例，您需要创建多个 ChatClient 实例，这些实例都使用相同的底层模型类型，但具有不同的配置。\n// Create ChatClient instances programmatically ChatModel myChatModel = ... // already autoconfigured by Spring Boot ChatClient chatClient = ChatClient.create(myChatModel); // Or use the builder for more control ChatClient.Builder builder = ChatClient.builder(myChatModel); ChatClient customChatClient = builder .defaultSystemPrompt(\u0026#34;You are a helpful assistant.\u0026#34;) .build(); 不同模型类型的 ChatClients # 当使用多个 AI 模型时，您可以为每个模型定义单独的 ChatClient bean：\nimport org.springframework.ai.chat.ChatClient; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; @Configuration public class ChatClientConfig { @Bean public ChatClient openAiChatClient(OpenAiChatModel chatModel) { return ChatClient.create(chatModel); } @Bean public ChatClient anthropicChatClient(AnthropicChatModel chatModel) { return ChatClient.create(chatModel); } } 然后，你可以使用 @Qualifier 注解将这些 bean 注入到你的应用程序组件中：\n@Configuration public class ChatClientExample { @Bean CommandLineRunner cli( @Qualifier(\u0026#34;openAiChatClient\u0026#34;) ChatClient openAiChatClient, @Qualifier(\u0026#34;anthropicChatClient\u0026#34;) ChatClient anthropicChatClient) { return args -\u0026gt; { var scanner = new Scanner(System.in); ChatClient chat; // Model selection System.out.println(\u0026#34;\\nSelect your AI model:\u0026#34;); System.out.println(\u0026#34;1. OpenAI\u0026#34;); System.out.println(\u0026#34;2. Anthropic\u0026#34;); System.out.print(\u0026#34;Enter your choice (1 or 2): \u0026#34;); String choice = scanner.nextLine().trim(); if (choice.equals(\u0026#34;1\u0026#34;)) { chat = openAiChatClient; System.out.println(\u0026#34;Using OpenAI model\u0026#34;); } else { chat = anthropicChatClient; System.out.println(\u0026#34;Using Anthropic model\u0026#34;); } // Use the selected chat client System.out.print(\u0026#34;\\nEnter your question: \u0026#34;); String input = scanner.nextLine(); String response = chat.prompt(input).call().content(); System.out.println(\u0026#34;ASSISTANT: \u0026#34; + response); scanner.close(); }; } } 多个兼容 OpenAI 的 API 终端节点 # OpenAiApi 和 OpenAiChatModel 类提供了一个 mutate（） 方法，允许您创建具有不同属性的现有实例的变体。当您需要使用多个与 OpenAI 兼容的 API 时，这尤其有用。\n@Service public class MultiModelService { private static final Logger logger = LoggerFactory.getLogger(MultiModelService.class); @Autowired private OpenAiChatModel baseChatModel; @Autowired private OpenAiApi baseOpenAiApi; public void multiClientFlow() { try { // Derive a new OpenAiApi for Groq (Llama3) OpenAiApi groqApi = baseOpenAiApi.mutate() .baseUrl(\u0026#34;https://api.groq.com/openai\u0026#34;) .apiKey(System.getenv(\u0026#34;GROQ_API_KEY\u0026#34;)) .build(); // Derive a new OpenAiApi for OpenAI GPT-4 OpenAiApi gpt4Api = baseOpenAiApi.mutate() .baseUrl(\u0026#34;https://api.openai.com\u0026#34;) .apiKey(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;)) .build(); // Derive a new OpenAiChatModel for Groq OpenAiChatModel groqModel = baseChatModel.mutate() .openAiApi(groqApi) .defaultOptions(OpenAiChatOptions.builder().model(\u0026#34;llama3-70b-8192\u0026#34;).temperature(0.5).build()) .build(); // Derive a new OpenAiChatModel for GPT-4 OpenAiChatModel gpt4Model = baseChatModel.mutate() .openAiApi(gpt4Api) .defaultOptions(OpenAiChatOptions.builder().model(\u0026#34;gpt-4\u0026#34;).temperature(0.7).build()) .build(); // Simple prompt for both models String prompt = \u0026#34;What is the capital of France?\u0026#34;; String groqResponse = ChatClient.builder(groqModel).build().prompt(prompt).call().content(); String gpt4Response = ChatClient.builder(gpt4Model).build().prompt(prompt).call().content(); logger.info(\u0026#34;Groq (Llama3) response: {}\u0026#34;, groqResponse); logger.info(\u0026#34;OpenAI GPT-4 response: {}\u0026#34;, gpt4Response); } catch (Exception e) { logger.error(\u0026#34;Error in multi-client flow\u0026#34;, e); } } } ChatClient Fluent API 允许您使用重载的提示方法以三种不同的方式创建提示，以启动 Fluent API：\nprompt（）：这种没有参数的方法可以让你开始使用 Fluent API，从而允许你构建 user、system 和 prompt 的其他部分。 prompt（Prompt prompt）： 此方法接受 Prompt 参数，允许您传入使用 Prompt 的非 Fluent API 创建的 Prompt 实例。 prompt（String content）： 这是一种类似于前面的重载的便捷方法。它获取用户的文本内容。 ChatClient 响应 # ChatClient API 提供了多种使用 Fluent API 格式化 AI 模型的响应的方法。\n返回 ChatResponse # 来自 AI 模型的响应是由 [[ChatResponse](chatmodel.html#ChatResponse)](chatmodel.html#[ChatResponse](chatmodel.html#ChatResponse)) 类型定义的丰富结构。它包括有关响应生成方式的元数据，还可以包含多个响应，称为 [ Generation](chatmodel.html# Generation)s，每个响应都有自己的元数据。元数据包括用于创建响应的标记数（每个标记大约是一个单词的 3/4）。此信息非常重要，因为托管 AI 模型根据每个请求使用的令牌数量收费。 通过在 call（） 方法后调用 chatResponse（） ，返回包含元数据的 ChatResponse 对象的示例如下所示。\nChatResponse chatResponse = chatClient.prompt() .user(\u0026#34;Tell me a joke\u0026#34;) .call() .chatResponse(); 返回实体 # 您通常希望返回从返回的 String 映射的实体类。entity（） 方法提供了此功能。 例如，给定 Java 记录：\nrecord ActorFilms(String actor, List\u0026lt;String\u0026gt; movies) {} 您可以使用 entity（） 方法轻松地将 AI 模型的输出映射到此记录，如下所示：\nActorFilms actorFilms = chatClient.prompt() .user(\u0026#34;Generate the filmography for a random actor.\u0026#34;) .call() .entity(ActorFilms.class); 还有一个带有签名 entity(ParameterizedTypeReference type) 的重载实体方法，可用于指定类型，例如泛型 List：\nList\u0026lt;ActorFilms\u0026gt; actorFilms = chatClient.prompt() .user(\u0026#34;Generate the filmography of 5 movies for Tom Hanks and Bill Murray.\u0026#34;) .call() .entity(new ParameterizedTypeReference\u0026lt;List\u0026lt;ActorFilms\u0026gt;\u0026gt;() {}); 流式响应 # stream（） 方法允许您获得异步响应，如下所示：\nFlux\u0026lt;String\u0026gt; output = chatClient.prompt() .user(\u0026#34;Tell me a joke\u0026#34;) .stream() .content(); 您还可以使用方法 Flux\u0026lt;ChatResponse\u0026gt; chatResponse() 流式传输 ChatResponse 。 将来，我们将提供一种便捷的方法，让您使用反应式 stream（） 方法返回 Java 实体。同时，您应该使用 [ Structured Output Converter](structured-output-converter.html#StructuredOutputConverter) 转换聚合响应显式，如下所示。这也演示了 Fluent API 中参数的使用，这将在文档的后面部分更详细地讨论。\nvar converter = new BeanOutputConverter\u0026lt;\u0026gt;(new ParameterizedTypeReference\u0026lt;List\u0026lt;ActorsFilms\u0026gt;\u0026gt;() {}); Flux\u0026lt;String\u0026gt; flux = this.chatClient.prompt() .user(u -\u0026gt; u.text(\u0026#34;\u0026#34;\u0026#34; Generate the filmography for a random actor. {format} \u0026#34;\u0026#34;\u0026#34;) .param(\u0026#34;format\u0026#34;, this.converter.getFormat())) .stream() .content(); String content = this.flux.collectList().block().stream().collect(Collectors.joining()); List\u0026lt;ActorFilms\u0026gt; actorFilms = this.converter.convert(this.content); 提示模板 # ChatClient Fluent API 允许您将用户和系统文本作为模板提供，其中包含在运行时替换的变量。\nString answer = ChatClient.create(chatModel).prompt() .user(u -\u0026gt; u .text(\u0026#34;Tell me the names of 5 movies whose soundtrack was composed by {composer}\u0026#34;) .param(\u0026#34;composer\u0026#34;, \u0026#34;John Williams\u0026#34;)) .call() .content(); 在内部，ChatClient 使用 PromptTemplate 类来处理用户和系统文本，并将变量替换为运行时提供的值，具体取决于给定的 TemplateRenderer 实现。默认情况下，Spring AI 使用 StTemplateRenderer 实现，该实现基于 Terence Parr 开发的开源 [ StringTemplate]( https://www.stringtemplate.org/) 引擎。 Spring AI 还为不需要模板处理的情况提供了 NoOpTemplateRenderer。 Spring AI 还提供了一个 NoOpTemplateRenderer。 如果您更愿意使用不同的模板引擎，则可以直接向 ChatClient 提供 TemplateRenderer 接口的自定义实现。您也可以继续使用默认的 StTemplateRenderer，但使用自定义配置。 例如，默认情况下，模板变量由 {} 语法标识。如果您计划在提示中包含 JSON，则可能需要使用不同的语法以避免与 JSON 语法冲突。例如，您可以使用 \u0026lt; 和 \u0026gt; 分隔符。\nString answer = ChatClient.create(chatModel).prompt() .user(u -\u0026gt; u .text(\u0026#34;Tell me the names of 5 movies whose soundtrack was composed by \u0026lt;composer\u0026gt;\u0026#34;) .param(\u0026#34;composer\u0026#34;, \u0026#34;John Williams\u0026#34;)) .templateRenderer(StTemplateRenderer.builder().startDelimiterToken(\u0026#39;\u0026lt;\u0026#39;).endDelimiterToken(\u0026#39;\u0026gt;\u0026#39;).build()) .call() .content(); call（） 返回值 # 在 ChatClient 上指定 call（） 方法后，响应类型有几种不同的选项。\nString content（）： 返回响应的 String 内容 ChatResponse chatResponse（）：返回包含多个代以及有关响应的元数据的 ChatResponse 对象，例如用于创建响应的令牌数。 ChatClientResponse chatClientResponse() ：返回一个 ChatClientResponse 对象，其中包含 ChatResponse 对象和 ChatClient 执行上下文，允许您访问在执行顾问程序期间使用的其他数据（例如，在 RAG 流中检索的相关文档）。 entity（） 返回 Java 类型 entity(ParameterizedTypeReference type) ：用于返回实体类型的 Collection。 entity（Class type）： 用于返回特定的实体类型。 entity(StructuredOutputConverter structuredOutputConverter) ：用于指定 StructuredOutputConverter 的实例，以将 String 转换为实体类型。 您还可以调用 stream（） 方法而不是 call（）。 stream（） 返回值 # 在 ChatClient 上指定 stream（） 方法后，响应类型有几个选项：\nFlux content（）：返回 AI 模型生成的字符串的 Flux。 Flux chatResponse() ：返回 ChatResponse 对象的 Flux，其中包含有关响应的其他元数据。 Flux chatClientResponse() ：返回包含 ChatResponse 对象和 ChatClient 执行上下文的 ChatClientResponse 对象的 Flux，使您能够访问在执行顾问程序期间使用的其他数据（例如，在 RAG 流中检索的相关文档）。 使用默认值 # 在 @Configuration 类中创建具有默认系统文本的 ChatClient 可以简化运行时代码。通过设置默认值，您只需在调用 ChatClient 时指定用户文本，无需为运行时代码路径中的每个请求设置系统文本。\n默认系统文本 # 在以下示例中，我们将系统文本配置为始终以海盗的声音回复。为避免在运行时代码中重复系统文本，我们将在 @Configuration 类中创建一个 ChatClient 实例。\n@Configuration class Config { @Bean ChatClient chatClient(ChatClient.Builder builder) { return builder.defaultSystem(\u0026#34;You are a friendly chat bot that answers question in the voice of a Pirate\u0026#34;) .build(); } } 以及一个 @RestController 来调用它：\n@RestController class AIController { private final ChatClient chatClient; AIController(ChatClient chatClient) { this.chatClient = chatClient; } @GetMapping(\u0026#34;/ai/simple\u0026#34;) public Map\u0026lt;String, String\u0026gt; completion(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;completion\u0026#34;, this.chatClient.prompt().user(message).call().content()); } } 通过 curl 调用应用程序端点时，结果为：\n❯ curl localhost:8080/ai/simple {\u0026#34;completion\u0026#34;:\u0026#34;Why did the pirate go to the comedy club? To hear some arrr-rated jokes! Arrr, matey!\u0026#34;} 带参数的默认系统文本 # 在以下示例中，我们将在系统文本中使用占位符来指定在运行时（而不是设计时）完成的声音。\n@Configuration class Config { @Bean ChatClient chatClient(ChatClient.Builder builder) { return builder.defaultSystem(\u0026#34;You are a friendly chat bot that answers question in the voice of a {voice}\u0026#34;) .build(); } } @RestController class AIController { private final ChatClient chatClient; AIController(ChatClient chatClient) { this.chatClient = chatClient; } @GetMapping(\u0026#34;/ai\u0026#34;) Map\u0026lt;String, String\u0026gt; completion(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message, String voice) { return Map.of(\u0026#34;completion\u0026#34;, this.chatClient.prompt() .system(sp -\u0026gt; sp.param(\u0026#34;voice\u0026#34;, voice)) .user(message) .call() .content()); } } 通过 httpie 调用应用程序终端节点时，结果为：\nhttp localhost:8080/ai voice==\u0026#39;Robert DeNiro\u0026#39; { \u0026#34;completion\u0026#34;: \u0026#34;You talkin\u0026#39; to me? Okay, here\u0026#39;s a joke for ya: Why couldn\u0026#39;t the bicycle stand up by itself? Because it was two tired! Classic, right?\u0026#34; } 其他默认值 # 在 ChatClient.Builder 级别，您可以指定默认提示配置。\ndefaultOptions(ChatOptions chatOptions) ：传入 ChatOptions 类中定义的可移植选项或特定于模型的选项，例如 OpenAiChatOptions 中的选项。有关特定于模型的 ChatOptions 实现的更多信息，请参阅 JavaDocs。 defaultFunction(String name, String description, java.util.function.Function\u0026lt;I, O\u0026gt; function) ： 该名称用于在用户文本中引用函数。 该描述解释了函数的用途，并帮助 AI 模型选择正确的函数以获得准确的响应。function 参数是模型将在必要时执行的 Java 函数实例。 defaultFunctions(String…​ functionNames) ：在应用程序上下文中定义的 \u0026lsquo;java.util.Function\u0026rsquo; 的 bean 名称。 defaultUser（String text）， defaultUser（Resource text）， defaultUser(Consumer userSpecConsumer) ： 这些方法允许您定义用户文本。Consumer 允许您使用 lambda 指定用户文本和任何默认参数。 defaultAdvisors(Advisor…​ advisor) ：顾问程序允许修改用于创建提示的数据。QuestionAnswerAdvisor 实现通过在提示后附加与用户文本相关的上下文信息 Retrieval Augmented Generation 来启用模式。 defaultAdvisors(Consumer advisorSpecConsumer) ：此方法允许您定义一个 Consumer 以使用 AdvisorSpec 配置多个 advisor。顾问可以修改用于创建最终 Prompt 的数据。Consumer 允许您指定一个 lambda 来添加顾问，例如 QuestionAnswerAdvisor，它支持 Retrieval Augmented Generation 根据用户文本在提示中附加相关上下文信息。 您可以在运行时使用不带 default 前缀的相应方法覆盖这些默认值。 user（字符串文本）， user（资源文本）， user(Consumer userSpecConsumer) 顾问 # [ Advisors API](advisors.html) 提供了一种灵活而强大的方法来拦截、修改和增强 Spring 应用程序中的 AI 驱动的交互。 使用用户文本调用 AI 模型时，一种常见模式是使用上下文数据附加或增强提示。 此上下文数据可以是不同的类型。常见类型包括：\n您自己的数据 ：这是 AI 模型尚未训练的数据。即使模型看到了类似的数据，附加的上下文数据在生成响应时也优先。 对话历史记录 ：聊天模型的 API 是无状态的。如果您告诉 AI 模型您的名字，它将在后续交互中不会记住它。必须随每个请求发送对话历史记录，以确保在生成响应时考虑以前的交互。 ChatClient 中的 Advisor 配置 # ChatClient Fluent API 提供了用于配置 advisor 的 AdvisorSpec 接口。此接口提供了添加参数、一次设置多个参数以及将一个或多个 advisor 添加到链的方法。\ninterface AdvisorSpec { AdvisorSpec param(String k, Object v); AdvisorSpec params(Map\u0026lt;String, Object\u0026gt; p); AdvisorSpec advisors(Advisor... advisors); AdvisorSpec advisors(List\u0026lt;Advisor\u0026gt; advisors); } ChatClient.builder(chatModel) .build() .prompt() .advisors( MessageChatMemoryAdvisor.builder(chatMemory).build(), QuestionAnswerAdvisor.builder(vectorStore).build() ) .user(userText) .call() .content(); 在此配置中，将首先执行 MessageChatMemoryAdvisor，并将对话历史记录添加到提示符中。然后，QuestionAnswerAdvisor 将根据用户的问题和添加的对话历史记录执行搜索，从而可能提供更相关的结果。 了解 Question Answer Advisor\n检索增强一代 # 请参阅 [ Retrieval Augmented Generation](retrieval-augmented-generation.html) 指南。\n伐木 # SimpleLoggerAdvisor 是一个 advisor，用于记录 ChatClient 的请求和响应数据。这对于调试和监控 AI 交互非常有用。 要启用日志记录，请在创建 ChatClient 时将 SimpleLoggerAdvisor 添加到 advisor 链中。建议将其添加到链的末尾：\nChatResponse response = ChatClient.create(chatModel).prompt() .advisors(new SimpleLoggerAdvisor()) .user(\u0026#34;Tell me a joke?\u0026#34;) .call() .chatResponse(); 要查看日志，请将 advisor 包的日志记录级别设置为 DEBUG： 将此文件添加到您的 application.properties 或 application.yaml 文件中。 您可以使用以下构造函数自定义记录 AdvisedRequest 和 ChatResponse 中的数据：\nSimpleLoggerAdvisor( Function\u0026lt;AdvisedRequest, String\u0026gt; requestToString, Function\u0026lt;ChatResponse, String\u0026gt; responseToString ) SimpleLoggerAdvisor customLogger = new SimpleLoggerAdvisor( request -\u0026gt; \u0026#34;Custom request: \u0026#34; + request.userText, response -\u0026gt; \u0026#34;Custom response: \u0026#34; + response.getResult() ); 有关更多详细信息和使用示例，请参阅 [ Chat Memory](chat-memory.html) 文档。\n实施说明 # 在 ChatClient 中结合使用命令式和反应式编程模型是 API 的一个独特方面。通常，应用程序要么是反应式的，要么是命令式的，但不能同时是两者。\n在自定义 Model 实现的 HTTP 客户端交互时，必须同时配置 RestClient 和 WebClient。 流仅支持通过 Reactive 堆栈。因此，命令式应用程序必须包含 Reactive 堆栈（例如 spring-boot-starter-webflux）。 非流式处理仅支持 Servlet 堆栈。出于这个原因，反应式应用程序必须包含 Servlet 堆栈（例如 spring-boot-starter-web），并预期某些调用会阻塞。 工具调用势在必行，这会导致工作流受阻。这也会导致部分/中断的 Micrometer 观察（例如，ChatClient span 和工具调用 span 未连接，由于该原因，第一个 span 保持不完整）。 内置顾问程序对标准调用执行阻塞作，对流式调用执行非阻塞作。的 Reactor Scheduler 用于 advisor 流式调用，可以通过每个 Advisor 类上的 Builder 进行配置。 "},{"id":38,"href":"/docs/vector-databases/chroma/","title":"色度","section":"矢量数据库","content":" 色度 # 本节将引导您设置 Chroma VectorStore 以存储文档嵌入并执行相似性搜索。 [ Chroma]( https://docs.trychroma.com/) 是开源嵌入数据库。它为您提供了存储文档嵌入、内容和元数据以及搜索这些嵌入（包括元数据筛选）的工具。\n先决条件 # 启动时，ChromaVectorStore 会创建所需的集合（如果尚未预置）。\n自动配置 # Spring AI 为 Chroma Vector Store 提供了 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-chroma\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-vector-store-chroma\u0026#39; } 矢量存储实现可以为您初始化必要的架构，但您必须通过在相应的构造函数中指定 initializeSchema 布尔值或设置 ...initialize-schema=true 在 application.properties 文件中。 此外，您将需要一个已配置的 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) bean。请参阅 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) 部分以了解更多信息。 以下是所需 bean 的示例：\n@Bean public EmbeddingModel embeddingModel() { // Can be any other EmbeddingModel implementation. return new OpenAiEmbeddingModel(OpenAiApi.builder().apiKey(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;)).build()); } 要连接到 Chroma，您需要提供实例的访问详细信息。可以通过 Spring Boot 的 application.properties 提供简单的配置，\n# Chroma Vector Store connection properties spring.ai.vectorstore.chroma.client.host=\u0026lt;your Chroma instance host\u0026gt; spring.ai.vectorstore.chroma.client.port=\u0026lt;your Chroma instance port\u0026gt; spring.ai.vectorstore.chroma.client.key-token=\u0026lt;your access token (if configure)\u0026gt; spring.ai.vectorstore.chroma.client.username=\u0026lt;your username (if configure)\u0026gt; spring.ai.vectorstore.chroma.client.password=\u0026lt;your password (if configure)\u0026gt; # Chroma Vector Store collection properties spring.ai.vectorstore.chroma.initialize-schema=\u0026lt;true or false\u0026gt; spring.ai.vectorstore.chroma.collection-name=\u0026lt;your collection name\u0026gt; # Chroma Vector Store configuration properties # OpenAI API key if the OpenAI auto-configuration is used. spring.ai.openai.api.key=\u0026lt;OpenAI Api-key\u0026gt; 请查看 vector store 的[ 配置参数](#_configuration_properties)列表，了解默认值和配置选项。 现在，您可以在应用程序中自动连接 Chroma Vector Store 并使用它\n@Autowired VectorStore vectorStore; // ... List \u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); // Add the documents vectorStore.add(documents); // Retrieve documents similar to a query List\u0026lt;Document\u0026gt; results = this.vectorStore.similaritySearch(SearchRequest.builder().query(\u0026#34;Spring\u0026#34;).topK(5).build()); 配置属性 # 您可以在 Spring Boot 配置中使用以下属性来自定义矢量存储。\n元数据筛选 # 您还可以将通用的可移植[ 元数据过滤器]( https://docs.spring.io/spring-ai/reference/api/vectordbs.html#_metadata_filters)与 ChromaVector 存储结合使用。 例如，您可以使用文本表达式语言：\nvectorStore.similaritySearch( SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(\u0026#34;author in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; article_type == \u0026#39;blog\u0026#39;\u0026#34;).build()); 或使用 Filter.Expression DSL 以编程方式：\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(b.and( b.in(\u0026#34;john\u0026#34;, \u0026#34;jill\u0026#34;), b.eq(\u0026#34;article_type\u0026#34;, \u0026#34;blog\u0026#34;)).build()).build()); 例如，此可移植筛选条件表达式：\nauthor in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; article_type == \u0026#39;blog\u0026#39; 转换为专有的 Chroma 格式\n{\u0026#34;$and\u0026#34;:[ {\u0026#34;author\u0026#34;: {\u0026#34;$in\u0026#34;: [\u0026#34;john\u0026#34;, \u0026#34;jill\u0026#34;]}}, {\u0026#34;article_type\u0026#34;:{\u0026#34;$eq\u0026#34;:\u0026#34;blog\u0026#34;}}] } 手动配置 # 如果你更喜欢手动配置 Chroma Vector Store，则可以通过在 Spring Boot 应用程序中创建一个 ChromaVectorStore bean 来实现。 将以下依赖项添加到您的项目中： * Chroma VectorStore.\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-chroma-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; OpenAI：计算嵌入时需要。您可以使用任何其他嵌入模型实现。 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 示例代码 # 使用适当的 ChromaDB 授权配置创建一个 RestClient.Builder 实例，并使用它来创建一个 ChromaApi 实例：\n@Bean public RestClient.Builder builder() { return RestClient.builder().requestFactory(new SimpleClientHttpRequestFactory()); } @Bean public ChromaApi chromaApi(RestClient.Builder restClientBuilder) { String chromaUrl = \u0026#34;http://localhost:8000\u0026#34;; ChromaApi chromaApi = new ChromaApi(chromaUrl, restClientBuilder); return chromaApi; } 通过将 Spring Boot OpenAI 启动器添加到您的项目，与 OpenAI 的嵌入集成。这为您提供了 Embeddings 客户端的实现：\n@Bean public VectorStore chromaVectorStore(EmbeddingModel embeddingModel, ChromaApi chromaApi) { return ChromaVectorStore.builder(chromaApi, embeddingModel) .collectionName(\u0026#34;TestCollection\u0026#34;) .initializeSchema(true) .build(); } 在您的主代码中，创建一些文档：\nList\u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); 将文档添加到您的 vector 存储中：\nvectorStore.add(documents); 最后，检索类似于查询的文档：\nList\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch(\u0026#34;Spring\u0026#34;); 如果一切顺利，您应该检索包含文本 “Spring AI rocks！！” 的文档。\n在本地运行 Chroma # docker run -it --rm --name chroma -p 8000:8000 ghcr.io/chroma-core/chroma:1.0.0 在 [ localhost：8000/api/v1](http://localhost:8000/api/v1) 启动 chroma 存储\n"},{"id":39,"href":"/docs/models/chat-models/deepseek/","title":"DeepSeek 聊天","section":"聊天模型 API","content":" DeepSeek 聊天 # Spring AI 支持 DeepSeek 中的各种 AI 语言模型。您可以与 DeepSeek 语言模型交互，并基于 DeepSeek 模型创建多语言对话助手。\n先决条件 # 您需要使用 DeepSeek 创建一个 API 密钥才能访问 DeepSeek 语言模型。 在 [ DeepSeek 注册页面]( https://platform.deepseek.com/sign_up)创建一个帐户，并在 [ API 密钥页面上]( https://platform.deepseek.com/api_keys)生成令牌。 Spring AI 项目定义了一个名为 spring.ai.deepseek.api-key 的配置属性，您应该将其设置为从 API 密钥页面获取的 API 密钥的值。 您可以在 application.properties 文件中设置此配置属性：\nspring.ai.deepseek.api-key=\u0026lt;your-deepseek-api-key\u0026gt; 为了在处理 API 密钥等敏感信息时增强安全性，您可以使用 Spring 表达式语言 （SpEL） 来引用自定义环境变量：\n# In application.yml spring: ai: deepseek: api-key: ${DEEPSEEK_API_KEY} # In your environment or .env file export DEEPSEEK_API_KEY=\u0026lt;your-deepseek-api-key\u0026gt; 您还可以在应用程序代码中以编程方式设置此配置：\n// Retrieve API key from a secure source or environment variable String apiKey = System.getenv(\u0026#34;DEEPSEEK_API_KEY\u0026#34;); 添加存储库和 BOM # Spring AI 工件发布在 Spring Milestone 和 Snapshot 存储库中。请参阅 [ Artifact Repositories](../../getting-started.html#artifact-repositories) 部分，将这些存储库添加到您的构建系统中。 为了帮助进行[ 依赖项管理](../../getting-started.html#dependency-management)，Spring AI 提供了一个 BOM（物料清单），以确保在整个项目中使用一致的 Spring AI 版本。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # Spring AI 为 DeepSeek 聊天模型提供 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-deepseek\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-deepseek\u0026#39; } 聊天属性 # 重试属性 # 前缀 spring.ai.retry 用作属性前缀，允许您为 DeepSeek Chat 模型配置重试机制。\n连接属性 # 前缀 spring.ai.deepseek 用作允许您连接到 DeepSeek 的属性前缀。\n配置属性 # 前缀 spring.ai.deepseek.chat 是允许您为 DeepSeek 配置聊天模型实现的属性前缀。\n运行时选项 # [ DeepSeekChatOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-deepseek/src/main/java/org/springframework/ai/deepseek/[DeepSeekChatOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-deepseek/src/main/java/org/springframework/ai/deepseek/DeepSeekChatOptions.java)) 提供模型配置，例如要使用的模型、温度、频率损失等。 启动时，可以使用 DeepSeekChatModel(api, options) constructor 或 spring.ai.deepseek.chat.options.* properties 配置默认选项。 在运行时，您可以通过向 Prompt 调用添加新的特定于请求的选项来覆盖默认选项。例如，要覆盖特定请求的默认型号和温度：\nChatResponse response = chatModel.call( new Prompt( \u0026#34;Generate the names of 5 famous pirates. Please provide the JSON response without any code block markers such as ```json```.\u0026#34;, DeepSeekChatOptions.builder() .withModel(DeepSeekApi.ChatModel.DEEPSEEK_CHAT.getValue()) .withTemperature(0.8f) .build() )); 样品控制器（自动配置） # [ 创建一个新]( https://start.spring.io/)的 Spring Boot 项目，并将 添加到您的 spring-ai-starter-model-deepseek pom（或 gradle）依赖项中。 在 src/main/resources 目录下添加一个 application.properties 文件，以启用和配置 DeepSeek Chat 模型：\nspring.ai.deepseek.api-key=YOUR_API_KEY spring.ai.deepseek.chat.options.model=deepseek-chat spring.ai.deepseek.chat.options.temperature=0.8 这将创建一个 DeepSeekChatModel 实现，您可以将其注入到您的类中。下面是一个使用 chat 模型生成文本的简单 @Controller 类的示例。\n@RestController public class ChatController { private final DeepSeekChatModel chatModel; @Autowired public ChatController(DeepSeekChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, chatModel.call(message)); } @GetMapping(\u0026#34;/ai/generateStream\u0026#34;) public Flux\u0026lt;ChatResponse\u0026gt; generateStream(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { var prompt = new Prompt(new UserMessage(message)); return chatModel.stream(prompt); } } 聊天前缀完成 # 聊天前缀完成遵循 Chat Completion API，其中用户为模型提供助手的前缀消息以完成消息的其余部分。 使用前缀补全时，用户必须确保消息列表中的最后一条消息是 DeepSeekAssistantMessage。 以下是用于完成聊天前缀的完整 Java 代码示例。在此示例中，我们将助手的前缀消息设置为 “\u0026lsquo;\u0026lsquo;\u0026lsquo;python\\n” 以强制模型输出 Python 代码，并将 stop 参数设置为 [\u0026rsquo;\u0026rsquo;\u0026rsquo;] 以防止模型提供额外的解释。\n@RestController public class CodeGenerateController { private final DeepSeekChatModel chatModel; @Autowired public ChatController(DeepSeekChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generatePythonCode\u0026#34;) public String generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Please write quick sort code\u0026#34;) String message) { UserMessage userMessage = new UserMessage(message); Message assistantMessage = DeepSeekAssistantMessage.prefixAssistantMessage(\u0026#34;```python\\\\n\u0026#34;); Prompt prompt = new Prompt(List.of(userMessage, assistantMessage), ChatOptions.builder().stopSequences(List.of(\u0026#34;```\u0026#34;)).build()); ChatResponse response = chatModel.call(prompt); return response.getResult().getOutput().getText(); } } 推理模型 （deepseek-reasoner） # deepseek-reasoner 是由 DeepSeek 开发的一个推理模型。在提供最终答案之前，模型首先生成一个思维链 （CoT） 以提高其响应的准确性。我们的 API 为用户提供了对 deepseek-reasoner 生成的 CoT 内容的访问权限，使他们能够查看、显示和提取这些内容。 您可以使用 DeepSeekAssistantMessage 获取 deepseek-reasoner 生成的 CoT 内容。\npublic void deepSeekReasonerExample() { DeepSeekChatOptions promptOptions = DeepSeekChatOptions.builder() .model(DeepSeekApi.ChatModel.DEEPSEEK_REASONER.getValue()) .build(); Prompt prompt = new Prompt(\u0026#34;9.11 and 9.8, which is greater?\u0026#34;, promptOptions); ChatResponse response = chatModel.call(prompt); // Get the CoT content generated by deepseek-reasoner, only available when using deepseek-reasoner model DeepSeekAssistantMessage deepSeekAssistantMessage = (DeepSeekAssistantMessage) response.getResult().getOutput(); String reasoningContent = deepSeekAssistantMessage.getReasoningContent(); String text = deepSeekAssistantMessage.getText(); } 推理模型 多轮对话 # 在每一轮对话中，模型都会输出 CoT （reasoning_content） 和最终答案 （content）。在下一轮对话中，前几轮的 CoT 不会连接到上下文中，如下图所示： 请注意，如果 reasoning_content 字段包含在输入消息序列中，则 API 将返回 400 错误。因此，在发出 API 请求之前，您应该从 API 响应中删除 reasoning_content 字段，如 API 示例所示。\npublic String deepSeekReasonerMultiRoundExample() { List\u0026lt;Message\u0026gt; messages = new ArrayList\u0026lt;\u0026gt;(); messages.add(new UserMessage(\u0026#34;9.11 and 9.8, which is greater?\u0026#34;)); DeepSeekChatOptions promptOptions = DeepSeekChatOptions.builder() .model(DeepSeekApi.ChatModel.DEEPSEEK_REASONER.getValue()) .build(); Prompt prompt = new Prompt(messages, promptOptions); ChatResponse response = chatModel.call(prompt); DeepSeekAssistantMessage deepSeekAssistantMessage = (DeepSeekAssistantMessage) response.getResult().getOutput(); String reasoningContent = deepSeekAssistantMessage.getReasoningContent(); String text = deepSeekAssistantMessage.getText(); messages.add(new AssistantMessage(Objects.requireNonNull(text))); messages.add(new UserMessage(\u0026#34;How many Rs are there in the word \u0026#39;strawberry\u0026#39;?\u0026#34;)); Prompt prompt2 = new Prompt(messages, promptOptions); ChatResponse response2 = chatModel.call(prompt2); DeepSeekAssistantMessage deepSeekAssistantMessage2 = (DeepSeekAssistantMessage) response2.getResult().getOutput(); String reasoningContent2 = deepSeekAssistantMessage2.getReasoningContent(); return deepSeekAssistantMessage2.getText(); } 手动配置 # DeepSeekChatModel 实现 ChatModel 和 StreamingChatModel，并使用[ 低级 DeepSeekApi 客户端](#low-level-api)连接到 DeepSeek 服务。 将 spring-ai-deepseek 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-deepseek\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-deepseek\u0026#39; } 接下来，创建一个 DeepSeekChatModel 并将其用于文本生成：\nvar deepSeekApi = new DeepSeekApi(System.getenv(\u0026#34;DEEPSEEK_API_KEY\u0026#34;)); var chatModel = new DeepSeekChatModel(deepSeekApi, DeepSeekChatOptions.builder() .withModel(DeepSeekApi.ChatModel.DEEPSEEK_CHAT.getValue()) .withTemperature(0.4f) .withMaxTokens(200) .build()); ChatResponse response = chatModel.call( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); // Or with streaming responses Flux\u0026lt;ChatResponse\u0026gt; streamResponse = chatModel.stream( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); DeepSeekChatOptions 提供聊天请求的配置信息。DeepSeekChatOptions.Builder 是一个 Fluent 选项构建器。\n低级 DeepSeekApi 客户端 # [ DeepSeekApi]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-deepseek/src/main/java/org/springframework/ai/deepseek/api/[DeepSeekApi](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-deepseek/src/main/java/org/springframework/ai/deepseek/api/DeepSeekApi.java).java) 是 [ DeepSeek API]( https://platform.deepseek.com/api-docs/) 的轻量级 Java 客户端。 下面是一个简单的代码片段，演示如何以编程方式使用 API：\nDeepSeekApi deepSeekApi = new DeepSeekApi(System.getenv(\u0026#34;DEEPSEEK_API_KEY\u0026#34;)); ChatCompletionMessage chatCompletionMessage = new ChatCompletionMessage(\u0026#34;Hello world\u0026#34;, Role.USER); // Sync request ResponseEntity\u0026lt;ChatCompletion\u0026gt; response = deepSeekApi.chatCompletionEntity( new ChatCompletionRequest(List.of(chatCompletionMessage), DeepSeekApi.ChatModel.DEEPSEEK_CHAT.getValue(), 0.7f, false)); // Streaming request Flux\u0026lt;ChatCompletionChunk\u0026gt; streamResponse = deepSeekApi.chatCompletionStream( new ChatCompletionRequest(List.of(chatCompletionMessage), DeepSeekApi.ChatModel.DEEPSEEK_CHAT.getValue(), 0.7f, true)); 有关详细信息，请遵循 [ DeepSeekApi.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-deepseek/src/main/java/org/springframework/ai/deepseek/api/[DeepSeekApi.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-deepseek/src/main/java/org/springframework/ai/deepseek/api/DeepSeekApi.java)) 的 JavaDoc。\nDeepSeekApi 示例 # DeepSeekApiIT.java 测试提供了一些有关如何使用轻量级库的一般示例。 "},{"id":40,"href":"/docs/models/embedding-models/oci-genai/","title":"Oracle Cloud Infrastructure （OCI） GenAI 嵌入","section":"嵌入模型 API","content":" Oracle Cloud Infrastructure （OCI） GenAI 嵌入 # [ OCI GenAI 服务]( https://www.oracle.com/artificial-intelligence/generative-ai/generative-ai-service/)提供带有按需模型或专用 AI 集群的文本嵌入。 [ OCI 嵌入模型页]( https://docs.oracle.com/en-us/iaas/Content/generative-ai/embed-models.htm)和 [ OCI 文本嵌入页]( https://docs.oracle.com/en-us/iaas/Content/generative-ai/use-playground-embed.htm)提供了有关在 OCI 上使用和托管嵌入模型的详细信息。\n先决条件 # 添加存储库和 BOM # Spring AI 工件发布在 Maven Central 和 Spring Snapshot 存储库中。请参阅 [ Artifact Repositories](../../getting-started.html#artifact-repositories) 部分，将这些存储库添加到您的构建系统中。 为了帮助进行[ 依赖项管理](../../getting-started.html#dependency-management)，Spring AI 提供了一个 BOM（物料清单），以确保在整个项目中使用一致的 Spring AI 版本。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # Spring AI 为 OCI GenAI 嵌入客户端提供 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-oci-genai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-oci-genai\u0026#39; } 嵌入属性 # 前缀 spring.ai.oci.genai 是用于配置与 OCI GenAI 的连接的属性前缀。 前缀 spring.ai.oci.genai.embedding 是为 OCI GenAI 配置 EmbeddingModel 实施的属性前缀\n运行时选项 # OCIEmbeddingOptions 提供嵌入请求的配置信息。OCIEmbeddingOptions 提供了一个构建器来创建选项。 开始时，使用 OCIEmbeddingOptions 构造函数设置用于所有嵌入请求的默认选项。在运行时，您可以通过将 OCIEmbeddingOptions 实例与 your 一起传递给 EmbeddingRequest 请求来覆盖默认选项。 例如，要覆盖特定请求的默认模型名称：\nEmbeddingResponse embeddingResponse = embeddingModel.call( new EmbeddingRequest(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;), OCIEmbeddingOptions.builder() .model(\u0026#34;my-other-embedding-model\u0026#34;) .build() )); 示例代码 # 这将创建一个 EmbeddingModel 实现，您可以将其注入到您的类中。下面是一个使用 EmbeddingModel 实现的简单 @Controller 类的示例。\nspring.ai.oci.genai.embedding.model=\u0026lt;your model\u0026gt; spring.ai.oci.genai.embedding.compartment=\u0026lt;your model compartment\u0026gt; @RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(\u0026#34;/ai/embedding\u0026#34;) public Map embed(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { EmbeddingResponse embeddingResponse = this.embeddingModel.embedForResponse(List.of(message)); return Map.of(\u0026#34;embedding\u0026#34;, embeddingResponse); } } 手动配置 # 如果您不想使用 Spring Boot 自动配置，则可以在应用程序中手动配置 OCIEmbeddingModel。为此，将 spring-oci-genai-openai 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-oci-genai-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-oci-genai-openai\u0026#39; } 接下来，创建一个 OCIEmbeddingModel 实例并使用它来计算两个输入文本之间的相似性：\nfinal String EMBEDDING_MODEL = \u0026#34;cohere.embed-english-light-v2.0\u0026#34;; final String CONFIG_FILE = Paths.get(System.getProperty(\u0026#34;user.home\u0026#34;), \u0026#34;.oci\u0026#34;, \u0026#34;config\u0026#34;).toString(); final String PROFILE = \u0026#34;DEFAULT\u0026#34;; final String REGION = \u0026#34;us-chicago-1\u0026#34;; final String COMPARTMENT_ID = System.getenv(\u0026#34;OCI_COMPARTMENT_ID\u0026#34;); var authProvider = new ConfigFileAuthenticationDetailsProvider( this.CONFIG_FILE, this.PROFILE); var aiClient = GenerativeAiInferenceClient.builder() .region(Region.valueOf(this.REGION)) .build(this.authProvider); var options = OCIEmbeddingOptions.builder() .model(this.EMBEDDING_MODEL) .compartment(this.COMPARTMENT_ID) .servingMode(\u0026#34;on-demand\u0026#34;) .build(); var embeddingModel = new OCIEmbeddingModel(this.aiClient, this.options); List\u0026lt;Double\u0026gt; embedding = this.embeddingModel.embed(new Document(\u0026#34;How many provinces are in Canada?\u0026#34;)); "},{"id":41,"href":"/docs/models/image-models/qianfan/","title":"千帆镜像","section":"图像模型 API","content":" 千帆镜像 # 此功能已移至 Spring AI Community 存储库。 请访问 [ github.com/spring-ai-community/qianfan](https:// github.com/spring-ai-community/qianfan) 获取最新版本。\n"},{"id":42,"href":"/docs/prompts/","title":"提示","section":"Docs","content":" 提示 # 提示是指导 AI 模型生成特定输出的输入。这些提示的设计和措辞会显著影响模型的响应。 在 Spring AI 中与 AI 模型交互的最低级别上，在 Spring AI 中处理提示有点类似于在 Spring MVC 中管理“视图”。这涉及使用动态内容的占位符创建大量文本。然后，根据用户请求或应用程序中的其他代码替换这些占位符。另一个类比是包含某些表达式的占位符的 SQL 语句。 随着 Spring AI 的发展，它将引入更高级别的抽象来与 AI 模型交互。本节中描述的基础类在角色和功能方面可以比作 JDBC。例如，ChatModel 类类似于 JDK 中的核心 JDBC 库。ChatClient 类可以比作 JdbcClient，它构建在 ChatModel 之上，并通过 Advisor 提供更高级的构造 要考虑过去与模型的交互，请使用额外的上下文文档来增强提示，并引入代理行为。 随着时间的推移，提示的结构在 AI 领域内不断发展。最初，提示是简单的字符串。随着时间的推移，它们逐渐包括特定输入的占位符，例如 AI 模型可以识别的“USER：”。OpenAI 通过在 AI 模型处理之前将多个消息字符串分类为不同的角色，为提示引入了更多结构。\nAPI 概述 # 提示 # 通常使用 ChatModel 的 call（） 方法，该方法采用 Prompt 实例并返回 ChatResponse。 Prompt 类充当一系列有组织的 Message 对象和请求 ChatOptions 的容器。每条 Message 在提示中都包含一个独特的角色，其内容和意图不同。这些角色可以包含各种元素，从用户查询到 AI 生成的对相关背景信息的响应。这种安排支持与 AI 模型进行复杂而详细的交互，因为提示是由多条消息构建的，每条消息都分配了在对话中扮演的特定角色。 下面是 Prompt 类的截断版本，为简洁起见，省略了构造函数和实用程序方法：\npublic class Prompt implements ModelRequest\u0026lt;List\u0026lt;Message\u0026gt;\u0026gt; { private final List\u0026lt;Message\u0026gt; messages; private ChatOptions chatOptions; } 消息 # Message 接口封装了 Prompt 文本内容、元数据属性的集合和称为 MessageType 的分类。 接口定义如下：\npublic interface Content { String getContent(); Map\u0026lt;String, Object\u0026gt; getMetadata(); } public interface Message extends Content { MessageType getMessageType(); } 多模式消息类型还实现了 ```MediaContent` 接口，该接口提供了 Media`` 内容对象列表。\npublic interface MediaContent extends Content { Collection\u0026lt;Media\u0026gt; getMedia(); } Message 接口的各种实现对应于 AI 模型可以处理的不同类别的消息。模型根据对话角色区分消息类别。 这些角色由 MessageType 有效地映射，如下所述。\n角色 # 每条消息都分配有一个特定的角色。这些角色对消息进行分类，阐明 AI 模型提示的每个部分的上下文和目的。这种结构化的方法增强了与 AI 通信的细微差别和有效性，因为提示的每个部分在交互中都发挥着独特且定义的作用。 主要角色包括：\nSystem Role（系统角色）：指导 AI 的行为和响应风格，为 AI 如何解释和回复输入设置参数或规则。这类似于在开始对话之前向 AI 提供指令。 User Role：表示用户的输入 – 他们对 AI 的问题、命令或陈述。这个角色是基础，因为它构成了 AI 响应的基础。 Assistant Role（助理角色）：AI 对用户输入的响应。它不仅仅是一个答案或反应，它对于保持对话的流畅性至关重要。通过跟踪 AI 之前的响应（其“助理角色”消息），系统可确保连贯且与上下文相关的交互。Assistant 消息也可能包含函数工具调用请求信息。它就像 AI 中的一项特殊功能，在需要时用于执行特定功能，例如计算、获取数据或除交谈之外的其他任务。 工具/功能角色： 工具/功能角色 侧重于返回其他信息以响应工具调用助手消息。 角色在 Spring AI 中表示为枚举，如下所示 public enum MessageType { USER(\u0026#34;user\u0026#34;), ASSISTANT(\u0026#34;assistant\u0026#34;), SYSTEM(\u0026#34;system\u0026#34;), TOOL(\u0026#34;tool\u0026#34;); ... } 提示模板 # Spring AI 中提示模板的一个关键组件是 PromptTemplate 类，该类旨在促进创建结构化提示，然后将其发送到 AI 模型进行处理\npublic class PromptTemplate implements PromptTemplateActions, PromptTemplateMessageActions { // Other methods to be discussed later } 此类使用 TemplateRenderer API 来渲染模板。默认情况下，Spring AI 使用 StTemplateRenderer 实现，该实现基于 Terence Parr 开发的开源 [ StringTemplate]( https://www.stringtemplate.org/) 引擎。模板变量由 {} 语法标识，但您也可以将分隔符配置为使用其他语法。\npublic interface TemplateRenderer extends BiFunction\u0026lt;String, Map\u0026lt;String, Object\u0026gt;, String\u0026gt; { @Override String apply(String template, Map\u0026lt;String, Object\u0026gt; variables); } Spring AI 使用 TemplateRenderer 接口来处理变量到模板字符串的实际替换。默认实现使用 [ [StringTemplate]。](#StringTemplate) 如果您需要自定义逻辑，您可以提供自己的 TemplateRenderer 实现。对于不需要模板渲染的场景（例如，模板字符串已经完成），你可以使用提供的 NoOpTemplateRenderer。\nPromptTemplate promptTemplate = PromptTemplate.builder() .renderer(StTemplateRenderer.builder().startDelimiterToken(\u0026#39;\u0026lt;\u0026#39;).endDelimiterToken(\u0026#39;\u0026gt;\u0026#39;).build()) .template(\u0026#34;\u0026#34;\u0026#34; Tell me the names of 5 movies whose soundtrack was composed by \u0026lt;composer\u0026gt;. \u0026#34;\u0026#34;\u0026#34;) .build(); String prompt = promptTemplate.render(Map.of(\u0026#34;composer\u0026#34;, \u0026#34;John Williams\u0026#34;)); 此类实现的接口支持提示创建的不同方面： PromptTemplateStringActions 侧重于创建和呈现提示字符串，表示提示生成的最基本形式。 PromptTemplate``Message``Actions 是为通过生成和作 Message 对象来创建提示而量身定制的。 ```PromptTemplateActions` 旨在返回 Prompt对象，该对象可以传递给ChatModel`` 以生成响应。 虽然这些接口可能在许多项目中没有广泛使用，但它们显示了创建提示的不同方法。 实现的接口包括\npublic interface PromptTemplateStringActions { String render(); String render(Map\u0026lt;String, Object\u0026gt; model); } 方法 String render（）： 将提示模板渲染为最终字符串格式，无需外部输入，适用于没有占位符或动态内容的模板。 方法 String render(Map\u0026lt;String, Object\u0026gt; model) ：增强渲染功能以包含动态内容。它使用 Map\u0026lt;String、Object\u0026gt;，其中映射键是提示模板中的占位符名称，值是要插入的动态内容。\npublic interface PromptTemplateMessageActions { Message createMessage(); Message createMessage(List\u0026lt;Media\u0026gt; mediaList); Message createMessage(Map\u0026lt;String, Object\u0026gt; model); } 方法 Message createMessage（）： 创建一个不带额外数据的 Message 对象，用于静态或预定义的消息内容。 方法 ```Message createMessage(List\u0026lt;Media\u0026gt; mediaList)` ：创建具有静态文本和媒体内容的 Message`` 对象。 方法 Message createMessage(Map\u0026lt;String, Object\u0026gt; model) ：扩展消息创建以集成动态内容，接受 Map\u0026lt;String， Object\u0026gt;，其中每个条目表示消息模板中的占位符及其相应的动态值。\npublic interface PromptTemplateActions extends PromptTemplateStringActions { Prompt create(); Prompt create(ChatOptions modelOptions); Prompt create(Map\u0026lt;String, Object\u0026gt; model); Prompt create(Map\u0026lt;String, Object\u0026gt; model, ChatOptions modelOptions); } 方法 Prompt create（）： 生成一个没有外部数据输入的 Prompt 对象，非常适合静态或预定义的提示。 方法 ```Promptcreate(ChatOptions modelOptions)` ：生成一个Prompt`` 对象，该对象没有外部数据输入，并且具有聊天请求的特定选项。 方法 Prompt create(Map\u0026lt;String, Object\u0026gt; model) ：扩展提示创建功能以包含动态内容，采用 Map\u0026lt;String， Object\u0026gt;，其中每个映射条目都是提示模板及其关联的动态值中的占位符。 方法 Prompt create(Map\u0026lt;String, Object\u0026gt; model, ChatOptions modelOptions) ：扩展提示创建功能以包含动态内容，采用 Map\u0026lt;String、Object\u0026gt;，其中每个映射条目都是提示模板中的占位符及其关联的动态值，以及聊天请求的特定选项。\n示例用法 # 下面显示了一个来自 [ PromptTemplates 上的 AI Workshop]( https://github.com/Azure-Samples/spring-ai-azure-workshop/blob/main/2-README-prompt-templating.md) 的简单示例。\nPromptTemplate promptTemplate = new PromptTemplate(\u0026#34;Tell me a {adjective} joke about {topic}\u0026#34;); Prompt prompt = promptTemplate.create(Map.of(\u0026#34;adjective\u0026#34;, adjective, \u0026#34;topic\u0026#34;, topic)); return chatModel.call(prompt).getResult(); 另一个示例摘自 [ AI Workshop on Roles]( https://github.com/Azure-Samples/spring-ai-azure-workshop/blob/main/3-README-prompt-roles.md) 如下所示。\nString userText = \u0026#34;\u0026#34;\u0026#34; Tell me about three famous pirates from the Golden Age of Piracy and why they did. Write at least a sentence for each pirate. \u0026#34;\u0026#34;\u0026#34;; Message userMessage = new UserMessage(userText); String systemText = \u0026#34;\u0026#34;\u0026#34; You are a helpful AI assistant that helps people find information. Your name is {name} You should reply to the user\u0026#39;s request with your name and also in the style of a {voice}. \u0026#34;\u0026#34;\u0026#34;; SystemPromptTemplate systemPromptTemplate = new SystemPromptTemplate(systemText); Message systemMessage = systemPromptTemplate.createMessage(Map.of(\u0026#34;name\u0026#34;, name, \u0026#34;voice\u0026#34;, voice)); Prompt prompt = new Prompt(List.of(userMessage, systemMessage)); List\u0026lt;Generation\u0026gt; response = chatModel.call(prompt).getResults(); 这显示了如何通过使用 SystemPromptTemplate 创建一个 Message 来构建 Prompt 实例，其中系统角色传入占位符值。然后，带有角色 user 的消息将与角色系统的消息组合在一起以形成提示。然后将提示传递给 ChatModel 以获得生成响应。\n使用自定义模板渲染器 # 您可以通过实现 TemplateRenderer 接口并将其传递给 PromptTemplate 构造函数来使用自定义模板渲染器。您也可以继续使用默认的 StTemplateRenderer，但使用自定义配置。 默认情况下，模板变量由 {} 语法标识。如果您计划在提示中包含 JSON，则可能需要使用不同的语法以避免与 JSON 语法冲突。例如，您可以使用 \u0026lt; 和 \u0026gt; 分隔符。\nPromptTemplate promptTemplate = PromptTemplate.builder() .renderer(StTemplateRenderer.builder().startDelimiterToken(\u0026#39;\u0026lt;\u0026#39;).endDelimiterToken(\u0026#39;\u0026gt;\u0026#39;).build()) .template(\u0026#34;\u0026#34;\u0026#34; Tell me the names of 5 movies whose soundtrack was composed by \u0026lt;composer\u0026gt;. \u0026#34;\u0026#34;\u0026#34;) .build(); String prompt = promptTemplate.render(Map.of(\u0026#34;composer\u0026#34;, \u0026#34;John Williams\u0026#34;)); 使用资源而不是原始字符串 # Spring AI 支持抽象 `org.springframework.core.io.Resource``` ，因此您可以将提示数据放在可以直接在 PromptTemplate中使用的文件中。例如，您可以在 Spring 托管组件中定义一个字段来检索Resource``。\n@Value(\u0026#34;classpath:/prompts/system-message.st\u0026#34;) private Resource systemResource; 然后将该资源直接传递给 SystemPromptTemplate。\nSystemPromptTemplate systemPromptTemplate = new SystemPromptTemplate(systemResource); 快速工程 # 在生成式 AI 中，创建提示是开发人员的一项关键任务。这些提示的质量和结构会显著影响 AI 输出的有效性。投入时间和精力来设计周到的提示可以大大改善 AI 的结果。 共享和讨论提示是 AI 社区的常见做法。这种协作方法不仅创造了一个共享的学习环境，而且还导致了高效提示的识别和使用。 该领域的研究通常涉及分析和比较不同的提示，以评估它们在各种情况下的有效性。例如，一项重要研究表明，以“深呼吸并逐步解决这个问题”开始提示会显著提高解决问题的效率。这凸显了精心选择的语言对生成式 AI 系统性能的影响。 掌握最有效地使用提示，尤其是在 AI 技术快速发展的情况下，是一项持续的挑战。您应该认识到提示工程的重要性，并考虑使用来自社区和研究的见解来改进提示创建策略。\n创建有效的提示 # 在开发提示时，重要的是要集成几个关键组件以确保清晰度和有效性：\n说明 ：向 AI 提供清晰直接的指示，类似于您与人交流的方式。这种清晰度对于帮助 AI “理解” 预期内容至关重要。 外部背景 ：必要时包括 AI 响应的相关背景信息或具体指导。这个 “外部上下文” 构建了提示并帮助 AI 掌握整体场景。 用户输入 ：这是简单的部分 - 用户的直接请求或问题构成了提示的核心。 输出指示器 ： 这方面可能很棘手。它涉及为 AI 的响应指定所需的格式，例如 JSON。但是，请注意，AI 可能并不总是严格遵守此格式。例如，它可能会在实际 JSON 数据之前预置“here is your JSON”之类的短语，或者有时会生成不准确的类似 JSON 的结构。 在制作提示时，向 AI 提供预期问答格式的示例可能非常有益。这种做法有助于 AI “理解” 查询的结构和意图，从而获得更精确和相关的响应。虽然本文档没有深入探讨这些技术，但它们为进一步探索 AI 提示工程提供了一个起点。 以下是用于进一步调查的资源列表。 简单的技术 # 将大量文本缩减为简洁的摘要，捕捉关键点和主要思想，同时省略不太关键的细节。 侧重于根据用户提出的问题从提供的文本中得出特定答案。它是关于精确定位和提取相关信息以响应查询。 系统地将文本分类为预定义的类别或组，分析文本并根据其内容将其分配到最合适的类别。 创建交互式对话，AI 可以在其中与用户进行来回通信，模拟自然的对话流。 根据特定的用户要求或描述生成功能代码片段，将自然语言指令转换为可执行代码。 高级技术 # 使模型能够做出准确的预测或响应，只需最少或没有特定问题类型的先前示例，即可使用学习的泛化理解新任务并采取行动。 链接多个 AI 响应以创建连贯且上下文感知的对话。它帮助 AI 维护讨论的主线，确保相关性和连续性。 在这种方法中，AI 首先分析（推理）输入，然后确定最合适的行动方案或响应。它将理解与决策相结合。 Microsoft 指南 # Microsoft 提供了一种结构化的方法来开发和优化提示。该框架指导用户创建有效的提示，从 AI 模型中引出所需的响应，优化交互以提高清晰度和效率。 令 牌 # 令牌在 AI 模型处理文本的方式中至关重要，它充当将单词（正如我们理解的）转换为 AI 模型可以处理的格式的桥梁。这种转换分两个阶段进行：单词在输入时转换为词元，然后在输出中将这些词元转换回词。 分词化是将文本分解为分词的过程，是 AI 模型理解和处理语言的基础。AI 模型使用这种标记化格式来理解和响应提示。 为了更好地理解标记，请将它们视为单词的一部分。通常，一个标记表示大约四分之三的单词。例如，莎士比亚全集总计约 900,000 字，可翻译成约 120 万个代币。 尝试使用 [ OpenAI Tokenizer UI]( https://platform.openai.com/tokenizer)，看看单词是如何转换为标记的。 除了在 AI 处理中的技术作用之外，令牌还具有实际意义，尤其是在计费和模型功能方面：\n计费：AI 模型服务通常根据 Token 使用情况计费。输入 （prompt） 和输出 （response） 都会计入总令牌计数，因此较短的提示更具成本效益。 模型限制：不同的 AI 模型具有不同的令牌限制，定义了它们的“上下文窗口”——它们一次可以处理的最大信息量。例如，GPT-3 的限制为 4K 令牌，而 Claude 2 和 Meta Llama 2 等其他模型限制为 100K 令牌，一些研究模型最多可以处理 100 万个令牌。 Context Window：模型的 Token 限制决定了其 Context Window。模型不会处理超过此限制的输入。仅发送最少的有效信息集进行处理至关重要。例如，在查询“哈姆雷特”时，无需包含莎士比亚所有其他作品中的标记。 响应元数据：来自 AI 模型的响应元数据包括使用的令牌数量，这是管理使用和成本的重要信息。 "},{"id":43,"href":"/docs/vector-databases/couchbase/","title":"沙发基地","section":"矢量数据库","content":" 沙发基地 # 本节将引导您设置 CouchbaseSearchVectorStore 以存储文档嵌入并使用 Couchbase 执行相似性搜索。 [ Couchbase]( https://docs.couchbase.com/server/current/vector-search/vector-search.html) 是一个分布式 JSON 文档数据库，具有关系 DBMS 所需的所有功能。除其他功能外，它还允许用户使用基于向量的存储和检索来查询信息。\n先决条件 # 一个正在运行的 Couchbase 实例。以下选项可用：Couchbase * [ Docker]( https://hub.docker.com/_/couchbase/)\n[ Capella - Couchbase 即服务]( https://cloud.couchbase.com/) [ 在本地安装 Couchbase]( https://www.couchbase.com/downloads/?family=couchbase-server) [ Couchbase Kubernetes 作员]( https://www.couchbase.com/downloads/?family=open-source-kubernetes) 自动配置 # Spring AI 为 Couchbase Vector Store 提供了 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-couchbase\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-couchbase-store-spring-boot-starter\u0026#39; } 矢量存储实现可以使用默认选项为您初始化配置的存储桶、范围、集合和搜索索引，但您必须通过在相应的构造函数中指定 initializeSchema 布尔值来选择加入。 请查看 vector store 的[ 配置参数](#couchbasevector-properties)列表，了解默认值和配置选项。 此外，您将需要一个已配置的 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) bean。请参阅 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) 部分以了解更多信息。 现在，您可以将 CouchbaseSearchVectorStore 自动连接为应用程序中的矢量存储。\n@Autowired VectorStore vectorStore; // ... List \u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); // Add the documents to Qdrant vectorStore.add(documents); // Retrieve documents similar to a query List\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch(SearchRequest.query(\u0026#34;Spring\u0026#34;).withTopK(5)); 配置属性 # 要连接到 Couchbase 并使用 CouchbaseSearchVectorStore，您需要提供实例的访问详细信息。可以通过 Spring Boot 的 application.properties 提供配置：\nspring.ai.openai.api-key=\u0026lt;key\u0026gt; spring.couchbase.connection-string=\u0026lt;conn_string\u0026gt; spring.couchbase.username=\u0026lt;username\u0026gt; spring.couchbase.password=\u0026lt;password\u0026gt; 如果您更喜欢对密码或 API 密钥等敏感信息使用环境变量，则有多种选择：\n选项 1：使用 Spring 表达式语言 （SpEL） # 你可以使用自定义环境变量名称，并使用 SpEL 在应用程序配置中引用它们：\n# In application.yml spring: ai: openai: api-key: ${OPENAI_API_KEY} couchbase: connection-string: ${COUCHBASE_CONN_STRING} username: ${COUCHBASE_USER} password: ${COUCHBASE_PASSWORD} # In your environment or .env file export OPENAI_API_KEY=\u0026lt;api-key\u0026gt; export COUCHBASE_CONN_STRING=\u0026lt;couchbase connection string like couchbase://localhost\u0026gt; export COUCHBASE_USER=\u0026lt;couchbase username\u0026gt; export COUCHBASE_PASSWORD=\u0026lt;couchbase password\u0026gt; 选项 2：以编程方式访问环境变量 # 或者，您可以在 Java 代码中访问环境变量：\nString apiKey = System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;); 此方法使您可以灵活地命名环境变量，同时将敏感信息排除在应用程序配置文件之外。 Spring Boot 对 Couchbase Cluster 的自动配置功能将创建一个 bean 实例，该实例将由 CouchbaseSearchVectorStore 使用。 以 spring.couchbase.* 开头的 Spring Boot 属性用于配置 Couchbase 集群实例： 以 spring.ai.vectorstore.couchbase.* 前缀开头的属性用于配置 CouchbaseSearchVectorStore。 可以使用以下相似性函数： 可以使用以下索引优化：\n召回 延迟 有关每个选项的更多详细信息，请参阅有关向量搜索的 [ Couchbase 文档]( https://docs.couchbase.com/server/current/search/child-field-options-reference.html) 。 元数据筛选 # 您可以将通用的可移植[ 元数据筛选器]( https://docs.spring.io/spring-ai/reference/api/vectordbs.html#_metadata_filters)用于 Couchbase 存储。 例如，您可以使用文本表达式语言：\nvectorStore.similaritySearch( SearchRequest.defaults() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .filterExpression(\u0026#34;author in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; article_type == \u0026#39;blog\u0026#39;\u0026#34;)); 或使用 Filter.Expression DSL 以编程方式：\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.defaults() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .filterExpression(b.and( b.in(\u0026#34;author\u0026#34;,\u0026#34;john\u0026#34;, \u0026#34;jill\u0026#34;), b.eq(\u0026#34;article_type\u0026#34;, \u0026#34;blog\u0026#34;)).build())); 手动配置 # 您可以手动配置 Couchbase 向量存储，而不是使用 Spring Boot 自动配置。为此，您需要将 spring-ai-couchbase-store 添加到您的项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-couchbase-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-couchbase-store\u0026#39; } 创建 Couchbase Cluster Bean。阅读 [ Couchbase 文档]( https://docs.couchbase.com/java-sdk/current/hello-world/start-using-sdk.html) ，了解有关自定义 Cluster 实例配置的更深入信息。\n@Bean public Cluster cluster() { return Cluster.connect(\u0026#34;couchbase://localhost\u0026#34;, \u0026#34;username\u0026#34;, \u0026#34;password\u0026#34;); } ，然后使用构建器模式创建 CouchbaseSearchVectorStore Bean：\n@Bean public VectorStore couchbaseSearchVectorStore(Cluster cluster, EmbeddingModel embeddingModel, Boolean initializeSchema) { return CouchbaseSearchVectorStore .builder(cluster, embeddingModel) .bucketName(\u0026#34;test\u0026#34;) .scopeName(\u0026#34;test\u0026#34;) .collectionName(\u0026#34;test\u0026#34;) .initializeSchema(initializeSchema) .build(); } // This can be any EmbeddingModel implementation. @Bean public EmbeddingModel embeddingModel() { return new OpenAiEmbeddingModel(OpenAiApi.builder().apiKey(this.openaiKey).build()); } 局限性 # "},{"id":44,"href":"/docs/models/chat-models/docker-model-runner/","title":"Docker Model Runner 聊天","section":"聊天模型 API","content":" Docker Model Runner 聊天 # [ Docker Model Runner]( https://docs.docker.com/desktop/features/model-runner/) 是一个 AI 推理引擎，提供来自[ 各种提供商]( https://hub.docker.com/u/ai)的各种模型。 Spring AI 通过重用现有的 [ OpenAI](openai-chat.html) 支持的 ChatClient 与 Docker Model Runner 集成。为此，请将基本 URL 设置为 [[localhost：12434/engines](http://localhost:12434/engines)](http://localhost:12434/engines) 并选择提供的 [ LLM 模型]( https://hub.docker.com/u/ai)之一。 查看 [ DockerModelRunnerWithOpenAiChatModelIT.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/chat/proxy/[DockerModelRunnerWithOpenAiChatModelIT.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/chat/proxy/DockerModelRunnerWithOpenAiChatModelIT.java)) 测试，了解如何将 Docker Model Runner 与 Spring AI 结合使用的示例。\n先决条件 # 下载适用于 Mac 的 Docker Desktop 4.40.0。 选择以下选项之一以启用 Model Runner： 选项 1： 启用 Model Runner docker desktop enable model-runner \u0026ndash;tcp 12434 。 将 base-url 设置为 localhost：12434/engines 选项 2： 启用 Model Runner docker desktop enable model-runner 。 使用 Testcontainers 并设置 base-url，如下所示： @Container private static final SocatContainer socat = new SocatContainer().withTarget(80, \u0026#34;model-runner.docker.internal\u0026#34;); @Bean public OpenAiApi chatCompletionApi() { var baseUrl = \u0026#34;http://%s:%d/engines\u0026#34;.formatted(socat.getHost(), socat.getMappedPort(80)); return OpenAiApi.builder().baseUrl(baseUrl).apiKey(\u0026#34;test\u0026#34;).build(); } 您可以通过阅读[ 使用 Docker 在本地运行 LLM]( https://www.docker.com/blog/run-llms-locally/) 博客文章了解有关 Docker Model Runner 的更多信息。\n自动配置 # Spring AI 为 OpenAI Chat 客户端提供 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或将以下内容添加到您的 Gradle build.gradle 构建文件中。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-openai\u0026#39; } 聊天属性 # 重试属性 # 前缀 spring.ai.retry 用作属性前缀，允许您为 OpenAI 聊天模型配置重试机制。\n连接属性 # 前缀 spring.ai.openai 用作允许您连接到 OpenAI 的属性前缀。\n配置属性 # 前缀 spring.ai.openai.chat 是属性前缀，允许您为 OpenAI 配置聊天模型实现。\n运行时选项 # [ OpenAiChatOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/[OpenAiChatOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/OpenAiChatOptions.java)) 提供模型配置，例如要使用的模型、温度、频率损失等。 启动时，可以使用 OpenAiChatModel（api， options） 构造函数或 spring.ai.openai.chat.options.* 属性配置默认选项。 在运行时，您可以通过向 Prompt 调用添加新的、特定于请求的选项来覆盖默认选项。例如，要覆盖特定请求的默认型号和温度：\nChatResponse response = chatModel.call( new Prompt( \u0026#34;Generate the names of 5 famous pirates.\u0026#34;, OpenAiChatOptions.builder() .model(\u0026#34;ai/gemma3:4B-F16\u0026#34;) .build() )); 函数调用 # Docker Model Runner 在选择支持它的模型时支持工具/函数调用。 您可以使用 ChatModel 注册自定义 Java 函数，并让提供的模型智能地选择输出包含参数的 JSON 对象，以调用一个或多个已注册的函数。这是一种将 LLM 功能与外部工具和 API 连接起来的强大技术。\n工具示例 # 以下是如何将 Docker Model Runner 函数调用与 Spring AI 一起使用的简单示例：\nspring.ai.openai.api-key=test spring.ai.openai.base-url=http://localhost:12434/engines spring.ai.openai.chat.options.model=ai/gemma3:4B-F16 @SpringBootApplication public class DockerModelRunnerLlmApplication { public static void main(String[] args) { SpringApplication.run(DockerModelRunnerLlmApplication.class, args); } @Bean CommandLineRunner runner(ChatClient.Builder chatClientBuilder) { return args -\u0026gt; { var chatClient = chatClientBuilder.build(); var response = chatClient.prompt() .user(\u0026#34;What is the weather in Amsterdam and Paris?\u0026#34;) .functions(\u0026#34;weatherFunction\u0026#34;) // reference by bean name. .call() .content(); System.out.println(response); }; } @Bean @Description(\u0026#34;Get the weather in location\u0026#34;) public Function\u0026lt;WeatherRequest, WeatherResponse\u0026gt; weatherFunction() { return new MockWeatherService(); } public static class MockWeatherService implements Function\u0026lt;WeatherRequest, WeatherResponse\u0026gt; { public record WeatherRequest(String location, String unit) {} public record WeatherResponse(double temp, String unit) {} @Override public WeatherResponse apply(WeatherRequest request) { double temperature = request.location().contains(\u0026#34;Amsterdam\u0026#34;) ? 20 : 25; return new WeatherResponse(temperature, request.unit); } } } 在此示例中，当模型需要天气信息时，它将自动调用 weatherFunction Bean，然后该 bean 可以获取实时天气数据。预期的回答是：“阿姆斯特丹的天气目前是 20 摄氏度，巴黎的天气目前是 25 摄氏度。 阅读有关 OpenAI [ 函数调用]( https://docs.spring.io/spring-ai/reference/api/chat/functions/openai-chat-functions.html)的更多信息。\n样品控制器 # [ 创建一个新]( https://start.spring.io/)的 Spring Boot 项目，并将 添加到您的 spring-ai-starter-model-openai pom（或 gradle）依赖项中。 在 src/main/resources 目录下添加一个 application.properties 文件，以启用和配置 OpenAi 聊天模型：\nspring.ai.openai.api-key=test spring.ai.openai.base-url=http://localhost:12434/engines spring.ai.openai.chat.options.model=ai/gemma3:4B-F16 # Docker Model Runner doesn\u0026#39;t support embeddings, so we need to disable them. spring.ai.openai.embedding.enabled=false 下面是一个使用聊天模型生成文本的简单 @Controller 类的示例。\n@RestController public class ChatController { private final OpenAiChatModel chatModel; @Autowired public ChatController(OpenAiChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, this.chatModel.call(message)); } @GetMapping(\u0026#34;/ai/generateStream\u0026#34;) public Flux\u0026lt;ChatResponse\u0026gt; generateStream(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { Prompt prompt = new Prompt(new UserMessage(message)); return this.chatModel.stream(prompt); } } "},{"id":45,"href":"/docs/vector-databases/elasticsearch/","title":"None","section":"矢量数据库","content":" None # 本节将指导您设置 Elasticsearch VectorStore 以存储文档嵌入并执行相似性搜索。 [ Elasticsearch]( https://www.elastic.co/elasticsearch) 是基于 Apache Lucene 库的开源搜索和分析引擎。\n先决条件 # 正在运行的 Elasticsearch 实例。以下选项可用：\n码头工人 自我管理的 Elasticsearch 弹性云 自动配置 # Spring AI 为 Elasticsearch Vector Store 提供 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 或 Gradle build.gradle 构建文件中： 矢量存储实现可以为您初始化必要的架构，但您必须通过在相应的构造函数中指定 initializeSchema 布尔值或设置 ...initialize-schema=true 在 application.properties 文件中。或者，您也可以选择退出初始化并使用 Elasticsearch 客户端手动创建索引，如果索引需要高级映射或其他配置，这可能非常有用。 请查看 vector store 的[ 配置参数](#elasticsearchvector-properties)列表，了解默认值和配置选项。还可以通过配置 ElasticsearchVectorStoreOptions Bean 来设置这些属性。 此外，您将需要一个已配置的 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) bean。请参阅 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) 部分以了解更多信息。 现在，您可以将 ElasticsearchVectorStore 自动连接为应用程序中的矢量存储。\n@Autowired VectorStore vectorStore; // ... List \u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); // Add the documents to Elasticsearch vectorStore.add(documents); // Retrieve documents similar to a query List\u0026lt;Document\u0026gt; results = this.vectorStore.similaritySearch(SearchRequest.builder().query(\u0026#34;Spring\u0026#34;).topK(5).build()); 配置属性 # 要连接到 Elasticsearch 并使用 ElasticsearchVectorStore，您需要提供实例的访问详细信息。可以通过 Spring Boot 的 application.yml 提供简单的配置，\nspring: elasticsearch: uris: \u0026lt;elasticsearch instance URIs\u0026gt; username: \u0026lt;elasticsearch username\u0026gt; password: \u0026lt;elasticsearch password\u0026gt; ai: vectorstore: elasticsearch: initialize-schema: true index-name: custom-index dimensions: 1536 similarity: cosine 以 spring.elasticsearch.* 开头的 Spring Boot 属性用于配置 Elasticsearch 客户端： 以 开头的属性 spring.ai.vectorstore.elasticsearch.* 用于配置 ElasticsearchVectorStore： 可以使用以下相似性函数：\ncosine - 默认，适用于大多数用例。测量向量之间的余弦相似性。 l2_norm - 向量之间的欧几里得距离。值越低表示相似度越高。 dot_product - 归一化向量（例如 OpenAI 嵌入）的最佳性能。 有关每个方法的更多详细信息，请参阅有关密集向量的 [ Elasticsearch 文档]( https://www.elastic.co/guide/en/elasticsearch/reference/master/dense-vector.html#dense-vector-params) 。 元数据筛选 # 您也可以在 Elasticsearch 中使用通用的可移植[ 元数据过滤器](../vectordbs.html#metadata-filters) 。 例如，您可以使用文本表达式语言：\nvectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(\u0026#34;author in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; \u0026#39;article_type\u0026#39; == \u0026#39;blog\u0026#39;\u0026#34;).build()); 或使用 Filter.Expression DSL 以编程方式：\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(b.and( b.in(\u0026#34;author\u0026#34;, \u0026#34;john\u0026#34;, \u0026#34;jill\u0026#34;), b.eq(\u0026#34;article_type\u0026#34;, \u0026#34;blog\u0026#34;)).build()).build()); 例如，此可移植筛选条件表达式：\nauthor in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; \u0026#39;article_type\u0026#39; == \u0026#39;blog\u0026#39; 转换为专有的 Elasticsearch 过滤器格式：\n(metadata.author:john OR jill) AND metadata.article_type:blog 手动配置 # 您可以手动配置 Elasticsearch 向量存储，而不是使用 Spring Boot 自动配置。为此，您需要将 spring-ai-elasticsearch-store 添加到您的项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-elasticsearch-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-elasticsearch-store\u0026#39; } 创建 Elasticsearch RestClient Bean。阅读 [ Elasticsearch 文档]( https://www.elastic.co/guide/en/elasticsearch/client/java-api-client/current/java-rest-low-usage-initialization.html) ，了解有关自定义 RestClient 配置的更深入信息。\n@Bean public RestClient restClient() { return RestClient.builder(new HttpHost(\u0026#34;\u0026lt;host\u0026gt;\u0026#34;, 9200, \u0026#34;http\u0026#34;)) .setDefaultHeaders(new Header[]{ new BasicHeader(\u0026#34;Authorization\u0026#34;, \u0026#34;Basic \u0026lt;encoded username and password\u0026gt;\u0026#34;) }) .build(); } 然后使用构建器模式创建 ElasticsearchVectorStore Bean：\n@Bean public VectorStore vectorStore(RestClient restClient, EmbeddingModel embeddingModel) { ElasticsearchVectorStoreOptions options = new ElasticsearchVectorStoreOptions(); options.setIndexName(\u0026#34;custom-index\u0026#34;); // Optional: defaults to \u0026#34;spring-ai-document-index\u0026#34; options.setSimilarity(COSINE); // Optional: defaults to COSINE options.setDimensions(1536); // Optional: defaults to model dimensions or 1536 return ElasticsearchVectorStore.builder(restClient, embeddingModel) .options(options) // Optional: use custom options .initializeSchema(true) // Optional: defaults to false .batchingStrategy(new TokenCountBatchingStrategy()) // Optional: defaults to TokenCountBatchingStrategy .build(); } // This can be any EmbeddingModel implementation @Bean public EmbeddingModel embeddingModel() { return new OpenAiEmbeddingModel(new OpenAiApi(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;))); } 访问 Native Client # Elasticsearch Vector Store 实现通过 getNativeClient（） 方法提供对底层原生 Elasticsearch 客户端 （ElasticsearchClient） 的访问：\nElasticsearchVectorStore vectorStore = context.getBean(ElasticsearchVectorStore.class); Optional\u0026lt;ElasticsearchClient\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { ElasticsearchClient client = nativeClient.get(); // Use the native client for Elasticsearch-specific operations } 本机客户端允许您访问特定于 Elasticsearch 的功能和作，这些功能和作可能无法通过 VectorStore 接口公开。\n"},{"id":46,"href":"/docs/models/embedding-models/ollama/","title":"Ollama 嵌入","section":"嵌入模型 API","content":" Ollama 嵌入 # 使用 [ Ollama]( https://ollama.ai/)，您可以在本地运行各种 [ AI 模型]( https://ollama.com/search?c=embedding)并从中生成嵌入。嵌入是浮点数的向量（列表）。两个向量之间的距离衡量它们的相关性。小距离表示高相关性，大距离表示低相关性。 OllamaEmbeddingModel 实现利用 Ollama [ Embeddings API]( https://github.com/ollama/ollama/blob/main/docs/api.md#generate-embeddings) 端点。\n先决条件 # 您首先需要访问 Ollama 实例。有几个选项，包括：\n在本地计算机上下载并安装 Ollama。 通过 Testcontainers 配置和运行 Ollama。 通过 Kubernetes 服务绑定绑定到 Ollama 实例。 您可以从 [ Ollama 模型库]( https://ollama.com/search?c=embedding)中提取要在应用程序中使用的模型： ollama pull \u0026lt;model-name\u0026gt; 您还可以提取数千个免费的 [ GGUF 紧贴脸模型]( https://huggingface.co/models?library=gguf\u0026sort=trending)中的任何一个：\nollama pull hf.co/\u0026lt;username\u0026gt;/\u0026lt;model-repository\u0026gt; 或者，您可以启用选项以自动下载任何需要的模型：[ Auto-pull Models （自动拉取模型](#auto-pulling-models) ）。\n自动配置 # Spring AI 为 Azure Ollama 嵌入模型提供 Spring Boot 自动配置。要启用它，请将以下依赖项添加到您的 Maven pom.xml 或 Gradle build.gradle 构建文件中：\n基本属性 # 前缀 spring.ai.ollama 是配置与 Ollama 的连接的属性前缀 以下是用于初始化 Ollama 集成和[ 自动拉取模型](#auto-pulling-models)的属性。\n嵌入属性 # prefix spring.ai.ollama.embedding.``options``` 是配置 Ollama 嵌入模型的属性前缀。它包括 Ollama 请求（高级）参数，例如 ``model``、``keep-alive`` 和 ``truncate`` 以及 Ollama ``model`` ``options`` 属性。 以下是 Ollama 嵌入模型的高级请求参数： 其余选项`属性基于 [ Ollama Valid Parameters and Values]( https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values) 和 [ Ollama Types]( https://github.com/ollama/ollama/blob/main/api/types.go)。默认值基于：[ Ollama 类型默认值]( https://github.com/ollama/ollama/blob/b538dc3858014f94b099730a592751a5454cab0a/api/types.go#L364) 。\n运行时选项 # [ OllamaOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-ollama/src/main/java/org/springframework/ai/ollama/api/[OllamaOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-ollama/src/main/java/org/springframework/ai/ollama/api/OllamaOptions.java)) 提供了 Ollama 配置，例如要使用的模型、低级 GPU 和 CPU 调优等。 也可以使用 spring.ai.ollama.embedding.options properties 配置默认选项。 在启动时，使用 配置 OllamaEmbeddingModel(OllamaApi ollamaApi, ``OllamaOptions`` defaultOptions) 用于所有嵌入请求的默认选项。在运行时，您可以使用 OllamaOptions 实例作为 EmbeddingRequest 的一部分来覆盖默认选项。 例如，要覆盖特定请求的默认模型名称：\nEmbeddingResponse embeddingResponse = embeddingModel.call( new EmbeddingRequest(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;), OllamaOptions.builder() .model(\u0026#34;Different-Embedding-Model-Deployment-Name\u0026#34;)) .truncates(false) .build()); 自动拉取模型 # Spring AI Ollama 可以在模型在 Ollama 实例中不可用时自动拉取模型。此功能对于开发和测试以及将应用程序部署到新环境特别有用。 拉取模型有三种策略：\nalways（在 PullModelStrategy.ALWAYS 中定义）：始终拉取模型，即使它已经可用。有助于确保您使用的是最新版本的模型。 when_missing （定义于 PullModelStrategy.WHEN_MISSING ）：仅在模型尚不可用时拉取模型。这可能会导致使用旧版本的模型。 never （defined in PullModelStrategy.NEVER）：从不自动拉取模型。 通过配置属性和默认选项定义的所有模型都可以在启动时自动拉取。您可以使用配置属性配置拉取策略、超时和最大重试次数： spring: ai: ollama: init: pull-model-strategy: always timeout: 60s max-retries: 1 您可以在启动时初始化其他模型，这对于在运行时动态使用的模型非常有用：\nspring: ai: ollama: init: pull-model-strategy: always embedding: additional-models: - mxbai-embed-large - nomic-embed-text 如果只想将拉取策略应用于特定类型的模型，则可以从初始化任务中排除嵌入模型：\nspring: ai: ollama: init: pull-model-strategy: always embedding: include: false 此配置会将拉取策略应用于除 embedding 模型之外的所有模型。\nHuggingFace 模特 # Ollama 可以开箱即用地访问所有 [ GGUF Hugging Face]( https://huggingface.co/models?library=gguf\u0026sort=trending) Embedding 模型。您可以按名称拉取这些模型中的任何一个： ollama pull hf.co/\u0026lt;username\u0026gt;/\u0026lt;model-repository\u0026gt; 或配置自动拉取策略： [ 自动拉取模型](#auto-pulling-models) ：\nspring.ai.ollama.embedding.options.model=hf.co/mixedbread-ai/mxbai-embed-large-v1 spring.ai.ollama.init.pull-model-strategy=always spring.ai.ollama.embedding.options.model ：指定要使用的紧贴面部 GGUF 模型 。 spring.ai.ollama.init.pull-model-strategy=always ：（可选）在启动时启用自动拉取模型。对于生产环境，您应该预先下载模型以避免延迟： ollama pull hf.co/mixedbread-ai/mxbai-embed-large-v1 . 样品控制器 # 这将创建一个 EmbeddingModel 实现，您可以将其注入到您的类中。下面是一个使用 EmbeddingModel 实现的简单 @Controller 类的示例。\n@RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(\u0026#34;/ai/embedding\u0026#34;) public Map embed(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { EmbeddingResponse embeddingResponse = this.embeddingModel.embedForResponse(List.of(message)); return Map.of(\u0026#34;embedding\u0026#34;, embeddingResponse); } } 手动配置 # 如果您不使用 Spring Boot，则可以手动配置 OllamaEmbeddingModel。为此，将 spring-ai-ollama 依赖项添加到项目的 Maven pom.xml 或 Gradle build.gradle 构建文件中： 接下来，创建一个 OllamaEmbeddingModel 实例，并使用它使用专用的 chroma/all-minilm-l6-v2-f32 嵌入模型计算两个输入文本的嵌入：\nvar ollamaApi = OllamaApi.builder().build(); var embeddingModel = new OllamaEmbeddingModel(this.ollamaApi, OllamaOptions.builder() .model(OllamaModel.MISTRAL.id()) .build()); EmbeddingResponse embeddingResponse = this.embeddingModel.call( new EmbeddingRequest(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;), OllamaOptions.builder() .model(\u0026#34;chroma/all-minilm-l6-v2-f32\u0026#34;)) .truncate(false) .build()); OllamaOptions 提供所有嵌入请求的配置信息。\n"},{"id":47,"href":"/docs/structured-output/","title":"结构化输出转换器","section":"Docs","content":" 结构化输出转换器 # LLM 生成结构化输出的能力对于依赖可靠解析输出值的下游应用程序非常重要。开发人员希望将 AI 模型的结果快速转换为数据类型，例如 JSON、XML 或 Java 类，这些数据类型可以传递给其他应用程序函数和方法。 Spring AI 结构化输出转换器有助于将 LLM 输出转换为结构化格式。如下图所示，此方法围绕 LLM 文本完成端点运行： 使用通用完成 API 从大型语言模型 （LLM） 生成结构化输出需要仔细处理输入和输出。结构化输出转换器在 LLM 调用之前和之后起着至关重要的作用，确保实现所需的输出结构。 在 LLM 调用之前，转换器会将格式说明附加到提示符中，为模型生成所需的输出结构提供明确的指导。这些指令充当蓝图，调整模型的响应以符合指定的格式。 在 LLM 调用之后，转换器获取模型的输出文本并将其转换为结构化类型的实例。此转换过程包括解析原始文本输出并将其映射到相应的结构化数据表示形式，例如 JSON、XML 或特定于域的数据结构。\n结构化输出 API # StructuredOutputConverter 接口允许您获取结构化输出，例如将输出映射到基于文本的 AI 模型输出中的 Java 类或值数组。接口定义为：\npublic interface StructuredOutputConverter\u0026lt;T\u0026gt; extends Converter\u0026lt;String, T\u0026gt;, FormatProvider { } 它结合了 Spring [ Converter\u0026lt;String、T\u0026gt;]( https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/core/convert/converter/Converter.html) 接口和 FormatProvider 接口\npublic interface FormatProvider { String getFormat(); } 下图显示了使用结构化输出 API 时的数据流。 FormatProvider 为 AI 模型提供特定的格式设置准则，使其能够生成文本输出，这些输出可以使用 Converter 转换为指定的目标类型 T。以下是此类格式设置说明的示例： 格式说明通常使用 [ PromptTemplate](prompt.html#_prompttemplate) 附加到用户输入的末尾，如下所示：\nStructuredOutputConverter outputConverter = ... String userInputTemplate = \u0026#34;\u0026#34;\u0026#34; ... user text input .... {format} \u0026#34;\u0026#34;\u0026#34;; // user input with a \u0026#34;format\u0026#34; placeholder. Prompt prompt = new Prompt( new PromptTemplate( this.userInputTemplate, Map.of(..., \u0026#34;format\u0026#34;, outputConverter.getFormat()) // replace the \u0026#34;format\u0026#34; placeholder with the converter\u0026#39;s format. ).createMessage()); Converter\u0026lt;String， T\u0026gt; 负责将模型的输出文本转换为指定类型 T 的实例。\n可用转换器 # 目前，Spring AI 提供了 AbstractConversionServiceOutputConverter 、、 AbstractMessageOutputConverter BeanOutputConverter、MapOutputConverter 和 ListOutputConverter 实现： AbstractConversionServiceOutputConverter - 提供预配置的 GenericConversionService，用于将 LLM 输出转换为所需的格式。未提供默认的 FormatProvider 实现。 AbstractMessageOutputConverter - 提供预配置的 MessageConverter，用于将 LLM 输出转换为所需的格式。未提供默认的 FormatProvider 实现。 BeanOutputConverter - 配置了指定的 Java 类（例如 Bean）或 ParameterizedTypeReference，此转换器采用 FormatProvider 实现，该实现指示 AI 模型生成符合 DRAFT_2020_12 的 JSON 响应，该响应从指定的 Java 类派生的 JSON 架构 。随后，它利用 ObjectMapper 将 JSON 输出反序列化为目标类的 Java 对象实例。 MapOutputConverter - AbstractMessageOutputConverter 使用 FormatProvider 实现扩展其功能，该实现指导 AI 模型生成符合 RFC8259 的 JSON 响应。此外，它还包含一个转换器实现，该实现利用提供的 MessageConverter 将 JSON 有效负载转换为 java.util.Map\u0026lt;String， Object\u0026gt; 实例。 ListOutputConverter - 扩展 AbstractConversionServiceOutputConverter 并包含为逗号分隔的列表输出量身定制的 FormatProvider 实现。转换器实现使用提供的 ConversionService 将模型文本输出转换为 java.util.List。 使用转换器 # 以下部分提供了如何使用可用转换器生成结构化输出的指南。\nBean 输出转换器 # 下面的示例演示如何使用 BeanOutputConverter 为 Actor 生成电影作品。 表示演员电影作品的目标记录：\nrecord ActorsFilms(String actor, List\u0026lt;String\u0026gt; movies) { } 以下是如何使用高级、流畅的 ChatClient API 应用 BeanOutputConverter：\nActorsFilms actorsFilms = ChatClient.create(chatModel).prompt() .user(u -\u0026gt; u.text(\u0026#34;Generate the filmography of 5 movies for {actor}.\u0026#34;) .param(\u0026#34;actor\u0026#34;, \u0026#34;Tom Hanks\u0026#34;)) .call() .entity(ActorsFilms.class); 或直接使用低级 ChatModel API：\nBeanOutputConverter\u0026lt;ActorsFilms\u0026gt; beanOutputConverter = new BeanOutputConverter\u0026lt;\u0026gt;(ActorsFilms.class); String format = this.beanOutputConverter.getFormat(); String actor = \u0026#34;Tom Hanks\u0026#34;; String template = \u0026#34;\u0026#34;\u0026#34; Generate the filmography of 5 movies for {actor}. {format} \u0026#34;\u0026#34;\u0026#34;; Generation generation = chatModel.call( new PromptTemplate(this.template, Map.of(\u0026#34;actor\u0026#34;, this.actor, \u0026#34;format\u0026#34;, this.format)).create()).getResult(); ActorsFilms actorsFilms = this.beanOutputConverter.convert(this.generation.getOutput().getText()); 生成的架构中的属性排序 # BeanOutputConverter 支持通过 @JsonPropertyOrder 注解在生成的 JSON 模式中进行自定义属性排序。此注释允许您指定属性在架构中出现的确切顺序，而不管它们在类或记录中的声明顺序如何。 例如，要确保 ActorsFilms 记录中属性的特定排序：\n@JsonPropertyOrder({\u0026#34;actor\u0026#34;, \u0026#34;movies\u0026#34;}) record ActorsFilms(String actor, List\u0026lt;String\u0026gt; movies) {} 此 Annotation 适用于记录和常规 Java 类。\n泛型 Bean 类型 # 使用 ParameterizedTypeReference 构造函数指定更复杂的目标类结构。例如，要表示演员及其电影作品的列表：\nList\u0026lt;ActorsFilms\u0026gt; actorsFilms = ChatClient.create(chatModel).prompt() .user(\u0026#34;Generate the filmography of 5 movies for Tom Hanks and Bill Murray.\u0026#34;) .call() .entity(new ParameterizedTypeReference\u0026lt;List\u0026lt;ActorsFilms\u0026gt;\u0026gt;() {}); 或直接使用低级 ChatModel API：\nBeanOutputConverter\u0026lt;List\u0026lt;ActorsFilms\u0026gt;\u0026gt; outputConverter = new BeanOutputConverter\u0026lt;\u0026gt;( new ParameterizedTypeReference\u0026lt;List\u0026lt;ActorsFilms\u0026gt;\u0026gt;() { }); String format = this.outputConverter.getFormat(); String template = \u0026#34;\u0026#34;\u0026#34; Generate the filmography of 5 movies for Tom Hanks and Bill Murray. {format} \u0026#34;\u0026#34;\u0026#34;; Prompt prompt = new PromptTemplate(this.template, Map.of(\u0026#34;format\u0026#34;, this.format)).create(); Generation generation = chatModel.call(this.prompt).getResult(); List\u0026lt;ActorsFilms\u0026gt; actorsFilms = this.outputConverter.convert(this.generation.getOutput().getText()); 贴图输出转换器 # 以下代码片段显示了如何使用 MapOutputConverter 将模型输出转换为地图中的数字列表。\nMap\u0026lt;String, Object\u0026gt; result = ChatClient.create(chatModel).prompt() .user(u -\u0026gt; u.text(\u0026#34;Provide me a List of {subject}\u0026#34;) .param(\u0026#34;subject\u0026#34;, \u0026#34;an array of numbers from 1 to 9 under they key name \u0026#39;numbers\u0026#39;\u0026#34;)) .call() .entity(new ParameterizedTypeReference\u0026lt;Map\u0026lt;String, Object\u0026gt;\u0026gt;() {}); 或直接使用低级 ChatModel API：\nMapOutputConverter mapOutputConverter = new MapOutputConverter(); String format = this.mapOutputConverter.getFormat(); String template = \u0026#34;\u0026#34;\u0026#34; Provide me a List of {subject} {format} \u0026#34;\u0026#34;\u0026#34;; Prompt prompt = new PromptTemplate(this.template, Map.of(\u0026#34;subject\u0026#34;, \u0026#34;an array of numbers from 1 to 9 under they key name \u0026#39;numbers\u0026#39;\u0026#34;, \u0026#34;format\u0026#34;, this.format)).create(); Generation generation = chatModel.call(this.prompt).getResult(); Map\u0026lt;String, Object\u0026gt; result = this.mapOutputConverter.convert(this.generation.getOutput().getText()); 列出输出转换器 # 以下代码段显示了如何使用 ListOutputConverter 将模型输出转换为冰淇淋口味列表。\nList\u0026lt;String\u0026gt; flavors = ChatClient.create(chatModel).prompt() .user(u -\u0026gt; u.text(\u0026#34;List five {subject}\u0026#34;) .param(\u0026#34;subject\u0026#34;, \u0026#34;ice cream flavors\u0026#34;)) .call() .entity(new ListOutputConverter(new DefaultConversionService())); 或直接使用低级 ChatModel API：\nListOutputConverter listOutputConverter = new ListOutputConverter(new DefaultConversionService()); String format = this.listOutputConverter.getFormat(); String template = \u0026#34;\u0026#34;\u0026#34; List five {subject} {format} \u0026#34;\u0026#34;\u0026#34;; Prompt prompt = new PromptTemplate(this.template, Map.of(\u0026#34;subject\u0026#34;, \u0026#34;ice cream flavors\u0026#34;, \u0026#34;format\u0026#34;, this.format)).create(); Generation generation = this.chatModel.call(this.prompt).getResult(); List\u0026lt;String\u0026gt; list = this.listOutputConverter.convert(this.generation.getOutput().getText()); 支持的 AI 模型 # 以下 AI 模型已经过测试，可支持 List、Map 和 Bean 结构化输出。\n内置 JSON 模式 # 一些 AI 模型提供专用的配置选项来生成结构化（通常是 JSON）输出。\nOpenAI 结构化输出可以确保您的模型生成严格符合您提供的 JSON 架构的响应。您可以选择保证模型生成的消息是有效 JSON 的 JSON_OBJECT，也可以选择 JSON_SCHEMA 提供的架构，以保证模型将生成与您提供的架构匹配的响应（ spring.ai.openai.chat.options.responseFormat 选项）。 Azure OpenAI - 提供指定模型必须输出的格式 spring.ai.azure.openai.chat.options.responseFormat 的选项。设置为 { “type”： “json_object” } 将启用 JSON 模式，从而保证模型生成的消息是有效的 JSON。 Ollama - 提供一个 spring.ai.ollama.chat.options.format 选项，用于指定返回响应的格式。目前，唯一接受的值是 json。 Mistral AI - 提供一个 spring.ai.mistralai.chat.options.responseFormat 选项，用于指定返回响应的格式。将其设置为 { “type”： “json_object” } 将启用 JSON 模式，从而保证模型生成的消息是有效的 JSON。 "},{"id":48,"href":"/docs/vector-databases/gemfire/","title":"GemFire 矢量商店","section":"矢量数据库","content":" GemFire 矢量商店 # 本节将指导您设置 GemFireVectorStore 以存储文档嵌入并执行相似性搜索。 [ GemFire]( https://tanzu.vmware.com/gemfire) 是一种分布式内存中的键值存储，以极快的速度执行读写作。它提供高度可用的并行消息队列、持续可用性和事件驱动型架构，您可以在不停机的情况下动态扩展。随着您的数据大小要求增加以支持高性能、实时应用程序，[ GemFire]( https://tanzu.vmware.com/gemfire) 可以轻松地线性扩展。 [ GemFire VectorDB]( https://docs.vmware.com/en/VMware-GemFire-VectorDB/1.0/gemfire-vectordb/overview.html) 扩展了 GemFire 的功能，作为一个多功能的矢量数据库，可以有效地存储、检索和执行矢量相似性搜索。\n先决条件 # 自动配置 # 将 GemFire VectorStore Spring Boot 启动器添加到项目的 Maven 构建文件 pom.xml：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-gemfire\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或复制到 Gradle build.gradle 文件\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-vector-store-gemfire\u0026#39; } 配置属性 # 你可以在 Spring Boot 配置中使用以下属性来进一步配置 GemFireVectorStore。\n手动配置 # 要仅使用 GemFireVectorStore，请在没有 Spring Boot 的自动配置的情况下，将以下依赖项添加到项目的 Maven pom.xml 中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-gemfire-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 对于 Gradle 用户，请将以下内容添加到 dependencies 块下的 build.gradle 文件中，以仅使用 GemFireVectorStore：\n用法 # 下面是一个创建 GemfireVectorStore 实例的示例，而不是使用 AutoConfiguration\n@Bean public GemFireVectorStore vectorStore(EmbeddingModel embeddingModel) { return GemFireVectorStore.builder(embeddingModel) .host(\u0026#34;localhost\u0026#34;) .port(7071) .indexName(\u0026#34;my-vector-index\u0026#34;) .initializeSchema(true) .build(); } 在您的应用程序中，创建一些文档： List\u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;country\u0026#34;, \u0026#34;UK\u0026#34;, \u0026#34;year\u0026#34;, 2020)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;, Map.of()), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;country\u0026#34;, \u0026#34;NL\u0026#34;, \u0026#34;year\u0026#34;, 2023))); 将文档添加到向量存储： vectorStore.add(documents); 要使用相似性搜索检索文档： List\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch( SearchRequest.builder().query(\u0026#34;Spring\u0026#34;).topK(5).build()); 您应该检索包含文本 “Spring AI rocks！！” 的文档。 您还可以使用相似性阈值来限制结果的数量：\nList\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch( SearchRequest.builder().query(\u0026#34;Spring\u0026#34;).topK(5) .similarityThreshold(0.5d).build()); "},{"id":49,"href":"/docs/models/embedding-models/onnx-transformers/","title":"变压器 （ONNX） 嵌入","section":"嵌入模型 API","content":" 变压器 （ONNX） 嵌入 # `TransformersEmbeddingModel``` 是一个 EmbeddingModel`` 实现，它使用选定的[ 句子转换器]( https://www.sbert.net/)在本地计算[ 句子嵌入]( https://www.sbert.net/examples/applications/computing-embeddings/README.html#sentence-embeddings-with-transformers) 。 您可以使用任何 [ HuggingFace Embedding 模型]( https://huggingface.co/spaces/mteb/leaderboard) 。 它使用[ 预先训练的]( https://www.sbert.net/docs/pretrained_models.html) transformer 模型，序列化为 [ Open Neural Network Exchange （ONNX）]( https://onnx.ai/) 格式。 [ Deep Java 库]( https://djl.ai/)和 Microsoft [ ONNX Java 运行时]( https://onnxruntime.ai/docs/get-started/with-java.html)库用于运行 ONNX 模型并计算 Java 中的嵌入。\n先决条件 # 要在 Java 中运行，我们需要将 Tokenizer 和 Transformer 模型序列化为 ONNX 格式。 使用 [[optimum-cli](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model#exporting-a-model-to-onnx-using-the-cli)](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model#exporting-a-model-to-onnx-using-the-cli) 进行序列化 - 实现此目的的一种快速方法是使用 [[optimum-cli](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model#exporting-a-model-to-onnx-using-the-cli)](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model#exporting-a-model-to-onnx-using-the-cli) 命令行工具。以下代码段准备了一个 python 虚拟环境，安装所需的包并使用 [[optimum-cli](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model#exporting-a-model-to-onnx-using-the-cli)](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model#exporting-a-model-to-onnx-using-the-cli) 序列化（例如导出）指定的模型：\npython3 -m venv venv source ./venv/bin/activate (venv) pip install --upgrade pip (venv) pip install optimum onnx onnxruntime sentence-transformers (venv) optimum-cli export onnx --model sentence-transformers/all-MiniLM-L6-v2 onnx-output-folder 该代码段将 [ sentence-transformers/all-MiniLM-L6-v2]( https://huggingface.co/[sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)) 转换器导出到 onnx-output-folder 文件夹中。后者包括嵌入模型使用的 tokenizer.json 和 model.onnx 文件。 您可以选择任何 huggingface 转换器标识符或提供直接文件路径，而不是全 MiniLM-L6-v2。\n自动配置 # Spring AI 为 ONNX Transformer 嵌入模型提供了 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-transformers\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-transformers\u0026#39; } 要配置它，请使用 spring.ai.embedding.transformer.* properties. 例如，将此内容添加到您的 application.properties 文件中，以使用 [ intfloat/e5-small-v2]( https://huggingface.co/[intfloat/e5-small-v2](https://huggingface.co/intfloat/e5-small-v2)) 文本嵌入模型配置客户端： 支持的属性的完整列表包括：\n嵌入属性 # 错误和特殊情况 # 手动配置 # 如果您不使用 Spring Boot，则可以手动配置 Onnx Transformers Embedding Model。为此，将 spring-ai-transformers 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-transformers\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 然后创建一个新的 TransformersEmbeddingModel 实例，并使用 setTokenizerResource(tokenizerJsonUri) and setModelResource(modelOnnxUri) 方法设置导出的 tokenizer.json 和 model.onnx 文件的 URI。（classpath：、file： 或 https： 支持 URI 架构）。 如果未显式设置模型， 则 TransformersEmbeddingModel 默认为 [ sentence-transformers/all-MiniLM-L6-v2]( https://huggingface.co/[sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2))： 以下代码段说明了如何手动使用 TransformersEmbeddingModel：\nTransformersEmbeddingModel embeddingModel = new TransformersEmbeddingModel(); // (optional) defaults to classpath:/onnx/all-MiniLM-L6-v2/tokenizer.json embeddingModel.setTokenizerResource(\u0026#34;classpath:/onnx/all-MiniLM-L6-v2/tokenizer.json\u0026#34;); // (optional) defaults to classpath:/onnx/all-MiniLM-L6-v2/model.onnx embeddingModel.setModelResource(\u0026#34;classpath:/onnx/all-MiniLM-L6-v2/model.onnx\u0026#34;); // (optional) defaults to ${java.io.tmpdir}/spring-ai-onnx-model // Only the http/https resources are cached by default. embeddingModel.setResourceCacheDirectory(\u0026#34;/tmp/onnx-zoo\u0026#34;); // (optional) Set the tokenizer padding if you see an errors like: // \u0026#34;ai.onnxruntime.OrtException: Supplied array is ragged, ...\u0026#34; embeddingModel.setTokenizerOptions(Map.of(\u0026#34;padding\u0026#34;, \u0026#34;true\u0026#34;)); embeddingModel.afterPropertiesSet(); List\u0026lt;List\u0026lt;Double\u0026gt;\u0026gt; embeddings = this.embeddingModel.embed(List.of(\u0026#34;Hello world\u0026#34;, \u0026#34;World is big\u0026#34;)); 第一个 embed（） 调用下载大型 ONNX 模型并将其缓存在本地文件系统上。因此，第一次调用可能需要比平时更长的时间。使用该方法 #setResourceCacheDirectory(\u0026lt;path\u0026gt;) 将 ONNX 模型的存储位置设置为本地文件夹。默认缓存文件夹为 ${java.io.tmpdir}/spring-ai-onnx-model . 将 TransformersEmbeddingModel 创建为 Bean 更方便（也是首选）。这样，你就不必手动调用 afterPropertiesSet（） 了。\n@Bean public EmbeddingModel embeddingModel() { return new TransformersEmbeddingModel(); } "},{"id":50,"href":"/docs/multimodality/","title":"多模态 API","section":"Docs","content":" 多模态 API # 人类跨多种数据输入模式同时处理知识。我们的学习方式、我们的体验都是多模式的。我们不仅有视觉，只有音频和文本。 与这些原则相反，机器学习通常专注于为处理单一模态而量身定制的专用模型。例如，我们为文本转语音或语音转文本等任务开发了音频模型，并为对象检测和分类等任务开发了计算机视觉模型。 然而，新一波多模态大型语言模型开始出现。示例包括 OpenAI 的 GPT-4o、Google 的 Vertex AI Gemini 1.5、Anthropic 的 Claude3 以及开源产品 Llama3.2、LLaVA 和 BakLLaVA 能够接受多个输入，包括文本图像、音频和视频，并通过集成这些输入来生成文本响应。\nSpring AI 多模态 # 多模态是指模型同时理解和处理来自各种来源的信息（包括文本、图像、音频和其他数据格式）的能力。 Spring AI 消息 API 提供了支持多模态 LLM 的所有必要抽象。 UserMessage 的 `````content````` 字段主要用于文本输入，而可选的 ````````media```````` 字段允许添加一个或多个不同形式的其他内容，例如图像、音频和视频。````````MimeType```````` 指定模态类型。根据使用的 LLM，`````Media````` data 字段可以是作为 R``e``s``o``u``r``c``e 对象的原始媒体内容，也可以是内容的 U``R``I。 例如，我们可以将下面的图片 （multimodal.test.png``） 作为输入，并要求 LLM 解释它所看到的内容。 对于大多数多模态 LLM，Spring AI 代码将如下所示：\nvar imageResource = new ClassPathResource(\u0026#34;/multimodal.test.png\u0026#34;); var userMessage = new UserMessage( \u0026#34;Explain what do you see in this picture?\u0026#34;, // content new Media(MimeTypeUtils.IMAGE_PNG, this.imageResource)); // media ChatResponse response = chatModel.call(new Prompt(this.userMessage)); 或使用 Fluent [ ChatClient](chatclient.html) API：\nString response = ChatClient.create(chatModel).prompt() .user(u -\u0026gt; u.text(\u0026#34;Explain what do you see on this picture?\u0026#34;) .media(MimeTypeUtils.IMAGE_PNG, new ClassPathResource(\u0026#34;/multimodal.test.png\u0026#34;))) .call() .content(); 并生成如下响应： Spring AI 为以下聊天模型提供多模态支持：\n人类学克劳德 3 AWS 基岩匡威 Azure Open AI（例如 GPT-4o 模型） Mistral AI（例如 Mistral Pixtral 模型） Ollama（例如 LLaVA、BakLLaVA、Llama3.2 型号） OpenAI（例如 GPT-4 和 GPT-4o 模型） Vertex AI Gemini（例如 gemini-1.5-pro-001、gemini-1.5-flash-001 型号） "},{"id":51,"href":"/docs/models/chat-models/google-vertexai/","title":"谷歌 VertexAI API","section":"聊天模型 API","content":" 谷歌 VertexAI API # [ VertexAI API]( https://cloud.google.com/vertex-ai/docs/reference) 以最少的机器学习专业知识和工作量提供高质量的自定义机器学习模型。 Spring AI 通过以下客户端提供与 VertexAI API 的集成：\nVertexAI Gemini 聊天 "},{"id":52,"href":"/docs/models/chat-models/groq/","title":"Groq 聊天","section":"聊天模型 API","content":" Groq 聊天 # [ Groq]( https://groq.com/) 是一种非常快速、基于 LPU™ 的 AI 推理引擎，支持各种 [ AI 模型]( https://console.groq.com/docs/models) ，支持工具/函数调用 ，并公开与 OpenAI API 兼容的端点。 Spring AI 通过重用现有的 [ OpenAI](openai-chat.html) 客户端与 [ Groq]( https://groq.com/) 集成。为此，您需要获取 [ Groq]( https://groq.com/) Api 密钥 ，将 base-url 设置为 [ api.groq.com/openai](https:// api.groq.com/openai) 并选择提供的 [ Groq]( https://groq.com/) 模型之一。 查看 [ GroqWithOpenAiChatModelIT.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/chat/proxy/[GroqWithOpenAiChatModelIT.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/chat/proxy/GroqWithOpenAiChatModelIT.java)) 测试，了解将 Groq 与 Spring AI 结合使用的示例。\n先决条件 # 创建 API 密钥 ：访问此处创建 API 密钥。Spring AI 项目定义了一个名为 spring.ai.openai.api-key 的配置属性，您应该将其设置为从 groq.com 获取的 API 密钥的值。 设置 Groq URL：您必须将 spring.ai.openai.base-url 属性设置为 api.groq.com/openai。 Select a Groq Model：使用属性 spring.ai.openai.chat.model= 从可用的 Groq 模型中进行选择。 您可以在 application.properties 文件中设置这些配置属性： spring.ai.openai.api-key=\u0026lt;your-groq-api-key\u0026gt; spring.ai.openai.base-url=https://api.groq.com/openai spring.ai.openai.chat.model=llama3-70b-8192 为了在处理 API 密钥等敏感信息时增强安全性，您可以使用 Spring 表达式语言 （SpEL） 来引用自定义环境变量：\n# In application.yml spring: ai: openai: api-key: ${GROQ_API_KEY} base-url: ${GROQ_BASE_URL} chat: model: ${GROQ_MODEL} # In your environment or .env file export GROQ_API_KEY=\u0026lt;your-groq-api-key\u0026gt; export GROQ_BASE_URL=https://api.groq.com/openai export GROQ_MODEL=llama3-70b-8192 您还可以在应用程序代码中以编程方式设置这些配置：\n// Retrieve configuration from secure sources or environment variables String apiKey = System.getenv(\u0026#34;GROQ_API_KEY\u0026#34;); String baseUrl = System.getenv(\u0026#34;GROQ_BASE_URL\u0026#34;); String model = System.getenv(\u0026#34;GROQ_MODEL\u0026#34;); 添加存储库和 BOM # Spring AI 工件发布在 Maven Central 和 Spring Snapshot 存储库中。请参阅 [ Artifact Repositories](../../getting-started.html#artifact-repositories) 部分，将这些存储库添加到您的构建系统中。 为了帮助进行[ 依赖项管理](../../getting-started.html#dependency-management)，Spring AI 提供了一个 BOM（物料清单），以确保在整个项目中使用一致的 Spring AI 版本。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # Spring AI 为 OpenAI Chat 客户端提供 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 或 Gradle build.gradle 构建文件中：\n聊天属性 # 重试属性 # 前缀 spring.ai.retry 用作属性前缀，允许您为 OpenAI 聊天模型配置重试机制。\n连接属性 # 前缀 spring.ai.openai 用作允许您连接到 OpenAI 的属性前缀。\n配置属性 # 前缀 spring.ai.openai.chat 是属性前缀，允许您为 OpenAI 配置聊天模型实现。\n运行时选项 # [ OpenAiChatOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/[OpenAiChatOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/OpenAiChatOptions.java)) 提供模型配置，例如要使用的模型、温度、频率损失等。 启动时，可以使用 OpenAiChatModel（api， options） 构造函数或 spring.ai.openai.chat.options.* 属性配置默认选项。 在运行时，您可以通过向 Prompt 调用添加新的、特定于请求的选项来覆盖默认选项。例如，要覆盖特定请求的默认模型和温度：\nChatResponse response = chatModel.call( new Prompt( \u0026#34;Generate the names of 5 famous pirates.\u0026#34;, OpenAiChatOptions.builder() .model(\u0026#34;mixtral-8x7b-32768\u0026#34;) .temperature(0.4) .build() )); 函数调用 # Groq API 端点在选择其中一个工具/函数支持模型时支持[ 工具/函数调用]( https://console.groq.com/docs/tool-use) 。 您可以使用 ChatModel 注册自定义 Java 函数，并让提供的 Groq 模型智能地选择输出包含参数的 JSON 对象，以调用一个或多个已注册的函数。这是一种将 LLM 功能与外部工具和 API 连接起来的强大技术。\n工具示例 # 以下是如何将 Groq 函数调用与 Spring AI 一起使用的简单示例：\n@SpringBootApplication public class GroqApplication { public static void main(String[] args) { SpringApplication.run(GroqApplication.class, args); } @Bean CommandLineRunner runner(ChatClient.Builder chatClientBuilder) { return args -\u0026gt; { var chatClient = chatClientBuilder.build(); var response = chatClient.prompt() .user(\u0026#34;What is the weather in Amsterdam and Paris?\u0026#34;) .functions(\u0026#34;weatherFunction\u0026#34;) // reference by bean name. .call() .content(); System.out.println(response); }; } @Bean @Description(\u0026#34;Get the weather in location\u0026#34;) public Function\u0026lt;WeatherRequest, WeatherResponse\u0026gt; weatherFunction() { return new MockWeatherService(); } public static class MockWeatherService implements Function\u0026lt;WeatherRequest, WeatherResponse\u0026gt; { public record WeatherRequest(String location, String unit) {} public record WeatherResponse(double temp, String unit) {} @Override public WeatherResponse apply(WeatherRequest request) { double temperature = request.location().contains(\u0026#34;Amsterdam\u0026#34;) ? 20 : 25; return new WeatherResponse(temperature, request.unit); } } } 在此示例中，当模型需要天气信息时，它将自动调用 weatherFunction Bean，然后该 bean 可以获取实时天气数据。预期的响应如下所示：“阿姆斯特丹的天气目前是 20 摄氏度，巴黎的天气目前是 25 摄氏度。 阅读有关 OpenAI [ 函数调用]( https://docs.spring.io/spring-ai/reference/api/chat/functions/openai-chat-functions.html)的更多信息。\n模 态 # 样品控制器 # [ 创建一个新]( https://start.spring.io/)的 Spring Boot 项目，并将 添加到您的 spring-ai-starter-model-openai pom（或 gradle）依赖项中。 在 src/main/resources 目录下添加一个 application.properties 文件，以启用和配置 OpenAi 聊天模型：\nspring.ai.openai.api-key=\u0026lt;GROQ_API_KEY\u0026gt; spring.ai.openai.base-url=https://api.groq.com/openai spring.ai.openai.chat.options.model=llama3-70b-8192 spring.ai.openai.chat.options.temperature=0.7 这将创建一个 OpenAiChatModel 实现，您可以将其注入到您的类中。下面是一个简单的 @Controller 类示例，该类使用 chat 模型生成文本。\n@RestController public class ChatController { private final OpenAiChatModel chatModel; @Autowired public ChatController(OpenAiChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, this.chatModel.call(message)); } @GetMapping(\u0026#34;/ai/generateStream\u0026#34;) public Flux\u0026lt;ChatResponse\u0026gt; generateStream(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { Prompt prompt = new Prompt(new UserMessage(message)); return this.chatModel.stream(prompt); } } 手动配置 # OpenAiChatModel 实现了 ChatModel 和 StreamingChatModel，并使用 [ [low-level-api]](#low-level-api) 连接到 OpenAI 服务。 将 spring-ai-openai 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-openai\u0026#39; } 接下来，创建一个 OpenAiChatModel 并将其用于文本生成：\nvar openAiApi = new OpenAiApi(\u0026#34;https://api.groq.com/openai\u0026#34;, System.getenv(\u0026#34;GROQ_API_KEY\u0026#34;)); var openAiChatOptions = OpenAiChatOptions.builder() .model(\u0026#34;llama3-70b-8192\u0026#34;) .temperature(0.4) .maxTokens(200) .build(); var chatModel = new OpenAiChatModel(this.openAiApi, this.openAiChatOptions); ChatResponse response = this.chatModel.call( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); // Or with streaming responses Flux\u0026lt;ChatResponse\u0026gt; response = this.chatModel.stream( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); OpenAiChatOptions 提供聊天请求的配置信息。OpenAiChatOptions.Builder 是 Fluent 选项构建器。\n"},{"id":53,"href":"/docs/vector-databases/mariadb-vector-store/","title":"MariaDB 矢量存储","section":"矢量数据库","content":" MariaDB 矢量存储 # 本节将引导您设置 MariaDBVectorStore 以存储文档嵌入并执行相似性搜索。 [ MariaDB Vector]( https://mariadb.org/projects/mariadb-vector/) 是 MariaDB 11.7 的一部分，支持存储和搜索机器学习生成的嵌入。它使用向量索引提供高效的向量相似性搜索功能，同时支持余弦相似度和欧几里得距离度量。\n先决条件 # 一个正在运行的 MariaDB （11.7+） 实例。以下选项可用： Docker 镜像 MariaDB 服务器 如果需要，EmbeddingModel 的 API 密钥，用于生成 MariaDBVectorStore 存储的嵌入。 自动配置 # Spring AI 为 MariaDB Vector Store 提供 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-mariadb\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-vector-store-mariadb\u0026#39; } 矢量存储实现可以为您初始化所需的架构，但您必须通过在相应的构造函数中指定 initializeSchema 布尔值或设置 ...initialize-schema=true 在 application.properties 文件中。 此外，您将需要一个已配置的 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) bean。请参阅 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) 部分以了解更多信息。 例如，要使用 [ OpenAI EmbeddingModel](../embeddings/openai-embeddings.html)，请添加以下依赖项：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 现在你可以在你的应用程序中自动连接 MariaDBVectorStore：\n@Autowired VectorStore vectorStore; // ... List\u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); // Add the documents to MariaDB vectorStore.add(documents); // Retrieve documents similar to a query List\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch(SearchRequest.builder().query(\u0026#34;Spring\u0026#34;).topK(5).build()); 配置属性 # 要连接到 MariaDB 并使用 MariaDBVectorStore，您需要提供实例的访问详细信息。可以通过 Spring Boot 的 application.yml 提供一个简单的配置：\nspring: datasource: url: jdbc:mariadb://localhost/db username: myUser password: myPassword ai: vectorstore: mariadb: initialize-schema: true distance-type: COSINE dimensions: 1536 以 开头的属性 spring.ai.vectorstore.mariadb.* 用于配置 MariaDBVectorStore：\n手动配置 # 您可以手动配置 MariaDB 向量存储，而不是使用 Spring Boot 自动配置。为此，您需要将以下依赖项添加到您的项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-jdbc\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.mariadb.jdbc\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mariadb-java-client\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;runtime\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-mariadb-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 然后使用构建器模式创建 MariaDBVectorStore Bean：\n@Bean public VectorStore vectorStore(JdbcTemplate jdbcTemplate, EmbeddingModel embeddingModel) { return MariaDBVectorStore.builder(jdbcTemplate, embeddingModel) .dimensions(1536) // Optional: defaults to 1536 .distanceType(MariaDBDistanceType.COSINE) // Optional: defaults to COSINE .schemaName(\u0026#34;mydb\u0026#34;) // Optional: defaults to null .vectorTableName(\u0026#34;custom_vectors\u0026#34;) // Optional: defaults to \u0026#34;vector_store\u0026#34; .contentFieldName(\u0026#34;text\u0026#34;) // Optional: defaults to \u0026#34;content\u0026#34; .embeddingFieldName(\u0026#34;embedding\u0026#34;) // Optional: defaults to \u0026#34;embedding\u0026#34; .idFieldName(\u0026#34;doc_id\u0026#34;) // Optional: defaults to \u0026#34;id\u0026#34; .metadataFieldName(\u0026#34;meta\u0026#34;) // Optional: defaults to \u0026#34;metadata\u0026#34; .initializeSchema(true) // Optional: defaults to false .schemaValidation(true) // Optional: defaults to false .removeExistingVectorStoreTable(false) // Optional: defaults to false .maxDocumentBatchSize(10000) // Optional: defaults to 10000 .build(); } // This can be any EmbeddingModel implementation @Bean public EmbeddingModel embeddingModel() { return new OpenAiEmbeddingModel(new OpenAiApi(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;))); } 元数据筛选 # 您可以将通用的可移植[ 元数据过滤器](../vectordbs.html#metadata-filters)与 MariaDB Vector store 结合使用。 例如，您可以使用文本表达式语言：\nvectorStore.similaritySearch( SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(\u0026#34;author in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; article_type == \u0026#39;blog\u0026#39;\u0026#34;).build()); 或使用 Filter.Expression DSL 以编程方式：\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(b.and( b.in(\u0026#34;author\u0026#34;, \u0026#34;john\u0026#34;, \u0026#34;jill\u0026#34;), b.eq(\u0026#34;article_type\u0026#34;, \u0026#34;blog\u0026#34;)).build()).build()); 访问 Native Client # MariaDB Vector Store 实现通过 getNativeClient（） 方法提供对底层原生 JDBC 客户端（JdbcTemplate）的访问：\nMariaDBVectorStore vectorStore = context.getBean(MariaDBVectorStore.class); Optional\u0026lt;JdbcTemplate\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { JdbcTemplate jdbc = nativeClient.get(); // Use the native client for MariaDB-specific operations } 本机客户端允许您访问特定于 MariaDB 的功能和作，这些功能和作可能无法通过 VectorStore 接口公开。\n"},{"id":54,"href":"/docs/models/","title":"None","section":"Docs","content":" None # 介绍 # Spring AI API 涵盖了广泛的功能。每个主要功能在其自己的专用部分中进行了详细说明。为了提供概述，可以使用以下关键功能：\nAI 模型 API # 跨 AI 提供商的可移植模型 API，用于聊天 、 文本到图像 、 音频转录 、 文本到语音``和``嵌入模型。同步和``流 API 选项均受支持。还支持下拉以访问特定于模型的功能。 支持来自 OpenAI、Microsoft、Amazon、Google、Amazon Bedrock、Hugging Face 等的 AI 模型。 矢量存储 API # 跨多个提供程序的可移植矢量存储 API，包括一个新颖的类似 SQL 的元数据过滤器 API，该 API 也是可移植的。支持 14 个矢量数据库。\n工具调用 API # Spring AI 可以轻松地让 AI 模型作为 @Tool 注释的方法或 POJO java.util.Function 对象调用您的服务。 查看 Spring AI [ 工具调用](tools.html)文档。\n自动配置 # AI 模型和向量存储的 Spring Boot 自动配置和启动器。\nETL 数据工程 # 用于数据工程的 ETL 框架。这为将数据加载到向量数据库提供了基础，有助于实施检索增强生成模式，使您能够将数据带到 AI 模型以合并到其响应中。 反馈和贡献 # 该项目的 [ GitHub 讨论]( https://github.com/spring-projects/spring-ai/discussions)是发送反馈的好地方。\n"},{"id":55,"href":"/docs/models/embedding-models/openai/","title":"OpenAI 嵌入","section":"嵌入模型 API","content":" OpenAI 嵌入 # Spring AI 支持 OpenAI 的文本嵌入模型。OpenAI 的文本嵌入测量文本字符串的相关性。嵌入是浮点数的向量（列表）。两个向量之间的距离衡量它们的相关性。小距离表示高相关性，大距离表示低相关性。\n先决条件 # 您需要使用 OpenAI 创建 API 才能访问 OpenAI 嵌入模型。 在 [ OpenAI 注册页面]( https://platform.openai.com/signup)创建一个帐户，并在 [ API 密钥页面上]( https://platform.openai.com/account/api-keys)生成令牌。 Spring AI 项目定义了一个名为 spring.ai.openai.api-key 的配置属性，您应该将其设置为从 openai.com 获取的 API 密钥的值。 您可以在 application.properties 文件中设置此配置属性：\nspring.ai.openai.api-key=\u0026lt;your-openai-api-key\u0026gt; 为了在处理 API 密钥等敏感信息时增强安全性，您可以使用 Spring 表达式语言 （SpEL） 来引用环境变量：\n# In application.yml spring: ai: openai: api-key: ${OPENAI_API_KEY} # In your environment or .env file export OPENAI_API_KEY=\u0026lt;your-openai-api-key\u0026gt; 您还可以在应用程序代码中以编程方式设置此配置：\n// Retrieve API key from a secure source or environment variable String apiKey = System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;); 添加存储库和 BOM # Spring AI 工件发布在 Maven Central 和 Spring Snapshot 存储库中。请参阅 [ Artifact Repositories](../../getting-started.html#artifact-repositories) 部分，将这些存储库添加到您的构建系统中。 为了帮助进行[ 依赖项管理](../../getting-started.html#dependency-management)，Spring AI 提供了一个 BOM（物料清单），以确保在整个项目中使用一致的 Spring AI 版本。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # Spring AI 为 OpenAI 嵌入模型提供 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-openai\u0026#39; } 嵌入属性 # 重试属性 # 前缀 spring.ai.retry 用作属性前缀，允许您为 OpenAI 嵌入模型配置重试机制。\n连接属性 # 前缀 spring.ai.openai 用作允许您连接到 OpenAI 的属性前缀。\n配置属性 # 前缀 spring.ai.openai.embedding 是属性前缀，用于为 OpenAI 配置 EmbeddingModel 实现。\n运行时选项 # [ OpenAiEmbeddingOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/[OpenAiEmbeddingOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/OpenAiEmbeddingOptions.java)) 提供了 OpenAI 的配置，例如要使用的模型等。 也可以使用 spring.ai.openai.embedding.options properties 配置默认选项。 在启动时，使用 OpenAiEmbeddingModel 构造函数设置用于所有嵌入请求的默认选项。在运行时，您可以使用 OpenAiEmbeddingOptions 实例作为 EmbeddingRequest 的一部分来覆盖默认选项。 例如，要覆盖特定请求的默认模型名称：\nEmbeddingResponse embeddingResponse = embeddingModel.call( new EmbeddingRequest(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;), OpenAiEmbeddingOptions.builder() .model(\u0026#34;Different-Embedding-Model-Deployment-Name\u0026#34;) .build())); 样品控制器 # 这将创建一个 EmbeddingModel 实现，您可以将其注入到您的类中。下面是一个使用 EmbeddingModel 实现的简单 @Controller 类的示例。\nspring.ai.openai.api-key=YOUR_API_KEY spring.ai.openai.embedding.options.model=text-embedding-ada-002 @RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(\u0026#34;/ai/embedding\u0026#34;) public Map embed(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { EmbeddingResponse embeddingResponse = this.embeddingModel.embedForResponse(List.of(message)); return Map.of(\u0026#34;embedding\u0026#34;, embeddingResponse); } } 手动配置 # 如果您不使用 Spring Boot，则可以手动配置 OpenAI 嵌入模型。为此，将 spring-ai-openai 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-openai\u0026#39; } 接下来，创建一个 OpenAiEmbeddingModel 实例，并使用它来计算两个输入文本之间的相似性：\nvar openAiApi = OpenAiApi.builder() .apiKey(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;)) .build(); var embeddingModel = new OpenAiEmbeddingModel( this.openAiApi, MetadataMode.EMBED, OpenAiEmbeddingOptions.builder() .model(\u0026#34;text-embedding-ada-002\u0026#34;) .user(\u0026#34;user-6\u0026#34;) .build(), RetryUtils.DEFAULT_RETRY_TEMPLATE); EmbeddingResponse embeddingResponse = this.embeddingModel .embedForResponse(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;)); OpenAiEmbeddingOptions 提供嵌入请求的配置信息。api 和 options 类提供了一个 builder（） 来轻松创建选项。\n"},{"id":56,"href":"/docs/models/embedding-models/postgresml/","title":"PostgresML 嵌入","section":"嵌入模型 API","content":" PostgresML 嵌入 # Spring AI 支持 PostgresML 文本嵌入模型。 嵌入是文本的数字表示形式。它们用于将单词和句子表示为向量，即数字数组。嵌入可以通过使用距离度量比较数值向量的相似性来查找相似的文本片段，也可以用作其他机器学习模型的输入特征，因为大多数算法不能直接使用文本。 许多预先训练的 LLM 可用于从 PostgresML 中的文本生成嵌入。您可以浏览所有可用的[ 模型]( https://huggingface.co/models?library=sentence-transformers)以在 Hugging Face 上找到最佳解决方案。\n添加存储库和 BOM # Spring AI 工件发布在 Maven Central 和 Spring Snapshot 存储库中。请参阅 [ Artifact Repositories](../../getting-started.html#artifact-repositories) 部分，将这些存储库添加到您的构建系统中。 为了帮助进行[ 依赖项管理](../../getting-started.html#dependency-management)，Spring AI 提供了一个 BOM（物料清单），以确保在整个项目中使用一致的 Spring AI 版本。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # Spring AI 为 Azure PostgresML 嵌入模型提供 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-postgresml-embedding\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-postgresml-embedding\u0026#39; } 使用 spring.ai.postgresml.embedding.options.* 属性配置 PostgresMlEmbeddingModel。链接\n嵌入属性 # 前缀 spring.ai.postgresml.embedding 是属性前缀，用于配置 PostgresML 嵌入的 EmbeddingModel 实现。\n运行时选项 # 使用 [ PostgresMlEmbeddingOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/postgresml/[PostgresMlEmbeddingOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/postgresml/PostgresMlEmbeddingOptions.java)) 配置带有选项的 PostgresMlEmbeddingModel，例如要使用的模型等。 在启动时，您可以将 PostgresMlEmbeddingOptions 传递给 PostgresMlEmbeddingModel 构造函数，以配置用于所有嵌入请求的默认选项。 在运行时，您可以在 EmbeddingRequest 中使用 PostgresMlEmbeddingOptions 覆盖默认选项。 例如，要覆盖特定请求的默认模型名称：\nEmbeddingResponse embeddingResponse = embeddingModel.call( new EmbeddingRequest(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;), PostgresMlEmbeddingOptions.builder() .transformer(\u0026#34;intfloat/e5-small\u0026#34;) .vectorType(VectorType.PG_ARRAY) .kwargs(Map.of(\u0026#34;device\u0026#34;, \u0026#34;gpu\u0026#34;)) .build())); 样品控制器 # 这将创建一个 EmbeddingModel 实现，您可以将其注入到您的类中。下面是一个使用 EmbeddingModel 实现的简单 @Controller 类的示例。\nspring.ai.postgresml.embedding.options.transformer=distilbert-base-uncased spring.ai.postgresml.embedding.options.vectorType=PG_ARRAY spring.ai.postgresml.embedding.options.metadataMode=EMBED spring.ai.postgresml.embedding.options.kwargs.device=cpu @RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(\u0026#34;/ai/embedding\u0026#34;) public Map embed(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { EmbeddingResponse embeddingResponse = this.embeddingModel.embedForResponse(List.of(message)); return Map.of(\u0026#34;embedding\u0026#34;, embeddingResponse); } } 手动配置 # 您可以手动创建 PostgresMlEmbeddingModel，而不是使用 Spring Boot 自动配置。为此，将 spring-ai-postgresml 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-postgresml\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-postgresml\u0026#39; } 接下来，创建一个 PostgresMlEmbeddingModel 实例，并使用它来计算两个输入文本之间的相似性：\nvar jdbcTemplate = new JdbcTemplate(dataSource); // your posgresml data source PostgresMlEmbeddingModel embeddingModel = new PostgresMlEmbeddingModel(this.jdbcTemplate, PostgresMlEmbeddingOptions.builder() .transformer(\u0026#34;distilbert-base-uncased\u0026#34;) // huggingface transformer model name. .vectorType(VectorType.PG_VECTOR) //vector type in PostgreSQL. .kwargs(Map.of(\u0026#34;device\u0026#34;, \u0026#34;cpu\u0026#34;)) // optional arguments. .metadataMode(MetadataMode.EMBED) // Document metadata mode. .build()); embeddingModel.afterPropertiesSet(); // initialize the jdbc template and database. EmbeddingResponse embeddingResponse = this.embeddingModel .embedForResponse(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;)); @Bean public EmbeddingModel embeddingModel(JdbcTemplate jdbcTemplate) { return new PostgresMlEmbeddingModel(jdbcTemplate, PostgresMlEmbeddingOptions.builder() .... .build()); } "},{"id":57,"href":"/docs/models/chat-models/hugging-face/","title":"拥抱脸聊天","section":"聊天模型 API","content":" 拥抱脸聊天 # Hugging Face Text Generation Inference （TGI） 是一种专门的部署解决方案，用于在云中为大型语言模型 （LLM） 提供服务，使其可通过 API 访问。TGI 通过连续批处理、令牌流式处理和高效内存管理等功能为文本生成任务提供优化的性能。\n先决条件 # 您需要在 Hugging Face 上创建一个推理终端节点，并创建一个 API 令牌来访问该终端节点。更多详情可[ 在此处]( https://huggingface.co/docs/inference-endpoints/index)找到。 Spring AI 项目定义了两个配置属性： 您可以[ 在此处]( https://ui.endpoints.huggingface.co/)的推理终端节点的 UI 上找到您的推理终端节点 URL。 您可以在 application.properties 文件中设置这些配置属性：\nspring.ai.huggingface.chat.api-key=\u0026lt;your-huggingface-api-key\u0026gt; spring.ai.huggingface.chat.url=\u0026lt;your-inference-endpoint-url\u0026gt; 为了在处理 API 密钥等敏感信息时增强安全性，您可以使用 Spring 表达式语言 （SpEL） 来引用自定义环境变量：\n# In application.yml spring: ai: huggingface: chat: api-key: ${HUGGINGFACE_API_KEY} url: ${HUGGINGFACE_ENDPOINT_URL} # In your environment or .env file export HUGGINGFACE_API_KEY=\u0026lt;your-huggingface-api-key\u0026gt; export HUGGINGFACE_ENDPOINT_URL=\u0026lt;your-inference-endpoint-url\u0026gt; 您还可以在应用程序代码中以编程方式设置这些配置：\n// Retrieve API key and endpoint URL from secure sources or environment variables String apiKey = System.getenv(\u0026#34;HUGGINGFACE_API_KEY\u0026#34;); String endpointUrl = System.getenv(\u0026#34;HUGGINGFACE_ENDPOINT_URL\u0026#34;); 添加存储库和 BOM # Spring AI 工件发布在 Maven Central 和 Spring Snapshot 存储库中。请参阅 [ Artifact Repositories](../../getting-started.html#artifact-repositories) 部分，将这些存储库添加到您的构建系统中。 为了帮助进行[ 依赖项管理](../../getting-started.html#dependency-management)，Spring AI 提供了一个 BOM（物料清单），以确保在整个项目中使用一致的 Spring AI 版本。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # Spring AI 为 Hugging Face Chat 客户端提供了 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-huggingface\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-huggingface\u0026#39; } 聊天属性 # 前缀 spring.ai.huggingface 是属性前缀，它允许你为 Hugging Face 配置聊天模型实现。\n样品控制器（自动配置） # [ 创建一个新]( https://start.spring.io/)的 Spring Boot 项目，并将 添加到您的 spring-ai-starter-model-huggingface pom（或 gradle）依赖项中。 在 src/main/resources 目录下添加一个 application.properties 文件，以启用和配置 Hugging Face 聊天模型：\nspring.ai.huggingface.chat.api-key=YOUR_API_KEY spring.ai.huggingface.chat.url=YOUR_INFERENCE_ENDPOINT_URL 这将创建一个 HuggingfaceChatModel 实现，您可以将其注入到您的类中。下面是一个简单的 @Controller 类示例，该类使用 chat 模型生成文本。\n@RestController public class ChatController { private final HuggingfaceChatModel chatModel; @Autowired public ChatController(HuggingfaceChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, this.chatModel.call(message)); } } 手动配置 # HuggingfaceChatModel 实现 ChatModel 接口，并使用 [ [low-level-api]](#low-level-api) 连接到 Hugging Face 推理终端节点。 将 spring-ai-huggingface 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-huggingface\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-huggingface\u0026#39; } 接下来，创建一个 HuggingfaceChatModel 并将其用于文本生成：\nHuggingfaceChatModel chatModel = new HuggingfaceChatModel(apiKey, url); ChatResponse response = this.chatModel.call( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); System.out.println(response.getGeneration().getResult().getOutput().getContent()); "},{"id":58,"href":"/docs/vector-databases/milvus/","title":"米尔沃斯","section":"矢量数据库","content":" 米尔沃斯 # [ Milvus]( https://milvus.io/) 是一个开源向量数据库，在数据科学和机器学习领域引起了广泛关注。它的突出特点之一在于它对向量索引和查询的强大支持。[ Milvus]( https://milvus.io/) 采用最先进的尖端算法来加速搜索过程，使其在检索相似向量时非常高效，即使在处理大量数据集时也是如此。\n先决条件 # 一个正在运行的 Milvus 实例。以下选项可用： Milvus 独立版：Docker、Operator、helm、DEB/RPM、Docker Compose。 Milvus Cluster：Operator、Helm。 如果需要，EmbeddingModel 的 API 密钥，用于生成 MilvusVectorStore 存储的嵌入。 依赖 # 然后将 Milvus VectorStore 启动启动依赖项添加到你的项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-milvus\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-vector-store-milvus\u0026#39; } 矢量存储实现可以为您初始化必要的架构，但您必须通过在相应的构造函数中指定 initializeSchema 布尔值或设置 ...initialize-schema=true 在 application.properties 文件中。 Vector Store 还需要一个 EmbeddingModel 实例来计算文档的嵌入。您可以选择一个可用的 EmbeddingModel Implementations。 要连接和配置 MilvusVectorStore，您需要提供实例的访问详细信息。可以通过 Spring Boot 的 application.yml 提供简单的配置 现在，你可以在应用程序中自动连接 Milvus Vector Store 并使用它\n@Autowired VectorStore vectorStore; // ... List \u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); // Add the documents to Milvus Vector Store vectorStore.add(documents); // Retrieve documents similar to a query List\u0026lt;Document\u0026gt; results = this.vectorStore.similaritySearch(SearchRequest.builder().query(\u0026#34;Spring\u0026#34;).topK(5).build()); 手动配置 # 除了使用 Spring Boot 自动配置之外，你可以手动配置 MilvusVectorStore。要将以下依赖项添加到您的项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-milvus-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 要在应用程序中配置 MilvusVectorStore，可以使用以下设置：\n@Bean public VectorStore vectorStore(MilvusServiceClient milvusClient, EmbeddingModel embeddingModel) { return MilvusVectorStore.builder(milvusClient, embeddingModel) .collectionName(\u0026#34;test_vector_store\u0026#34;) .databaseName(\u0026#34;default\u0026#34;) .indexType(IndexType.IVF_FLAT) .metricType(MetricType.COSINE) .batchingStrategy(new TokenCountBatchingStrategy()) .initializeSchema(true) .build(); } @Bean public MilvusServiceClient milvusClient() { return new MilvusServiceClient(ConnectParam.newBuilder() .withAuthorization(\u0026#34;minioadmin\u0026#34;, \u0026#34;minioadmin\u0026#34;) .withUri(milvusContainer.getEndpoint()) .build()); } 元数据筛选 # 您可以在 Milvus 商店中使用通用的可移植[ 元数据过滤器]( https://docs.spring.io/spring-ai/reference/api/vectordbs.html#_metadata_filters) 。 例如，您可以使用文本表达式语言：\nvectorStore.similaritySearch( SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(\u0026#34;author in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; article_type == \u0026#39;blog\u0026#39;\u0026#34;).build()); 或使用 Filter.Expression DSL 以编程方式：\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(b.and( b.in(\u0026#34;author\u0026#34;,\u0026#34;john\u0026#34;, \u0026#34;jill\u0026#34;), b.eq(\u0026#34;article_type\u0026#34;, \u0026#34;blog\u0026#34;)).build()).build()); 使用 MilvusSearchRequest # MilvusSearchRequest 扩展了 SearchRequest，允许你使用 Milvus 特定的搜索参数，例如原生表达式和搜索参数 JSON。\nMilvusSearchRequest request = MilvusSearchRequest.milvusBuilder() .query(\u0026#34;sample query\u0026#34;) .topK(5) .similarityThreshold(0.7) .nativeExpression(\u0026#34;metadata[\\\u0026#34;age\\\u0026#34;] \u0026gt; 30\u0026#34;) // Overrides filterExpression if both are set .filterExpression(\u0026#34;age \u0026lt;= 30\u0026#34;) // Ignored if nativeExpression is set .searchParamsJson(\u0026#34;{\\\u0026#34;nprobe\\\u0026#34;:128}\u0026#34;) .build(); List results = vectorStore.similaritySearch(request); 这在使用 Milvus 特定的搜索功能时提供了更大的灵活性。\nnativeExpression 和 searchParamsJson 在 MilvusSearchRequest 中的重要性 # 这两个参数可以提高 Milvus 的搜索精度，保证查询的最优性能： nativeExpression：使用 Milvus 的原生过滤表达式启用额外的过滤功能。 [ Milvus 过滤]( https://milvus.io/docs/boolean.md) 例：\nMilvusSearchRequest request = MilvusSearchRequest.milvusBuilder() .query(\u0026#34;sample query\u0026#34;) .topK(5) .nativeExpression(\u0026#34;metadata[\u0026#39;category\u0026#39;] == \u0026#39;science\u0026#39;\u0026#34;) .build(); searchParamsJson：在使用 Milvus 的默认索引 IVF_FLAT 时调整搜索行为时必不可少。 [ Milvus 向量索引]( https://milvus.io/docs/index.md?tab=floating) 默认情况下，IVF_FLAT 需要设置 nprobe 以获得准确的结果。如果未指定， 则 nprobe 默认为 1，这可能会导致召回率不佳，甚至搜索结果为零。 例：\nMilvusSearchRequest request = MilvusSearchRequest.milvusBuilder() .query(\u0026#34;sample query\u0026#34;) .topK(5) .searchParamsJson(\u0026#34;{\\\u0026#34;nprobe\\\u0026#34;:128}\u0026#34;) .build(); 使用 nativeExpression 可确保高级筛选，而 searchParamsJson 可防止因默认 nprobe 值较低而导致的无效搜索。\nMilvus VectorStore 属性 # 你可以在 Spring Boot 配置中使用以下属性来自定义 Milvus 向量存储。\n启动 Milvus Store # 在 src/test/resources/ 文件夹中运行：\ndocker-compose up 要清洁环境：\ndocker-compose down; rm -Rf ./volumes 然后连接到 [ http://localhost:19530]( http://localhost:19530) 上的矢量存储或用于管理 [ http://localhost:9001]( http://localhost:9001)（用户：minioadmin，通行证：minioadmin）\n故障 排除 # 如果 Docker 抱怨资源，则执行：\ndocker system prune --all --force --volumes 访问 Native Client # Milvus Vector Store 实现通过 getNativeClient（） 方法提供对底层原生 Milvus 客户端（MilvusServiceClient）的访问：\nMilvusVectorStore vectorStore = context.getBean(MilvusVectorStore.class); Optional\u0026lt;MilvusServiceClient\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { MilvusServiceClient client = nativeClient.get(); // Use the native client for Milvus-specific operations } 原生客户端允许你访问 Milvus 特定的功能和作，这些功能和作可能不会通过 VectorStore 接口公开。\n"},{"id":59,"href":"/docs/chat-memory/","title":"聊天记忆","section":"Docs","content":" 聊天记忆 # 大型语言模型 （LLM） 是无状态的，这意味着它们不会保留有关以前交互的信息。当您希望在多个交互中维护上下文或状态时，这可能是一个限制。为了解决这个问题，Spring AI 提供了聊天内存功能，允许您在与 LLM 的多次交互中存储和检索信息。 ChatMemory 抽象允许您实现各种类型的内存以支持不同的使用案例。消息的底层存储由 ChatMemoryRepository 处理，其唯一职责是存储和检索消息。由 ChatMemory 实现决定要保留哪些消息以及何时删除它们。策略示例可能包括保留最后 N 条消息、将消息保留一段时间或将消息保持在某个令牌限制内。 在选择内存类型之前，必须了解聊天内存和聊天记录之间的区别。 ChatMemory 抽象旨在管理聊天内存 。它允许您存储和检索与当前对话上下文相关的消息。但是，它并不是存储聊天记录的最佳选择。如果您需要维护所有交换消息的完整记录，您应该考虑使用不同的方法，例如依靠 Spring Data 来有效存储和检索完整的聊天历史记录。\n快速开始 # Spring AI 会自动配置一个 ChatMemory bean，您可以直接在应用程序中使用。默认情况下，它使用内存存储库来存储消息 （InMemoryChatMemoryRepository），并使用 MessageWindowChatMemory 实现来管理对话历史记录。如果已经配置了不同的存储库（例如，Cassandra、JDBC 或 Neo4j），Spring AI 将改用该存储库。\n@Autowired ChatMemory chatMemory; 以下部分将进一步描述 Spring AI 中可用的不同内存类型和存储库。\n内存类型 # ChatMemory 抽象允许您实现各种类型的内存以适应不同的用例。内存类型的选择会显著影响应用程序的性能和行为。本节介绍了 Spring AI 提供的内置内存类型及其特性。\n消息窗口聊天内存 # MessageWindowChatMemory 将消息窗口维护到指定的最大大小。当消息数超过最大值时，将删除较旧的消息，同时保留系统消息。默认窗口大小为 20 封邮件。\nMessageWindowChatMemory memory = MessageWindowChatMemory.builder() .maxMessages(10) .build(); 这是 Spring AI 用于自动配置 ChatMemory bean 的默认消息类型。\n内存存储 # Spring AI 提供了 ChatMemoryRepository 抽象来存储聊天内存。本节描述了 Spring AI 提供的内置存储库及其使用方法，但如果需要，您也可以实现自己的存储库。\n内存存储库 # InMemoryChatMemoryRepository 使用 ConcurrentHashMap 将消息存储在内存中。 默认情况下，如果尚未配置其他存储库，则 Spring AI 会自动配置 InMemoryChatMemoryRepository 类型的 ChatMemoryRepository bean ，您可以直接在应用程序中使用该 bean。\n@Autowired ChatMemoryRepository chatMemoryRepository; 如果你希望手动创建 InMemoryChatMemoryRepository，可以按如下方式执行此作：\nChatMemoryRepository repository = new InMemoryChatMemoryRepository(); JdbcChatMemoryRepository 是一个内置实现，它使用 JDBC 将消息存储在关系数据库中。它支持多个开箱即用的数据库，适用于需要持久存储聊天内存的应用程序。 首先，将以下依赖项添加到您的项目中： Spring AI 为 JdbcChatMemoryRepository 提供了自动配置，你可以直接在应用程序中使用它。\n@Autowired JdbcChatMemoryRepository chatMemoryRepository; ChatMemory chatMemory = MessageWindowChatMemory.builder() .chatMemoryRepository(chatMemoryRepository) .maxMessages(10) .build(); 如果你更愿意手动创建 JdbcChatMemoryRepository，你可以通过提供 JdbcTemplate 实例和一个 JdbcChatMemoryRepositoryDialect ：\nChatMemoryRepository chatMemoryRepository = JdbcChatMemoryRepository.builder() .jdbcTemplate(jdbcTemplate) .dialect(new PostgresChatMemoryDialect()) .build(); ChatMemory chatMemory = MessageWindowChatMemory.builder() .chatMemoryRepository(chatMemoryRepository) .maxMessages(10) .build(); 支持的数据库和 Dialect 抽象 # Spring AI 通过 dialect 抽象支持多个关系数据库。以下数据库是开箱即用的：\nPostgreSQL 数据库 SQL 服务器 HSQLDB 数据库 使用 ```JdbcChatMemoryRepositoryDialect.from(DataSource)` 时，可以从 JDBC URL 中自动检测正确的方言。您可以通过实现该 JdbcChatMemoryRepositoryDialect`` 接口来扩展对其他数据库的支持。 配置属性 # Schema 初始化 # 自动配置将在启动时使用特定于供应商的数据库 SQL 脚本自动创建 SPRING_AI_CHAT_MEMORY 表。默认情况下，架构初始化仅针对嵌入式数据库（H2、HSQL、Derby 等）运行。 您可以使用以下 spring.ai.chat.memory.repository.jdbc.initialize-schema 属性控制架构初始化：\nspring.ai.chat.memory.repository.jdbc.initialize-schema=embedded # Only for embedded DBs (default) spring.ai.chat.memory.repository.jdbc.initialize-schema=always # Always initialize spring.ai.chat.memory.repository.jdbc.initialize-schema=never # Never initialize (useful with Flyway/Liquibase) 要覆盖架构脚本位置，请使用：\nspring.ai.chat.memory.repository.jdbc.schema=classpath:/custom/path/schema-mysql.sql 扩展方言 # 要添加对新数据库的支持，请实现 JdbcChatMemoryRepositoryDialect 该接口并提供用于选择、插入和删除消息的 SQL。然后，您可以将自定义 dialect 传递给存储库构建器。\nChatMemoryRepository chatMemoryRepository = JdbcChatMemoryRepository.builder() .jdbcTemplate(jdbcTemplate) .dialect(new MyCustomDbDialect()) .build(); CassandraChatMemoryRepository 使用 Apache Cassandra 来存储消息。它适用于需要持久存储聊天内存的应用程序，尤其是可用性、持久性、规模以及利用生存时间 （TTL） 功能时。 CassandraChatMemoryRepository 具有时间序列架构，用于记录所有过去的聊天窗口，这对于治理和审计很有价值。建议将 time to live 设置为某个值，例如三年。 要首先使用 CassandraChatMemoryRepository，请将依赖项添加到您的项目中： Spring AI 为 CassandraChatMemoryRepository 提供了自动配置，您可以直接在应用程序中使用。\n@Autowired CassandraChatMemoryRepository chatMemoryRepository; ChatMemory chatMemory = MessageWindowChatMemory.builder() .chatMemoryRepository(chatMemoryRepository) .maxMessages(10) .build(); 如果你更愿意手动创建 CassandraChatMemoryRepository，你可以通过提供一个 CassandraChatMemoryRepositoryConfig 实例来实现：\nChatMemoryRepository chatMemoryRepository = CassandraChatMemoryRepository .create(CassandraChatMemoryConfig.builder().withCqlSession(cqlSession)); ChatMemory chatMemory = MessageWindowChatMemory.builder() .chatMemoryRepository(chatMemoryRepository) .maxMessages(10) .build(); 配置属性 # Schema 初始化 # 自动配置将自动创建 ai_chat_memory 表。 您可以通过将 property spring.ai.chat.memory.repository.cassandra.initialize-schema 设置为 false 来禁用 schema 初始化。\nNeo4j ChatMemory 仓库 # Neo4jChatMemoryRepository 是一个内置实现，它使用 Neo4j 将聊天消息作为节点和关系存储在属性图数据库中。它适用于希望利用 Neo4j 的图形功能实现聊天内存持久性的应用程序。 首先，将以下依赖项添加到您的项目中： Spring AI 为 Neo4jChatMemoryRepository 提供了自动配置，您可以直接在应用程序中使用它。\n@Autowired Neo4jChatMemoryRepository chatMemoryRepository; ChatMemory chatMemory = MessageWindowChatMemory.builder() .chatMemoryRepository(chatMemoryRepository) .maxMessages(10) .build(); 如果你更愿意手动创建 Neo4jChatMemoryRepository，你可以通过提供 Neo4j Driver 实例来实现：\nChatMemoryRepository chatMemoryRepository = Neo4jChatMemoryRepository.builder() .driver(driver) .build(); ChatMemory chatMemory = MessageWindowChatMemory.builder() .chatMemoryRepository(chatMemoryRepository) .maxMessages(10) .build(); 配置属性 # 索引初始化 # Neo4j 存储库将自动确保为对话 ID 和消息索引创建索引，以优化性能。如果您使用自定义标签，则也会为这些标签创建索引。不需要架构初始化，但您应该确保您的应用程序可以访问您的 Neo4j 实例。\n聊天客户端中的内存 # 使用 ChatClient API 时，您可以提供 ChatMemory 实现来维护多个交互中的对话上下文。 Spring AI 提供了一些内置的 Advisors，你可以根据自己的需要使用它们来配置 ChatClient 的内存行为。\nMessageChatMemoryAdvisor 的 MemoryAdvisor 中。此顾问使用提供的 ChatMemory 实现管理对话内存。在每次交互时，它都会从内存中检索对话历史记录，并将其作为消息集合包含在提示中。 PromptChatMemoryAdvisor 的 PromptChatMemoryAdvisor 中。此顾问使用提供的 ChatMemory 实现管理对话内存。在每次交互时，它都会从内存中检索对话历史记录，并将其作为纯文本附加到系统提示符中。 VectorStoreChatMemoryAdvisor 的 MemoryAdvisor 中。此 advisor 使用提供的 VectorStore 实现管理对话内存。在每次交互时，它都会从向量存储中检索对话历史记录，并将其作为纯文本附加到系统消息中。 例如，如果要将 MessageWindowChatMemory 与 MessageChatMemoryAdvisor 一起使用，则可以按如下方式对其进行配置： ChatMemory chatMemory = MessageWindowChatMemory.builder().build(); ChatClient chatClient = ChatClient.builder(chatModel) .defaultAdvisors(MessageChatMemoryAdvisor.builder(chatMemory).build()) .build(); 当执行对 ChatClient 的调用时，内存将由 MessageChatMemoryAdvisor 自动管理。将根据指定的对话 ID 从内存中检索对话历史记录：\nString conversationId = \u0026#34;007\u0026#34;; chatClient.prompt() .user(\u0026#34;Do I have license to code?\u0026#34;) .advisors(a -\u0026gt; a.param(ChatMemory.CONVERSATION_ID, conversationId)) .call() .content(); 提示 ChatMemoryAdvisor # 自定义模板 # PromptChatMemoryAdvisor 使用默认模板通过检索到的对话内存来扩充系统消息。您可以通过 .promptTemplate（） 构建器方法提供自己的 PromptTemplate 对象来自定义此行为。 自定义 PromptTemplate 可以使用任何 TemplateRenderer 实现（默认情况下，它使用基于 [ StringTemplate]( https://www.stringtemplate.org/) 引擎的 StPromptTemplate）。重要的要求是模板必须包含以下两个占位符：\n用于接收原始系统消息的 Instructions 占位符。 一个 Memory 占位符，用于接收检索到的对话内存。 自定义模板 # VectorStoreChatMemoryAdvisor 使用默认模板通过检索到的对话内存来增强系统消息。您可以通过 .promptTemplate（） 构建器方法提供自己的 PromptTemplate 对象来自定义此行为。 自定义 PromptTemplate 可以使用任何 TemplateRenderer 实现（默认情况下，它使用基于 [ StringTemplate]( https://www.stringtemplate.org/) 引擎的 StPromptTemplate）。重要的要求是模板必须包含以下两个占位符：\n用于接收原始系统消息的 Instructions 占位符。 用于接收检索到的对话内存的 long_term_memory 占位符。 聊天模型中的内存 # 如果您直接使用 ChatModel 而不是 ChatClient，则可以显式管理内存：\n// Create a memory instance ChatMemory chatMemory = MessageWindowChatMemory.builder().build(); String conversationId = \u0026#34;007\u0026#34;; // First interaction UserMessage userMessage1 = new UserMessage(\u0026#34;My name is James Bond\u0026#34;); chatMemory.add(conversationId, userMessage1); ChatResponse response1 = chatModel.call(new Prompt(chatMemory.get(conversationId))); chatMemory.add(conversationId, response1.getResult().getOutput()); // Second interaction UserMessage userMessage2 = new UserMessage(\u0026#34;What is my name?\u0026#34;); chatMemory.add(conversationId, userMessage2); ChatResponse response2 = chatModel.call(new Prompt(chatMemory.get(conversationId))); chatMemory.add(conversationId, response2.getResult().getOutput()); // The response will contain \u0026#34;James Bond\u0026#34; "},{"id":60,"href":"/docs/models/chat-models/mistral-ai/","title":"Mistral AI 聊天","section":"聊天模型 API","content":" Mistral AI 聊天 # Spring AI 支持来自 Mistral AI 的各种 AI 语言模型。您可以与 Mistral AI 语言模型交互，并基于 Mistral 模型创建多语言对话助手。\n先决条件 # 您需要使用 Mistral AI 创建 API 才能访问 Mistral AI 语言模型。 在 [ Mistral AI 注册页面]( https://auth.mistral.ai/ui/registration)创建账户，并在 [ API 密钥页面]( https://console.mistral.ai/api-keys/)生成令牌。 Spring AI 项目定义了一个名为 spring.ai.mistralai.api-key 的配置属性，您应该将其设置为从 console.mistral.ai 获取的 API 密钥的值。 您可以在 application.properties 文件中设置此配置属性：\nspring.ai.mistralai.api-key=\u0026lt;your-mistralai-api-key\u0026gt; 为了在处理 API 密钥等敏感信息时增强安全性，您可以使用 Spring 表达式语言 （SpEL） 来引用自定义环境变量：\n# In application.yml spring: ai: mistralai: api-key: ${MISTRALAI_API_KEY} # In your environment or .env file export MISTRALAI_API_KEY=\u0026lt;your-mistralai-api-key\u0026gt; 您还可以在应用程序代码中以编程方式设置此配置：\n// Retrieve API key from a secure source or environment variable String apiKey = System.getenv(\u0026#34;MISTRALAI_API_KEY\u0026#34;); 添加存储库和 BOM # Spring AI 工件发布在 Maven Central 和 Spring Snapshot 存储库中。请参阅 [ Artifact Repositories](../../getting-started.html#artifact-repositories) 部分，将这些存储库添加到您的构建系统中。 为了帮助进行[ 依赖项管理](../../getting-started.html#dependency-management)，Spring AI 提供了一个 BOM（物料清单），以确保在整个项目中使用一致的 Spring AI 版本。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # Spring AI 为 Mistral AI Chat 客户端提供 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-mistral-ai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-mistral-ai\u0026#39; } 聊天属性 # 重试属性 # 前缀 spring.ai.retry 用作属性前缀，允许您为 Mistral AI 聊天模型配置重试机制。\n连接属性 # 前缀 spring.ai.mistralai 用作允许您连接到 OpenAI 的属性前缀。\n配置属性 # 前缀 spring.ai.mistralai.chat 是属性前缀，它允许你为 Mistral AI 配置聊天模型实现。\n运行时选项 # [ MistralAiChatOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-mistral-ai/src/main/java/org/springframework/ai/mistralai/[MistralAiChatOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-mistral-ai/src/main/java/org/springframework/ai/mistralai/MistralAiChatOptions.java)) 提供模型配置，例如要使用的模型、温度、频率损失等。 启动时，可以使用 MistralAiChatModel(api, options) constructor 或 spring.ai.mistralai.chat.options.* properties 配置默认选项。 在运行时，您可以通过向 Prompt 调用添加新的特定于请求的选项来覆盖默认选项。例如，要覆盖特定请求的默认型号和温度：\nChatResponse response = chatModel.call( new Prompt( \u0026#34;Generate the names of 5 famous pirates.\u0026#34;, MistralAiChatOptions.builder() .model(MistralAiApi.ChatModel.LARGE.getValue()) .temperature(0.5) .build() )); 函数调用 # 您可以使用 MistralAiChatModel 注册自定义 Java 函数，并让 Mistral AI 模型智能地选择输出包含参数的 JSON 对象，以调用一个或多个已注册的函数。这是一种将 LLM 功能与外部工具和 API 连接起来的强大技术。阅读有关[ 工具调用](../tools.html)的更多信息。\n模 态 # 多模态是指模型同时理解和处理来自各种来源的信息（包括文本、图像、音频和其他数据格式）的能力。Mistral AI 支持文本和视觉模式。\n视觉 # 提供[ 视觉]( https://docs.mistral.ai/capabilities/vision/)多模式支持的 Mistral AI 模型包括 pixtral-large-latest。有关更多信息，请参阅[ 视觉]( https://docs.mistral.ai/capabilities/vision/)指南。 Mistral AI [ 用户消息 API]( https://docs.mistral.ai/api/#tag/chat/operation/chat_completion_v1_chat_completions_post) 可以将 base64 编码的图像列表或图像 URL 与消息合并。Spring AI 的 [ Message]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-client-chat/src/main/java/org/springframework/ai/chat/messages/[Message](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-client-chat/src/main/java/org/springframework/ai/chat/messages/Message.java).java) 接口通过引入 [ Media]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/model/[Media](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/model/Media.java).java) 类型来促进多模态 AI 模型。此类型包含有关消息中媒体附件的数据和详细信息，将 Spring org.springframework.util.MimeType 和 a org.springframework.core.io.Resource 用于原始媒体数据。 下面是从 MistralAiChatModelIT.java 摘录的代码示例，说明了用户文本与图像的融合。\nvar imageResource = new ClassPathResource(\u0026#34;/multimodal.test.png\u0026#34;); var userMessage = new UserMessage(\u0026#34;Explain what do you see on this picture?\u0026#34;, new Media(MimeTypeUtils.IMAGE_PNG, this.imageResource)); ChatResponse response = chatModel.call(new Prompt(this.userMessage, ChatOptions.builder().model(MistralAiApi.ChatModel.PIXTRAL_LARGE.getValue()).build())); 或等效的图片 URL：\nvar userMessage = new UserMessage(\u0026#34;Explain what do you see on this picture?\u0026#34;, new Media(MimeTypeUtils.IMAGE_PNG, URI.create(\u0026#34;https://docs.spring.io/spring-ai/reference/_images/multimodal.test.png\u0026#34;))); ChatResponse response = chatModel.call(new Prompt(this.userMessage, ChatOptions.builder().model(MistralAiApi.ChatModel.PIXTRAL_LARGE.getValue()).build())); 该示例显示了一个将 multimodal.test.png 图像作为输入的模型： 以及文本消息 “Explain what do you see on this picture？”，并生成如下响应：\nOpenAI API 兼容性 # Mistral 与 OpenAI API 兼容，您可以使用 [ Spring AI OpenAI](openai-chat.html) 客户端与 Mistrial 交谈。为此，您需要将 OpenAI 基本 URL 配置为 Mistral AI 平台： spring.ai.openai.chat.base-url=https://api.mistral.ai ，然后选择一个 Mistral 模型： spring.ai.openai.chat.options.model=mistral-small-latest 并设置 Mistral AI API 密钥： spring.ai.openai.chat.api-key=\u0026lt;YOUR MISTRAL API KEY 。 查看 [ MistralWithOpenAiChatModelIT.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/chat/proxy/[MistralWithOpenAiChatModelIT.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/chat/proxy/MistralWithOpenAiChatModelIT.java)) 测试，了解在 Spring AI OpenAI 上使用 Mistral 的示例。\n样品控制器（自动配置） # [ 创建一个新]( https://start.spring.io/)的 Spring Boot 项目，并将 添加到您的 spring-ai-starter-model-mistral-ai pom（或 gradle）依赖项中。 在 src/main/resources 目录下添加一个 application.properties 文件，用于启用和配置 Mistral AI 聊天模型：\nspring.ai.mistralai.api-key=YOUR_API_KEY spring.ai.mistralai.chat.options.model=mistral-small spring.ai.mistralai.chat.options.temperature=0.7 这将创建一个 MistralAiChatModel 实现，您可以将其注入到您的类中。下面是一个简单的 @RestController 类的示例，该类使用 chat 模型生成文本。\n@RestController public class ChatController { private final MistralAiChatModel chatModel; @Autowired public ChatController(MistralAiChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map\u0026lt;String,String\u0026gt; generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, this.chatModel.call(message)); } @GetMapping(\u0026#34;/ai/generateStream\u0026#34;) public Flux\u0026lt;ChatResponse\u0026gt; generateStream(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { var prompt = new Prompt(new UserMessage(message)); return this.chatModel.stream(prompt); } } 手动配置 # MistralAiChatModel 实现 ChatModel 和 StreamingChatModel，并使用[ 低级 MistralAiApi 客户端](#low-level-api)连接到 Mistral AI 服务。 将 spring-ai-mistral-ai 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-mistral-ai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-mistral-ai\u0026#39; } 接下来，创建一个 MistralAiChatModel 并将其用于文本生成：\nvar mistralAiApi = new MistralAiApi(System.getenv(\u0026#34;MISTRAL_AI_API_KEY\u0026#34;)); var chatModel = new MistralAiChatModel(this.mistralAiApi, MistralAiChatOptions.builder() .model(MistralAiApi.ChatModel.LARGE.getValue()) .temperature(0.4) .maxTokens(200) .build()); ChatResponse response = this.chatModel.call( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); // Or with streaming responses Flux\u0026lt;ChatResponse\u0026gt; response = this.chatModel.stream( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); MistralAiChatOptions 提供聊天请求的配置信息。MistralAiChatOptions.Builder 是一个流畅的选项构建器。\n低级 MistralAiApi 客户端 # [ MistralAiApi]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-mistral-ai/src/main/java/org/springframework/ai/mistralai/api/[MistralAiApi](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-mistral-ai/src/main/java/org/springframework/ai/mistralai/api/MistralAiApi.java).java) 提供的是 [ Mistral AI API]( https://docs.mistral.ai/api/) 的轻量级 Java 客户端。 下面是一个简单的代码片段，演示如何以编程方式使用 API：\nMistralAiApi mistralAiApi = new MistralAiApi(System.getenv(\u0026#34;MISTRAL_AI_API_KEY\u0026#34;)); ChatCompletionMessage chatCompletionMessage = new ChatCompletionMessage(\u0026#34;Hello world\u0026#34;, Role.USER); // Sync request ResponseEntity\u0026lt;ChatCompletion\u0026gt; response = this.mistralAiApi.chatCompletionEntity( new ChatCompletionRequest(List.of(this.chatCompletionMessage), MistralAiApi.ChatModel.LARGE.getValue(), 0.8, false)); // Streaming request Flux\u0026lt;ChatCompletionChunk\u0026gt; streamResponse = this.mistralAiApi.chatCompletionStream( new ChatCompletionRequest(List.of(this.chatCompletionMessage), MistralAiApi.ChatModel.LARGE.getValue(), 0.8, true)); 有关详细信息，请遵循 [ MistralAiApi.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-mistral-ai/src/main/java/org/springframework/ai/mistralai/api/[MistralAiApi.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-mistral-ai/src/main/java/org/springframework/ai/mistralai/api/MistralAiApi.java)) 的 JavaDoc。\nMistralAiApi 示例 # MistralAiApiIT.java 测试提供了一些有关如何使用轻量级库的一般示例。 PaymentStatusFunctionCallingIT.java 测试演示如何使用低级 API 调用工具函数。基于 Mistral AI 函数调用教程。 "},{"id":61,"href":"/docs/vector-databases/mongodb-atlas/","title":"None","section":"矢量数据库","content":" None # 本节将引导您将 MongoDB Atlas 设置为矢量存储以用于 Spring AI。\n什么是 MongoDB Atlas？ # [ MongoDB Atlas]( https://www.mongodb.com/products/platform/atlas-database) 是 MongoDB 提供的完全托管式云数据库，可在 AWS、Azure 和 GCP 中使用。Atlas 支持对 MongoDB 文档数据进行原生矢量搜索和全文搜索。 [ MongoDB Atlas Vector Search]( https://www.mongodb.com/products/platform/atlas-vector-search) 允许您将嵌入存储在 MongoDB 文档中，创建向量搜索索引，并使用近似最近邻算法（分层可导航小世界）执行 KNN 搜索。您可以在 MongoDB 聚合阶段中使用 $vectorSearch 聚合运算符对向量嵌入执行搜索。\n先决条件 # 运行 MongoDB 版本 6.0.11、7.0.2 或更高版本的 Atlas 集群。要开始使用 MongoDB Atlas，您可以按照此处的说明进行作。确保您的 IP 地址包含在 Atlas 项目的访问列表中 。 启用了 Vector Search 的正在运行的 MongoDB Atlas 实例 配置了向量搜索索引的集合 具有 id （string）、content （string）、元数据 （document） 和嵌入 （vector） 字段的集合架构 索引和集合作的适当访问权限 自动配置 # Spring AI 为 MongoDB Atlas Vector Store 提供 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-mongodb-atlas\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或复制到您的 Gradle build.gradle 构建文件：\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-vector-store-mongodb-atlas\u0026#39; } 矢量存储实现可以为您初始化必要的架构，但您必须通过在 application.properties 文件中进行设置 spring.ai.vectorstore.mongodb.initialize-schema=true 来选择加入。或者，您也可以选择退出初始化并使用 MongoDB Atlas UI、Atlas 管理 API 或 Atlas CLI 手动创建索引，如果索引需要高级映射或其他配置，这可能很有用。 请查看 vector store 的[ 配置参数](#mongodbvector-properties)列表，了解默认值和配置选项。 此外，您将需要一个已配置的 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) bean。请参阅 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) 部分以了解更多信息。 现在，您可以在应用程序中将 MongoDBAtlasVectorStore 作为矢量存储自动连接：\n@Autowired VectorStore vectorStore; // ... List\u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); // Add the documents to MongoDB Atlas vectorStore.add(documents); // Retrieve documents similar to a query List\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch(SearchRequest.builder().query(\u0026#34;Spring\u0026#34;).topK(5).build()); 配置属性 # 要连接到 MongoDB Atlas 并使用 MongoDBAtlasVectorStore，您需要提供实例的访问详细信息。可以通过 Spring Boot 的 application.yml 提供一个简单的配置：\nspring: data: mongodb: uri: \u0026lt;mongodb atlas connection string\u0026gt; database: \u0026lt;database name\u0026gt; ai: vectorstore: mongodb: initialize-schema: true collection-name: custom_vector_store index-name: custom_vector_index path-name: custom_embedding metadata-fields-to-filter: author,year 以 开头的属性 spring.ai.vectorstore.mongodb.* 用于配置 MongoDBAtlasVectorStore：\n手动配置 # 您可以手动配置 MongoDB Atlas 向量存储，而不是使用 Spring Boot 自动配置。为此，您需要将 spring-ai-mongodb-atlas-store 添加到您的项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-mongodb-atlas-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或复制到您的 Gradle build.gradle 构建文件：\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-mongodb-atlas-store\u0026#39; } 创建一个 MongoTemplate Bean：\n@Bean public MongoTemplate mongoTemplate() { return new MongoTemplate(MongoClients.create(\u0026#34;\u0026lt;mongodb atlas connection string\u0026gt;\u0026#34;), \u0026#34;\u0026lt;database name\u0026gt;\u0026#34;); } 然后使用构建器模式创建 MongoDBAtlasVectorStore Bean：\n@Bean public VectorStore vectorStore(MongoTemplate mongoTemplate, EmbeddingModel embeddingModel) { return MongoDBAtlasVectorStore.builder(mongoTemplate, embeddingModel) .collectionName(\u0026#34;custom_vector_store\u0026#34;) // Optional: defaults to \u0026#34;vector_store\u0026#34; .vectorIndexName(\u0026#34;custom_vector_index\u0026#34;) // Optional: defaults to \u0026#34;vector_index\u0026#34; .pathName(\u0026#34;custom_embedding\u0026#34;) // Optional: defaults to \u0026#34;embedding\u0026#34; .numCandidates(500) // Optional: defaults to 200 .metadataFieldsToFilter(List.of(\u0026#34;author\u0026#34;, \u0026#34;year\u0026#34;)) // Optional: defaults to empty list .initializeSchema(true) // Optional: defaults to false .batchingStrategy(new TokenCountBatchingStrategy()) // Optional: defaults to TokenCountBatchingStrategy .build(); } // This can be any EmbeddingModel implementation @Bean public EmbeddingModel embeddingModel() { return new OpenAiEmbeddingModel(new OpenAiApi(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;))); } 元数据筛选 # 您也可以在 MongoDB Atlas 中使用通用的可移植[ 元数据过滤器](../vectordbs.html#metadata-filters) 。 例如，您可以使用文本表达式语言：\nvectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(5) .similarityThreshold(0.7) .filterExpression(\u0026#34;author in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; article_type == \u0026#39;blog\u0026#39;\u0026#34;).build()); 或使用 Filter.Expression DSL 以编程方式：\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(5) .similarityThreshold(0.7) .filterExpression(b.and( b.in(\u0026#34;author\u0026#34;, \u0026#34;john\u0026#34;, \u0026#34;jill\u0026#34;), b.eq(\u0026#34;article_type\u0026#34;, \u0026#34;blog\u0026#34;)).build()).build()); 例如，此可移植筛选条件表达式：\nauthor in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; article_type == \u0026#39;blog\u0026#39; 转换为专有的 MongoDB Atlas 过滤器格式：\n{ \u0026#34;$and\u0026#34;: [ { \u0026#34;$or\u0026#34;: [ { \u0026#34;metadata.author\u0026#34;: \u0026#34;john\u0026#34; }, { \u0026#34;metadata.author\u0026#34;: \u0026#34;jill\u0026#34; } ] }, { \u0026#34;metadata.article_type\u0026#34;: \u0026#34;blog\u0026#34; } ] } 教程和代码示例 # 要开始使用 Spring AI 和 MongoDB：\n请参阅 Spring AI 集成的入门指南 。 有关演示使用 Spring AI 和 MongoDB 进行检索增强生成 （RAG） 的综合代码示例，请参阅此详细教程 。 访问 Native Client # MongoDB Atlas Vector Store 实现通过 getNativeClient（） 方法提供对底层原生 MongoDB 客户端 （MongoClient） 的访问：\nMongoDBAtlasVectorStore vectorStore = context.getBean(MongoDBAtlasVectorStore.class); Optional\u0026lt;MongoClient\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { MongoClient client = nativeClient.get(); // Use the native client for MongoDB-specific operations } 本机客户端允许您访问特定于 MongoDB 的功能和作，这些功能和作可能无法通过 VectorStore 接口公开。\n"},{"id":62,"href":"/docs/models/embedding-models/qianfan/","title":"QianFan 嵌入","section":"嵌入模型 API","content":" QianFan 嵌入 # 此功能已移至 Spring AI Community 存储库。 请访问 [ github.com/spring-ai-community/qianfan](https:// github.com/spring-ai-community/qianfan) 获取最新版本。\n"},{"id":63,"href":"/docs/tool-calling/","title":"工具调用","section":"Docs","content":" 工具调用 # 工具调用 （也称为函数调用 ）是 AI 应用程序中的一种常见模式，允许模型与一组 API 或工具进行交互，从而增强其功能。 工具主要用于： 尽管我们通常将工具调用称为模型功能，但实际上由客户端应用程序提供工具调用逻辑。模型只能请求工具调用并提供输入参数，而应用程序负责从输入参数执行工具调用并返回结果。该模型永远无法访问作为工具提供的任何 API，这是一个关键的安全考虑因素。 Spring AI 提供了方便的 API 来定义工具、解决来自模型的工具调用请求以及执行工具调用。以下部分概述了 Spring AI 中的工具调用功能。\n快速开始 # 让我们看看如何在 Spring AI 中开始使用工具调用。我们将实现两个简单的工具：一个用于信息检索，一个用于采取行动。信息检索工具将用于获取用户所在时区的当前日期和时间。作工具将用于设置指定时间的闹钟。\n信息检索 # AI 模型无法访问实时信息。模型无法回答任何假设了解信息（如当前日期或天气预报）的问题。但是，我们可以提供一个可以检索此信息的工具，并让模型在需要访问实时信息时调用此工具。 让我们在 DateTimeTools 类中实现一个工具来获取用户时区的当前日期和时间。该工具将不接受任何参数。Spring Framework 的 LocaleContextHolder 可以提供用户的时区。该工具将被定义为带有 @Tool 注释的方法。为了帮助模型了解是否以及何时调用此工具，我们将提供这些工具的作用的详细说明。\nimport java.time.LocalDateTime; import org.springframework.ai.tool.annotation.Tool; import org.springframework.context.i18n.LocaleContextHolder; class DateTimeTools { @Tool(description = \u0026#34;Get the current date and time in the user\u0026#39;s timezone\u0026#34;) String getCurrentDateTime() { return LocalDateTime.now().atZone(LocaleContextHolder.getTimeZone().toZoneId()).toString(); } } 接下来，让我们使该工具可用于模型。在此示例中，我们将使用 ChatClient 与模型交互。我们将通过 tools（） 方法传递 DateTimeTools 的实例，从而为模型提供该工具。当模型需要知道当前日期和时间时，它将请求调用该工具。在内部，ChatClient 将调用该工具并将结果返回给模型，然后模型将使用工具调用结果生成对原始问题的最终响应。\nChatModel chatModel = ... String response = ChatClient.create(chatModel) .prompt(\u0026#34;What day is tomorrow?\u0026#34;) .tools(new DateTimeTools()) .call() .content(); System.out.println(response); 输出将如下所示：\nTomorrow is 2015-10-21. 您可以再次尝试询问相同的问题。这一次，不要向模型提供工具。输出将如下所示：\nI am an AI and do not have access to real-time information. Please provide the current date so I can accurately determine what day tomorrow will be. 如果没有该工具，模型就不知道如何回答问题，因为它无法确定当前日期和时间。\n采取行动 # AI 模型可用于生成实现某些目标的计划。例如，模型可以生成预订丹麦旅行的计划。但是，该模型无法执行该计划。这就是工具的用武之地：它们可用于执行模型生成的计划。 在前面的示例中，我们使用了一个工具来确定当前日期和时间。在此示例中，我们将定义第二个工具，用于在特定时间设置闹钟。目标是从现在开始设置 10 分钟的闹钟，因此我们需要为模型提供这两种工具来完成此任务。 我们将新工具添加到与以前相同的 DateTimeTools 类中。新工具将采用单个参数，即 ISO-8601 格式的时间。然后，该工具将向控制台打印一条消息，指示已为给定时间设置警报。与以前一样，该工具被定义为一个带有 @Tool 注释的方法，我们还使用它来提供详细的描述，以帮助模型了解何时以及如何使用该工具。\nimport java.time.LocalDateTime; import java.time.format.DateTimeFormatter; import org.springframework.ai.tool.annotation.Tool; import org.springframework.context.i18n.LocaleContextHolder; class DateTimeTools { @Tool(description = \u0026#34;Get the current date and time in the user\u0026#39;s timezone\u0026#34;) String getCurrentDateTime() { return LocalDateTime.now().atZone(LocaleContextHolder.getTimeZone().toZoneId()).toString(); } @Tool(description = \u0026#34;Set a user alarm for the given time, provided in ISO-8601 format\u0026#34;) void setAlarm(String time) { LocalDateTime alarmTime = LocalDateTime.parse(time, DateTimeFormatter.ISO_DATE_TIME); System.out.println(\u0026#34;Alarm set for \u0026#34; + alarmTime); } } 接下来，让我们使这两个工具都可用于模型。我们将使用 ChatClient 与模型进行交互。我们将通过 tools（） 方法传递 DateTimeTools 的实例，从而为模型提供工具。当我们要求在从 10 分钟后设置闹钟时，模型首先需要知道当前日期和时间。然后，它将使用当前日期和时间来计算闹钟时间。最后，它将使用闹钟工具设置闹钟。在内部，ChatClient 将处理来自模型的任何工具调用请求，并将任何工具调用执行结果发送回给它，以便模型可以生成最终响应。\nChatModel chatModel = ... String response = ChatClient.create(chatModel) .prompt(\u0026#34;Can you set an alarm 10 minutes from now?\u0026#34;) .tools(new DateTimeTools()) .call() .content(); System.out.println(response); 在应用程序日志中，您可以检查警报是否已在正确的时间设置。\n概述 # Spring AI 通过一组灵活的抽象支持工具调用，这些抽象允许您以一致的方式定义、解析和执行工具。本节概述了 Spring AI 中工具调用的主要概念和组件。 工具是工具调用的构建块，它们由 ToolCallback 接口建模。Spring AI 为从方法和函数指定 ToolCallback 提供了内置支持，但您始终可以定义自己的 ToolCallback 实现以支持更多用例。 ChatModel 实现以透明方式将工具调用请求分派给相应的 ToolCallback 实现，并将工具调用结果发送回模型，最终生成最终响应。他们使用 ToolCallingManager 接口执行此作，该接口负责管理工具执行生命周期。 ChatClient 和 ChatModel 都接受 ToolCallback 对象列表，以使工具可用于模型和最终将执行它们的 ToolCallingManager。 除了直接传递 ToolCallback 对象外，您还可以传递工具名称列表，这些名称将使用 ToolCallbackResolver 接口动态解析。 以下部分将详细介绍所有这些概念和 API，包括如何自定义和扩展它们以支持更多用例。\n方法即工具 # Spring AI 以两种方式为从方法指定工具（即 ToolCallback）提供了内置支持：\n以声明方式使用 @Tool 注解 以编程方式，使用低级 MethodToolCallback 实现。 声明性规范：@Tool # 您可以通过使用 @Tool 注释方法将方法转换为工具。\nclass DateTimeTools { @Tool(description = \u0026#34;Get the current date and time in the user\u0026#39;s timezone\u0026#34;) String getCurrentDateTime() { return LocalDateTime.now().atZone(LocaleContextHolder.getTimeZone().toZoneId()).toString(); } } @Tool 注释允许您提供有关该工具的关键信息：\nname：工具的名称。如果未提供，则将使用方法名称。AI 模型在调用工具时使用此名称来识别工具。因此，不允许在同一个类中有两个同名的工具。该名称在模型可用于特定聊天请求的所有工具中必须是唯一的。 description：工具的描述，模型可以使用它来了解何时以及如何调用工具。如果未提供，则方法名称将用作工具描述。但是，强烈建议提供详细的描述，因为这对于模型了解工具的用途以及如何使用它至关重要。未能提供良好的描述可能会导致模型在应该使用该工具时未使用该工具或错误地使用该工具。 returnDirect：工具结果应直接返回给客户端还是传递回模型。有关更多详细信息，请参阅直接返回 。 resultConverter：用于将工具调用的结果转换为 String 对象以发送回 AI 模型的 ToolCallResultConverter 实现。有关更多详细信息，请参阅 Result Conversion。 该方法可以是 static 或 instance，并且可以具有任何可见性（public、protected、package-private 或 private）。包含该方法的类可以是顶级类或嵌套类，并且还可以具有任何可见性（只要它位于您计划实例化的位置可访问）。 您可以为大多数类型（基元、POJO、枚举、列表、数组、映射等）的方法定义任意数量的参数（包括无参数）。同样，该方法可以返回大多数类型，包括 void。如果该方法返回一个值，则返回类型必须是可序列化类型，因为结果将被序列化并发送回模型。 Spring AI 将自动为 @Tool 注释方法的 input 参数生成 JSON 模式。模型使用架构来了解如何调用工具和准备工具请求。@ToolParam 注释可用于提供有关输入参数的其他信息，例如描述或参数是必需的还是可选的。默认情况下，所有输入参数都被视为必需参数。 import java.time.LocalDateTime; import java.time.format.DateTimeFormatter; import org.springframework.ai.tool.annotation.Tool; import org.springframework.ai.tool.annotation.ToolParam; class DateTimeTools { @Tool(description = \u0026#34;Set a user alarm for the given time\u0026#34;) void setAlarm(@ToolParam(description = \u0026#34;Time in ISO-8601 format\u0026#34;) String time) { LocalDateTime alarmTime = LocalDateTime.parse(time, DateTimeFormatter.ISO_DATE_TIME); System.out.println(\u0026#34;Alarm set for \u0026#34; + alarmTime); } } @ToolParam 注记允许您提供有关工具参数的关键信息：\ndescription：参数的描述，模型可以使用该参数来更好地了解如何使用它。例如，参数应采用什么格式、允许哪些值等。 required：参数是 required 还是 optional。默认情况下，所有参数都被视为必需参数。 如果参数被注释为 @Nullable，则除非使用 @ToolParam 注释明确标记为必需，否则该参数将被视为可选参数。 除了 @ToolParam 注释之外，您还可以使用 Swagger 的 @Schema 注释或 Jackson 的 @JsonProperty。有关更多详细信息，请参阅 [ JSON 架构](#_json_schema) 。 向 ChatClient 添加工具 # 使用声明性规范方法时，可以在调用 ChatClient 时将 tool 类实例传递给 tools（） 方法。此类工具仅适用于它们被添加到的特定聊天请求。\nChatClient.create(chatModel) .prompt(\u0026#34;What day is tomorrow?\u0026#34;) .tools(new DateTimeTools()) .call() .content(); 在后台，ChatClient 将从工具类实例中的每个 @Tool 注释方法生成一个 ToolCallback，并将它们传递给模型。如果您希望自己生成 ToolCallback，则可以使用 ToolCallbacks 实用程序类。\nToolCallback[] dateTimeTools = ToolCallbacks.from(new DateTimeTools()); 向 ChatClient 添加默认工具 # 使用声明性规范方法时，您可以通过将工具类实例传递给 defaultTools（） 方法，将默认工具添加到 ChatClient.Builder。如果同时提供了 default 和 runtime 工具，则 runtime 工具将完全覆盖 default 工具。\nChatModel chatModel = ... ChatClient chatClient = ChatClient.builder(chatModel) .defaultTools(new DateTimeTools()) .build(); 向 ChatModel 添加工具 # 使用声明性规范方法时，您可以将工具类实例传递给用于调用 ChatModel 的 ToolCallingChatOptions 的 toolCallbacks（） 方法。此类工具仅适用于它们被添加到的特定聊天请求。\nChatModel chatModel = ... ToolCallback[] dateTimeTools = ToolCallbacks.from(new DateTimeTools()); ChatOptions chatOptions = ToolCallingChatOptions.builder() .toolCallbacks(dateTimeTools) .build(); Prompt prompt = new Prompt(\u0026#34;What day is tomorrow?\u0026#34;, chatOptions); chatModel.call(prompt); 向 ChatModel 添加默认工具 # 使用声明式规范方法时，您可以通过将工具类实例传递给用于创建 ChatModel 的 ToolCallingChatOptions 实例的 toolCallbacks（） 方法，在构造时将默认工具添加到 ChatModel。如果同时提供了 default 和 runtime 工具，则 runtime 工具将完全覆盖 default 工具。\nToolCallback[] dateTimeTools = ToolCallbacks.from(new DateTimeTools()); ChatModel chatModel = OllamaChatModel.builder() .ollamaApi(OllamaApi.builder().build()) .defaultOptions(ToolCallingChatOptions.builder() .toolCallbacks(dateTimeTools) .build()) .build(); 编程规范：MethodToolCallback # 您可以通过以编程方式构建 MethodToolCallback 将方法转换为工具。\nclass DateTimeTools { String getCurrentDateTime() { return LocalDateTime.now().atZone(LocaleContextHolder.getTimeZone().toZoneId()).toString(); } } ```MethodToolCallback.Builder` 允许您构建 MethodToolCallback`` 实例并提供有关该工具的关键信息：\ntoolDefinition：定义工具名称、描述和输入架构的 ToolDefinition 实例。您可以使用 ToolDefinition.Builder 类构建它。必需。 toolMetadata：ToolMetadata 实例，用于定义其他设置，例如是否应将结果直接返回给客户端，以及要使用的结果转换器。您可以使用 ToolMetadata.Builder 类构建它。 toolMethod：表示工具方法的 Method 实例。必填。 toolObject：包含工具方法的对象实例。如果 method 是 static，则可以省略该参数。 toolCallResultConverter：用于将工具调用的结果转换为 String 对象以发送回 AI 模型的 ToolCallResultConverter 实例。如果未提供，将使用默认转换器 （ DefaultToolCallResultConverter ）。 ```ToolDefinition.Builder` 允许您构建 ToolDefinition`` 实例并定义工具名称、描述和输入架构： name：工具的名称。如果未提供，则将使用方法名称。AI 模型在调用工具时使用此名称来识别工具。因此，不允许在同一个类中有两个同名的工具。该名称在模型可用于特定聊天请求的所有工具中必须是唯一的。 description：工具的描述，模型可以使用它来了解何时以及如何调用工具。如果未提供，则方法名称将用作工具描述。但是，强烈建议提供详细的描述，因为这对于模型了解工具的用途以及如何使用它至关重要。未能提供良好的描述可能会导致模型在应该使用该工具时未使用该工具或错误地使用该工具。 inputSchema：工具输入参数的 JSON 架构。如果未提供，将根据方法参数自动生成 schema。您可以使用 @ToolParam 注释提供有关输入参数的其他信息，例如描述或参数是必需的还是可选的。默认情况下，所有输入参数都被视为必需参数。有关更多详细信息，请参阅 JSON 架构 。 ```ToolMetadata.Builder` 允许您构建 ToolMetadata`` 实例并为该工具定义其他设置： returnDirect：工具结果应直接返回给客户端还是传递回模型。有关更多详细信息，请参阅直接返回 。 Method method = ReflectionUtils.findMethod(DateTimeTools.class, \u0026#34;getCurrentDateTime\u0026#34;); ToolCallback toolCallback = MethodToolCallback.builder() .toolDefinition(ToolDefinition.builder(method) .description(\u0026#34;Get the current date and time in the user\u0026#39;s timezone\u0026#34;) .build()) .toolMethod(method) .toolObject(new DateTimeTools()) .build(); 该方法可以是 static 或 instance，并且可以具有任何可见性（public、protected、package-private 或 private）。包含该方法的类可以是顶级类或嵌套类，并且还可以具有任何可见性（只要它位于您计划实例化的位置可访问）。 您可以为大多数类型（基元、POJO、枚举、列表、数组、映射等）的方法定义任意数量的参数（包括无参数）。同样，该方法可以返回大多数类型，包括 void。如果该方法返回一个值，则返回类型必须是可序列化类型，因为结果将被序列化并发送回模型。 如果方法是静态的，则可以省略 toolObject（） 方法，因为它不是必需的。\nclass DateTimeTools { static String getCurrentDateTime() { return LocalDateTime.now().atZone(LocaleContextHolder.getTimeZone().toZoneId()).toString(); } } Method method = ReflectionUtils.findMethod(DateTimeTools.class, \u0026#34;getCurrentDateTime\u0026#34;); ToolCallback toolCallback = MethodToolCallback.builder() .toolDefinition(ToolDefinition.builder(method) .description(\u0026#34;Get the current date and time in the user\u0026#39;s timezone\u0026#34;) .build()) .toolMethod(method) .build(); Spring AI 将自动为方法的输入参数生成 JSON 模式。模型使用架构来了解如何调用工具和准备工具请求。@ToolParam 注释可用于提供有关输入参数的其他信息，例如描述或参数是必需的还是可选的。默认情况下，所有输入参数都被视为必需参数。\nimport java.time.LocalDateTime; import java.time.format.DateTimeFormatter; import org.springframework.ai.tool.annotation.ToolParam; class DateTimeTools { void setAlarm(@ToolParam(description = \u0026#34;Time in ISO-8601 format\u0026#34;) String time) { LocalDateTime alarmTime = LocalDateTime.parse(time, DateTimeFormatter.ISO_DATE_TIME); System.out.println(\u0026#34;Alarm set for \u0026#34; + alarmTime); } } @ToolParam 注记允许您提供有关工具参数的关键信息：\ndescription：参数的描述，模型可以使用该参数来更好地了解如何使用它。例如，参数应采用什么格式、允许哪些值等。 required：参数是 required 还是 optional。默认情况下，所有参数都被视为必需参数。 如果参数被注释为 @Nullable，则除非使用 @ToolParam 注释明确标记为必需，否则该参数将被视为可选参数。 除了 @ToolParam 注释之外，您还可以使用 Swagger 的 @Schema 注释或 Jackson 的 @JsonProperty。有关更多详细信息，请参阅 [ JSON 架构](#_json_schema) 。 向 ChatClient 和 ChatModel 添加工具 # 使用编程规范方法时，您可以将 MethodToolCallback 实例传递给 ChatClient 的 tools（） 方法。该工具仅适用于它所添加到的特定聊天请求。\nToolCallback toolCallback = ... ChatClient.create(chatModel) .prompt(\u0026#34;What day is tomorrow?\u0026#34;) .tools(toolCallback) .call() .content(); 向 ChatClient 添加默认工具 # 使用编程规范方法时，您可以通过将 MethodToolCallback 实例传递给 defaultTools（） 方法，将默认工具添加到 ChatClient.Builder。如果同时提供了 default 和 runtime 工具，则 runtime 工具将完全覆盖 default 工具。\nChatModel chatModel = ... ToolCallback toolCallback = ... ChatClient chatClient = ChatClient.builder(chatModel) .defaultTools(toolCallback) .build(); 向 ChatModel 添加工具 # 使用编程规范方法时，可以将 MethodToolCallback 实例传递给用于调用 ChatModel 的 ToolCallingChatOptions 的 toolCallbacks（） 方法。该工具仅适用于它所添加到的特定聊天请求。\nChatModel chatModel = ... ToolCallback toolCallback = ... ChatOptions chatOptions = ToolCallingChatOptions.builder() .toolCallbacks(toolCallback) .build(): Prompt prompt = new Prompt(\u0026#34;What day is tomorrow?\u0026#34;, chatOptions); chatModel.call(prompt); 向 ChatModel 添加默认工具 # 使用编程规范方法时，您可以通过将 MethodToolCallback 实例传递给用于创建 ChatModel 的 ToolCallingChatOptions 实例的 toolCallbacks（） 方法，在构造时向 ChatModel 添加默认工具。如果同时提供了 default 和 runtime 工具，则 runtime 工具将完全覆盖 default 工具。\nToolCallback toolCallback = ... ChatModel chatModel = OllamaChatModel.builder() .ollamaApi(OllamaApi.builder().build()) .defaultOptions(ToolCallingChatOptions.builder() .toolCallbacks(toolCallback) .build()) .build(); 方法工具限制 # 目前不支持将以下类型用作工具的方法的参数或返回类型：\n异步类型（例如 CompletableFuture、Future） 反应类型（例如 Flow、Mono、Flux） 功能类型（例如 Function、Supplier、Consumer）。 使用基于函数的工具规范方法支持函数类型。有关更多详细信息，请参阅[ 函数即工具](#_functions_as_tools) 。 作为工具的功能 # Spring AI 为从函数指定工具提供了内置支持，既可以使用低级 FunctionToolCallback 实现以编程方式指定工具，也可以动态地作为运行时解析 @Bean。\n编程规范：FunctionToolCallback # 您可以通过以编程方式构建 FunctionToolCallback，将函数类型（Function、Supplier、Consumer 或 BiFunction）转换为工具。\npublic class WeatherService implements Function\u0026lt;WeatherRequest, WeatherResponse\u0026gt; { public WeatherResponse apply(WeatherRequest request) { return new WeatherResponse(30.0, Unit.C); } } public enum Unit { C, F } public record WeatherRequest(String location, Unit unit) {} public record WeatherResponse(double temp, Unit unit) {} ```FunctionToolCallback.Builder` 允许您构建 FunctionToolCallback`` 实例并提供有关该工具的关键信息：\nname：工具的名称。AI 模型在调用工具时使用此名称来识别工具。因此，不允许在同一上下文中有两个同名的工具。该名称在模型可用于特定聊天请求的所有工具中必须是唯一的。必填。 toolFunction：表示工具方法（Function、Supplier、Consumer 或 BiFunction）的函数对象。必填。 description：工具的描述，模型可以使用它来了解何时以及如何调用工具。如果未提供，则方法名称将用作工具描述。但是，强烈建议提供详细的描述，因为这对于模型了解工具的用途以及如何使用它至关重要。未能提供良好的描述可能会导致模型在应该使用该工具时未使用该工具或错误地使用该工具。 inputType：函数 input 的类型。必填。 inputSchema：工具输入参数的 JSON 架构。如果未提供，将根据 inputType 自动生成架构。您可以使用 @ToolParam 注释提供有关输入参数的其他信息，例如描述或参数是必需的还是可选的。默认情况下，所有输入参数都被视为必需参数。有关更多详细信息，请参阅 JSON 架构 。 toolMetadata：ToolMetadata 实例，用于定义其他设置，例如是否应将结果直接返回给客户端，以及要使用的结果转换器。您可以使用 ToolMetadata.Builder 类构建它。 toolCallResultConverter：用于将工具调用的结果转换为 String 对象以发送回 AI 模型的 ToolCallResultConverter 实例。如果未提供，将使用默认转换器 （ DefaultToolCallResultConverter ）。 ```ToolMetadata.Builder` 允许您构建 ToolMetadata`` 实例并为该工具定义其他设置： returnDirect：工具结果应直接返回给客户端还是传递回模型。有关更多详细信息，请参阅直接返回 。 ToolCallback toolCallback = FunctionToolCallback .builder(\u0026#34;currentWeather\u0026#34;, new WeatherService()) .description(\u0026#34;Get the weather in location\u0026#34;) .inputType(WeatherRequest.class) .build(); 函数输入和输出可以是 Void 或 POJO。输入和输出 POJO 必须是可序列化的，因为结果将被序列化并发送回模型。函数以及输入和输出类型必须是 public。\n向 ChatClient 添加工具 # 使用编程规范方法时，您可以将 FunctionToolCallback 实例传递给 ChatClient 的 tools（） 方法。该工具仅适用于它所添加到的特定聊天请求。\nToolCallback toolCallback = ... ChatClient.create(chatModel) .prompt(\u0026#34;What\u0026#39;s the weather like in Copenhagen?\u0026#34;) .tools(toolCallback) .call() .content(); 向 ChatClient 添加默认工具 # 使用编程规范方法时，您可以通过将 FunctionToolCallback 实例传递给 defaultTools（） 方法，将默认工具添加到 ChatClient.Builder。如果同时提供了 default 和 runtime 工具，则 runtime 工具将完全覆盖 default 工具。\nChatModel chatModel = ... ToolCallback toolCallback = ... ChatClient chatClient = ChatClient.builder(chatModel) .defaultTools(toolCallback) .build(); 向 ChatModel 添加工具 # 使用编程规范方法时，您可以将 FunctionToolCallback 实例传递给 ToolCallingChatOptions 的 toolCallbacks（） 方法。该工具仅适用于它所添加到的特定聊天请求。\nChatModel chatModel = ... ToolCallback toolCallback = ... ChatOptions chatOptions = ToolCallingChatOptions.builder() .toolCallbacks(toolCallback) .build(): Prompt prompt = new Prompt(\u0026#34;What\u0026#39;s the weather like in Copenhagen?\u0026#34;, chatOptions); chatModel.call(prompt); 向 ChatModel 添加默认工具 # 使用编程规范方法时，您可以通过将 FunctionToolCallback 实例传递给用于创建 ChatModel 的 ToolCallingChatOptions 实例的 toolCallbacks（） 方法，在构造时将默认工具添加到 ChatModel。如果同时提供了 default 和 runtime 工具，则 runtime 工具将完全覆盖 default 工具。\nToolCallback toolCallback = ... ChatModel chatModel = OllamaChatModel.builder() .ollamaApi(OllamaApi.builder().build()) .defaultOptions(ToolCallingChatOptions.builder() .toolCallbacks(toolCallback) .build()) .build(); 动态规格：@Bean # 您可以将工具定义为 Spring bean，并让 Spring AI 在运行时使用 ToolCallbackResolver 接口（通过实现） SpringBeanToolCallbackResolver 动态解析它们，而不是以编程方式指定工具。此选项使您可以使用任何 Function、Supplier、Consumer 或 BiFunction bean 作为工具。bean 名称将用作工具名称，Spring Framework 中的 @Description 注释可用于提供工具的描述，模型使用它来了解何时以及如何调用该工具。如果您未提供描述，则方法名称将用作工具描述。但是，强烈建议提供详细的描述，因为这对于模型了解工具的用途以及如何使用它至关重要。未能提供良好的描述可能会导致模型在应该使用该工具时未使用该工具或错误地使用该工具。\n@Configuration(proxyBeanMethods = false) class WeatherTools { WeatherService weatherService = new WeatherService(); @Bean @Description(\u0026#34;Get the weather in location\u0026#34;) Function\u0026lt;WeatherRequest, WeatherResponse\u0026gt; currentWeather() { return weatherService; } } 将自动生成工具输入参数的 JSON 方案。您可以使用 @ToolParam 注释提供有关输入参数的其他信息，例如描述或参数是必需的还是可选的。默认情况下，所有输入参数都被视为必需参数。有关更多详细信息，请参阅 [ JSON 架构](#_json_schema) 。\nrecord WeatherRequest(@ToolParam(description = \u0026#34;The name of a city or a country\u0026#34;) String location, Unit unit) {} 这种工具规范方法的缺点是不能保证类型安全，因为工具解析是在运行时完成的。要缓解这种情况，您可以使用 @Bean 注释显式指定工具名称，并将值存储在常量中，以便您可以在聊天请求中使用它，而不是对工具名称进行硬编码。\n@Configuration(proxyBeanMethods = false) class WeatherTools { public static final String CURRENT_WEATHER_TOOL = \u0026#34;currentWeather\u0026#34;; @Bean(CURRENT_WEATHER_TOOL) @Description(\u0026#34;Get the weather in location\u0026#34;) Function\u0026lt;WeatherRequest, WeatherResponse\u0026gt; currentWeather() { ... } } 向 ChatClient 添加工具 # 当使用动态规范方法时，你可以将工具名称（即函数 Bean 名称）传递给 ChatClient 的 tools（） 方法。该工具仅适用于它所添加到的特定聊天请求。\nChatClient.create(chatModel) .prompt(\u0026#34;What\u0026#39;s the weather like in Copenhagen?\u0026#34;) .tools(\u0026#34;currentWeather\u0026#34;) .call() .content(); 向 ChatClient 添加默认工具 # 使用动态规范方法时，您可以通过将工具名称传递给 defaultTools（） 方法，将默认工具添加到 ChatClient.Builder。如果同时提供了 default 和 runtime 工具，则 runtime 工具将完全覆盖 default 工具。\nChatModel chatModel = ... ChatClient chatClient = ChatClient.builder(chatModel) .defaultTools(\u0026#34;currentWeather\u0026#34;) .build(); 向 ChatModel 添加工具 # 使用动态规范方法时，可以将工具名称传递给用于调用 ChatModel 的 ToolCallingChatOptions 的 toolNames（） 方法。该工具仅适用于它所添加到的特定聊天请求。\nChatModel chatModel = ... ChatOptions chatOptions = ToolCallingChatOptions.builder() .toolNames(\u0026#34;currentWeather\u0026#34;) .build(): Prompt prompt = new Prompt(\u0026#34;What\u0026#39;s the weather like in Copenhagen?\u0026#34;, chatOptions); chatModel.call(prompt); 向 ChatModel 添加默认工具 # 使用动态规范方法时，您可以通过将工具名称传递给用于创建 ChatModel 的 ToolCallingChatOptions 实例的 toolNames（） 方法，在构造时将默认工具添加到 ChatModel。如果同时提供了 default 和 runtime 工具，则 runtime 工具将完全覆盖 default 工具。\nChatModel chatModel = OllamaChatModel.builder() .ollamaApi(OllamaApi.builder().build()) .defaultOptions(ToolCallingChatOptions.builder() .toolNames(\u0026#34;currentWeather\u0026#34;) .build()) .build(); 函数工具限制 # 当前不支持将以下类型用作用作工具的函数的输入或输出类型：\n基元类型 集合类型（例如 List、Map、Array、Set） 异步类型（例如 CompletableFuture、Future） 反应类型（例如 Flow、Mono、Flux）。 使用基于方法的工具规范方法支持基元类型和集合。有关更多详细信息，请参阅[ 方法作为工具](#_methods_as_tools) 。 工具规格 # 在 Spring AI 中，工具是通过 ToolCallback 接口建模的。在前面的部分中，我们已经看到了如何使用 Spring AI 提供的内置支持从方法和函数定义工具（参见[ 作为工具和](#_methods_as_tools)[ 作为工具的函数](#_functions_as_tools) ）。本节将深入探讨工具规范以及如何自定义和扩展它以支持更多用例。\n工具回调 # ToolCallback 接口提供了一种定义可由 AI 模型调用的工具的方法，包括定义和执行逻辑。当您想从头开始定义一个工具时，它是要实现的主界面。例如，您可以从 MCP Client（使用模型上下文协议）或 ChatClient（以构建模块化代理应用程序）定义 ToolCallback。 该接口提供如下方法：\npublic interface ToolCallback { /** * Definition used by the AI model to determine when and how to call the tool. */ ToolDefinition getToolDefinition(); /** * Metadata providing additional information on how to handle the tool. */ ToolMetadata getToolMetadata(); /** * Execute tool with the given input and return the result to send back to the AI model. */ String call(String toolInput); /** * Execute tool with the given input and context, and return the result to send back to the AI model. */ String call(String toolInput, ToolContext tooContext); } Spring AI 为工具方法（MethodToolCallback）和工具函数（FunctionToolCallback）提供了内置实现。\n工具定义 # ToolDefinition 接口为 AI 模型提供了解工具可用性所需的信息，包括工具名称、描述和输入架构。每个 ToolCallback 实现都必须提供一个 ToolDefinition 实例来定义工具。 该接口提供如下方法：\npublic interface ToolDefinition { /** * The tool name. Unique within the tool set provided to a model. */ String name(); /** * The tool description, used by the AI model to determine what the tool does. */ String description(); /** * The schema of the parameters used to call the tool. */ String inputSchema(); } ```ToolDefinition.Builder` 允许您使用默认实现 （DefaultToolDefinition） 构建 ToolDefinition`` 实例。\nToolDefinition toolDefinition = ToolDefinition.builder() .name(\u0026#34;currentWeather\u0026#34;) .description(\u0026#34;Get the weather in location\u0026#34;) .inputSchema(\u0026#34;\u0026#34;\u0026#34; { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;unit\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;enum\u0026#34;: [\u0026#34;C\u0026#34;, \u0026#34;F\u0026#34;] } }, \u0026#34;required\u0026#34;: [\u0026#34;location\u0026#34;, \u0026#34;unit\u0026#34;] } \u0026#34;\u0026#34;\u0026#34;) .build(); 方法工具定义 # 从方法构建工具时，将为您自动生成 ToolDefinition。如果您更喜欢自己生成 ToolDefinition，则可以使用这个方便的构建器。\nMethod method = ReflectionUtils.findMethod(DateTimeTools.class, \u0026#34;getCurrentDateTime\u0026#34;); ToolDefinition toolDefinition = ToolDefinition.from(method); 从方法生成的 ToolDefinition 包括作为工具名称的方法名称、作为工具描述的方法名称以及方法输入参数的 JSON 架构。如果方法使用 @Tool 进行注释，则工具名称和描述将从注释 （如果已设置） 中获取。 如果您希望显式提供部分或全部属性，则可以使用 ```ToolDefinition.Builder` 构建自定义 ToolDefinition`` 实例。\nMethod method = ReflectionUtils.findMethod(DateTimeTools.class, \u0026#34;getCurrentDateTime\u0026#34;); ToolDefinition toolDefinition = ToolDefinition.builder(method) .name(\u0026#34;currentDateTime\u0026#34;) .description(\u0026#34;Get the current date and time in the user\u0026#39;s timezone\u0026#34;) .inputSchema(JsonSchemaGenerator.generateForMethodInput(method)) .build(); 功能工具定义 # 从函数构建工具时，将为您自动生成 ToolDefinition。当您使用 ```FunctionToolCallback.Builder` 构建 FunctionToolCallback`` 实例时，您可以提供将用于生成 ToolDefinition 的工具名称、描述和输入架构。有关更多详细信息，请参阅[ 函数即工具](#_functions_as_tools) 。\nJSON 架构 # 向 AI 模型提供工具时，模型需要知道用于调用该工具的输入类型的架构。架构用于了解如何调用工具和准备工具请求。Spring AI 提供了内置支持，用于通过 JsonSchemaGenerator 类为工具生成 input 类型的 JSON Schema。该架构作为 ToolDefinition 的一部分提供。 JsonSchemaGenerator 类在后台用于为方法或函数的输入参数生成 JSON 架构，使用[ 方法作为工具和](#_methods_as_tools)[ 函数作为工具](#_functions_as_tools)中描述的任何策略。JSON 架构生成逻辑支持一系列注释，您可以在方法和函数的输入参数上使用这些注释来自定义生成的架构。 本节介绍在为工具的输入参数生成 JSON 架构时可以自定义的两个主要选项：description 和 required status。\n描述 # 除了提供工具本身的描述外，您还可以提供工具输入参数的描述。描述可用于提供有关输入参数的关键信息，例如参数应采用的格式、允许的值等。这有助于模型了解输入架构以及如何使用它。Spring AI 提供了内置支持，以使用以下注释之一为 Importing 参数生成描述：\n@ToolParam(description = \u0026ldquo;…​\u0026rdquo;) 来自 Spring AI @JsonClassDescription(description = \u0026ldquo;…​\u0026rdquo;) 与 Jackson 相比 @JsonPropertyDescription(description = \u0026ldquo;…​\u0026rdquo;) 与 Jackson 相比 @Schema（description = “\u0026hellip;“）。 此方法适用于方法和函数，您可以递归地将其用于嵌套类型。 import java.time.LocalDateTime; import java.time.format.DateTimeFormatter; import org.springframework.ai.tool.annotation.Tool; import org.springframework.ai.tool.annotation.ToolParam; import org.springframework.context.i18n.LocaleContextHolder; class DateTimeTools { @Tool(description = \u0026#34;Set a user alarm for the given time\u0026#34;) void setAlarm(@ToolParam(description = \u0026#34;Time in ISO-8601 format\u0026#34;) String time) { LocalDateTime alarmTime = LocalDateTime.parse(time, DateTimeFormatter.ISO_DATE_TIME); System.out.println(\u0026#34;Alarm set for \u0026#34; + alarmTime); } } 必需/可选 # 默认情况下，每个输入参数都被视为必需参数，这会强制 AI 模型在调用工具时为其提供值。但是，您可以使用以下注释之一将 input 参数设置为可选，按以下优先顺序：\n来自 Spring AI 的 @ToolParam（必需 = false） @JsonProperty(required = false) 与 Jackson 相比 @Schema（必需 = false） 来自 Swagger @Nullable Spring Framework 中。 此方法适用于方法和函数，您可以递归地将其用于嵌套类型。 class CustomerTools { @Tool(description = \u0026#34;Update customer information\u0026#34;) void updateCustomerInfo(Long id, String name, @ToolParam(required = false) String email) { System.out.println(\u0026#34;Updated info for customer with id: \u0026#34; + id); } } 结果转换 # 工具调用的结果使用 ToolCallResultConverter 进行序列化，然后发送回 AI 模型。ToolCallResultConverter 接口提供了一种将工具调用的结果转换为 String 对象的方法。 该接口提供如下方法：\n@FunctionalInterface public interface ToolCallResultConverter { /** * Given an Object returned by a tool, convert it to a String compatible with the * given class type. */ String convert(@Nullable Object result, @Nullable Type returnType); } 结果必须是可序列化类型。默认情况下，结果使用 Jackson （） 序列化为 JSON `DefaultToolCallResultConverter``` ，但您可以通过提供自己的 ToolCallResultConverter实现来自定义序列化过程。 Spring AI 在方法和函数工具中都依赖于ToolCallResultConverter``。\n方法工具调用结果转换 # 当使用声明性方法从方法构建工具时，您可以通过设置 @Tool 注解的 resultConverter（） 属性来提供用于该工具的自定义 ToolCallResultConverter。\nclass CustomerTools { @Tool(description = \u0026#34;Retrieve customer information\u0026#34;, resultConverter = CustomToolCallResultConverter.class) Customer getCustomerInfo(Long id) { return customerRepository.findById(id); } } 如果使用编程方法，则可以通过设置 MethodToolCallback.Builder 的 resultConverter（） 属性来提供用于该工具的自定义 ToolCallResultConverter。 有关更多详细信息，请参阅[ 方法作为工具](#_methods_as_tools) 。\n函数工具调用结果转换 # 使用编程方法从函数构建工具时，您可以通过设置 FunctionToolCallback.Builder 的 resultConverter（） 属性来提供用于该工具的自定义 ToolCallResultConverter。 有关更多详细信息，请参阅[ 函数即工具](#_functions_as_tools) 。\n工具上下文 # Spring AI 支持通过 ToolContext API 将额外的上下文信息传递给工具。此功能允许您提供额外的用户提供的数据，这些数据可与 AI 模型传递的工具参数一起在工具执行中使用。 class CustomerTools { @Tool(description = \u0026#34;Retrieve customer information\u0026#34;) Customer getCustomerInfo(Long id, ToolContext toolContext) { return customerRepository.findById(id, toolContext.get(\u0026#34;tenantId\u0026#34;)); } } ToolContext 使用用户在调用 ChatClient 时提供的数据进行填充。\nChatModel chatModel = ... String response = ChatClient.create(chatModel) .prompt(\u0026#34;Tell me more about the customer with ID 42\u0026#34;) .tools(new CustomerTools()) .toolContext(Map.of(\u0026#34;tenantId\u0026#34;, \u0026#34;acme\u0026#34;)) .call() .content(); System.out.println(response); 同样，您可以在直接调用 ChatModel 时定义工具上下文数据。\nChatModel chatModel = ... ToolCallback[] customerTools = ToolCallbacks.from(new CustomerTools()); ChatOptions chatOptions = ToolCallingChatOptions.builder() .toolCallbacks(customerTools) .toolContext(Map.of(\u0026#34;tenantId\u0026#34;, \u0026#34;acme\u0026#34;)) .build(); Prompt prompt = new Prompt(\u0026#34;Tell me more about the customer with ID 42\u0026#34;, chatOptions); chatModel.call(prompt); 如果在默认选项和运行时选项中都设置了 toolContext 选项，则生成的 ToolContext 将是两者的合并，其中运行时选项优先于默认选项。\n直接返回 # 默认情况下，工具调用的结果将作为响应发送回模型。然后，模型可以使用结果继续对话。 在某些情况下，您宁愿将结果直接返回给调用方，而不是将其发送回模型。例如，如果您构建了一个依赖于 RAG 工具的代理，您可能希望将结果直接返回给调用方，而不是将其发送回模型进行不必要的后处理。或者，也许您有一些工具可以结束代理的推理循环。 每个 ToolCallback 实现都可以定义是应将工具调用的结果直接返回给调用方还是发送回模型。默认情况下，结果将发送回模型。但是，您可以按工具更改此行为。 负责管理工具执行生命周期的 ToolCallingManager 负责处理与工具关联的 returnDirect 属性。如果该属性设置为 true，则工具调用的结果将直接返回给调用方。否则，结果将发送回模型。 class CustomerTools { @Tool(description = \u0026#34;Retrieve customer information\u0026#34;, returnDirect = true) Customer getCustomerInfo(Long id) { return customerRepository.findById(id); } } ToolMetadata toolMetadata = ToolMetadata.builder() .returnDirect(true) .build(); ToolMetadata toolMetadata = ToolMetadata.builder() .returnDirect(true) .build(); public interface ToolCallingManager { /** * Resolve the tool definitions from the model\u0026#39;s tool calling options. */ List\u0026lt;ToolDefinition\u0026gt; resolveToolDefinitions(ToolCallingChatOptions chatOptions); /** * Execute the tool calls requested by the model. */ ToolExecutionResult executeToolCalls(Prompt prompt, ChatResponse chatResponse); } @Bean ToolCallingManager toolCallingManager() { return ToolCallingManager.builder().build(); } public class DefaultToolExecutionEligibilityPredicate implements ToolExecutionEligibilityPredicate { @Override public boolean test(ChatOptions promptOptions, ChatResponse chatResponse) { return ToolCallingChatOptions.isInternalToolExecutionEnabled(promptOptions) \u0026amp;\u0026amp; chatResponse != null \u0026amp;\u0026amp; chatResponse.hasToolCalls(); } } ChatModel chatModel = ... ToolCallingManager toolCallingManager = ToolCallingManager.builder().build(); ChatOptions chatOptions = ToolCallingChatOptions.builder() .toolCallbacks(new CustomerTools()) .internalToolExecutionEnabled(false) .build(); Prompt prompt = new Prompt(\u0026#34;Tell me more about the customer with ID 42\u0026#34;, chatOptions); ChatResponse chatResponse = chatModel.call(prompt); while (chatResponse.hasToolCalls()) { ToolExecutionResult toolExecutionResult = toolCallingManager.executeToolCalls(prompt, chatResponse); prompt = new Prompt(toolExecutionResult.conversationHistory(), chatOptions); chatResponse = chatModel.call(prompt); } System.out.println(chatResponse.getResult().getOutput().getText()); ToolCallingManager toolCallingManager = DefaultToolCallingManager.builder().build(); ChatMemory chatMemory = MessageWindowChatMemory.builder().build(); String conversationId = UUID.randomUUID().toString(); ChatOptions chatOptions = ToolCallingChatOptions.builder() .toolCallbacks(ToolCallbacks.from(new MathTools())) .internalToolExecutionEnabled(false) .build(); Prompt prompt = new Prompt( List.of(new SystemMessage(\u0026#34;You are a helpful assistant.\u0026#34;), new UserMessage(\u0026#34;What is 6 * 8?\u0026#34;)), chatOptions); chatMemory.add(conversationId, prompt.getInstructions()); Prompt promptWithMemory = new Prompt(chatMemory.get(conversationId), chatOptions); ChatResponse chatResponse = chatModel.call(promptWithMemory); chatMemory.add(conversationId, chatResponse.getResult().getOutput()); while (chatResponse.hasToolCalls()) { ToolExecutionResult toolExecutionResult = toolCallingManager.executeToolCalls(promptWithMemory, chatResponse); chatMemory.add(conversationId, toolExecutionResult.conversationHistory() .get(toolExecutionResult.conversationHistory().size() - 1)); promptWithMemory = new Prompt(chatMemory.get(conversationId), chatOptions); chatResponse = chatModel.call(promptWithMemory); chatMemory.add(conversationId, chatResponse.getResult().getOutput()); } UserMessage newUserMessage = new UserMessage(\u0026#34;What did I ask you earlier?\u0026#34;); chatMemory.add(conversationId, newUserMessage); ChatResponse newResponse = chatModel.call(new Prompt(chatMemory.get(conversationId))); @FunctionalInterface public interface ToolExecutionExceptionProcessor { /** * Convert an exception thrown by a tool to a String that can be sent back to the AI * model or throw an exception to be handled by the caller. */ String process(ToolExecutionException exception); } @Bean ToolExecutionExceptionProcessor toolExecutionExceptionProcessor() { return new DefaultToolExecutionExceptionProcessor(true); } public interface ToolCallbackResolver { /** * Resolve the {@link ToolCallback} for the given tool name. */ @Nullable ToolCallback resolve(String toolName); } @Bean ToolCallbackResolver toolCallbackResolver(List\u0026lt;FunctionCallback\u0026gt; toolCallbacks) { StaticToolCallbackResolver staticToolCallbackResolver = new StaticToolCallbackResolver(toolCallbacks); return new DelegatingToolCallbackResolver(List.of(staticToolCallbackResolver)); } ToolCallingManager 在内部使用 ToolCallingManager 在运行时动态解析工具，同时支持[ 框架控制的工具执行](#_framework_controlled_tool_execution)和[ 用户控制的工具执行](#_user_controlled_tool_execution) 。\n可观察性 # 工具调用包括对 spring.ai.tool 观察的可观察性支持，这些观察可以测量完成时间并传播跟踪信息。请参阅[ 工具调用可观测性](../observability/index.html#_tool_calling) 。 或者，Spring AI 可以将工具调用参数和结果导出为 span 属性，出于敏感原因，默认情况下处于禁用状态。详细信息： [ 工具调用参数和结果数据](../observability/index.html#_tool_call_arguments_and_result_data) 。\n伐木 # 工具调用功能的所有主要作都记录在 DEBUG 级别。您可以通过将 org.springframework.ai 包的日志级别设置为 DEBUG 来启用日志记录。\n"},{"id":64,"href":"/docs/models/chat-models/minimax/","title":"MiniMax 聊天","section":"聊天模型 API","content":" MiniMax 聊天 # Spring AI 支持 MiniMax 的各种 AI 语言模型。您可以与 MiniMax 语言模型交互，并基于 MiniMax 模型创建多语言对话助手。\n先决条件 # 您需要使用 MiniMax 创建 API 才能访问 MiniMax 语言模型。 在 [ MiniMax 注册页面]( https://www.minimaxi.com/login)创建一个帐户，并在 [ API 密钥页面上]( https://www.minimaxi.com/user-center/basic-information/interface-key)生成令牌。 Spring AI 项目定义了一个名为 spring.ai.minimax.api-key 的配置属性，您应该将其设置为从“API 密钥”页面获取的 API 密钥的值。 您可以在 application.properties 文件中设置此配置属性：\nspring.ai.minimax.api-key=\u0026lt;your-minimax-api-key\u0026gt; 为了在处理 API 密钥等敏感信息时增强安全性，您可以使用 Spring 表达式语言 （SpEL） 来引用环境变量：\n# In application.yml spring: ai: minimax: api-key: ${MINIMAX_API_KEY} # In your environment or .env file export MINIMAX_API_KEY=\u0026lt;your-minimax-api-key\u0026gt; 您还可以在应用程序代码中以编程方式设置此配置：\n// Retrieve API key from a secure source or environment variable String apiKey = System.getenv(\u0026#34;MINIMAX_API_KEY\u0026#34;); 添加存储库和 BOM # Spring AI 工件发布在 Maven Central 和 Spring Snapshot 存储库中。请参阅 [ Artifact Repositories](../../getting-started.html#artifact-repositories) 部分，将这些存储库添加到您的构建系统中。 为了帮助进行[ 依赖项管理](../../getting-started.html#dependency-management)，Spring AI 提供了一个 BOM（物料清单），以确保在整个项目中使用一致的 Spring AI 版本。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # Spring AI 为 MiniMax Chat 客户端提供 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-minimax\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-minimax\u0026#39; } 聊天属性 # 重试属性 # 前缀 spring.ai.retry 用作属性前缀，它允许你为 MiniMax 聊天模型配置重试机制。\n连接属性 # 前缀 spring.ai.minimax 用作属性前缀，可让您连接到 MiniMax。\n配置属性 # 前缀 spring.ai.minimax.chat 是属性前缀，它允许你为 MiniMax 配置聊天模型实现。\n运行时选项 # [ MiniMaxChatOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-minimax/src/main/java/org/springframework/ai/minimax/[MiniMaxChatOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-minimax/src/main/java/org/springframework/ai/minimax/MiniMaxChatOptions.java)) 提供模型配置，例如要使用的模型、温度、频率损失等。 启动时，可以使用 MiniMaxChatModel(api, options) constructor 或 spring.ai.minimax.chat.options.* properties 配置默认选项。 在运行时，您可以通过向 Prompt 调用添加新的、特定于请求的选项来覆盖默认选项。例如，要覆盖特定请求的默认模型和温度：\nChatResponse response = chatModel.call( new Prompt( \u0026#34;Generate the names of 5 famous pirates.\u0026#34;, MiniMaxChatOptions.builder() .model(MiniMaxApi.ChatModel.ABAB_6_5_S_Chat.getValue()) .temperature(0.5) .build() )); 样品控制器 # [ 创建一个新]( https://start.spring.io/)的 Spring Boot 项目，并将 添加到您的 spring-ai-starter-model-minimax pom（或 gradle）依赖项中。 在 src/main/resources 目录下添加一个 application.properties 文件，以启用和配置 MiniMax 聊天模型：\nspring.ai.minimax.api-key=YOUR_API_KEY spring.ai.minimax.chat.options.model=abab6.5g-chat spring.ai.minimax.chat.options.temperature=0.7 这将创建一个 MiniMaxChatModel 实现，您可以将其注入到您的类中。下面是一个简单的 @Controller 类示例，该类使用 chat 模型生成文本。\n@RestController public class ChatController { private final MiniMaxChatModel chatModel; @Autowired public ChatController(MiniMaxChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, this.chatModel.call(message)); } @GetMapping(\u0026#34;/ai/generateStream\u0026#34;) public Flux\u0026lt;ChatResponse\u0026gt; generateStream(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { var prompt = new Prompt(new UserMessage(message)); return this.chatModel.stream(prompt); } } 手动配置 # MiniMaxChatModel 实现 ChatModel 和 StreamingChatModel，并使用[ 低级 MiniMaxApi 客户端](#low-level-api)连接到 MiniMax 服务。 将 spring-ai-minimax 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-minimax\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-minimax\u0026#39; } 接下来，创建一个 MiniMaxChatModel 并将其用于文本生成：\nvar miniMaxApi = new MiniMaxApi(System.getenv(\u0026#34;MINIMAX_API_KEY\u0026#34;)); var chatModel = new MiniMaxChatModel(this.miniMaxApi, MiniMaxChatOptions.builder() .model(MiniMaxApi.ChatModel.ABAB_6_5_S_Chat.getValue()) .temperature(0.4) .maxTokens(200) .build()); ChatResponse response = this.chatModel.call( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); // Or with streaming responses Flux\u0026lt;ChatResponse\u0026gt; streamResponse = this.chatModel.stream( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); MiniMaxChatOptions 提供聊天请求的配置信息。MiniMaxChatOptions.Builder 是 Fluent 选项生成器。\n低级 MiniMaxApi 客户端 # [ MiniMaxApi]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-minimax/src/main/java/org/springframework/ai/minimax/api/[MiniMaxApi](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-minimax/src/main/java/org/springframework/ai/minimax/api/MiniMaxApi.java).java) 为 [ MiniMax API]( https://www.minimaxi.com/document/guides/chat-model/V2) 提供了轻量级的 Java 客户端。 以下是如何以编程方式使用 api 的简单代码段：\nMiniMaxApi miniMaxApi = new MiniMaxApi(System.getenv(\u0026#34;MINIMAX_API_KEY\u0026#34;)); ChatCompletionMessage chatCompletionMessage = new ChatCompletionMessage(\u0026#34;Hello world\u0026#34;, Role.USER); // Sync request ResponseEntity\u0026lt;ChatCompletion\u0026gt; response = this.miniMaxApi.chatCompletionEntity( new ChatCompletionRequest(List.of(this.chatCompletionMessage), MiniMaxApi.ChatModel.ABAB_6_5_S_Chat.getValue(), 0.7f, false)); // Streaming request Flux\u0026lt;ChatCompletionChunk\u0026gt; streamResponse = this.miniMaxApi.chatCompletionStream( new ChatCompletionRequest(List.of(this.chatCompletionMessage), MiniMaxApi.ChatModel.ABAB_6_5_S_Chat.getValue(), 0.7f, true)); 有关详细信息，请遵循 [ MiniMaxApi.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-minimax/src/main/java/org/springframework/ai/minimax/api/[MiniMaxApi.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-minimax/src/main/java/org/springframework/ai/minimax/api/MiniMaxApi.java)) 的 JavaDoc。\nWebSearch 聊天 # MiniMax 型号支持 Web 搜索功能。Web 搜索功能允许您在 Web 上搜索信息并在聊天响应中返回结果。 关于网页搜索，请关注 [ MiniMax ChatCompletion]( https://platform.minimaxi.com/document/ChatCompletion%20v2) 了解更多信息。 以下是如何使用 Web 搜索的简单代码段：\nUserMessage userMessage = new UserMessage( \u0026#34;How many gold medals has the United States won in total at the 2024 Olympics?\u0026#34;); List\u0026lt;Message\u0026gt; messages = new ArrayList\u0026lt;\u0026gt;(List.of(this.userMessage)); List\u0026lt;MiniMaxApi.FunctionTool\u0026gt; functionTool = List.of(MiniMaxApi.FunctionTool.webSearchFunctionTool()); MiniMaxChatOptions options = MiniMaxChatOptions.builder() .model(MiniMaxApi.ChatModel.ABAB_6_5_S_Chat.value) .tools(this.functionTool) .build(); // Sync request ChatResponse response = chatModel.call(new Prompt(this.messages, this.options)); // Streaming request Flux\u0026lt;ChatResponse\u0026gt; streamResponse = chatModel.stream(new Prompt(this.messages, this.options)); MiniMaxApi 示例 # MiniMaxApiIT.java 测试提供了一些如何使用轻量级库的一般示例。 MiniMaxApiToolFunctionCallIT.java 测试显示了如何使用低级 API 调用工具函数。\u0026gt; "},{"id":65,"href":"/docs/models/embedding-models/zhipu-ai/","title":"ZhiPuAI 嵌入","section":"嵌入模型 API","content":" ZhiPuAI 嵌入 # Spring AI 支持 ZhiPuAI 的文本嵌入模型。智普 AI 的文本嵌入向量可以测量文本字符串的相关性。嵌入是浮点数的向量（列表）。两个向量之间的距离衡量它们的相关性。小距离表示高相关性，大距离表示低相关性。\n先决条件 # 您需要使用 ZhiPuAI 创建 API 才能访问 ZhiPu AI 语言模型。 在 [ 智普 AI 注册页面]( https://open.bigmodel.cn/login)创建账号，并在 [ API Keys 页面生成 Token]( https://open.bigmodel.cn/usercenter/apikeys)。 Spring AI 项目定义了一个名为 spring.ai.zhipu.api-key 的配置属性，您应该将其设置为从 API 密钥页面获取的 API 密钥的值。 您可以在 application.properties 文件中设置此配置属性：\nspring.ai.zhipu.api-key=\u0026lt;your-zhipu-api-key\u0026gt; 为了在处理 API 密钥等敏感信息时增强安全性，您可以使用 Spring 表达式语言 （SpEL） 来引用环境变量：\n# In application.yml spring: ai: zhipu: api-key: ${ZHIPU_API_KEY} # In your environment or .env file export ZHIPU_API_KEY=\u0026lt;your-zhipu-api-key\u0026gt; 您还可以在应用程序代码中以编程方式设置此配置：\n// Retrieve API key from a secure source or environment variable String apiKey = System.getenv(\u0026#34;ZHIPU_API_KEY\u0026#34;); 添加存储库和 BOM # Spring AI 工件发布在 Maven Central 和 Spring Snapshot 存储库中。请参阅 [ Artifact Repositories](../../getting-started.html#artifact-repositories) 部分，将这些存储库添加到您的构建系统中。 为了帮助进行[ 依赖项管理](../../getting-started.html#dependency-management)，Spring AI 提供了一个 BOM（物料清单），以确保在整个项目中使用一致的 Spring AI 版本。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # Spring AI 为 Azure ZhiPuAI 嵌入模型提供 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-zhipuai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-zhipuai\u0026#39; } 嵌入属性 # 重试属性 # 前缀 spring.ai.retry 用作属性前缀，允许您为 ZhiPuAI Embedding 模型配置重试机制。\n连接属性 # 前缀 spring.ai.zhipuai 用作允许您连接到 ZhiPuAI 的属性前缀。\n配置属性 # 前缀 spring.ai.zhipuai.embedding 是配置 ZhiPuAI 的 EmbeddingModel 实现的属性前缀。\n运行时选项 # [ ZhiPuAiEmbeddingOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-zhipuai/src/main/java/org/springframework/ai/zhipuai/[ZhiPuAiEmbeddingOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-zhipuai/src/main/java/org/springframework/ai/zhipuai/ZhiPuAiEmbeddingOptions.java)) 提供了 ZhiPuAI 的配置，例如要使用的模型等。 也可以使用 spring.ai.zhipuai.embedding.options properties 配置默认选项。 在开始时，使用 ZhiPuAiEmbeddingModel 构造函数设置用于所有嵌入请求的默认选项。在运行时，您可以使用 ZhiPuAiEmbeddingOptions 实例作为 EmbeddingRequest 的一部分来覆盖默认选项。 例如，要覆盖特定请求的默认模型名称：\nEmbeddingResponse embeddingResponse = embeddingModel.call( new EmbeddingRequest(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;), ZhiPuAiEmbeddingOptions.builder() .model(\u0026#34;Different-Embedding-Model-Deployment-Name\u0026#34;) .build())); 样品控制器 # 这将创建一个 EmbeddingModel 实现，您可以将其注入到您的类中。下面是一个使用 EmbeddingModel 实现的简单 @Controller 类的示例。\nspring.ai.zhipuai.api-key=YOUR_API_KEY spring.ai.zhipuai.embedding.options.model=embedding-2 @RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(\u0026#34;/ai/embedding\u0026#34;) public Map embed(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { EmbeddingResponse embeddingResponse = this.embeddingModel.embedForResponse(List.of(message)); return Map.of(\u0026#34;embedding\u0026#34;, embeddingResponse); } } 手动配置 # 如果你没有使用 Spring Boot，可以手动配置 ZhiPuAI Embedding Model。为此，将 spring-ai-zhipuai 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-zhipuai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-zhipuai\u0026#39; } 接下来，创建一个 ZhiPuAiEmbeddingModel 实例，并使用它来计算两个输入文本之间的相似度：\nvar zhiPuAiApi = new ZhiPuAiApi(System.getenv(\u0026#34;ZHIPU_AI_API_KEY\u0026#34;)); var embeddingModel = new ZhiPuAiEmbeddingModel(api, MetadataMode.EMBED, ZhiPuAiEmbeddingOptions.builder() .model(\u0026#34;embedding-3\u0026#34;) .dimensions(1536) .build()); EmbeddingResponse embeddingResponse = this.embeddingModel .embedForResponse(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;)); ZhiPuAiEmbeddingOptions 提供嵌入请求的配置信息。options 类提供了一个 builder（） 来轻松创建选项。\n"},{"id":66,"href":"/docs/vector-databases/neo4j/","title":"新 4j","section":"矢量数据库","content":" 新 4j # 本节将引导您完成设置 Neo4jVectorStore 以存储文档嵌入并执行相似性搜索。 [ Neo4j]( https://neo4j.com) 是一个开源的 NoSQL 图形数据库。它是一个完全事务性的数据库 （ACID），将数据存储为由节点组成的图形，通过关系连接。受现实世界结构的启发，它允许对复杂数据进行高查询性能，同时为开发人员保持直观和简单。 [ Neo4j 的 Vector Search]( https://neo4j.com/docs/cypher-manual/current/indexes-for-vector-search/) 允许用户从大型数据集中查询向量嵌入。嵌入是数据对象（如文本、图像、音频或文档）的数字表示形式。嵌入可以存储在 Node 属性上，并且可以使用 db.index.vector.queryNodes（） 函数进行查询。这些索引由 Lucene 提供支持，使用分层可导航小世界图 （HNSW） 对向量字段执行 k 个近似最近邻 （k-ANN） 查询。\n先决条件 # 一个正在运行的 Neo4j （5.15+） 实例。以下选项可用： Docker 镜像 Neo4j 桌面 Neo4j 光环 Neo4j 服务器实例 如果需要，EmbeddingModel 的 API 密钥，用于生成 Neo4jVectorStore 存储的嵌入。 自动配置 # Spring AI 为 Neo4j Vector Store 提供 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-neo4j\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-vector-store-neo4j\u0026#39; } 请查看向量存储的 [ Configuration Properties](#neo4jvector-properties) 列表，了解默认值和配置选项。 矢量存储实现可以为您初始化必要的架构，但您必须通过在相应的构造函数中指定 initializeSchema 布尔值或设置 ...initialize-schema=true 在 application.properties 文件中。 此外，您将需要一个已配置的 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) bean。请参阅 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) 部分以了解更多信息。 现在，您可以将 Neo4jVectorStore 自动连接为应用程序中的矢量存储。\n@Autowired VectorStore vectorStore; // ... List\u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); // Add the documents to Neo4j vectorStore.add(documents); // Retrieve documents similar to a query List\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch(SearchRequest.builder().query(\u0026#34;Spring\u0026#34;).topK(5).build()); 配置属性 # 要连接到 Neo4j 并使用 Neo4jVectorStore，您需要提供实例的访问详细信息。可以通过 Spring Boot 的 application.yml 提供一个简单的配置：\nspring: neo4j: uri: \u0026lt;neo4j instance URI\u0026gt; authentication: username: \u0026lt;neo4j username\u0026gt; password: \u0026lt;neo4j password\u0026gt; ai: vectorstore: neo4j: initialize-schema: true database-name: neo4j index-name: custom-index embedding-dimension: 1536 distance-type: cosine 以 spring.neo4j.* 开头的 Spring Boot 属性用于配置 Neo4j 客户端： 以 spring.ai.vectorstore.neo4j.* 开头的属性用于配置 Neo4jVectorStore： 以下距离函数可用：\ncosine - 默认，适用于大多数用例。测量向量之间的余弦相似性。 euclidean - 向量之间的欧几里得距离。值越低表示相似度越高。 手动配置 # 您可以手动配置 Neo4j 矢量存储，而不是使用 Spring Boot 自动配置。为此，你需要将 spring-ai-neo4j-store 添加到你的项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-neo4j-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-neo4j-store\u0026#39; } 创建 Neo4j 驱动程序 Bean。阅读 [ Neo4j 文档]( https://neo4j.com/docs/java-manual/current/client-applications/) 以获取有关自定义驱动程序配置的更深入信息。\n@Bean public Driver driver() { return GraphDatabase.driver(\u0026#34;neo4j://\u0026lt;host\u0026gt;:\u0026lt;bolt-port\u0026gt;\u0026#34;, AuthTokens.basic(\u0026#34;\u0026lt;username\u0026gt;\u0026#34;, \u0026#34;\u0026lt;password\u0026gt;\u0026#34;)); } 然后使用构建器模式创建 Neo4jVectorStore Bean：\n@Bean public VectorStore vectorStore(Driver driver, EmbeddingModel embeddingModel) { return Neo4jVectorStore.builder(driver, embeddingModel) .databaseName(\u0026#34;neo4j\u0026#34;) // Optional: defaults to \u0026#34;neo4j\u0026#34; .distanceType(Neo4jDistanceType.COSINE) // Optional: defaults to COSINE .embeddingDimension(1536) // Optional: defaults to 1536 .label(\u0026#34;Document\u0026#34;) // Optional: defaults to \u0026#34;Document\u0026#34; .embeddingProperty(\u0026#34;embedding\u0026#34;) // Optional: defaults to \u0026#34;embedding\u0026#34; .indexName(\u0026#34;custom-index\u0026#34;) // Optional: defaults to \u0026#34;spring-ai-document-index\u0026#34; .initializeSchema(true) // Optional: defaults to false .batchingStrategy(new TokenCountBatchingStrategy()) // Optional: defaults to TokenCountBatchingStrategy .build(); } // This can be any EmbeddingModel implementation @Bean public EmbeddingModel embeddingModel() { return new OpenAiEmbeddingModel(new OpenAiApi(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;))); } 元数据筛选 # 您也可以在 Neo4j store 中使用通用的可移植[ 元数据过滤器](../vectordbs.html#metadata-filters) 。 例如，您可以使用文本表达式语言：\nvectorStore.similaritySearch( SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(\u0026#34;author in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; \u0026#39;article_type\u0026#39; == \u0026#39;blog\u0026#39;\u0026#34;).build()); 或使用 Filter.Expression DSL 以编程方式：\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(b.and( b.in(\u0026#34;author\u0026#34;, \u0026#34;john\u0026#34;, \u0026#34;jill\u0026#34;), b.eq(\u0026#34;article_type\u0026#34;, \u0026#34;blog\u0026#34;)).build()).build()); 例如，此可移植筛选条件表达式：\nauthor in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; \u0026#39;article_type\u0026#39; == \u0026#39;blog\u0026#39; 转换为专有的 Neo4j 过滤器格式：\nnode.`metadata.author` IN [\u0026#34;john\u0026#34;,\u0026#34;jill\u0026#34;] AND node.`metadata.\u0026#39;article_type\u0026#39;` = \u0026#34;blog\u0026#34; 访问 Native Client # Neo4j Vector Store 实现通过 getNativeClient（） 方法提供对底层原生 Neo4j 客户端（ 驱动程序 ）的访问：\nNeo4jVectorStore vectorStore = context.getBean(Neo4jVectorStore.class); Optional\u0026lt;Driver\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { Driver driver = nativeClient.get(); // Use the native client for Neo4j-specific operations } 本机客户端允许您访问特定于 Neo4j 的功能和作，这些功能和作可能不会通过 VectorStore 接口公开。\n"},{"id":67,"href":"/docs/model-context-protocol-mcp/","title":"模型上下文协议 （MCP）","section":"Docs","content":" 模型上下文协议 （MCP） # [ 模型上下文协议]( https://modelcontextprotocol.org/docs/concepts/architecture) （MCP） 是一种标准化协议，使 AI 模型能够以结构化方式与外部工具和资源交互。它支持多种传输机制，以便在不同环境中提供灵活性。 [ MCP Java SDK]( https://modelcontextprotocol.io/sdk/java) 提供模型上下文协议的 Java 实现，通过同步和异步通信模式实现与 AI 模型和工具的标准化交互。 Spring AI MCP 通过 Spring Boot 集成扩展了 MCP Java SDK，同时提供了[ 客户端](mcp-client-boot-starter-docs.html)和[ 服务器](mcp-server-boot-starter-docs.html)启动器。使用 [ Spring Initializer]( https://start.spring.io) 通过 MCP 支持引导您的 AI 应用程序。\nMCP Java SDK 架构 # Java MCP 实现遵循三层架构： 有关使用低级 MCP 客户端/服务器 API 的详细实施指南，请参阅 [ MCP Java SDK 文档]( https://modelcontextprotocol.io/sdk/java) 。要使用 Spring Boot 简化设置，请使用下面描述的 MCP Boot Starters。\nSpring AI MCP 集成 # Spring AI 通过以下 Spring Boot 启动器提供 MCP 集成：\n客户端启动器 # spring-ai-starter-mcp-client - 提供 STDIO 和基于 HTTP 的 SSE 支持的核心启动器 spring-ai-starter-mcp-client-webflux - 基于 WebFlux 的 SSE 传输实现 服务器启动器 # spring-ai-starter-mcp-server - 支持 STDIO 传输的核心服务器 spring-ai-starter-mcp-server-webmvc - 基于 Spring MVC 的 SSE 传输实现 spring-ai-starter-mcp-server-webflux - 基于 WebFlux 的 SSE 传输实现 其他资源 # MCP 客户端引导启动程序文档 MCP Server Boot Starters 文档 MCP 实用程序文档 Model 上下文协议规范 "},{"id":68,"href":"/docs/models/chat-models/moonshot-ai/","title":"Moonshot AI 聊天","section":"聊天模型 API","content":" Moonshot AI 聊天 # 此功能已移至 Spring AI Community 存储库。 请访问 [ github.com/spring-ai-community/moonshot](https:// github.com/spring-ai-community/moonshot) 获取最新版本。\n"},{"id":69,"href":"/docs/vector-databases/opensearch/","title":"开放搜索","section":"矢量数据库","content":" 开放搜索 # 本节将指导您设置 OpenSearchVectorStore 以存储文档嵌入并执行相似性搜索。 [ OpenSearch]( https://opensearch.org) 是一个开源搜索和分析引擎，最初是从 Elasticsearch 分叉而来的，在 Apache License 2.0 下分发。它通过简化 AI 生成资产的集成和管理来增强 AI 应用程序开发。[ OpenSearch]( https://opensearch.org) 支持向量、词法和混合搜索功能，利用高级向量数据库功能来促进低延迟查询和相似性搜索，如[ 向量数据库页面上]( https://opensearch.org/platform/search/vector-database.html)详述的那样。 [ OpenSearch k-NN]( https://opensearch.org/docs/latest/search-plugins/knn/index/) 功能允许用户从大型数据集中查询向量嵌入。嵌入是数据对象（如文本、图像、音频或文档）的数字表示形式。嵌入可以存储在索引中，并使用各种相似性函数进行查询。\n先决条件 # 正在运行的 OpenSearch 实例。以下选项可用： 自我管理的 OpenSearch Amazon OpenSearch 服务 如果需要，EmbeddingModel 的 API 密钥，用于生成由 OpenSearchVectorStore 存储的嵌入。 自动配置 # Spring AI 为 OpenSearch Vector Store 提供 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-opensearch\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或复制到您的 Gradle build.gradle 构建文件：\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-vector-store-opensearch\u0026#39; } 对于 Amazon OpenSearch Service，请改用以下依赖项：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-opensearch\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或者对于 Gradle：\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-vector-store-opensearch\u0026#39; } 请查看 vector store 的[ 配置参数](#_configuration_properties)列表，了解默认值和配置选项。 此外，您将需要一个已配置的 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) bean。请参阅 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) 部分以了解更多信息。 现在，您可以在应用程序中将 OpenSearchVectorStore 自动连接为矢量存储：\n@Autowired VectorStore vectorStore; // ... List\u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); // Add the documents to OpenSearch vectorStore.add(documents); // Retrieve documents similar to a query List\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch(SearchRequest.builder().query(\u0026#34;Spring\u0026#34;).topK(5).build()); 配置属性 # 要连接到 OpenSearch 并使用 OpenSearchVectorStore，您需要提供实例的访问详细信息。可以通过 Spring Boot 的 application.yml 提供一个简单的配置：\nspring: ai: vectorstore: opensearch: uris: \u0026lt;opensearch instance URIs\u0026gt; username: \u0026lt;opensearch username\u0026gt; password: \u0026lt;opensearch password\u0026gt; index-name: spring-ai-document-index initialize-schema: true similarity-function: cosinesimil read-timeout: \u0026lt;time to wait for response\u0026gt; connect-timeout: \u0026lt;time to wait until connection established\u0026gt; path-prefix: \u0026lt;custom path prefix\u0026gt; ssl-bundle: \u0026lt;name of SSL bundle\u0026gt; aws: # Only for Amazon OpenSearch Service host: \u0026lt;aws opensearch host\u0026gt; service-name: \u0026lt;aws service name\u0026gt; access-key: \u0026lt;aws access key\u0026gt; secret-key: \u0026lt;aws secret key\u0026gt; region: \u0026lt;aws region\u0026gt; 以 开头的属性 spring.ai.vectorstore.opensearch.* 用于配置 OpenSearchVectorStore： 可以使用以下相似性函数：\ncosinesimil - 默认，适用于大多数用例。测量向量之间的余弦相似性。 l1 - 向量之间的曼哈顿距离。 l2 - 向量之间的欧几里得距离。 linf - 向量之间的切比雪夫距离。 手动配置 # 您可以手动配置 OpenSearch 矢量存储，而不是使用 Spring Boot 自动配置。为此，您需要将 spring-ai-opensearch-store 添加到您的项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-opensearch-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或复制到您的 Gradle build.gradle 构建文件：\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-opensearch-store\u0026#39; } 创建 OpenSearch 客户端 Bean：\n@Bean public OpenSearchClient openSearchClient() { RestClient restClient = RestClient.builder( HttpHost.create(\u0026#34;http://localhost:9200\u0026#34;)) .build(); return new OpenSearchClient(new RestClientTransport( restClient, new JacksonJsonpMapper())); } 然后使用构建器模式创建 OpenSearchVectorStore Bean：\n@Bean public VectorStore vectorStore(OpenSearchClient openSearchClient, EmbeddingModel embeddingModel) { return OpenSearchVectorStore.builder(openSearchClient, embeddingModel) .index(\u0026#34;custom-index\u0026#34;) // Optional: defaults to \u0026#34;spring-ai-document-index\u0026#34; .similarityFunction(\u0026#34;l2\u0026#34;) // Optional: defaults to \u0026#34;cosinesimil\u0026#34; .initializeSchema(true) // Optional: defaults to false .batchingStrategy(new TokenCountBatchingStrategy()) // Optional: defaults to TokenCountBatchingStrategy .build(); } // This can be any EmbeddingModel implementation @Bean public EmbeddingModel embeddingModel() { return new OpenAiEmbeddingModel(new OpenAiApi(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;))); } 元数据筛选 # 您还可以将通用的可移植[ 元数据筛选器](../vectordbs.html#metadata-filters)与 OpenSearch 结合使用。 例如，您可以使用文本表达式语言：\nvectorStore.similaritySearch( SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(\u0026#34;author in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; \u0026#39;article_type\u0026#39; == \u0026#39;blog\u0026#39;\u0026#34;).build()); 或使用 Filter.Expression DSL 以编程方式：\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(b.and( b.in(\u0026#34;author\u0026#34;, \u0026#34;john\u0026#34;, \u0026#34;jill\u0026#34;), b.eq(\u0026#34;article_type\u0026#34;, \u0026#34;blog\u0026#34;)).build()).build()); 例如，此可移植筛选条件表达式：\nauthor in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; \u0026#39;article_type\u0026#39; == \u0026#39;blog\u0026#39; 转换为专有的 OpenSearch 筛选条件格式：\n(metadata.author:john OR jill) AND metadata.article_type:blog 访问 Native Client # OpenSearch 矢量存储实现通过 getNativeClient（） 方法提供对底层原生 OpenSearch 客户端 （OpenSearchClient） 的访问：\nOpenSearchVectorStore vectorStore = context.getBean(OpenSearchVectorStore.class); Optional\u0026lt;OpenSearchClient\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { OpenSearchClient client = nativeClient.get(); // Use the native client for OpenSearch-specific operations } 本机客户端允许您访问特定于 OpenSearch 的功能和作，这些功能和作可能无法通过 VectorStore 界面公开。\n"},{"id":70,"href":"/docs/retrieval-augmented-generation-rag/","title":"检索增强一代","section":"Docs","content":" 检索增强一代 # 检索增强生成 （RAG） 是一种有用的技术，可用于克服大型语言模型的局限性，这些模型在长篇内容、事实准确性和上下文感知方面存在困难。 Spring AI 通过提供模块化架构来支持 RAG，该架构允许您自己构建自定义 RAG 流或使用 Advisor API 使用开箱即用的 RAG 流。\n顾问 # Spring AI 使用 Advisor API 为常见的 RAG 流提供开箱即用的支持。 要使用 QuestionAnswerAdvisor 或 RetrievalAugmentationAdvisor，您需要将 spring-ai-advisors-vector-store 依赖项添加到您的项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-advisors-vector-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; QuestionAnswer 顾问 # 矢量数据库存储 AI 模型不知道的数据。将用户问题发送到 AI 模型时，QuestionAnswerAdvisor 会在向量数据库中查询与用户问题相关的文档。 来自向量数据库的响应将附加到用户文本中，以便为 AI 模型生成响应提供上下文。 假设您已经将数据加载到 VectorStore 中，您可以通过向 ChatClient 提供 QuestionAnswerAdvisor 的实例来执行检索增强生成 （RAG）。\nChatResponse response = ChatClient.builder(chatModel) .build().prompt() .advisors(new QuestionAnswerAdvisor(vectorStore)) .user(userText) .call() .chatResponse(); 在此示例中，QuestionAnswerAdvisor 将对 Vector Database 中的所有文档执行相似性搜索。为了限制搜索的文档类型，SearchRequest 采用可在所有 VectorStore 之间移植的类似 SQL 的筛选表达式。 此筛选条件表达式可以在创建 QuestionAnswerAdvisor 时进行配置，因此将始终应用于所有 ChatClient 请求，也可以在运行时为每个请求提供。 下面介绍如何创建阈值为 0.8 的 QuestionAnswerAdvisor 实例并返回前 6 个结果。\nvar qaAdvisor = QuestionAnswerAdvisor.builder(vectorStore) .searchRequest(SearchRequest.builder().similarityThreshold(0.8d).topK(6).build()) .build(); 动态筛选表达式 # 在运行时使用 FILTER_EXPRESSION advisor 上下文参数更新 SearchRequest 筛选条件表达式：\nChatClient chatClient = ChatClient.builder(chatModel) .defaultAdvisors(QuestionAnswerAdvisor.builder(vectorStore) .searchRequest(SearchRequest.builder().build()) .build()) .build(); // Update filter expression at runtime String content = this.chatClient.prompt() .user(\u0026#34;Please answer my question XYZ\u0026#34;) .advisors(a -\u0026gt; a.param(QuestionAnswerAdvisor.FILTER_EXPRESSION, \u0026#34;type == \u0026#39;Spring\u0026#39;\u0026#34;)) .call() .content(); 的 FILTER_EXPRESSION 参数允许您根据提供的表达式动态筛选搜索结果。\n自定义模板 # QuestionAnswerAdvisor 使用默认模板通过检索到的文档来扩充用户问题。您可以通过 .promptTemplate（） 构建器方法提供自己的 PromptTemplate 对象来自定义此行为。 自定义 PromptTemplate 可以使用任何 TemplateRenderer 实现（默认情况下，它使用基于 [ StringTemplate]( https://www.stringtemplate.org/) 引擎的 StPromptTemplate）。重要的要求是模板必须包含以下两个占位符：\n用于接收用户问题的查询占位符。 一个 question_answer_context 占位符，用于接收检索到的上下文。 PromptTemplate customPromptTemplate = PromptTemplate.builder() .renderer(StTemplateRenderer.builder().startDelimiterToken(\u0026#39;\u0026lt;\u0026#39;).endDelimiterToken(\u0026#39;\u0026gt;\u0026#39;).build()) .template(\u0026#34;\u0026#34;\u0026#34; \u0026lt;query\u0026gt; Context information is below. --------------------- \u0026lt;question_answer_context\u0026gt; --------------------- Given the context information and no prior knowledge, answer the query. Follow these rules: 1. If the answer is not in the context, just say that you don\u0026#39;t know. 2. Avoid statements like \u0026#34;Based on the context...\u0026#34; or \u0026#34;The provided information...\u0026#34;. \u0026#34;\u0026#34;\u0026#34;) .build(); String question = \u0026#34;Where does the adventure of Anacletus and Birba take place?\u0026#34;; QuestionAnswerAdvisor qaAdvisor = QuestionAnswerAdvisor.builder(vectorStore) .promptTemplate(customPromptTemplate) .build(); String response = ChatClient.builder(chatModel).build() .prompt(question) .advisors(qaAdvisor) .call() .content(); 检索增强顾问 # Spring AI 包括一个 [ RAG 模块库](#modules) ，您可以使用这些模块来构建自己的 RAG 流。`RetrievalAugmentationAdvisor``` 是一个 Advisor``，它基于模块化架构为最常见的 RAG 流提供开箱即用的实现。\n顺序 RAG 流 # 天真的 RAG # Advisor retrievalAugmentationAdvisor = RetrievalAugmentationAdvisor.builder() .documentRetriever(VectorStoreDocumentRetriever.builder() .similarityThreshold(0.50) .vectorStore(vectorStore) .build()) .build(); String answer = chatClient.prompt() .advisors(retrievalAugmentationAdvisor) .user(question) .call() .content(); 默认情况下，RetrievalAugmentationAdvisor 不允许检索到的上下文为空。发生这种情况时，它会指示模型不回答用户查询。您可以允许空上下文，如下所示。\nAdvisor retrievalAugmentationAdvisor = RetrievalAugmentationAdvisor.builder() .documentRetriever(VectorStoreDocumentRetriever.builder() .similarityThreshold(0.50) .vectorStore(vectorStore) .build()) .queryAugmenter(ContextualQueryAugmenter.builder() .allowEmptyContext(true) .build()) .build(); String answer = chatClient.prompt() .advisors(retrievalAugmentationAdvisor) .user(question) .call() .content(); VectorStoreDocumentRetriever 接受 FilterExpression 以根据元数据筛选搜索结果。您可以在实例化 VectorStoreDocumentRetriever 时或在运行时为每个请求提供一个 FILTER_EXPRESSION advisor 上下文参数。\nAdvisor retrievalAugmentationAdvisor = RetrievalAugmentationAdvisor.builder() .documentRetriever(VectorStoreDocumentRetriever.builder() .similarityThreshold(0.50) .vectorStore(vectorStore) .build()) .build(); String answer = chatClient.prompt() .advisors(retrievalAugmentationAdvisor) .advisors(a -\u0026gt; a.param(VectorStoreDocumentRetriever.FILTER_EXPRESSION, \u0026#34;type == \u0026#39;Spring\u0026#39;\u0026#34;)) .user(question) .call() .content(); 有关更多信息，请参阅 [ VectorStoreDocumentRetriever](#_vectorstoredocumentretriever)。\n高级 RAG # Advisor retrievalAugmentationAdvisor = RetrievalAugmentationAdvisor.builder() .queryTransformers(RewriteQueryTransformer.builder() .chatClientBuilder(chatClientBuilder.build().mutate()) .build()) .documentRetriever(VectorStoreDocumentRetriever.builder() .similarityThreshold(0.50) .vectorStore(vectorStore) .build()) .build(); String answer = chatClient.prompt() .advisors(retrievalAugmentationAdvisor) .user(question) .call() .content(); 您还可以使用 DocumentPostProcessor API 对检索到的文档进行后处理，然后再将其传递给模型。例如，您可以使用此类接口根据检索到的文档与查询的相关性对检索到的文档执行重新排名，删除不相关或冗余的文档，或压缩每个文档的内容以减少干扰和冗余。\n模块 # Spring AI 实现了模块化 RAG 架构，其灵感来自论文“ [ 模块化 RAG：将 RAG 系统转变为类似乐高的可重构框架]( https://arxiv.org/abs/2407.21059) ”中详述的模块化概念。\n预取 # 预检索模块负责处理用户查询以实现最佳检索结果。\n查询转换 # 一个组件，用于转换输入查询以使其更有效地执行检索任务，解决诸如格式不佳的查询、模棱两可的术语、复杂的词汇表或不支持的语言等挑战。\nCompressionQueryTransformer （压缩查询转换器） # CompressionQueryTransformer 使用大型语言模型将对话历史记录和后续查询压缩为捕获对话本质的独立查询。 当对话历史记录很长并且后续查询与对话上下文相关时，此转换器非常有用。\nQuery query = Query.builder() .text(\u0026#34;And what is its second largest city?\u0026#34;) .history(new UserMessage(\u0026#34;What is the capital of Denmark?\u0026#34;), new AssistantMessage(\u0026#34;Copenhagen is the capital of Denmark.\u0026#34;)) .build(); QueryTransformer queryTransformer = CompressionQueryTransformer.builder() .chatClientBuilder(chatClientBuilder) .build(); Query transformedQuery = queryTransformer.transform(query); 此组件使用的提示可以通过构建器中提供的 promptTemplate（） 方法进行自定义。\n重写 QueryTransformer # RewriteQueryTransformer 使用大型语言模型重写用户查询，以便在查询目标系统（如矢量存储或 Web 搜索引擎）时提供更好的结果。 当用户查询冗长、模棱两可或包含可能影响搜索结果质量的不相关信息时，此转换器非常有用。\nQuery query = new Query(\u0026#34;I\u0026#39;m studying machine learning. What is an LLM?\u0026#34;); QueryTransformer queryTransformer = RewriteQueryTransformer.builder() .chatClientBuilder(chatClientBuilder) .build(); Query transformedQuery = queryTransformer.transform(query); 此组件使用的提示可以通过构建器中提供的 promptTemplate（） 方法进行自定义。 TranslationQueryTransformer 使用大型语言模型将查询转换为用于生成文档嵌入的嵌入模型支持的目标语言。如果查询已使用目标语言，则返回该查询时不会更改。如果查询的语言未知，则也会原封不动地返回它。 当嵌入模型使用特定语言进行训练并且用户查询使用其他语言时，此转换器非常有用。\nQuery query = new Query(\u0026#34;Hvad er Danmarks hovedstad?\u0026#34;); QueryTransformer queryTransformer = TranslationQueryTransformer.builder() .chatClientBuilder(chatClientBuilder) .targetLanguage(\u0026#34;english\u0026#34;) .build(); Query transformedQuery = queryTransformer.transform(query); 此组件使用的提示可以通过构建器中提供的 promptTemplate（） 方法进行自定义。\n查询扩展 # 一个组件，用于将输入查询扩展为查询列表，通过提供替代查询公式或将复杂问题分解为更简单的子查询来解决难题，例如格式不正确的查询。 MultiQueryExpander 使用大型语言模型将查询扩展为多个语义不同的变体，以捕获不同的视角，这对于检索其他上下文信息和增加找到相关结果的机会非常有用。\nMultiQueryExpander queryExpander = MultiQueryExpander.builder() .chatClientBuilder(chatClientBuilder) .numberOfQueries(3) .build(); List\u0026lt;Query\u0026gt; queries = queryExpander.expand(new Query(\u0026#34;How to run a Spring Boot app?\u0026#34;)); 默认情况下，MultiQueryExpander 在展开的查询列表中包含原始查询。您可以通过生成器中的 includeOriginal 方法禁用此行为。\nMultiQueryExpander queryExpander = MultiQueryExpander.builder() .chatClientBuilder(chatClientBuilder) .includeOriginal(false) .build(); 此组件使用的提示可以通过构建器中提供的 promptTemplate（） 方法进行自定义。\n检索 # 检索模块负责查询数据系统（如 vector store）并检索最相关的文档。\n文件搜索 # 负责从底层数据源（如搜索引擎、矢量存储、数据库或知识图谱）检索 Documents 的组件。 VectorStoreDocumentRetriever 从向量存储中检索在语义上类似于输入查询的文档。它支持根据元数据、相似性阈值和 top-k 结果进行筛选。\nDocumentRetriever retriever = VectorStoreDocumentRetriever.builder() .vectorStore(vectorStore) .similarityThreshold(0.73) .topK(5) .filterExpression(new FilterExpressionBuilder() .eq(\u0026#34;genre\u0026#34;, \u0026#34;fairytale\u0026#34;) .build()) .build(); List\u0026lt;Document\u0026gt; documents = retriever.retrieve(new Query(\u0026#34;What is the main character of the story?\u0026#34;)); 筛选条件表达式可以是 static 或 dynamic。对于动态筛选条件表达式，您可以传递 Supplier。\nDocumentRetriever retriever = VectorStoreDocumentRetriever.builder() .vectorStore(vectorStore) .filterExpression(() -\u0026gt; new FilterExpressionBuilder() .eq(\u0026#34;tenant\u0026#34;, TenantContextHolder.getTenantIdentifier()) .build()) .build(); List\u0026lt;Document\u0026gt; documents = retriever.retrieve(new Query(\u0026#34;What are the KPIs for the next semester?\u0026#34;)); 您还可以使用 Query API 使用 FILTER_EXPRESSION 参数提供特定于请求的筛选条件表达式。如果同时提供了特定于请求的筛选条件表达式和特定于检索器的筛选条件表达式，则特定于请求的筛选条件表达式优先。\nQuery query = Query.builder() .text(\u0026#34;Who is Anacletus?\u0026#34;) .context(Map.of(VectorStoreDocumentRetriever.FILTER_EXPRESSION, \u0026#34;location == \u0026#39;Whispering Woods\u0026#39;\u0026#34;)) .build(); List\u0026lt;Document\u0026gt; retrievedDocuments = documentRetriever.retrieve(query); 文档联接 # 一个组件，用于将基于多个查询和从多个数据源检索到的文档合并到单个文档集合中。作为联接过程的一部分，它还可以处理重复文档和互惠排名策略。 ConcatenationDocumentJoiner 通过将基于多个查询和多个数据源检索到的文档连接到单个文档集合中，将它们组合在一起。如果存在重复的文档，则保留第一次出现。每个文档的分数保持原样。\nMap\u0026lt;Query, List\u0026lt;List\u0026lt;Document\u0026gt;\u0026gt;\u0026gt; documentsForQuery = ... DocumentJoiner documentJoiner = new ConcatenationDocumentJoiner(); List\u0026lt;Document\u0026gt; documents = documentJoiner.join(documentsForQuery); 检索后 # 检索后模块负责处理检索到的文档，以实现最佳的生成结果。\n文档后处理 # 一个组件，用于根据查询对检索到的文档进行后处理，解决诸如中间丢失 、模型中的上下文长度限制以及减少检索信息中的噪声和冗余等挑战。 例如，它可以根据文档与查询的相关性对文档进行排名，删除不相关或冗余的文档，或者压缩每个文档的内容以减少干扰和冗余。\n代 # 生成模块负责根据用户查询和检索到的文档生成最终响应。\n查询增加 # 一个组件，用于使用其他数据来扩充输入查询，可用于为大型语言模型提供必要的上下文来回答用户查询。\n上下文 QueryAugmenter # ContextualQueryAugmenter 使用来自所提供文档内容的上下文数据来增强用户查询。\nQueryAugmenter queryAugmenter = ContextualQueryAugmenter.builder().build(); 默认情况下，ContextualQueryAugmenter 不允许检索到的上下文为空。发生这种情况时，它会指示模型不回答用户查询。 您可以启用 allowEmptyContext 选项，以允许模型生成响应，即使检索到的上下文为空。\nQueryAugmenter queryAugmenter = ContextualQueryAugmenter.builder() .allowEmptyContext(true) .build(); 此组件使用的提示可以通过构建器中提供的 promptTemplate（） 和 emptyContextPromptTemplate（） 方法进行自定义。\n"},{"id":71,"href":"/docs/models/chat-models/nvidia/","title":"NVIDIA 聊天","section":"聊天模型 API","content":" NVIDIA 聊天 # [ NVIDIA LLM API]( https://docs.api.nvidia.com/nim/reference/llm-apis) 是一个代理 AI 推理引擎，提供来自[ 各种提供商]( https://docs.api.nvidia.com/nim/reference/llm-apis#models)的各种模型。 Spring AI 通过重用现有的 [ OpenAI](openai-chat.html) 客户端与 NVIDIA LLM API 集成。为此，您需要将 base-url 设置为 [[integrate.api.nvidia.com](https://integrate.api.nvidia.com)](https://[integrate.api.nvidia.com](https://integrate.api.nvidia.com))，选择一个提供的 [ LLM 模型]( https://docs.api.nvidia.com/nim/reference/llm-apis#model)并为其获取 api-key。 查看 [ NvidiaWithOpenAiChatModelIT.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/chat/proxy/[NvidiaWithOpenAiChatModelIT.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/chat/proxy/NvidiaWithOpenAiChatModelIT.java)) 测试，了解将 NVIDIA LLM API 与 Spring AI 结合使用的示例。\n先决条件 # 创建具有足够积分的 NVIDIA 帐户。 选择要使用的 LLM 模型。例如下面屏幕截图中的 meta/llama-3.1-70b-instruct。 从所选模型的页面中，您可以获取用于访问此模型的 api-key。 自动配置 # Spring AI 为 OpenAI Chat 客户端提供 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-openai\u0026#39; } 聊天属性 # 重试属性 # 前缀 spring.ai.retry 用作属性前缀，允许您为 OpenAI 聊天模型配置重试机制。\n连接属性 # 前缀 spring.ai.openai 用作允许您连接到 OpenAI 的属性前缀。\n配置属性 # 前缀 spring.ai.openai.chat 是属性前缀，允许您为 OpenAI 配置聊天模型实现。\n运行时选项 # [ OpenAiChatOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/[OpenAiChatOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/OpenAiChatOptions.java)) 提供模型配置，例如要使用的模型、温度、频率损失等。 启动时，可以使用 OpenAiChatModel（api， options） 构造函数或 spring.ai.openai.chat.options.* 属性配置默认选项。 在运行时，您可以通过向 Prompt 调用添加新的、特定于请求的选项来覆盖默认选项。例如，要覆盖特定请求的默认模型和温度：\nChatResponse response = chatModel.call( new Prompt( \u0026#34;Generate the names of 5 famous pirates.\u0026#34;, OpenAiChatOptions.builder() .model(\u0026#34;mixtral-8x7b-32768\u0026#34;) .temperature(0.4) .build() )); 函数调用 # NVIDIA LLM API 在选择支持它的模型时支持工具/函数调用。 您可以使用 ChatModel 注册自定义 Java 函数，并让提供的模型智能地选择输出包含参数的 JSON 对象，以调用一个或多个已注册的函数。这是一种将 LLM 功能与外部工具和 API 连接起来的强大技术。\n工具示例 # 以下是如何将 NVIDIA LLM API 函数调用与 Spring AI 一起使用的简单示例：\nspring.ai.openai.api-key=${NVIDIA_API_KEY} spring.ai.openai.base-url=https://integrate.api.nvidia.com spring.ai.openai.chat.options.model=meta/llama-3.1-70b-instruct spring.ai.openai.chat.options.max-tokens=2048 @SpringBootApplication public class NvidiaLlmApplication { public static void main(String[] args) { SpringApplication.run(NvidiaLlmApplication.class, args); } @Bean CommandLineRunner runner(ChatClient.Builder chatClientBuilder) { return args -\u0026gt; { var chatClient = chatClientBuilder.build(); var response = chatClient.prompt() .user(\u0026#34;What is the weather in Amsterdam and Paris?\u0026#34;) .functions(\u0026#34;weatherFunction\u0026#34;) // reference by bean name. .call() .content(); System.out.println(response); }; } @Bean @Description(\u0026#34;Get the weather in location\u0026#34;) public Function\u0026lt;WeatherRequest, WeatherResponse\u0026gt; weatherFunction() { return new MockWeatherService(); } public static class MockWeatherService implements Function\u0026lt;WeatherRequest, WeatherResponse\u0026gt; { public record WeatherRequest(String location, String unit) {} public record WeatherResponse(double temp, String unit) {} @Override public WeatherResponse apply(WeatherRequest request) { double temperature = request.location().contains(\u0026#34;Amsterdam\u0026#34;) ? 20 : 25; return new WeatherResponse(temperature, request.unit); } } } 在此示例中，当模型需要天气信息时，它将自动调用 weatherFunction Bean，然后该 bean 可以获取实时天气数据。预期的响应如下所示：“阿姆斯特丹的天气目前是 20 摄氏度，巴黎的天气目前是 25 摄氏度。 阅读有关 OpenAI [ 函数调用]( https://docs.spring.io/spring-ai/reference/api/chat/functions/openai-chat-functions.html)的更多信息。\n样品控制器 # [ 创建一个新]( https://start.spring.io/)的 Spring Boot 项目，并将 添加到您的 spring-ai-starter-model-openai pom（或 gradle）依赖项中。 在 src/main/resources 目录下添加一个 application.properties 文件，以启用和配置 OpenAi 聊天模型：\nspring.ai.openai.api-key=${NVIDIA_API_KEY} spring.ai.openai.base-url=https://integrate.api.nvidia.com spring.ai.openai.chat.options.model=meta/llama-3.1-70b-instruct # The NVIDIA LLM API doesn\u0026#39;t support embeddings, so we need to disable it. spring.ai.openai.embedding.enabled=false # The NVIDIA LLM API requires this parameter to be set explicitly or server internal error will be thrown. spring.ai.openai.chat.options.max-tokens=2048 下面是一个简单的 @Controller 类的示例，该类使用聊天模型生成文本。\n@RestController public class ChatController { private final OpenAiChatModel chatModel; @Autowired public ChatController(OpenAiChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, this.chatModel.call(message)); } @GetMapping(\u0026#34;/ai/generateStream\u0026#34;) public Flux\u0026lt;ChatResponse\u0026gt; generateStream(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { Prompt prompt = new Prompt(new UserMessage(message)); return this.chatModel.stream(prompt); } } "},{"id":72,"href":"/docs/vector-databases/oracle/","title":"Oracle Database 23ai — AI 向量搜索","section":"矢量数据库","content":" Oracle Database 23ai — AI 向量搜索 # Oracle Database 23ai （23.4+） 的 [ AI Vector Search]( https://docs.oracle.com/en/database/oracle/oracle-database/23/vecse/overview-ai-vector-search.html) 功能以 Spring AI VectorStore 的形式提供，可帮助您存储文档嵌入并执行相似性搜索。当然，所有其他功能也都可用。\n自动配置 # 首先，将 Oracle Vector Store 引导启动程序依赖项添加到您的项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-oracle\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-vector-store-oracle\u0026#39; } 如果需要此向量存储来初始化架构，则需要在相应的构造函数中为 initializeSchema 布尔参数传递 true，或者通过设置 ...initialize-schema=true 在 application.properties 文件中。 Vector Store 还需要一个 EmbeddingModel 实例来计算文档的嵌入。您可以选择一个可用的 EmbeddingModel Implementations。 例如，要使用 [ OpenAI EmbeddingModel](../embeddings/openai-embeddings.html)，请将以下依赖项添加到您的项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-openai\u0026#39; } 要连接并配置 OracleVectorStore，您需要提供数据库的访问详细信息。可以通过 Spring Boot 的 application.yml 提供简单的配置 现在，您可以在应用程序中自动连接 OracleVectorStore 并使用它：\n@Autowired VectorStore vectorStore; // ... List\u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); // Add the documents to Oracle Vector Store vectorStore.add(documents); // Retrieve documents similar to a query List\u0026lt;Document\u0026gt; results = this.vectorStore.similaritySearch(SearchRequest.builder().query(\u0026#34;Spring\u0026#34;).topK(5).build()); 配置属性 # 您可以在 Spring Boot 配置中使用以下属性来自定义 OracleVectorStore。\n元数据筛选 # 您可以将通用的可移植[ 元数据过滤器]( https://docs.spring.io/spring-ai/reference/api/vectordbs.html#_metadata_filters)与 OracleVectorStore 结合使用。 例如，您可以使用文本表达式语言：\nvectorStore.similaritySearch( SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(\u0026#34;author in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; article_type == \u0026#39;blog\u0026#39;\u0026#34;).build()); 或使用 Filter.Expression DSL 以编程方式：\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(b.and( b.in(\u0026#34;author\u0026#34;,\u0026#34;john\u0026#34;, \u0026#34;jill\u0026#34;), b.eq(\u0026#34;article_type\u0026#34;, \u0026#34;blog\u0026#34;)).build()).build()); 手动配置 # 您可以手动配置 OracleVectorStore，而不是使用 Spring Boot 自动配置。为此，您需要将 Oracle JDBC 驱动程序和 JdbcTemplate 自动配置依赖项添加到您的项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-jdbc\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.oracle.database.jdbc\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;ojdbc11\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;runtime\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-oracle-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 要在应用程序中配置 OracleVectorStore，可以使用以下设置：\n@Bean public VectorStore vectorStore(JdbcTemplate jdbcTemplate, EmbeddingModel embeddingModel) { return OracleVectorStore.builder(jdbcTemplate, embeddingModel) .tableName(\u0026#34;my_vectors\u0026#34;) .indexType(OracleVectorStoreIndexType.IVF) .distanceType(OracleVectorStoreDistanceType.COSINE) .dimensions(1536) .searchAccuracy(95) .initializeSchema(true) .build(); } 在本地运行 Oracle Database 23ai # 然后，您可以使用以下方法连接到数据库：\n访问 Native Client # Oracle Vector Store 实现通过 getNativeClient（） 方法提供对底层原生 Oracle 客户端 （OracleConnection） 的访问：\nOracleVectorStore vectorStore = context.getBean(OracleVectorStore.class); Optional\u0026lt;OracleConnection\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { OracleConnection connection = nativeClient.get(); // Use the native client for Oracle-specific operations } 本机客户端允许您访问特定于 Oracle 的功能和作，这些功能和作可能无法通过 VectorStore 接口公开。\n"},{"id":73,"href":"/docs/model-evaluation/","title":"评估测试","section":"Docs","content":" 评估测试 # 测试 AI 应用程序需要评估生成的内容，以确保 AI 模型没有产生幻觉响应。 评估响应的一种方法是使用 AI 模型本身进行评估。选择最佳 AI 模型进行评估，该模型可能与用于生成响应的模型不同。 用于评估响应的 Spring AI 接口是 Evaluator，定义为： 评估的输入是 EvaluationRequest，定义为\n相关性评估器 # `RelevancyEvaluator``` 是 Evaluator接口的一种实现，旨在评估 AI 生成的响应与提供的上下文的相关性。此评估器通过确定 AI 模型的响应是否与用户对检索到的上下文的输入相关，帮助评估 RAG 流的质量。 评估基于用户输入、AI 模型的响应和上下文信息。它使用提示模板询问 AI 模型响应是否与用户输入和上下文相关。 这是RelevancyEvaluator`` 使用的默认提示模板：\nYour task is to evaluate if the response for the query is in line with the context information provided. You have two options to answer. Either YES or NO. Answer YES, if the response for the query is in line with context information otherwise NO. Query: {query} Response: {response} Context: {context} Answer: 集成测试中的使用 # 以下是在集成测试中使用 RelevancyEvaluator 的示例，使用 RetrievalAugmentationAdvisor 验证 RAG 流的结果：\n@Test void evaluateRelevancy() { String question = \u0026#34;Where does the adventure of Anacletus and Birba take place?\u0026#34;; RetrievalAugmentationAdvisor ragAdvisor = RetrievalAugmentationAdvisor.builder() .documentRetriever(VectorStoreDocumentRetriever.builder() .vectorStore(pgVectorStore) .build()) .build(); ChatResponse chatResponse = ChatClient.builder(chatModel).build() .prompt(question) .advisors(ragAdvisor) .call() .chatResponse(); EvaluationRequest evaluationRequest = new EvaluationRequest( // The original user question question, // The retrieved context from the RAG flow chatResponse.getMetadata().get(RetrievalAugmentationAdvisor.DOCUMENT_CONTEXT), // The AI model\u0026#39;s response chatResponse.getResult().getOutput().getText() ); RelevancyEvaluator evaluator = new RelevancyEvaluator(ChatClient.builder(chatModel)); EvaluationResponse evaluationResponse = evaluator.evaluate(evaluationRequest); assertThat(evaluationResponse.isPass()).isTrue(); } 你可以在 Spring AI 项目中找到几个集成测试，这些测试使用 RelevancyEvaluator 来测试 QuestionAnswerAdvisor（参见 [[[ tests](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration- tests/src/test/java/org/springframework/ai/integration/ tests/client/advisor/QuestionAnswerAdvisorIT.java)](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-[ tests](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration- tests/src/test/java/org/springframework/ai/integration/ tests/client/advisor/QuestionAnswerAdvisorIT.java)/src/test/java/org/springframework/ai/integration/ [tests](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/RetrievalAugmentationAdvisorIT.java)/client/advisor/RetrievalAugmentationAdvisorIT.java)]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-[[[tests](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/RetrievalAugmentationAdvisorIT.java)](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-[tests](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/RetrievalAugmentationAdvisorIT.java)/src/test/java/org/springframework/ai/integration/[tests](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/RetrievalAugmentationAdvisorIT.java)/client/advisor/QuestionAnswerAdvisorIT.java)](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-[[tests](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/RetrievalAugmentationAdvisorIT.java)](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-[tests](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/RetrievalAugmentationAdvisorIT.java)/src/test/java/org/springframework/ai/integration/[tests](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/RetrievalAugmentationAdvisorIT.java)/client/advisor/QuestionAnswerAdvisorIT.java)/src/test/java/org/springframework/ai/integration/[[tests](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/RetrievalAugmentationAdvisorIT.java)](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-[tests](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/RetrievalAugmentationAdvisorIT.java)/src/test/java/org/springframework/ai/integration/[tests](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/RetrievalAugmentationAdvisorIT.java)/client/advisor/QuestionAnswerAdvisorIT.java)/client/advisor/RetrievalAugmentationAdvisorIT.java)/src/test/java/org/springframework/ai/integration/[[[tests](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/RetrievalAugmentationAdvisorIT.java)](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-[tests](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/RetrievalAugmentationAdvisorIT.java)/src/test/java/org/springframework/ai/integration/[tests](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/RetrievalAugmentationAdvisorIT.java)/client/advisor/QuestionAnswerAdvisorIT.java)](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-[[tests](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/RetrievalAugmentationAdvisorIT.java)](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-[tests](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/RetrievalAugmentationAdvisorIT.java)/src/test/java/org/springframework/ai/integration/[tests](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/RetrievalAugmentationAdvisorIT.java)/client/advisor/QuestionAnswerAdvisorIT.java)/src/test/java/org/springframework/ai/integration/[[tests](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/RetrievalAugmentationAdvisorIT.java)](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-[tests](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/RetrievalAugmentationAdvisorIT.java)/src/test/java/org/springframework/ai/integration/[tests](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/RetrievalAugmentationAdvisorIT.java)/client/advisor/QuestionAnswerAdvisorIT.java)/client/advisor/RetrievalAugmentationAdvisorIT.java)/client/advisor/QuestionAnswerAdvisorIT.java)）和 RetrievalAugmentationAdvisor（参见 [[[ tests](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration- tests/src/test/java/org/springframework/ai/integration/ tests/client/advisor/QuestionAnswerAdvisorIT.java)](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-[ tests](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration- tests/src/test/java/org/springframework/ai/integration/ tests/client/advisor/QuestionAnswerAdvisorIT.java)/src/test/java/org/springframework/ai/integration/ [tests](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/RetrievalAugmentationAdvisorIT.java)/client/advisor/RetrievalAugmentationAdvisorIT.java)]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-[[[tests](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/RetrievalAugmentationAdvisorIT.java)](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-[tests](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/RetrievalAugmentationAdvisorIT.java)/src/test/java/org/springframework/ai/integration/[tests](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/RetrievalAugmentationAdvisorIT.java)/client/advisor/QuestionAnswerAdvisorIT.java)](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-[[tests](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/RetrievalAugmentationAdvisorIT.java)](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-[tests](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/RetrievalAugmentationAdvisorIT.java)/src/test/java/org/springframework/ai/integration/[tests](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/RetrievalAugmentationAdvisorIT.java)/client/advisor/QuestionAnswerAdvisorIT.java)/src/test/java/org/springframework/ai/integration/[[tests](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/RetrievalAugmentationAdvisorIT.java)](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-[tests](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/RetrievalAugmentationAdvisorIT.java)/src/test/java/org/springframework/ai/integration/[tests](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/RetrievalAugmentationAdvisorIT.java)/client/advisor/QuestionAnswerAdvisorIT.java)/client/advisor/RetrievalAugmentationAdvisorIT.java)/src/test/java/org/springframework/ai/integration/[[[tests](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/RetrievalAugmentationAdvisorIT.java)](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-[tests](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/RetrievalAugmentationAdvisorIT.java)/src/test/java/org/springframework/ai/integration/[tests](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/RetrievalAugmentationAdvisorIT.java)/client/advisor/QuestionAnswerAdvisorIT.java)](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-[[tests](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/RetrievalAugmentationAdvisorIT.java)](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-[tests](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/RetrievalAugmentationAdvisorIT.java)/src/test/java/org/springframework/ai/integration/[tests](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/RetrievalAugmentationAdvisorIT.java)/client/advisor/QuestionAnswerAdvisorIT.java)/src/test/java/org/springframework/ai/integration/[[tests](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/RetrievalAugmentationAdvisorIT.java)](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-[tests](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/RetrievalAugmentationAdvisorIT.java)/src/test/java/org/springframework/ai/integration/[tests](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-integration-tests/src/test/java/org/springframework/ai/integration/tests/client/advisor/RetrievalAugmentationAdvisorIT.java)/client/advisor/QuestionAnswerAdvisorIT.java)/client/advisor/RetrievalAugmentationAdvisorIT.java)/client/advisor/QuestionAnswerAdvisorIT.java)）的功能。\n自定义模板 # RelevancyEvaluator 使用默认模板提示 AI 模型进行评估。您可以通过 .promptTemplate（） 构建器方法提供自己的 PromptTemplate 对象来自定义此行为。 自定义 PromptTemplate 可以使用任何 TemplateRenderer 实现（默认情况下，它使用基于 [ StringTemplate]( https://www.stringtemplate.org/) 引擎的 StPromptTemplate）。重要的要求是模板必须包含以下占位符：\n用于接收用户问题的查询占位符。 用于接收 AI 模型的响应的响应占位符。 用于接收上下文信息的上下文占位符。 事实核查评估员 # FactCheckingEvaluator 是 Evaluator 接口的另一种实现，旨在根据提供的上下文评估 AI 生成的响应的事实准确性。此评估器通过验证给定的陈述（声明）是否在逻辑上得到提供的上下文（文档）的支持，帮助检测和减少 AI 输出中的幻觉。 “claim”和“document”将提交给 AI 模型进行评估。可以使用专门用于此目的的更小、更高效的 AI 模型，例如 Bespoke 的 Minicheck，与 GPT-4 等旗舰模型相比，它有助于降低执行这些检查的成本。Minicheck 也可通过 Ollama 使用。\n用法 # FactCheckingEvaluator 构造函数将 ChatClient.Builder 作为参数：\npublic FactCheckingEvaluator(ChatClient.Builder chatClientBuilder) { this.chatClientBuilder = chatClientBuilder; } 评估员使用以下提示模板进行事实核查：\nDocument: {document} Claim: {claim} 其中 {document} 是上下文信息，{claim} 是要评估的 AI 模型的响应。\n例 # 以下是如何将 FactCheckingEvaluator 与基于 Ollama 的 ChatModel（特别是 Bespoke-Minicheck 模型）一起使用的示例：\n@Test void testFactChecking() { // Set up the Ollama API OllamaApi ollamaApi = new OllamaApi(\u0026#34;http://localhost:11434\u0026#34;); ChatModel chatModel = new OllamaChatModel(ollamaApi, OllamaOptions.builder().model(BESPOKE_MINICHECK).numPredict(2).temperature(0.0d).build()) // Create the FactCheckingEvaluator var factCheckingEvaluator = new FactCheckingEvaluator(ChatClient.builder(chatModel)); // Example context and claim String context = \u0026#34;The Earth is the third planet from the Sun and the only astronomical object known to harbor life.\u0026#34;; String claim = \u0026#34;The Earth is the fourth planet from the Sun.\u0026#34;; // Create an EvaluationRequest EvaluationRequest evaluationRequest = new EvaluationRequest(context, Collections.emptyList(), claim); // Perform the evaluation EvaluationResponse evaluationResponse = factCheckingEvaluator.evaluate(evaluationRequest); assertFalse(evaluationResponse.isPass(), \u0026#34;The claim should not be supported by the context\u0026#34;); } "},{"id":74,"href":"/docs/vector-databases/pgvector/","title":"None","section":"矢量数据库","content":" None # 本节将指导您设置 PGvector VectorStore 以存储文档嵌入并执行相似性搜索。 [ PGvector]( https://github.com/pgvector/pgvector) 是 PostgreSQL 的开源扩展，支持存储和搜索机器学习生成的嵌入。它提供了不同的功能，使用户能够识别精确和近似的最近邻。它旨在与其他 PostgreSQL 功能无缝协作，包括索引和查询。\n先决条件 # 首先，您需要访问启用了 vector、hstore 和 uuid-ossp 扩展的 PostgreSQL 实例。 启动时，PgVectorStore 将尝试安装所需的数据库扩展，并创建所需的 vector_store 表（如果不存在）。 或者，您可以手动执行此作，如下所示： 接下来，如果需要，请提供 [ EmbeddingModel](../embeddings.html#available-implementations) 的 API 密钥，以生成 PgVectorStore 存储的嵌入。\n自动配置 # 然后将 PgVectorStore 启动启动器依赖项添加到您的项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-pgvector\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-vector-store-pgvector\u0026#39; } 矢量存储实现可以为您初始化所需的架构，但您必须通过在相应的构造函数中指定 initializeSchema 布尔值或设置 ...initialize-schema=true 在 application.properties 文件中。 Vector Store 还需要一个 EmbeddingModel 实例来计算文档的嵌入。您可以选择一个可用的 EmbeddingModel Implementations。 例如，要使用 [ OpenAI EmbeddingModel](../embeddings/openai-embeddings.html)，请将以下依赖项添加到您的项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-openai\u0026#39; } 要连接并配置 PgVectorStore，您需要提供实例的访问详细信息。可以通过 Spring Boot 的 application.yml 提供简单的配置。 现在，您可以在应用程序中自动连接 VectorStore 并使用它\n@Autowired VectorStore vectorStore; // ... List\u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); // Add the documents to PGVector vectorStore.add(documents); // Retrieve documents similar to a query List\u0026lt;Document\u0026gt; results = this.vectorStore.similaritySearch(SearchRequest.builder().query(\u0026#34;Spring\u0026#34;).topK(5).build()); 配置属性 # 您可以在 Spring Boot 配置中使用以下属性来自定义 PGVector 矢量存储。\n元数据筛选 # 您可以将通用的可移植[ 元数据过滤器]( https://docs.spring.io/spring-ai/reference/api/vectordbs.html#_metadata_filters)与 PgVector 存储结合使用。 例如，您可以使用文本表达式语言：\nvectorStore.similaritySearch( SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(\u0026#34;author in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; article_type == \u0026#39;blog\u0026#39;\u0026#34;).build()); 或使用 Filter.Expression DSL 以编程方式：\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(b.and( b.in(\u0026#34;author\u0026#34;,\u0026#34;john\u0026#34;, \u0026#34;jill\u0026#34;), b.eq(\u0026#34;article_type\u0026#34;, \u0026#34;blog\u0026#34;)).build()).build()); 手动配置 # 您可以手动配置 PgVectorStore，而不是使用 Spring Boot 自动配置。为此，您需要将 PostgreSQL 连接和 JdbcTemplate 自动配置依赖项添加到您的项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-jdbc\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.postgresql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;postgresql\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;runtime\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-pgvector-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 要在应用程序中配置 PgVector，您可以使用以下设置：\n@Bean public VectorStore vectorStore(JdbcTemplate jdbcTemplate, EmbeddingModel embeddingModel) { return PgVectorStore.builder(jdbcTemplate, embeddingModel) .dimensions(1536) // Optional: defaults to model dimensions or 1536 .distanceType(COSINE_DISTANCE) // Optional: defaults to COSINE_DISTANCE .indexType(HNSW) // Optional: defaults to HNSW .initializeSchema(true) // Optional: defaults to false .schemaName(\u0026#34;public\u0026#34;) // Optional: defaults to \u0026#34;public\u0026#34; .vectorTableName(\u0026#34;vector_store\u0026#34;) // Optional: defaults to \u0026#34;vector_store\u0026#34; .maxDocumentBatchSize(10000) // Optional: defaults to 10000 .build(); } 在本地运行 Postgres 和 PGVector DB # 您可以像这样连接到此服务器：\n访问 Native Client # PGVector Store 实现通过 getNativeClient（） 方法提供对底层原生 JDBC 客户端（JdbcTemplate）的访问：\nPgVectorStore vectorStore = context.getBean(PgVectorStore.class); Optional\u0026lt;JdbcTemplate\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { JdbcTemplate jdbc = nativeClient.get(); // Use the native client for PostgreSQL-specific operations } 本机客户端允许您访问特定于 PostgreSQL 的功能和作，这些功能和作可能不会通过 VectorStore 接口公开。\n"},{"id":75,"href":"/docs/models/chat-models/ollama/","title":"Ollama 聊天","section":"聊天模型 API","content":" Ollama 聊天 # 使用 [ Ollama]( https://ollama.ai/)，您可以在本地运行各种大型语言模型 （LLM） 并从中生成文本。Spring AI 通过 [[Ollama](https://ollama.ai/)](https://ollama.ai/)ChatModel API 支持 [ Ollama]( https://ollama.ai/) 聊天完成功能。\n先决条件 # 您首先需要访问 Ollama 实例。有几个选项，包括：\n在本地计算机上下载并安装 Ollama。 通过 Testcontainers 配置和运行 Ollama。 通过 Kubernetes 服务绑定绑定到 Ollama 实例。 您可以从 [ Ollama 模型库]( https://ollama.com/library)中提取要在应用程序中使用的模型： ollama pull \u0026lt;model-name\u0026gt; 您还可以提取数千个免费的 [ GGUF 紧贴脸模型]( https://huggingface.co/models?library=gguf\u0026sort=trending)中的任何一个：\nollama pull hf.co/\u0026lt;username\u0026gt;/\u0026lt;model-repository\u0026gt; 或者，您可以启用选项以自动下载任何需要的模型：[ Auto-pull Models （自动拉取模型](#auto-pulling-models) ）。\n自动配置 # Spring AI 为 Ollama 聊天集成提供了 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 或 Gradle build.gradle 构建文件中：\n基本属性 # 前缀 spring.ai.ollama 是配置与 Ollama 的连接的属性前缀。 以下是用于初始化 Ollama 集成和[ 自动拉取模型](#auto-pulling-models)的属性。\n聊天属性 # 前缀 spring.ai.ollama.chat.``options``` 是配置 Ollama 聊天模型的属性前缀。它包括 Ollama 请求（高级）参数，例如 ``model``、``keep-alive`` 和 ``format`` 以及 Ollama ``model`` ``options`` 属性。 以下是 Ollama 聊天模型的高级请求参数： 其余选项`属性基于 [ Ollama Valid Parameters and Values]( https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values) 和 [ Ollama Types]( https://github.com/ollama/ollama/blob/main/api/types.go)。默认值基于 [ Ollama Types]( https://github.com/ollama/ollama/blob/main/api/types.go) Defaults。\n运行时选项 # [ OllamaOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-ollama/src/main/java/org/springframework/ai/ollama/api/[OllamaOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-ollama/src/main/java/org/springframework/ai/ollama/api/OllamaOptions.java)) 类提供模型配置，例如要使用的模型、温度等。 启动时，可以使用 OllamaChatModel（api， options） 构造函数或 spring.ai.ollama.chat.options.* 属性来配置默认选项。 在运行时，您可以通过向 Prompt 调用添加新的特定于请求的选项来覆盖默认选项。例如，要覆盖特定请求的默认型号和温度：\nChatResponse response = chatModel.call( new Prompt( \u0026#34;Generate the names of 5 famous pirates.\u0026#34;, OllamaOptions.builder() .model(OllamaModel.LLAMA3_1) .temperature(0.4) .build() )); 自动拉取模型 # Spring AI Ollama 可以在模型在 Ollama 实例中不可用时自动拉取模型。此功能对于开发和测试以及将应用程序部署到新环境特别有用。 拉取模型有三种策略：\nalways（在 PullModelStrategy.ALWAYS 中定义）：始终拉取模型，即使它已经可用。有助于确保您使用的是最新版本的模型。 when_missing （定义于 PullModelStrategy.WHEN_MISSING ）：仅在模型尚不可用时拉取模型。这可能会导致使用旧版本的模型。 never （defined in PullModelStrategy.NEVER）：从不自动拉取模型。 通过配置属性和默认选项定义的所有模型都可以在启动时自动拉取。您可以使用配置属性配置拉取策略、超时和最大重试次数： spring: ai: ollama: init: pull-model-strategy: always timeout: 60s max-retries: 1 您可以在启动时初始化其他模型，这对于在运行时动态使用的模型非常有用：\nspring: ai: ollama: init: pull-model-strategy: always chat: additional-models: - llama3.2 - qwen2.5 如果只想将拉取策略应用于特定类型的模型，则可以从初始化任务中排除聊天模型：\nspring: ai: ollama: init: pull-model-strategy: always chat: include: false 此配置会将拉取策略应用于除聊天模型之外的所有模型。\n函数调用 # 您可以使用 OllamaChatModel 注册自定义 Java 函数，并让 Ollama 模型智能地选择输出包含参数的 JSON 对象，以调用一个或多个已注册的函数。这是一种将 LLM 功能与外部工具和 API 连接起来的强大技术。阅读有关[ 工具调用](../tools.html)的更多信息。\n模 态 # 多模态是指模型同时理解和处理来自各种来源的信息（包括文本、图像、音频和其他数据格式）的能力。 Ollama 中支持多模态的一些模型是 [ LLaVA]( https://ollama.com/library/llava) 和 Bak[ LLaVA]( https://ollama.com/library/llava)（请参阅[ 完整列表]( https://ollama.com/search?c=vision) ）。有关更多详细信息，请参阅 [ LLaVA]( https://ollama.com/library/llava)：大型语言和视觉助手 。 Ollama [ 消息 API]( https://github.com/ollama/ollama/blob/main/docs/api.md#parameters-1) 提供了一个 “images” 参数，用于将 base64 编码的图像列表与消息合并。 Spring AI 的 [ Message]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/messages/[Message](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/messages/Message.java).java) 接口通过引入 [ Media]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/model/[Media](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/model/Media.java).java) 类型来促进多模态 AI 模型。此类型包含有关消息中媒体附件的数据和详细信息，将 Spring org.springframework.util.MimeType 和 a org.springframework.core.io.Resource 用于原始媒体数据。 下面是一个摘自 [ OllamaChatModelMultimodalIT.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-ollama/src/test/java/org/springframework/ai/ollama/[OllamaChatModelMultimodalIT.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-ollama/src/test/java/org/springframework/ai/ollama/OllamaChatModelMultimodalIT.java)) 的简单代码示例，说明了用户文本与图像的融合。\nvar imageResource = new ClassPathResource(\u0026#34;/multimodal.test.png\u0026#34;); var userMessage = new UserMessage(\u0026#34;Explain what do you see on this picture?\u0026#34;, new Media(MimeTypeUtils.IMAGE_PNG, this.imageResource)); ChatResponse response = chatModel.call(new Prompt(this.userMessage, OllamaOptions.builder().model(OllamaModel.LLAVA)).build()); 该示例显示了一个将 multimodal.test.png 图像作为输入的模型： 以及文本消息 “Explain what do you see on this picture？”，并生成如下响应：\n结构化输出 # Ollama 提供自定义[ 结构化输出]( https://ollama.com/blog/structured-outputs) API，确保您的模型生成的响应严格符合您提供的 JSON 架构 。除了现有的 Spring AI 模型无关的[ 结构化输出]( https://ollama.com/blog/structured-outputs)转换器之外，这些 API 还提供了增强的控制和精度。\n配置 # Spring AI 允许您使用 OllamaOptions 构建器以编程方式配置响应格式。\n使用聊天选项生成器 # 您可以使用 OllamaOptions 构建器以编程方式设置响应格式，如下所示：\nString jsonSchema = \u0026#34;\u0026#34;\u0026#34; { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;steps\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, \u0026#34;items\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;explanation\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;output\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } }, \u0026#34;required\u0026#34;: [\u0026#34;explanation\u0026#34;, \u0026#34;output\u0026#34;], \u0026#34;additionalProperties\u0026#34;: false } }, \u0026#34;final_answer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } }, \u0026#34;required\u0026#34;: [\u0026#34;steps\u0026#34;, \u0026#34;final_answer\u0026#34;], \u0026#34;additionalProperties\u0026#34;: false } \u0026#34;\u0026#34;\u0026#34;; Prompt prompt = new Prompt(\u0026#34;how can I solve 8x + 7 = -23\u0026#34;, OllamaOptions.builder() .model(OllamaModel.LLAMA3_2.getName()) .format(new ObjectMapper().readValue(jsonSchema, Map.class)) .build()); ChatResponse response = this.ollamaChatModel.call(this.prompt); 与 BeanOutputConverter 实用程序集成 # 您可以利用现有的 [ BeanOutputConverter](../structured-output-converter.html#_bean_output_converter) 实用程序从域对象自动生成 JSON 模式，然后将结构化响应转换为特定于域的实例：\nrecord MathReasoning( @JsonProperty(required = true, value = \u0026#34;steps\u0026#34;) Steps steps, @JsonProperty(required = true, value = \u0026#34;final_answer\u0026#34;) String finalAnswer) { record Steps( @JsonProperty(required = true, value = \u0026#34;items\u0026#34;) Items[] items) { record Items( @JsonProperty(required = true, value = \u0026#34;explanation\u0026#34;) String explanation, @JsonProperty(required = true, value = \u0026#34;output\u0026#34;) String output) { } } } var outputConverter = new BeanOutputConverter\u0026lt;\u0026gt;(MathReasoning.class); Prompt prompt = new Prompt(\u0026#34;how can I solve 8x + 7 = -23\u0026#34;, OllamaOptions.builder() .model(OllamaModel.LLAMA3_2.getName()) .format(outputConverter.getJsonSchemaMap()) .build()); ChatResponse response = this.ollamaChatModel.call(this.prompt); String content = this.response.getResult().getOutput().getText(); MathReasoning mathReasoning = this.outputConverter.convert(this.content); OpenAI API 兼容性 # Ollama 与 OpenAI API 兼容，您可以使用 [ Spring AI OpenAI](openai-chat.html) 客户端与 Ollama 交谈并使用工具。为此，您需要将 OpenAI 基本 URL 配置为您的 Ollama 实例： spring.ai.openai.chat.base-url=http://localhost:11434 并选择提供的 Ollama 模型之一： spring.ai.openai.chat.options.model=mistral 。 查看 [ OllamaWithOpenAiChatModelIT.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/chat/proxy/[OllamaWithOpenAiChatModelIT.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/chat/proxy/OllamaWithOpenAiChatModelIT.java)) 测试，了解在 Spring AI OpenAI 上使用 Ollama 的示例。\nHuggingFace 模特 # Ollama 可以开箱即用地访问所有 [ GGUF Hugging Face]( https://huggingface.co/models?library=gguf\u0026sort=trending) 聊天模型。您可以按名称拉取这些模型中的任何一个： ollama pull hf.co/\u0026lt;username\u0026gt;/\u0026lt;model-repository\u0026gt; 或配置自动拉取策略： [ 自动拉取模型](#auto-pulling-models) ：\nspring.ai.ollama.chat.options.model=hf.co/bartowski/gemma-2-2b-it-GGUF spring.ai.ollama.init.pull-model-strategy=always spring.ai.ollama.chat.options.model ：指定要使用的紧贴面部 GGUF 模型 。 spring.ai.ollama.init.pull-model-strategy=always ：（可选）在启动时启用自动拉取模型。对于生产环境，您应该预先下载模型以避免延迟： ollama pull hf.co/bartowski/gemma-2-2b-it-GGUF . 样品控制器 # [ 创建一个新]( https://start.spring.io/)的 Spring Boot 项目，并将 添加到您的 spring-ai-starter-model-ollama pom（或 gradle）依赖项中。 在 src/main/resources 目录下添加一个 application.yaml 文件，以启用和配置 Ollama 聊天模型：\nspring: ai: ollama: base-url: http://localhost:11434 chat: options: model: mistral temperature: 0.7 这将创建一个 OllamaChatModel 实现，您可以将其注入到您的类中。下面是一个简单的 @RestController 类的示例，该类使用 chat 模型生成文本。\n@RestController public class ChatController { private final OllamaChatModel chatModel; @Autowired public ChatController(OllamaChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map\u0026lt;String,String\u0026gt; generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, this.chatModel.call(message)); } @GetMapping(\u0026#34;/ai/generateStream\u0026#34;) public Flux\u0026lt;ChatResponse\u0026gt; generateStream(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { Prompt prompt = new Prompt(new UserMessage(message)); return this.chatModel.stream(prompt); } } 手动配置 # 如果您不想使用 Spring Boot 自动配置，则可以在应用程序中手动配置 Ollama``ChatModel```。OllamaChatModel``` 实现 ChatModel 和 StreamingChatModel，并使用[[低级 OllamaApi 客户端](#low-level-api)](#low-level-api)连接到 Ollama 服务。 要使用它，请将 spring-ai-ollama依赖项添加到项目的 Mavenpom.xml或 Gradlebuild.gradle构建文件中： 接下来，创建一个OllamaChatModel`` 实例，并使用它来发送文本生成请求：\nvar ollamaApi = OllamaApi.builder().build(); var chatModel = OllamaChatModel.builder() .ollamaApi(ollamaApi) .defaultOptions( OllamaOptions.builder() .model(OllamaModel.MISTRAL) .temperature(0.9) .build()) .build(); ChatResponse response = this.chatModel.call( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); // Or with streaming responses Flux\u0026lt;ChatResponse\u0026gt; response = this.chatModel.stream( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); OllamaOptions 提供所有聊天请求的配置信息。\n低级 OllamaApi 客户端 # [ OllamaApi]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-ollama/src/main/java/org/springframework/ai/ollama/api/[OllamaApi](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-ollama/src/main/java/org/springframework/ai/ollama/api/OllamaApi.java).java) 为 [ Ollama Chat Completion API]( https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-chat-completion) [ Ollama Chat Completion API]( https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-chat-completion) 提供了一个轻量级 Java 客户端。 下面的类图说明了 OllamaApi 聊天接口和构建块： OllamaApi ollamaApi = new OllamaApi(\u0026#34;YOUR_HOST:YOUR_PORT\u0026#34;); // Sync request var request = ChatRequest.builder(\u0026#34;orca-mini\u0026#34;) .stream(false) // not streaming .messages(List.of( Message.builder(Role.SYSTEM) .content(\u0026#34;You are a geography teacher. You are talking to a student.\u0026#34;) .build(), Message.builder(Role.USER) .content(\u0026#34;What is the capital of Bulgaria and what is the size? \u0026#34; + \u0026#34;What is the national anthem?\u0026#34;) .build())) .options(OllamaOptions.builder().temperature(0.9).build()) .build(); ChatResponse response = this.ollamaApi.chat(this.request); // Streaming request var request2 = ChatRequest.builder(\u0026#34;orca-mini\u0026#34;) .ttream(true) // streaming .messages(List.of(Message.builder(Role.USER) .content(\u0026#34;What is the capital of Bulgaria and what is the size? \u0026#34; + \u0026#34;What is the national anthem?\u0026#34;) .build())) .options(OllamaOptions.builder().temperature(0.9).build().toMap()) .build(); Flux\u0026lt;ChatResponse\u0026gt; streamingResponse = this.ollamaApi.streamingChat(this.request2); "},{"id":76,"href":"/docs/vector-databases/","title":"矢量数据库","section":"Docs","content":" 矢量数据库 # 矢量数据库是一种特殊类型的数据库，在 AI 应用程序中起着至关重要的作用。 在矢量数据库中，查询不同于传统的关系数据库。它们执行相似性搜索，而不是完全匹配。当给定一个向量作为查询时，向量数据库会返回与查询向量“相似”的向量。有关如何在高级别计算此相似性的更多详细信息，请参阅 [ 向量相似性](vectordbs/understand-vectordbs.html#vectordbs-similarity) . 矢量数据库用于将数据与 AI 模型集成。使用它们的第一步是将您的数据加载到矢量数据库中。然后，当要将用户查询发送到 AI 模型时，首先检索一组类似的文档。然后，这些文档用作用户问题的上下文，并与用户的查询一起发送到 AI 模型。这种技术被称为[ 检索增强生成 （RAG）。](../concepts.html#concept-rag) 以下部分描述了使用多个矢量数据库实现的 Spring AI 接口和一些高级示例用法。 最后一部分旨在揭开向量数据库中相似性搜索的基本方法的神秘面纱。\nAPI 概述 # 本节作为 Spring AI 框架中 VectorStore 接口及其关联类的指南。 Spring AI 提供了一个抽象的 API，用于通过 VectorStore 接口与向量数据库进行交互。 以下是 VectorStore 接口定义：\npublic interface VectorStore extends DocumentWriter { default String getName() { return this.getClass().getSimpleName(); } void add(List\u0026lt;Document\u0026gt; documents); void delete(List\u0026lt;String\u0026gt; idList); void delete(Filter.Expression filterExpression); default void delete(String filterExpression) { ... }; List\u0026lt;Document\u0026gt; similaritySearch(String query); List\u0026lt;Document\u0026gt; similaritySearch(SearchRequest request); default \u0026lt;T\u0026gt; Optional\u0026lt;T\u0026gt; getNativeClient() { return Optional.empty(); } } 和相关的 SearchRequest 构建器：\npublic class SearchRequest { public static final double SIMILARITY_THRESHOLD_ACCEPT_ALL = 0.0; public static final int DEFAULT_TOP_K = 4; private String query = \u0026#34;\u0026#34;; private int topK = DEFAULT_TOP_K; private double similarityThreshold = SIMILARITY_THRESHOLD_ACCEPT_ALL; @Nullable private Filter.Expression filterExpression; public static Builder from(SearchRequest originalSearchRequest) { return builder().query(originalSearchRequest.getQuery()) .topK(originalSearchRequest.getTopK()) .similarityThreshold(originalSearchRequest.getSimilarityThreshold()) .filterExpression(originalSearchRequest.getFilterExpression()); } public static class Builder { private final SearchRequest searchRequest = new SearchRequest(); public Builder query(String query) { Assert.notNull(query, \u0026#34;Query can not be null.\u0026#34;); this.searchRequest.query = query; return this; } public Builder topK(int topK) { Assert.isTrue(topK \u0026gt;= 0, \u0026#34;TopK should be positive.\u0026#34;); this.searchRequest.topK = topK; return this; } public Builder similarityThreshold(double threshold) { Assert.isTrue(threshold \u0026gt;= 0 \u0026amp;\u0026amp; threshold \u0026lt;= 1, \u0026#34;Similarity threshold must be in [0,1] range.\u0026#34;); this.searchRequest.similarityThreshold = threshold; return this; } public Builder similarityThresholdAll() { this.searchRequest.similarityThreshold = 0.0; return this; } public Builder filterExpression(@Nullable Filter.Expression expression) { this.searchRequest.filterExpression = expression; return this; } public Builder filterExpression(@Nullable String textExpression) { this.searchRequest.filterExpression = (textExpression != null) ? new FilterExpressionTextParser().parse(textExpression) : null; return this; } public SearchRequest build() { return this.searchRequest; } } public String getQuery() {...} public int getTopK() {...} public double getSimilarityThreshold() {...} public Filter.Expression getFilterExpression() {...} } 若要将数据插入矢量数据库，请将其封装在 Document 对象中。Document 类封装数据源（如 PDF 或 Word 文档）中的内容，并包含表示为字符串的文本。它还包含键值对形式的元数据，包括文件名等详细信息。 插入向量数据库后，使用嵌入模型将文本内容转换为数字数组或 float[]，称为向量嵌入。嵌入模型（如 [ Word2Vec]( https://en.wikipedia.org/wiki/Word2vec)、[ GLoVE]( https://en.wikipedia.org/wiki/GloVe_(machine_learning)) 和 [ BERT）]( https://en.wikipedia.org/wiki/BERT_(language_model)) 或 OpenAI 的 text-embedding-ada-002 用于将单词、句子或段落转换为这些向量嵌入。 向量数据库的作用是存储和促进这些嵌入的相似性搜索。它本身不会生成嵌入向量。要创建向量嵌入，应使用 EmbeddingModel。 界面中的 similaritySearch 方法允许检索与给定查询字符串类似的文档。可以使用以下参数对这些方法进行微调：\nk：一个整数，指定要返回的相似文档的最大数量。这通常被称为 “top K” 搜索或 “K 最近邻” （KNN）。 threshold：介于 0 到 1 之间的双精度值，其中值越接近 1 表示相似度越高。默认情况下，例如，如果将阈值设置为 0.75，则仅返回相似度高于此值的文档。 Filter.Expression：一个用于传递 Fluent DSL（域特定语言）表达式的类，其功能类似于 SQL 中的“where”子句，但它仅适用于 Document 的元数据键值对。 filterExpression：基于 ANTLR4 的外部 DSL，接受筛选表达式作为字符串。例如，对于 country、year 和 isActive 等元数据键，您可以使用如下表达式： country == \u0026lsquo;UK\u0026rsquo; \u0026amp;\u0026amp; year \u0026gt;= 2020 \u0026amp;\u0026amp; isActive == true. 在 [ Metadata Filters](#metadata-filters) 部分中查找有关 Filter.Expression 的更多信息。 Schema 初始化 # 某些 vector store 要求在使用前初始化其后端 schema。默认情况下，它不会为您初始化。您必须通过为适当的构造函数参数传递布尔值来选择加入，或者如果使用 Spring Boot，则在 application.properties 或 application.yml 中将适当的 initialize-schema 属性设置为 true。有关特定属性名称，请查看您正在使用的矢量存储的文档。\n分批处理策略 # 使用矢量存储时，通常需要嵌入大量文档。虽然一次调用嵌入所有文档似乎很简单，但这种方法可能会导致问题。嵌入模型将文本作为标记处理，并且具有最大标记限制，通常称为上下文窗口大小。此限制限制了可在单个嵌入请求中处理的文本量。尝试在一次调用中嵌入过多的标记可能会导致错误或嵌入被截断。 为了解决此令牌限制，Spring AI 实现了批处理策略。这种方法将大型文档集分解为适合嵌入模型的最大上下文窗口的较小批次。批处理不仅可以解决令牌限制问题，还可以提高性能并更有效地使用 API 速率限制。 Spring AI 通过 BatchingStrategy 接口提供此功能，该接口允许根据其令牌计数在子批处理中处理文档。 核心 BatchingStrategy 接口定义如下：\npublic interface BatchingStrategy { List\u0026lt;List\u0026lt;Document\u0026gt;\u0026gt; batch(List\u0026lt;Document\u0026gt; documents); } 此接口定义了一个方法 batch，该方法采用文档列表并返回文档批次列表。\n默认实现 # Spring AI 提供了一个名为 TokenCountBatchingStrategy 的默认实现。此策略根据文档的令牌计数对文档进行批处理，确保每个批处理不超过计算的最大输入令牌计数。 TokenCountBatchingStrategy 的主要特点： 该策略估计每个文档的令牌计数，在不超过最大输入令牌计数的情况下将它们分组为批次，如果单个文档超过此限制，则引发异常。 您还可以自定义 TokenCountBatchingStrategy 以更好地满足您的特定要求。这可以通过在 Spring Boot @Configuration 类中创建具有自定义参数的新实例来完成。 下面是如何创建自定义 TokenCountBatchingStrategy bean 的示例：\n@Configuration public class EmbeddingConfig { @Bean public BatchingStrategy customTokenCountBatchingStrategy() { return new TokenCountBatchingStrategy( EncodingType.CL100K_BASE, // Specify the encoding type 8000, // Set the maximum input token count 0.1 // Set the reserve percentage ); } } 在此配置中： 默认情况下，此构造函数用于 Document.DEFAULT_CONTENT_FORMATTER 内容格式设置，MetadataMode.NONE 用于元数据处理。如果需要自定义这些参数，可以将 full 构造函数与其他参数一起使用。 定义后，此应用程序中的 EmbeddingModel 实现将自动使用此自定义 TokenCountBatchingStrategy Bean，以替换默认策略。 TokenCountBatchingStrategy 在内部使用 TokenCountEstimator（特别是 JTokkitTokenCountEstimator）来计算令牌计数以实现高效批处理。这可确保根据指定的编码类型进行准确的令牌估计。 此外，TokenCountBatchingStrategy 允许您传入自己的 TokenCountEstimator 接口实现，从而提供了灵活性。此功能使您能够使用根据您的特定需求量身定制的自定义代币计数策略。例如：\nTokenCountEstimator customEstimator = new YourCustomTokenCountEstimator(); TokenCountBatchingStrategy strategy = new TokenCountBatchingStrategy( this.customEstimator, 8000, // maxInputTokenCount 0.1, // reservePercentage Document.DEFAULT_CONTENT_FORMATTER, MetadataMode.NONE ); 使用自动截断 # 某些嵌入模型（如 Vertex AI 文本嵌入）支持 auto_truncate 功能。启用后，模型会以静默方式截断超过最大大小的文本输入并继续处理;禁用后，它会为太大的输入引发显式错误。 在批处理策略中使用自动截断时，您必须为批处理策略配置的输入令牌计数远高于模型的实际最大值。这可以防止批处理策略为大型文档引发异常，从而允许嵌入模型在内部处理截断。\n自动截断的配置 # 启用自动截断时，请将批处理策略的最大输入令牌计数设置为远高于模型的实际限制。这可以防止批处理策略为大型文档引发异常，从而允许嵌入模型在内部处理截断。 以下是将 Vertex AI 与自动截断和自定义 BatchingStrategy 结合使用，然后在 PgVectorStore 中使用它们的示例配置：\n@Configuration public class AutoTruncationEmbeddingConfig { @Bean public VertexAiTextEmbeddingModel vertexAiEmbeddingModel( VertexAiEmbeddingConnectionDetails connectionDetails) { VertexAiTextEmbeddingOptions options = VertexAiTextEmbeddingOptions.builder() .model(VertexAiTextEmbeddingOptions.DEFAULT_MODEL_NAME) .autoTruncate(true) // Enable auto-truncation .build(); return new VertexAiTextEmbeddingModel(connectionDetails, options); } @Bean public BatchingStrategy batchingStrategy() { // Only use a high token limit if auto-truncation is enabled in your embedding model. // Set a much higher token count than the model actually supports // (e.g., 132,900 when Vertex AI supports only up to 20,000) return new TokenCountBatchingStrategy( EncodingType.CL100K_BASE, 132900, // Artificially high limit 0.1 // 10% reserve ); } @Bean public VectorStore vectorStore(JdbcTemplate jdbcTemplate, EmbeddingModel embeddingModel, BatchingStrategy batchingStrategy) { return PgVectorStore.builder(jdbcTemplate, embeddingModel) // other properties omitted here .build(); } } 在此配置中：\n为什么有效 # 这种方法之所以有效，是因为：\n最佳实践 # 使用自动截断时：\n将批处理策略的最大输入令牌计数设置为至少比模型的实际限制大 5-10 倍，以避免批处理策略过早出现异常。 监控日志中是否有来自嵌入模型的截断警告（注意：并非所有模型都会记录截断事件）。 考虑静默截断对嵌入质量的影响。 使用示例文档进行测试，以确保截断的嵌入仍然满足您的要求。 为未来的维护者记录此配置，因为它是非标准的。 Spring Boot 自动配置 # 如果你正在使用 Spring Boot 自动配置，则必须提供一个自定义的 BatchingStrategy Bean 来覆盖 Spring AI 附带的默认 Bean：\n@Bean public BatchingStrategy customBatchingStrategy() { // This bean will override the default BatchingStrategy return new TokenCountBatchingStrategy( EncodingType.CL100K_BASE, 132900, // Much higher than model\u0026#39;s actual limit 0.1 ); } 应用程序上下文中存在此 bean 将自动替换所有向量存储使用的默认批处理策略。\n自定义实现 # 虽然 TokenCountBatchingStrategy 提供了强大的默认实现，但您可以自定义批处理策略以满足您的特定需求。这可以通过 Spring Boot 的自动配置来完成。 要自定义批处理策略，请在 Spring Boot 应用程序中定义 BatchingStrategy Bean：\n@Configuration public class EmbeddingConfig { @Bean public BatchingStrategy customBatchingStrategy() { return new CustomBatchingStrategy(); } } 然后，此应用程序中的 EmbeddingModel 实现将自动使用此自定义 BatchingStrategy。\nVectorStore 实现 # 以下是 VectorStore 接口的可用实现：\nAzure 矢量搜索 - Azure 矢量存储。 Apache Cassandra - Apache Cassandra 矢量存储。 Chroma Vector Store （色度矢量存储 ） - Chroma 矢量存储。 Elasticsearch 矢量存储 - Elasticsearch 矢量存储。 GemFire 矢量存储 - GemFire 矢量存储。 MariaDB 矢量存储 - MariaDB 矢量存储。 Milvus Vector Store - Milvus 矢量存储。 MongoDB Atlas 矢量存储 - MongoDB Atlas 矢量存储。 Neo4j 矢量存储区 - Neo4j 矢量存储区。 OpenSearch 矢量存储 - OpenSearch 矢量存储。 Oracle 矢量存储 - Oracle Database 矢量存储。 PgVector Store - PostgreSQL/PGVector 矢量存储。 Pinecone Vector Store - PineCone 矢量存储。 Qdrant 向量存储 - Qdrant 向量存储。 Redis 矢量存储 - Redis 矢量存储。 SAP Hana 矢量存储 - SAP HANA 矢量存储。 Typesense Vector Store - Typesense 矢量存储。 Weaviate 矢量存储 - Weaviate 矢量存储。 SimpleVectorStore - 持久向量存储的简单实现，适用于教育目的。 未来版本可能支持更多实施。 如果你有一个需要 Spring AI 支持的向量数据库，请在 GitHub 上打开一个问题，或者更好的是，提交一个带有实现的拉取请求。 有关每个 VectorStore 实现的信息，请参阅本章的小节。 示例用法 # 要计算向量数据库的嵌入向量，您需要选择与正在使用的更高级别 AI 模型匹配的嵌入模型。 例如，对于 OpenAI 的 ChatGPT，我们使用 OpenAiEmbeddingModel 和一个名为 text-embedding-ada-002 的模型。 Spring Boot 启动器对 OpenAI 的自动配置使 EmbeddingModel 的实现在 Spring 应用程序上下文中可用于依赖项注入。 将数据加载到向量存储中的一般用法是你在类似批处理的工作中做的事情，首先将数据加载到 Spring AI 的 Document 类中，然后调用 save 方法。 给定对源文件的 String 引用，该源文件表示一个 JSON 文件，其中包含我们要加载到向量数据库中的数据，我们使用 Spring AI 的 JsonReader 加载 JSON 中的特定字段，该字段将它们拆分为小块，然后将这些小块传递给向量存储实现。VectorStore 实现计算嵌入向量并将 JSON 和嵌入向量存储在向量数据库中：\n@Autowired VectorStore vectorStore; void load(String sourceFile) { JsonReader jsonReader = new JsonReader(new FileSystemResource(sourceFile), \u0026#34;price\u0026#34;, \u0026#34;name\u0026#34;, \u0026#34;shortDescription\u0026#34;, \u0026#34;description\u0026#34;, \u0026#34;tags\u0026#34;); List\u0026lt;Document\u0026gt; documents = jsonReader.get(); this.vectorStore.add(documents); } 稍后，当用户问题传递到 AI 模型时，将执行相似性搜索以检索相似文档，然后将这些文档作为用户问题的上下文“塞入”提示中。\nString question = \u0026lt;question from user\u0026gt; List\u0026lt;Document\u0026gt; similarDocuments = store.similaritySearch(this.question); 可以将其他选项传递到 similaritySearch 方法中，以定义要检索的文档数和相似性搜索的阈值。\n元数据过滤器 # 本节介绍可用于查询结果的各种筛选条件。 您可以将类似 SQL 的筛选条件表达式作为 String 传递给其中一个 similaritySearch 重载。 请考虑以下示例：\nFilter.Expression （筛选.表达式） # 您可以使用公开 Fluent API 的 FilterExpressionBuilder 创建 Filter.Expression 的实例。一个简单的示例如下：\nFilterExpressionBuilder b = new FilterExpressionBuilder(); Expression expression = this.b.eq(\u0026#34;country\u0026#34;, \u0026#34;BG\u0026#34;).build(); 您可以使用以下运算符构建复杂的表达式：\nEQUALS: \u0026#39;==\u0026#39; MINUS : \u0026#39;-\u0026#39; PLUS: \u0026#39;+\u0026#39; GT: \u0026#39;\u0026gt;\u0026#39; GE: \u0026#39;\u0026gt;=\u0026#39; LT: \u0026#39;\u0026lt;\u0026#39; LE: \u0026#39;\u0026lt;=\u0026#39; NE: \u0026#39;!=\u0026#39; 您可以使用以下运算符组合表达式：\nAND: \u0026#39;AND\u0026#39; | \u0026#39;and\u0026#39; | \u0026#39;\u0026amp;\u0026amp;\u0026#39;; OR: \u0026#39;OR\u0026#39; | \u0026#39;or\u0026#39; | \u0026#39;||\u0026#39;; 考虑以下示例：\nExpression exp = b.and(b.eq(\u0026#34;genre\u0026#34;, \u0026#34;drama\u0026#34;), b.gte(\u0026#34;year\u0026#34;, 2020)).build(); 您还可以使用以下运算符：\nIN: \u0026#39;IN\u0026#39; | \u0026#39;in\u0026#39;; NIN: \u0026#39;NIN\u0026#39; | \u0026#39;nin\u0026#39;; NOT: \u0026#39;NOT\u0026#39; | \u0026#39;not\u0026#39;; 请考虑以下示例：\nExpression exp = b.and(b.in(\u0026#34;genre\u0026#34;, \u0026#34;drama\u0026#34;, \u0026#34;documentary\u0026#34;), b.not(b.lt(\u0026#34;year\u0026#34;, 2020))).build(); 从 Vector Store 中删除文档 # Vector Store 界面提供了多种删除文档的方法，允许您按特定文档 ID 或使用筛选表达式删除数据。\n按文档 ID 删除 # 删除文档的最简单方法是提供文档 ID 列表：\nvoid delete(List\u0026lt;String\u0026gt; idList); 此方法将删除其 ID 与所提供列表中的 ID 匹配的所有文档。如果列表中的任何 ID 在 store 中不存在，则将被忽略。\n// Create and add document Document document = new Document(\u0026#34;The World is Big\u0026#34;, Map.of(\u0026#34;country\u0026#34;, \u0026#34;Netherlands\u0026#34;)); vectorStore.add(List.of(document)); // Delete document by ID vectorStore.delete(List.of(document.getId())); 按筛选表达式删除 # 对于更复杂的删除条件，您可以使用筛选条件表达式：\nvoid delete(Filter.Expression filterExpression); 此方法接受一个 Filter.Expression 对象，该对象定义应删除哪些文档的条件。当您需要根据文档的元数据属性删除文档时，它特别有用。\n// Create test documents with different metadata Document bgDocument = new Document(\u0026#34;The World is Big\u0026#34;, Map.of(\u0026#34;country\u0026#34;, \u0026#34;Bulgaria\u0026#34;)); Document nlDocument = new Document(\u0026#34;The World is Big\u0026#34;, Map.of(\u0026#34;country\u0026#34;, \u0026#34;Netherlands\u0026#34;)); // Add documents to the store vectorStore.add(List.of(bgDocument, nlDocument)); // Delete documents from Bulgaria using filter expression Filter.Expression filterExpression = new Filter.Expression( Filter.ExpressionType.EQ, new Filter.Key(\u0026#34;country\u0026#34;), new Filter.Value(\u0026#34;Bulgaria\u0026#34;) ); vectorStore.delete(filterExpression); // Verify deletion with search SearchRequest request = SearchRequest.builder() .query(\u0026#34;World\u0026#34;) .filterExpression(\u0026#34;country == \u0026#39;Bulgaria\u0026#39;\u0026#34;) .build(); List\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch(request); // results will be empty as Bulgarian document was deleted Delete by String 筛选表达式 # 为方便起见，您还可以使用基于字符串的筛选表达式删除文档：\nvoid delete(String filterExpression); 此方法在内部将提供的字符串筛选器转换为 Filter.Expression 对象。当您具有字符串格式的筛选条件时，它非常有用。\n// Create and add documents Document bgDocument = new Document(\u0026#34;The World is Big\u0026#34;, Map.of(\u0026#34;country\u0026#34;, \u0026#34;Bulgaria\u0026#34;)); Document nlDocument = new Document(\u0026#34;The World is Big\u0026#34;, Map.of(\u0026#34;country\u0026#34;, \u0026#34;Netherlands\u0026#34;)); vectorStore.add(List.of(bgDocument, nlDocument)); // Delete Bulgarian documents using string filter vectorStore.delete(\u0026#34;country == \u0026#39;Bulgaria\u0026#39;\u0026#34;); // Verify remaining documents SearchRequest request = SearchRequest.builder() .query(\u0026#34;World\u0026#34;) .topK(5) .build(); List\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch(request); // results will only contain the Netherlands document try { vectorStore.delete(\u0026#34;country == \u0026#39;Bulgaria\u0026#39;\u0026#34;); } catch (Exception e) { logger.error(\u0026#34;Invalid filter expression\u0026#34;, e); } // Create initial document (v1) with version metadata Document documentV1 = new Document( \u0026#34;AI and Machine Learning Best Practices\u0026#34;, Map.of( \u0026#34;docId\u0026#34;, \u0026#34;AIML-001\u0026#34;, \u0026#34;version\u0026#34;, \u0026#34;1.0\u0026#34;, \u0026#34;lastUpdated\u0026#34;, \u0026#34;2024-01-01\u0026#34; ) ); // Add v1 to the vector store vectorStore.add(List.of(documentV1)); // Create updated version (v2) of the same document Document documentV2 = new Document( \u0026#34;AI and Machine Learning Best Practices - Updated\u0026#34;, Map.of( \u0026#34;docId\u0026#34;, \u0026#34;AIML-001\u0026#34;, \u0026#34;version\u0026#34;, \u0026#34;2.0\u0026#34;, \u0026#34;lastUpdated\u0026#34;, \u0026#34;2024-02-01\u0026#34; ) ); // First, delete the old version using filter expression Filter.Expression deleteOldVersion = new Filter.Expression( Filter.ExpressionType.AND, Arrays.asList( new Filter.Expression( Filter.ExpressionType.EQ, new Filter.Key(\u0026#34;docId\u0026#34;), new Filter.Value(\u0026#34;AIML-001\u0026#34;) ), new Filter.Expression( Filter.ExpressionType.EQ, new Filter.Key(\u0026#34;version\u0026#34;), new Filter.Value(\u0026#34;1.0\u0026#34;) ) ) ); vectorStore.delete(deleteOldVersion); // Add the new version vectorStore.add(List.of(documentV2)); // Verify only v2 exists SearchRequest request = SearchRequest.builder() .query(\u0026#34;AI and Machine Learning\u0026#34;) .filterExpression(\u0026#34;docId == \u0026#39;AIML-001\u0026#39;\u0026#34;) .build(); List\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch(request); // results will contain only v2 of the document 您还可以使用字符串筛选条件表达式完成相同的作：\n// Delete old version using string filter vectorStore.delete(\u0026#34;docId == \u0026#39;AIML-001\u0026#39; AND version == \u0026#39;1.0\u0026#39;\u0026#34;); // Add new version vectorStore.add(List.of(documentV2)); 删除文档时的性能注意事项 # 当您确切知道要删除哪些文档时，按 ID 列表删除通常会更快。 基于过滤器的删除可能需要扫描索引以查找匹配的文档;但是，这是特定于 Vector Store 实现的。 应批量执行大型删除作，以避免系统不堪重负。 根据文档属性删除时，请考虑使用筛选表达式，而不是先收集 ID。 了解向量 # 了解向量\n"},{"id":77,"href":"/docs/observability/","title":"可观察性","section":"Docs","content":" 可观察性 # Spring AI 基于 Spring 生态系统中的可观察性功能构建，以提供对 AI 相关作的见解。Spring AI 为其核心组件提供指标和跟踪功能：ChatClient（包括 Advisor）、 ChatModel、EmbeddingModel、ImageModel 和 VectorStore。\n聊天客户端 # spring.ai.chat.client 在调用 ChatClient 调用（） 或 stream（） 作时记录观察结果。它们测量执行调用所花费的时间并传播相关的跟踪信息。\n提示内容 # ChatClient 提示内容通常很大，并且可能包含敏感信息。由于这些原因，默认情况下不会导出它。 Spring AI 支持记录提示内容以帮助进行调试和故障排除。\n输入数据 （已弃用） # ChatClient 输入数据通常很大，并且可能包含敏感信息。由于这些原因，默认情况下不会导出它。 Spring AI 支持记录 Importing 数据以帮助进行调试和故障排除。\n聊天客户顾问 # spring.ai.advisor 观察值在执行 advisor 时记录。它们测量在 advisor 中花费的时间（包括在内部 advisor 上花费的时间）并传播相关的跟踪信息。\n聊天模型 # gen_ai.client.operation 的观察结果是在调用 ChatModel call 或 stream 方法时记录的。它们测量方法完成所花费的时间并传播相关的跟踪信息。\n聊天提示和完成数据 # 聊天提示和完成数据通常很大，并且可能包含敏感信息。由于这些原因，默认情况下不会导出它。 Spring AI 支持记录聊天提示和完成数据，这对于故障排除场景非常有用。当跟踪可用时，日志将包含跟踪信息，以便更好地关联。\n工具调用 # spring.ai.tool 在聊天模型交互的上下文中执行工具调用时，会记录观察结果。它们测量收费呼叫完成所花费的时间，并传播相关的跟踪信息。\n工具调用参数和结果数据 # 默认情况下，不会导出工具调用的输入参数和结果，因为它们可能很敏感。 Spring AI 支持将工具调用参数和结果数据导出为 span 属性。 gen_ai.client.operation 观察结果记录在嵌入模型方法调用时。它们测量方法完成所花费的时间并传播相关的跟踪信息。\n图像模型 # gen_ai.client.operation 观察结果记录在图像模型方法调用中。它们测量方法完成所花费的时间并传播相关的跟踪信息。\n图像提示数据 # 图像提示数据通常很大，并且可能包含敏感信息。由于这些原因，默认情况下不会导出它。 Spring AI 支持记录图像提示数据，这对于故障排除场景非常有用。当跟踪可用时，日志将包含跟踪信息，以便更好地关联。\n矢量存储 # Spring AI 中的所有向量存储实现都经过检测，以通过 Micrometer 提供指标和分布式跟踪数据。 db.vector.client.operation 在与 Vector Store 交互时记录观察结果。它们测量查询、 添加或删除``作所``花费的时间，并传播相关的跟踪信息。\n响应数据 # 向量搜索响应数据通常很大，并且可能包含敏感信息。由于这些原因，默认情况下不会导出它。 Spring AI 支持记录向量搜索响应数据，这对于故障排除场景非常有用。当跟踪可用时，日志将包含跟踪信息，以便更好地关联。\n"},{"id":78,"href":"/docs/models/chat-models/perplexity-ai/","title":"困惑聊天","section":"聊天模型 API","content":" 困惑聊天 # [ Perplexity AI]( https://perplexity.ai/) 提供独特的 AI 服务，将其语言模型与实时搜索功能集成在一起。它提供了多种模型，并支持对话式 AI 的流式响应。 Spring AI 通过重用现有的 [ OpenAI](openai-chat.html) 客户端与 Perplexity AI 集成。要开始使用，您需要获取 [ Perplexity API 密钥]( https://docs.perplexity.ai/guides/getting-started) ，配置基本 URL，然后选择一个受支持的[ 模型]( https://docs.perplexity.ai/guides/model-cards) 。 查看 [ PerplexityWithOpenAiChatModelIT.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/chat/proxy/[PerplexityWithOpenAiChatModelIT.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/chat/proxy/PerplexityWithOpenAiChatModelIT.java)) 测试，了解将 Perplexity 与 Spring AI 结合使用的示例。\n先决条件 # 创建 API 密钥 ：访问此处创建 API 密钥。使用 Spring AI 项目中的 spring.ai.openai.api-key 属性对其进行配置。 设置 Perplexity Base URL：将 spring.ai.openai.base-url 属性设置为 api.perplexity.ai。 Select a Perplexity Model（选择困惑度模型 ）：使用属性 spring.ai.openai.chat.model= 指定模型。有关可用选项，请参阅支持的型号 。 设置聊天完成路径 ：设置为 spring.ai.openai.chat.completions-path /chat/completions。有关更多详细信息，请参阅 聊天完成 API。 您可以在 application.properties 文件中设置这些配置属性： spring.ai.openai.api-key=\u0026lt;your-perplexity-api-key\u0026gt; spring.ai.openai.base-url=https://api.perplexity.ai spring.ai.openai.chat.model=llama-3.1-sonar-small-128k-online spring.ai.openai.chat.completions-path=/chat/completions 为了在处理 API 密钥等敏感信息时增强安全性，您可以使用 Spring 表达式语言 （SpEL） 来引用自定义环境变量：\n# In application.yml spring: ai: openai: api-key: ${PERPLEXITY_API_KEY} base-url: ${PERPLEXITY_BASE_URL} chat: model: ${PERPLEXITY_MODEL} completions-path: ${PERPLEXITY_COMPLETIONS_PATH} # In your environment or .env file export PERPLEXITY_API_KEY=\u0026lt;your-perplexity-api-key\u0026gt; export PERPLEXITY_BASE_URL=https://api.perplexity.ai export PERPLEXITY_MODEL=llama-3.1-sonar-small-128k-online export PERPLEXITY_COMPLETIONS_PATH=/chat/completions 您还可以在应用程序代码中以编程方式设置这些配置：\n// Retrieve configuration from secure sources or environment variables String apiKey = System.getenv(\u0026#34;PERPLEXITY_API_KEY\u0026#34;); String baseUrl = System.getenv(\u0026#34;PERPLEXITY_BASE_URL\u0026#34;); String model = System.getenv(\u0026#34;PERPLEXITY_MODEL\u0026#34;); String completionsPath = System.getenv(\u0026#34;PERPLEXITY_COMPLETIONS_PATH\u0026#34;); 添加存储库和 BOM # Spring AI 工件发布在 Maven Central 和 Spring Snapshot 存储库中。请参阅 [ Artifact Repositories](../../getting-started.html#artifact-repositories) 部分，将这些存储库添加到您的构建系统中。 为了帮助进行[ 依赖项管理](../../getting-started.html#dependency-management)，Spring AI 提供了一个 BOM（物料清单），以确保在整个项目中使用一致的 Spring AI 版本。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # Spring AI 为 OpenAI Chat 客户端提供 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 或 Gradle build.gradle 构建文件中：\n聊天属性 # 重试属性 # 前缀 spring.ai.retry 用作属性前缀，允许您为 OpenAI 聊天模型配置重试机制。\n连接属性 # 前缀 spring.ai.openai 用作允许您连接到 OpenAI 的属性前缀。\n配置属性 # 前缀 spring.ai.openai.chat 是属性前缀，允许您为 OpenAI 配置聊天模型实现。\n运行时选项 # [ OpenAiChatOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/[OpenAiChatOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/OpenAiChatOptions.java)) 提供模型配置，例如要使用的模型、温度、频率损失等。 启动时，可以使用 OpenAiChatModel（api， options） 构造函数或 spring.ai.openai.chat.options.* 属性配置默认选项。 在运行时，您可以通过向 Prompt 调用添加新的、特定于请求的选项来覆盖默认选项。例如，要覆盖特定请求的默认模型和温度：\nChatResponse response = chatModel.call( new Prompt( \u0026#34;Generate the names of 5 famous pirates.\u0026#34;, OpenAiChatOptions.builder() .model(\u0026#34;llama-3.1-sonar-large-128k-online\u0026#34;) .temperature(0.4) .build() )); 函数调用 # 模 态 # 样品控制器 # [ 创建一个新]( https://start.spring.io/)的 Spring Boot 项目，并将 添加到您的 spring-ai-starter-model-openai pom（或 gradle）依赖项中。 在 src/main/resources 目录下添加一个 application.properties 文件，以启用和配置 OpenAi 聊天模型：\nspring.ai.openai.api-key=\u0026lt;PERPLEXITY_API_KEY\u0026gt; spring.ai.openai.base-url=https://api.perplexity.ai spring.ai.openai.chat.completions-path=/chat/completions spring.ai.openai.chat.options.model=llama-3.1-sonar-small-128k-online spring.ai.openai.chat.options.temperature=0.7 # The Perplexity API doesn\u0026#39;t support embeddings, so we need to disable it. spring.ai.openai.embedding.enabled=false 这将创建一个 OpenAiChatModel 实现，您可以将其注入到您的类中。下面是一个简单的 @Controller 类示例，该类使用 chat 模型生成文本。\n@RestController public class ChatController { private final OpenAiChatModel chatModel; @Autowired public ChatController(OpenAiChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, this.chatModel.call(message)); } @GetMapping(\u0026#34;/ai/generateStream\u0026#34;) public Flux\u0026lt;ChatResponse\u0026gt; generateStream(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { Prompt prompt = new Prompt(new UserMessage(message)); return this.chatModel.stream(prompt); } } 支持的型号 # Perplexity 支持多个针对搜索增强型对话式 AI 优化的模型。有关详细信息，请参阅[ 支持的型号]( https://docs.perplexity.ai/guides/model-cards) 。\n引用 # 文档主页 API 参考 开始 速率限制 "},{"id":79,"href":"/docs/vector-databases/pinecone/","title":"松果","section":"矢量数据库","content":" 松果 # 本节将引导您设置 Pinecone VectorStore 以存储文档嵌入并执行相似性搜索。 [ Pinecone]( https://www.pinecone.io/) 是一种流行的基于云的矢量数据库，可让您有效地存储和搜索矢量。\n先决条件 # 要设置 PineconeVectorStore，请从您的 Pinecone 账户收集以下详细信息：\n松果 API 密钥 松果指数名称 Pinecone 命名空间 自动配置 # Spring AI 为 Pinecone Vector Store 提供 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-pinecone\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-vector-store-pinecone\u0026#39; } 此外，您将需要一个已配置的 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) bean。请参阅 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) 部分以了解更多信息。 以下是所需 bean 的示例：\n@Bean public EmbeddingModel embeddingModel() { // Can be any other EmbeddingModel implementation. return new OpenAiEmbeddingModel(new OpenAiApi(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;))); } 要连接到 Pinecone，您需要提供实例的访问详细信息。可以通过 Spring Boot 的 application.properties 提供简单的配置，\nspring.ai.vectorstore.pinecone.apiKey=\u0026lt;your api key\u0026gt; spring.ai.vectorstore.pinecone.index-name=\u0026lt;your index name\u0026gt; # API key if needed, e.g. OpenAI spring.ai.openai.api.key=\u0026lt;api-key\u0026gt; 请查看 vector store 的[ 配置参数](#_configuration_properties)列表，了解默认值和配置选项。 现在，您可以在应用程序中自动连接 Pinecone Vector Store 并使用它\n@Autowired VectorStore vectorStore; // ... List \u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); // Add the documents vectorStore.add(documents); // Retrieve documents similar to a query List\u0026lt;Document\u0026gt; results = this.vectorStore.similaritySearch(SearchRequest.builder().query(\u0026#34;Spring\u0026#34;).topK(5).build()); 配置属性 # 您可以在 Spring Boot 配置中使用以下属性来自定义 Pinecone 矢量存储。\n元数据筛选 # 您可以将通用的可移植[ 元数据过滤器]( https://docs.spring.io/spring-ai/reference/api/vectordbs.html#_metadata_filters)与 Pinecone 商店结合使用。 例如，您可以使用文本表达式语言：\nvectorStore.similaritySearch( SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(\u0026#34;author in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; article_type == \u0026#39;blog\u0026#39;\u0026#34;).build()); 或使用 Filter.Expression DSL 以编程方式：\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(b.and( b.in(\u0026#34;author\u0026#34;,\u0026#34;john\u0026#34;, \u0026#34;jill\u0026#34;), b.eq(\u0026#34;article_type\u0026#34;, \u0026#34;blog\u0026#34;)).build()).build()); 手动配置 # 如果你更喜欢手动配置 PineconeVectorStore，你可以使用 PineconeVectorStore#Builder 来实现。 将这些依赖项添加到您的项目中：\nOpenAI：计算嵌入时需要。 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 松果 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-pinecone-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 示例代码 # 要在应用程序中配置 Pinecone，您可以使用以下设置：\n@Bean public VectorStore pineconeVectorStore(EmbeddingModel embeddingModel) { return PineconeVectorStore.builder(embeddingModel) .apiKey(PINECONE_API_KEY) .indexName(PINECONE_INDEX_NAME) .namespace(PINECONE_NAMESPACE) // the free tier doesn\u0026#39;t support namespaces. .contentFieldName(CUSTOM_CONTENT_FIELD_NAME) // optional field to store the original content. Defaults to `document_content` .build(); } 在您的主代码中，创建一些文档：\nList\u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); 将文档添加到 Pinecone：\nvectorStore.add(documents); 最后，检索类似于查询的文档：\nList\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch(SearchRequest.query(\u0026#34;Spring\u0026#34;).topK(5).build()); 如果一切顺利，您应该检索包含文本 “Spring AI rocks！！” 的文档。\n访问 Native Client # Pinecone Vector Store 实现通过 getNativeClient（） 方法提供对底层原生 Pinecone 客户端 （PineconeConnection） 的访问：\nPineconeVectorStore vectorStore = context.getBean(PineconeVectorStore.class); Optional\u0026lt;PineconeConnection\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { PineconeConnection client = nativeClient.get(); // Use the native client for Pinecone-specific operations } 本机客户端允许您访问特定于 Pinecone 的功能和作，这些功能和作可能不会通过 VectorStore 接口公开。\n"},{"id":80,"href":"/docs/development-time-services/","title":"None","section":"Docs","content":" None # Spring AI 提供了 Spring Boot 自动配置，用于建立与通过 Docker Compose 运行的模型服务或矢量存储的连接。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 文件中： 或您的 Gradle build.gradle 构建文件。\n服务连接 # 该 spring-ai-spring-boot-docker-compose 模块中提供了以下服务连接工厂：\n"},{"id":81,"href":"/docs/vector-databases/qdrant/","title":"None","section":"矢量数据库","content":" None # 本节将指导您设置 Qdrant VectorStore 以存储文档嵌入并执行相似性搜索。 [ Qdrant]( https://www.qdrant.tech/) 是一个开源、高性能的矢量搜索引擎/数据库。它使用 HNSW （Hierarchical Navigable Small World） 算法进行高效的 k-NN 搜索作，并为基于元数据的查询提供高级过滤功能。\n先决条件 # Qdrant 实例：按照 Qdrant 文档中的安装说明设置 Qdrant 实例。 如果需要，EmbeddingModel 的 API 密钥，用于生成 QdrantVectorStore 存储的嵌入。 自动配置 # Spring AI 为 Qdrant Vector Store 提供 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-qdrant\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-vector-store-qdrant\u0026#39; } 请查看 vector store 的[ 配置参数](#qdrant-vectorstore-properties)列表，了解默认值和配置选项。 矢量存储实现可以为您初始化必要的架构，但您必须通过在生成器中指定 initializeSchema 布尔值或设置 ...initialize-schema=true 在 application.properties 文件中。 此外，您将需要一个已配置的 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) bean。请参阅 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) 部分以了解更多信息。 现在，您可以将 QdrantVectorStore 作为应用程序中的矢量存储进行自动连接。\n@Autowired VectorStore vectorStore; // ... List\u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); // Add the documents to Qdrant vectorStore.add(documents); // Retrieve documents similar to a query List\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch(SearchRequest.builder().query(\u0026#34;Spring\u0026#34;).topK(5).build()); 配置属性 # 要连接到 Qdrant 并使用 QdrantVectorStore，您需要提供实例的访问详细信息。可以通过 Spring Boot 的 application.yml 提供一个简单的配置：\nspring: ai: vectorstore: qdrant: host: \u0026lt;qdrant host\u0026gt; port: \u0026lt;qdrant grpc port\u0026gt; api-key: \u0026lt;qdrant api key\u0026gt; collection-name: \u0026lt;collection name\u0026gt; use-tls: false initialize-schema: true 以 开头的属性 spring.ai.vectorstore.qdrant.* 用于配置 QdrantVectorStore：\n手动配置 # 您可以手动配置 Qdrant 向量存储，而不是使用 Spring Boot 自动配置。为此，您需要将 spring-ai-qdrant-store 添加到您的项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-qdrant-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-qdrant-store\u0026#39; } 创建 Qdrant 客户端 Bean：\n@Bean public QdrantClient qdrantClient() { QdrantGrpcClient.Builder grpcClientBuilder = QdrantGrpcClient.newBuilder( \u0026#34;\u0026lt;QDRANT_HOSTNAME\u0026gt;\u0026#34;, \u0026lt;QDRANT_GRPC_PORT\u0026gt;, \u0026lt;IS_TLS\u0026gt;); grpcClientBuilder.withApiKey(\u0026#34;\u0026lt;QDRANT_API_KEY\u0026gt;\u0026#34;); return new QdrantClient(grpcClientBuilder.build()); } 然后使用构建器模式创建 QdrantVectorStore Bean：\n@Bean public VectorStore vectorStore(QdrantClient qdrantClient, EmbeddingModel embeddingModel) { return QdrantVectorStore.builder(qdrantClient, embeddingModel) .collectionName(\u0026#34;custom-collection\u0026#34;) // Optional: defaults to \u0026#34;vector_store\u0026#34; .initializeSchema(true) // Optional: defaults to false .batchingStrategy(new TokenCountBatchingStrategy()) // Optional: defaults to TokenCountBatchingStrategy .build(); } // This can be any EmbeddingModel implementation @Bean public EmbeddingModel embeddingModel() { return new OpenAiEmbeddingModel(new OpenAiApi(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;))); } 元数据筛选 # 您也可以将通用的可移植[ 元数据过滤器](../vectordbs.html#metadata-filters)与 Qdrant 存储结合使用。 例如，您可以使用文本表达式语言：\nvectorStore.similaritySearch( SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(\u0026#34;author in [\u0026#39;john\u0026#39;, \u0026#39;jill\u0026#39;] \u0026amp;\u0026amp; article_type == \u0026#39;blog\u0026#39;\u0026#34;).build()); 或使用 Filter.Expression DSL 以编程方式：\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(b.and( b.in(\u0026#34;author\u0026#34;, \u0026#34;john\u0026#34;, \u0026#34;jill\u0026#34;), b.eq(\u0026#34;article_type\u0026#34;, \u0026#34;blog\u0026#34;)).build()).build()); 访问 Native Client # Qdrant Vector Store 实现通过 getNativeClient（） 方法提供对底层原生 Qdrant 客户端 （QdrantClient） 的访问：\nQdrantVectorStore vectorStore = context.getBean(QdrantVectorStore.class); Optional\u0026lt;QdrantClient\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { QdrantClient client = nativeClient.get(); // Use the native client for Qdrant-specific operations } 本机客户端允许您访问特定于 Qdrant 的功能和作，这些功能和作可能无法通过 VectorStore 接口公开。\n"},{"id":82,"href":"/docs/models/chat-models/openai/","title":"OpenAI 聊天","section":"聊天模型 API","content":" OpenAI 聊天 # Spring AI 支持来自 OpenAI 的各种 AI 语言模型，OpenAI 是 ChatGPT 背后的公司，由于其创建了行业领先的文本生成模型和嵌入，该公司在激发人们对 AI 驱动的文本生成的兴趣方面发挥了重要作用。\n先决条件 # 您需要使用 OpenAI 创建一个 API 才能访问 ChatGPT 模型。 在 [ OpenAI 注册页面]( https://platform.openai.com/signup)创建一个帐户，并在 [ API 密钥页面上]( https://platform.openai.com/account/api-keys)生成令牌。 Spring AI 项目定义了一个名为 spring.ai.openai.api-key 的配置属性，您应该将其设置为从 openai.com 获取的 API 密钥的值。 您可以在 application.properties 文件中设置此配置属性：\nspring.ai.openai.api-key=\u0026lt;your-openai-api-key\u0026gt; 为了在处理 API 密钥等敏感信息时增强安全性，您可以使用 Spring 表达式语言 （SpEL） 来引用自定义环境变量：\n# In application.yml spring: ai: openai: api-key: ${OPENAI_API_KEY} # In your environment or .env file export OPENAI_API_KEY=\u0026lt;your-openai-api-key\u0026gt; 您还可以在应用程序代码中以编程方式设置此配置：\n// Retrieve API key from a secure source or environment variable String apiKey = System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;); 添加存储库和 BOM # Spring AI 工件发布在 Maven Central 和 Spring Snapshot 存储库中。请参阅 [ Artifact Repositories](../../getting-started.html#artifact-repositories) 部分，将这些存储库添加到您的构建系统中。 为了帮助进行[ 依赖项管理](../../getting-started.html#dependency-management)，Spring AI 提供了一个 BOM（物料清单），以确保在整个项目中使用一致的 Spring AI 版本。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # Spring AI 为 OpenAI Chat 客户端提供 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 或 Gradle build.gradle 构建文件中：\n聊天属性 # 重试属性 # 前缀 spring.ai.retry 用作属性前缀，允许您为 OpenAI 聊天模型配置重试机制。\n连接属性 # 前缀 spring.ai.openai 用作允许您连接到 OpenAI 的属性前缀。\n配置属性 # 前缀 spring.ai.openai.chat 是属性前缀，允许您为 OpenAI 配置聊天模型实现。\n运行时选项 # [ OpenAiChatOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/[OpenAiChatOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/OpenAiChatOptions.java)) 类提供模型配置，例如要使用的模型、温度、频率损失等。 启动时，可以使用 OpenAiChatModel（api， options） 构造函数或 spring.ai.openai.chat.options.* 属性配置默认选项。 在运行时，您可以通过向 Prompt 调用添加新的特定于请求的选项来覆盖默认选项。例如，要覆盖特定请求的默认型号和温度：\nChatResponse response = chatModel.call( new Prompt( \u0026#34;Generate the names of 5 famous pirates.\u0026#34;, OpenAiChatOptions.builder() .model(\u0026#34;gpt-4o\u0026#34;) .temperature(0.4) .build() )); 函数调用 # 您可以使用 OpenAiChatModel 注册自定义 Java 函数，并让 OpenAI 模型智能地选择输出包含参数的 JSON 对象，以调用一个或多个已注册的函数。这是一种将 LLM 功能与外部工具和 API 连接起来的强大技术。阅读有关[ 工具调用](../tools.html)的更多信息。\n模 态 # 多模态是指模型同时理解和处理来自各种来源的信息（包括文本、图像、音频和其他数据格式）的能力。OpenAI 支持文本、视觉和音频输入模式。\n视觉 # 提供[ 视觉]( https://platform.openai.com/docs/guides/vision)多模式支持的 OpenAI 模型包括 gpt-4、gpt-4o 和 gpt-4o-mini。有关更多信息，请参阅[ 视觉]( https://platform.openai.com/docs/guides/vision)指南。 OpenAI [ 用户消息 API]( https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages) 可以将 base64 编码的图像列表或图像 url 与消息合并。Spring AI 的 [ Message]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/messages/[Message](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/messages/Message.java).java) 接口通过引入 [ Media]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/model/[Media](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/model/Media.java).java) 类型来促进多模态 AI 模型。此类型包含有关消息中媒体附件的数据和详细信息，将 Spring org.springframework.util.MimeType 和 a org.springframework.core.io.Resource 用于原始媒体数据。 下面是摘自 [ OpenAiChatModelIT.java]( https://github.com/spring-projects/spring-ai/blob/c9a3e66f90187ce7eae7eb78c462ec622685de6c/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/chat/[OpenAiChatModelIT.java](https://github.com/spring-projects/spring-ai/blob/c9a3e66f90187ce7eae7eb78c462ec622685de6c/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/chat/OpenAiChatModelIT.java#L293)#L293) 的代码示例，说明了使用 gpt-4o 模型将用户文本与图像融合。\nvar imageResource = new ClassPathResource(\u0026#34;/multimodal.test.png\u0026#34;); var userMessage = new UserMessage(\u0026#34;Explain what do you see on this picture?\u0026#34;, new Media(MimeTypeUtils.IMAGE_PNG, this.imageResource)); ChatResponse response = chatModel.call(new Prompt(this.userMessage, OpenAiChatOptions.builder().model(OpenAiApi.ChatModel.GPT_4_O.getValue()).build())); 或使用 gpt-4o 模型的等效图像 URL：\nvar userMessage = new UserMessage(\u0026#34;Explain what do you see on this picture?\u0026#34;, new Media(MimeTypeUtils.IMAGE_PNG, URI.create(\u0026#34;https://docs.spring.io/spring-ai/reference/_images/multimodal.test.png\u0026#34;))); ChatResponse response = chatModel.call(new Prompt(this.userMessage, OpenAiChatOptions.builder().model(OpenAiApi.ChatModel.GPT_4_O.getValue()).build())); 该示例显示了一个将 multimodal.test.png 图像作为输入的模型： 以及文本消息 “Explain what do you see on this picture？”，并生成如下响应：\n音频 # 提供输入[ 音频]( https://platform.openai.com/docs/guides/audio)多模态支持的 OpenAI 模型包括 gpt-4o-audio-preview。请参阅 [ 音频]( https://platform.openai.com/docs/guides/audio)指南 以了解更多信息。 OpenAI [ 用户消息 API]( https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages) 可以将 base64 编码的音频文件列表与消息合并。Spring AI 的 [ Message]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/messages/[Message](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/messages/Message.java).java) 接口通过引入 [ Media]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-client-chat/src/main/java/org/springframework/ai/chat/messages/[Media](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-client-chat/src/main/java/org/springframework/ai/chat/messages/Media.java).java) 类型来促进多模态 AI 模型。此类型包含有关消息中媒体附件的数据和详细信息，将 Spring org.springframework.util.MimeType 和 a org.springframework.core.io.Resource 用于原始媒体数据。目前 OpenAI 仅支持 audio/mp3 和 audio/wav 两种媒体类型。 下面是摘自 [ OpenAiChatModelIT.java]( https://github.com/spring-projects/spring-ai/blob/c9a3e66f90187ce7eae7eb78c462ec622685de6c/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/chat/[OpenAiChatModelIT.java](https://github.com/spring-projects/spring-ai/blob/c9a3e66f90187ce7eae7eb78c462ec622685de6c/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/chat/OpenAiChatModelIT.java#L442)#L442) 的代码示例，说明了使用 gpt-4o-audio-preview 模型将用户文本与音频文件融合在一起。\nvar audioResource = new ClassPathResource(\u0026#34;speech1.mp3\u0026#34;); var userMessage = new UserMessage(\u0026#34;What is this recording about?\u0026#34;, List.of(new Media(MimeTypeUtils.parseMimeType(\u0026#34;audio/mp3\u0026#34;), audioResource))); ChatResponse response = chatModel.call(new Prompt(List.of(userMessage), OpenAiChatOptions.builder().model(OpenAiApi.ChatModel.GPT_4_O_AUDIO_PREVIEW).build())); 输出音频 # 提供输入[ 音频]( https://platform.openai.com/docs/guides/audio)多模态支持的 OpenAI 模型包括 gpt-4o-audio-preview。请参阅 [ 音频]( https://platform.openai.com/docs/guides/audio)指南 以了解更多信息。 OpenAI [ Assystant 消息 API]( https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages) 可以包含带有消息的 base64 编码音频文件列表。Spring AI 的 [ Message]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/messages/[Message](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/messages/Message.java).java) 接口通过引入 [ Media]( https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/messages/[Media](https://github.com/spring-projects/spring-ai/blob/main/spring-ai-model/src/main/java/org/springframework/ai/chat/messages/Media.java).java) 类型来促进多模态 AI 模型。此类型包含有关消息中媒体附件的数据和详细信息，将 Spring org.springframework.util.MimeType 和 a org.springframework.core.io.Resource 用于原始媒体数据。目前 OpenAI 仅支持 audio/mp3 和 audio/wav 音频类型。 下面是一个代码示例，它使用 gpt-4o-audio-preview 模型说明了用户文本以及音频字节数组的响应：\nvar userMessage = new UserMessage(\u0026#34;Tell me joke about Spring Framework\u0026#34;); ChatResponse response = chatModel.call(new Prompt(List.of(userMessage), OpenAiChatOptions.builder() .model(OpenAiApi.ChatModel.GPT_4_O_AUDIO_PREVIEW) .outputModalities(List.of(\u0026#34;text\u0026#34;, \u0026#34;audio\u0026#34;)) .outputAudio(new AudioParameters(Voice.ALLOY, AudioResponseFormat.WAV)) .build())); String text = response.getResult().getOutput().getContent(); // audio transcript byte[] waveAudio = response.getResult().getOutput().getMedia().get(0).getDataAsByteArray(); // audio data 您必须在 OpenAiChatOptions 中指定音频模态才能生成音频输出。AudioParameters 类为音频输出提供语音和音频格式。\n结构化输出 # OpenAI 提供自定义[ 结构化输出]( https://platform.openai.com/docs/guides/structured-outputs) API，确保您的模型生成的响应严格符合您提供的 JSON 架构 。除了现有的 Spring AI 模型无关的[ 结构化输出]( https://platform.openai.com/docs/guides/structured-outputs)转换器之外，这些 API 还提供了增强的控制和精度。\n配置 # Spring AI 允许您使用 OpenAiChatOptions 构建器以编程方式或通过应用程序属性配置响应格式。\n使用聊天选项生成器 # 您可以使用 OpenAiChatOptions 构建器以编程方式设置响应格式，如下所示：\nString jsonSchema = \u0026#34;\u0026#34;\u0026#34; { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;steps\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, \u0026#34;items\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;explanation\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;output\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } }, \u0026#34;required\u0026#34;: [\u0026#34;explanation\u0026#34;, \u0026#34;output\u0026#34;], \u0026#34;additionalProperties\u0026#34;: false } }, \u0026#34;final_answer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } }, \u0026#34;required\u0026#34;: [\u0026#34;steps\u0026#34;, \u0026#34;final_answer\u0026#34;], \u0026#34;additionalProperties\u0026#34;: false } \u0026#34;\u0026#34;\u0026#34;; Prompt prompt = new Prompt(\u0026#34;how can I solve 8x + 7 = -23\u0026#34;, OpenAiChatOptions.builder() .model(ChatModel.GPT_4_O_MINI) .responseFormat(new ResponseFormat(ResponseFormat.Type.JSON_SCHEMA, this.jsonSchema)) .build()); ChatResponse response = this.openAiChatModel.call(this.prompt); 与 BeanOutputConverter 实用程序集成 # 您可以利用现有的 [ BeanOutputConverter](../structured-output-converter.html#_bean_output_converter) 实用程序从域对象自动生成 JSON 模式，然后将结构化响应转换为特定于域的实例：\n通过应用程序属性进行配置 # 或者，在使用 OpenAI 自动配置时，您可以通过以下应用程序属性配置所需的响应格式：\nspring.ai.openai.api-key=YOUR_API_KEY spring.ai.openai.chat.options.model=gpt-4o-mini spring.ai.openai.chat.options.response-format.type=JSON_SCHEMA spring.ai.openai.chat.options.response-format.name=MySchemaName spring.ai.openai.chat.options.response-format.schema={\u0026#34;type\u0026#34;:\u0026#34;object\u0026#34;,\u0026#34;properties\u0026#34;:{\u0026#34;steps\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;array\u0026#34;,\u0026#34;items\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;object\u0026#34;,\u0026#34;properties\u0026#34;:{\u0026#34;explanation\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;},\u0026#34;output\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;}},\u0026#34;required\u0026#34;:[\u0026#34;explanation\u0026#34;,\u0026#34;output\u0026#34;],\u0026#34;additionalProperties\u0026#34;:false}},\u0026#34;final_answer\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;}},\u0026#34;required\u0026#34;:[\u0026#34;steps\u0026#34;,\u0026#34;final_answer\u0026#34;],\u0026#34;additionalProperties\u0026#34;:false} spring.ai.openai.chat.options.response-format.strict=true 样品控制器 # [ 创建一个新]( https://start.spring.io/)的 Spring Boot 项目，并将 添加到您的 spring-ai-starter-model-openai pom（或 gradle）依赖项中。 在 src/main/resources 目录下添加一个 application.properties 文件，用于启用和配置 OpenAi 聊天模型：\nspring.ai.openai.api-key=YOUR_API_KEY spring.ai.openai.chat.options.model=gpt-4o spring.ai.openai.chat.options.temperature=0.7 这将创建一个 OpenAiChatModel 实现，您可以将其注入到您的类中。下面是一个简单的 @RestController 类的示例，该类使用 chat 模型生成文本。\n@RestController public class ChatController { private final OpenAiChatModel chatModel; @Autowired public ChatController(OpenAiChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map\u0026lt;String,String\u0026gt; generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, this.chatModel.call(message)); } @GetMapping(\u0026#34;/ai/generateStream\u0026#34;) public Flux\u0026lt;ChatResponse\u0026gt; generateStream(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { Prompt prompt = new Prompt(new UserMessage(message)); return this.chatModel.stream(prompt); } } 手动配置 # OpenAiChatModel 实现了 ChatModel 和 StreamingChatModel，并使用[ 低级 OpenAiApi 客户端](#low-level-api)连接到 OpenAI 服务。 将 spring-ai-openai 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-openai\u0026#39; } 接下来，创建一个 OpenAiChatModel 并将其用于文本生成：\nvar openAiApi = OpenAiApi.builder() .apiKey(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;)) .build(); var openAiChatOptions = OpenAiChatOptions.builder() .model(\u0026#34;gpt-3.5-turbo\u0026#34;) .temperature(0.4) .maxTokens(200) .build(); var chatModel = new OpenAiChatModel(this.openAiApi, this.openAiChatOptions); ChatResponse response = this.chatModel.call( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); // Or with streaming responses Flux\u0026lt;ChatResponse\u0026gt; response = this.chatModel.stream( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); OpenAiChatOptions 提供聊天请求的配置信息。OpenAiApi.Builder 和 OpenAiChatOptions.Builder 分别是 API 客户端和聊天配置的流畅选项构建器。\n低级 OpenAiApi 客户端 # OpenAiApi openAiApi = OpenAiApi.builder() .apiKey(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;)) .build(); ChatCompletionMessage chatCompletionMessage = new ChatCompletionMessage(\u0026#34;Hello world\u0026#34;, Role.USER); // Sync request ResponseEntity\u0026lt;ChatCompletion\u0026gt; response = this.openAiApi.chatCompletionEntity( new ChatCompletionRequest(List.of(this.chatCompletionMessage), \u0026#34;gpt-3.5-turbo\u0026#34;, 0.8, false)); // Streaming request Flux\u0026lt;ChatCompletionChunk\u0026gt; streamResponse = this.openAiApi.chatCompletionStream( new ChatCompletionRequest(List.of(this.chatCompletionMessage), \u0026#34;gpt-3.5-turbo\u0026#34;, 0.8, true)); spring.ai.openai.api-key=your-api-key-here 您可以使用构建器模式使用您自己的 ApiKey 实现创建自定义 OpenAiApi 实例：\nApiKey customApiKey = new ApiKey() { @Override public String getValue() { // Custom logic to retrieve API key return \u0026#34;your-api-key-here\u0026#34;; } }; OpenAiApi openAiApi = OpenAiApi.builder() .apiKey(customApiKey) .build(); // Create a chat client with the custom OpenAiApi instance OpenAiChatClient chatClient = new OpenAiChatClient(openAiApi); 当您需要执行以下作时，这非常有用：\n从安全密钥存储中检索 API 密钥 动态轮换 API 密钥 实现自定义 API 密钥选择逻辑 "},{"id":83,"href":"/docs/models/chat-models/qianfan/","title":"千帆聊天","section":"聊天模型 API","content":" 千帆聊天 # 此功能已移至 Spring AI Community 存储库。 请访问 [ github.com/spring-ai-community/qianfan](https:// github.com/spring-ai-community/qianfan) 获取最新版本。\n"},{"id":84,"href":"/docs/vector-databases/redis/","title":"雷迪斯","section":"矢量数据库","content":" 雷迪斯 # 本节将指导您设置 RedisVectorStore 以存储文档嵌入并执行相似性搜索。 [ Redis]( https://redis.io) 是一种开源（BSD 许可）内存中数据结构存储，用作数据库、缓存、消息代理和流式处理引擎。[ Redis]( https://redis.io) 提供数据结构，例如字符串、哈希、列表、集、带有范围查询的排序集、位图、hyperloglog、地理空间索引和流。 [ Redis Search and Query]( https://redis.io/docs/interact/search-and-query/) 扩展了 Redis OSS 的核心功能，允许您将 Redis 作为矢量数据库使用：\n先决条件 # 自动配置 # Spring AI 为 Redis Vector Store 提供 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-redis\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-vector-store-redis\u0026#39; } 矢量存储实现可以为您初始化必要的架构，但您必须通过在相应的构造函数中指定 initializeSchema 布尔值或设置 ...initialize-schema=true 在 application.properties 文件中。 请查看 vector store 的[ 配置参数](#redisvector-properties)列表，了解默认值和配置选项。 此外，您将需要一个已配置的 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) bean。请参阅 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) 部分以了解更多信息。 现在，您可以将 RedisVectorStore 自动连接为应用程序中的矢量存储。\n@Autowired VectorStore vectorStore; // ... List \u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); // Add the documents to Redis vectorStore.add(documents); // Retrieve documents similar to a query List\u0026lt;Document\u0026gt; results = this.vectorStore.similaritySearch(SearchRequest.builder().query(\u0026#34;Spring\u0026#34;).topK(5).build()); 配置属性 # 要连接到 Redis 并使用 RedisVectorStore，您需要提供实例的访问详细信息。可以通过 Spring Boot 的 application.yml 提供一个简单的配置，\nspring: data: redis: url: \u0026lt;redis instance url\u0026gt; ai: vectorstore: redis: initialize-schema: true index-name: custom-index prefix: custom-prefix 对于 redis 连接配置，或者可以通过 Spring Boot 的 application.properties 提供简单的配置。\nspring.data.redis.host=localhost spring.data.redis.port=6379 spring.data.redis.username=default spring.data.redis.password= 以 spring.ai.vectorstore.redis.* 开头的属性用于配置 RedisVectorStore：\n元数据筛选 # 您也可以将通用的可移植[ 元数据筛选器](../vectordbs.html#metadata-filters)与 Redis 结合使用。 例如，您可以使用文本表达式语言：\nvectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(\u0026#34;country in [\u0026#39;UK\u0026#39;, \u0026#39;NL\u0026#39;] \u0026amp;\u0026amp; year \u0026gt;= 2020\u0026#34;).build()); 或使用 Filter.Expression DSL 以编程方式：\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(b.and( b.in(\u0026#34;country\u0026#34;, \u0026#34;UK\u0026#34;, \u0026#34;NL\u0026#34;), b.gte(\u0026#34;year\u0026#34;, 2020)).build()).build()); 例如，此可移植筛选条件表达式：\ncountry in [\u0026#39;UK\u0026#39;, \u0026#39;NL\u0026#39;] \u0026amp;\u0026amp; year \u0026gt;= 2020 转换为专有的 Redis 过滤器格式：\n@country:{UK | NL} @year:[2020 inf] 手动配置 # 您可以手动配置 Redis 矢量存储，而不是使用 Spring Boot 自动配置。为此，您需要将 spring-ai-redis-store 添加到您的项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-redis-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-redis-store\u0026#39; } 创建一个 JedisPooled bean：\n@Bean public JedisPooled jedisPooled() { return new JedisPooled(\u0026#34;\u0026lt;host\u0026gt;\u0026#34;, 6379); } 然后使用构建器模式创建 RedisVectorStore Bean：\n@Bean public VectorStore vectorStore(JedisPooled jedisPooled, EmbeddingModel embeddingModel) { return RedisVectorStore.builder(jedisPooled, embeddingModel) .indexName(\u0026#34;custom-index\u0026#34;) // Optional: defaults to \u0026#34;spring-ai-index\u0026#34; .prefix(\u0026#34;custom-prefix\u0026#34;) // Optional: defaults to \u0026#34;embedding:\u0026#34; .metadataFields( // Optional: define metadata fields for filtering MetadataField.tag(\u0026#34;country\u0026#34;), MetadataField.numeric(\u0026#34;year\u0026#34;)) .initializeSchema(true) // Optional: defaults to false .batchingStrategy(new TokenCountBatchingStrategy()) // Optional: defaults to TokenCountBatchingStrategy .build(); } // This can be any EmbeddingModel implementation @Bean public EmbeddingModel embeddingModel() { return new OpenAiEmbeddingModel(new OpenAiApi(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;))); } 访问 Native Client # Redis Vector Store 实现通过 getNativeClient（） 方法提供对底层原生 Redis 客户端 （JedisPooled） 的访问：\nRedisVectorStore vectorStore = context.getBean(RedisVectorStore.class); Optional\u0026lt;JedisPooled\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { JedisPooled jedis = nativeClient.get(); // Use the native client for Redis-specific operations } 本机客户端允许您访问特定于 Redis 的功能和作，这些功能和作可能不会通过 VectorStore 接口公开。\n"},{"id":85,"href":"/docs/vector-databases/sap-hana/","title":"None","section":"矢量数据库","content":" None # 先决条件 # 您需要一个 SAP HANA Cloud 矢量引擎账户 - 请参阅 SAP HANA Cloud 矢量引擎 - 配置试用账户指南以创建试用账户。 如果需要，EmbeddingModel 的 API 密钥，用于生成向量存储存储的嵌入。 自动配置 # Spring AI 没有为 SAP Hana 向量存储提供专用模块。用户应使用 Spring AI 中 SAP Hana 矢量存储的标准矢量存储模块 - spring-ai-hanadb-store 在应用程序中提供自己的配置。 请查看向量存储的 [ HanaCloudVectorStore 属性](#hanacloudvectorstore-properties)列表，了解默认值和配置选项。 此外，您将需要一个已配置的 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) bean。请参阅 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) 部分以了解更多信息。\nHanaCloudVectorStore 属性 # 您可以在 Spring Boot 配置中使用以下属性来自定义 SAP Hana 矢量存储。它使用 spring.datasource。 properties 来配置 Hana 数据源和 spring.ai.vectorstore.hanadb。 属性来配置 Hana 矢量存储。\n构建示例 RAG 应用程序 # 演示如何设置使用 SAP Hana Cloud 作为矢量数据库的项目，并利用 OpenAI 实现 RAG 模式\n在 SAP Hana DB 中创建表 CRICKET_WORLD_CUP： 在 pom.xml 中添加以下依赖项 您可以将属性 spring-ai-version 设置为 \u0026lt;spring-ai-version\u0026gt;1.0.0-SNAPSHOT\u0026lt;/spring-ai-version\u0026gt; ： \u0026lt;dependencyManagement\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-bom\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${spring-ai-version}\u0026lt;/version\u0026gt; \u0026lt;type\u0026gt;pom\u0026lt;/type\u0026gt; \u0026lt;scope\u0026gt;import\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/dependencyManagement\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-pdf-document-reader\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-openai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-hana\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lombok\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.18.30\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; 在 application.properties 文件中添加以下属性： 创建一个名为 CricketWorldCup 的 Entity 类，该类从 HanaVectorEntity 扩展而来： # package com.interviewpedia.spring.ai.hana; import jakarta.persistence.Column; import jakarta.persistence.Entity; import jakarta.persistence.Table; import lombok.Data; import lombok.NoArgsConstructor; import lombok.extern.jackson.Jacksonized; import org.springframework.ai.vectorstore.hanadb.HanaVectorEntity; @Entity @Table(name = \u0026#34;CRICKET_WORLD_CUP\u0026#34;) @Data @Jacksonized @NoArgsConstructor public class CricketWorldCup extends HanaVectorEntity { @Column(name = \u0026#34;content\u0026#34;) private String content; } 创建一个名为 CricketWorldCupRepository 的 Repository，该 Repository 实现 HanaVectorRepository 接口： package com.interviewpedia.spring.ai.hana; import jakarta.persistence.EntityManager; import jakarta.persistence.PersistenceContext; import jakarta.transaction.Transactional; import org.springframework.ai.vectorstore.hanadb.HanaVectorRepository; import org.springframework.stereotype.Repository; import java.util.List; @Repository public class CricketWorldCupRepository implements HanaVectorRepository\u0026lt;CricketWorldCup\u0026gt; { @PersistenceContext private EntityManager entityManager; @Override @Transactional public void save(String tableName, String id, String embedding, String content) { String sql = String.format(\u0026#34;\u0026#34;\u0026#34; INSERT INTO %s (_ID, EMBEDDING, CONTENT) VALUES(:_id, TO_REAL_VECTOR(:embedding), :content) \u0026#34;\u0026#34;\u0026#34;, tableName); this.entityManager.createNativeQuery(sql) .setParameter(\u0026#34;_id\u0026#34;, id) .setParameter(\u0026#34;embedding\u0026#34;, embedding) .setParameter(\u0026#34;content\u0026#34;, content) .executeUpdate(); } @Override @Transactional public int deleteEmbeddingsById(String tableName, List\u0026lt;String\u0026gt; idList) { String sql = String.format(\u0026#34;\u0026#34;\u0026#34; DELETE FROM %s WHERE _ID IN (:ids) \u0026#34;\u0026#34;\u0026#34;, tableName); return this.entityManager.createNativeQuery(sql) .setParameter(\u0026#34;ids\u0026#34;, idList) .executeUpdate(); } @Override @Transactional public int deleteAllEmbeddings(String tableName) { String sql = String.format(\u0026#34;\u0026#34;\u0026#34; DELETE FROM %s \u0026#34;\u0026#34;\u0026#34;, tableName); return this.entityManager.createNativeQuery(sql).executeUpdate(); } @Override public List\u0026lt;CricketWorldCup\u0026gt; cosineSimilaritySearch(String tableName, int topK, String queryEmbedding) { String sql = String.format(\u0026#34;\u0026#34;\u0026#34; SELECT TOP :topK * FROM %s ORDER BY COSINE_SIMILARITY(EMBEDDING, TO_REAL_VECTOR(:queryEmbedding)) DESC \u0026#34;\u0026#34;\u0026#34;, tableName); return this.entityManager.createNativeQuery(sql, CricketWorldCup.class) .setParameter(\u0026#34;topK\u0026#34;, topK) .setParameter(\u0026#34;queryEmbedding\u0026#34;, queryEmbedding) .getResultList(); } } 现在，创建一个 REST 控制器类 CricketWorldCupHanaController，并将 ChatModel 和 VectorStore 自动装配为依赖项在此控制器类中，创建以下 REST 端点： /ai/hana-vector-store/cricket-world-cup/purge-embeddings - 从 Vector Store 中清除所有嵌入 /ai/hana-vector-store/cricket-world-cup/upload - 上传 Cricket_World_Cup.pdf，以便其数据作为嵌入存储在 SAP Hana Cloud Vector DB 中 /ai/hana-vector-store/cricket-world-cup - 在 SAP Hana DB 中使用 Cosine_Similarity 实现 RAG package com.interviewpedia.spring.ai.hana; import lombok.extern.slf4j.Slf4j; import org.springframework.ai.chat.model.ChatModel; import org.springframework.ai.chat.messages.UserMessage; import org.springframework.ai.chat.prompt.Prompt; import org.springframework.ai.chat.prompt.SystemPromptTemplate; import org.springframework.ai.document.Document; import org.springframework.ai.reader.pdf.PagePdfDocumentReader; import org.springframework.ai.transformer.splitter.TokenTextSplitter; import org.springframework.ai.vectorstore.hanadb.HanaCloudVectorStore; import org.springframework.ai.vectorstore.VectorStore; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.core.io.Resource; import org.springframework.http.ResponseEntity; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.PostMapping; import org.springframework.web.bind.annotation.RequestParam; import org.springframework.web.bind.annotation.RestController; import org.springframework.web.multipart.MultipartFile; import java.io.IOException; import java.util.List; import java.util.Map; import java.util.function.Function; import java.util.function.Supplier; import java.util.stream.Collectors; @RestController @Slf4j public class CricketWorldCupHanaController { private final VectorStore hanaCloudVectorStore; private final ChatModel chatModel; @Autowired public CricketWorldCupHanaController(ChatModel chatModel, VectorStore hanaCloudVectorStore) { this.chatModel = chatModel; this.hanaCloudVectorStore = hanaCloudVectorStore; } @PostMapping(\u0026#34;/ai/hana-vector-store/cricket-world-cup/purge-embeddings\u0026#34;) public ResponseEntity\u0026lt;String\u0026gt; purgeEmbeddings() { int deleteCount = ((HanaCloudVectorStore) this.hanaCloudVectorStore).purgeEmbeddings(); log.info(\u0026#34;{} embeddings purged from CRICKET_WORLD_CUP table in Hana DB\u0026#34;, deleteCount); return ResponseEntity.ok().body(String.format(\u0026#34;%d embeddings purged from CRICKET_WORLD_CUP table in Hana DB\u0026#34;, deleteCount)); } @PostMapping(\u0026#34;/ai/hana-vector-store/cricket-world-cup/upload\u0026#34;) public ResponseEntity\u0026lt;String\u0026gt; handleFileUpload(@RequestParam(\u0026#34;pdf\u0026#34;) MultipartFile file) throws IOException { Resource pdf = file.getResource(); Supplier\u0026lt;List\u0026lt;Document\u0026gt;\u0026gt; reader = new PagePdfDocumentReader(pdf); Function\u0026lt;List\u0026lt;Document\u0026gt;, List\u0026lt;Document\u0026gt;\u0026gt; splitter = new TokenTextSplitter(); List\u0026lt;Document\u0026gt; documents = splitter.apply(reader.get()); log.info(\u0026#34;{} documents created from pdf file: {}\u0026#34;, documents.size(), pdf.getFilename()); this.hanaCloudVectorStore.accept(documents); return ResponseEntity.ok().body(String.format(\u0026#34;%d documents created from pdf file: %s\u0026#34;, documents.size(), pdf.getFilename())); } @GetMapping(\u0026#34;/ai/hana-vector-store/cricket-world-cup\u0026#34;) public Map\u0026lt;String, String\u0026gt; hanaVectorStoreSearch(@RequestParam(value = \u0026#34;message\u0026#34;) String message) { var documents = this.hanaCloudVectorStore.similaritySearch(message); var inlined = documents.stream().map(Document::getText).collect(Collectors.joining(System.lineSeparator())); var similarDocsMessage = new SystemPromptTemplate(\u0026#34;Based on the following: {documents}\u0026#34;) .createMessage(Map.of(\u0026#34;documents\u0026#34;, inlined)); var userMessage = new UserMessage(message); Prompt prompt = new Prompt(List.of(similarDocsMessage, userMessage)); String generation = this.chatModel.call(prompt).getResult().getOutput().getContent(); log.info(\u0026#34;Generation: {}\u0026#34;, generation); return Map.of(\u0026#34;generation\u0026#34;, generation); } } 由于 HanaDB vector store 支持不提供 autoconfiguration 模块，因此您还需要在应用程序中提供 vector store bean，如下所示，作为示例。\n@Bean public VectorStore hanaCloudVectorStore(CricketWorldCupRepository cricketWorldCupRepository, EmbeddingModel embeddingModel) { return HanaCloudVectorStore.builder(cricketWorldCupRepository, embeddingModel) .tableName(\u0026#34;CRICKET_WORLD_CUP\u0026#34;) .topK(1) .build(); } 使用来自维基百科的上下文 pdf 文件 转到[ 维基百科]( https://en.wikipedia.org/wiki/Cricket_World_Cup)并[ 下载]( https://en.wikipedia.org/w/index.php?title=Special:DownloadAsPdf\u0026page=Cricket_World_Cup\u0026action=show-download-screen)板球世界杯页面作为 PDF 文件。 使用我们在上一步中创建的文件上传 REST 端点上传此 PDF 文件。 "},{"id":86,"href":"/docs/prompt-engineering-patterns/","title":"提示工程模式","section":"Docs","content":" 提示工程模式 # 基于全面的 [ Prompt Engineering Guide]( https://www.kaggle.com/whitepaper-prompt-engineering) 的 Prompt Engineering 技术的实际实施。该指南涵盖了有效提示工程的理论、原则和模式，而在这里，我们演示了如何使用 Spring AI 的 Fluent [ ChatClient API](../chatclient.html) 将这些概念转换为有效的 Java 代码。本文中使用的演示源代码可在以下网址获得：[ Prompt Engineering Patterns Examples]( https://github.com/spring-projects/spring-ai-examples/tree/main/prompt-engineering/prompt-engineering-patterns)。\n1. 配置 # 配置部分概述了如何使用 Spring AI 设置和调整大型语言模型 （LLM）。它涵盖了为您的使用案例选择合适的 LLM 提供程序，以及配置控制模型输出的质量、样式和格式的重要生成参数。\nLLM 提供商选择 # 对于快速工程，您将首先选择一个模型。Spring AI 支持[ 多个 LLM 提供程序](comparison.html) （例如 OpenAI、Anthropic、Google Vertex AI、AWS Bedrock、Ollama 等），让您无需更改应用程序代码即可切换提供程序 - 只需更新配置即可。只需添加选定的 starter dependency spring-ai-starter-model-\u0026lt;MODEL-PROVIDER-NAME\u0026gt; .例如，以下是启用 Anthropic Claude API 的方法：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-anthropic\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 您可以指定 LLM 模型名称，如下所示：\n.options(ChatOptions.builder() .model(\u0026#34;claude-3-7-sonnet-latest\u0026#34;) // Use Anthropic\u0026#39;s Claude model .build()) 在[ 参考文档中](../chatmodel.html)查找有关启用每个模型的详细信息。\nLLM 输出配置 # 在我们深入研究提示工程技术之前，必须了解如何配置 LLM 的输出行为。Spring AI 提供了几个配置选项，允许您通过 [ ChatOptions](../chatmodel.html#_chat_options) 构建器控制生成的各个方面。 所有配置都可以以编程方式应用，如以下示例所示，或者在开始时通过 Spring 应用程序属性应用。\n温度 # 温度控制模型响应的随机性或“创造力”。\n下限值 （0.0-0.3）： 更具确定性、更集中的响应。更适合于一致性至关重要的事实性问题、分类或任务。 中等值 （0.4-0.7）： 在确定性和创造力之间取得平衡。适用于一般用例。 较高的值 （0.8-1.0）： 更具创意、变化多端且可能令人惊讶的回答。更适合创意写作、头脑风暴或生成多种选择。 .options(ChatOptions.builder() .temperature(0.1) // Very deterministic output .build()) 了解温度对于快速工程设计至关重要，因为不同的技术受益于不同的温度设置。\n输出长度 （MaxTokens） # maxTokens 参数限制模型可以在其响应中生成的标记（单词片段）数量。\n低值 （5-25）： 适用于单个单词、短语或分类标签。 中等值 （50-500）： 用于段落或简短说明。 高值 （1000+）： 用于长篇内容、故事或复杂的解释。 .options(ChatOptions.builder() .maxTokens(250) // Medium-length response .build()) 设置适当的输出长度对于确保您获得完整的响应而没有不必要的冗长非常重要。\n采样控制（Top-K 和 Top-P） # 这些参数使您可以在生成过程中对令牌选择过程进行精细控制。\nTop-K：将 Token 选择限制为 K 个最有可能的下一个 Token。较高的值（例如，40-50）会引入更多的多样性。 Top-P （nucleus sampling）： 从累积概率超过 P 的最小标记集中动态选择。0.8-0.95 等值很常见。 .options(ChatOptions.builder() .topK(40) // Consider only the top 40 tokens .topP(0.8) // Sample from tokens that cover 80% of probability mass .build()) 这些采样控制与温度结合使用，以塑造响应特性。\n结构化响应格式 # 除了纯文本响应（使用 .content（）） 之外，Spring AI 还可以轻松地使用 .entity（） 方法将 LLM 响应直接映射到 Java 对象。\nenum Sentiment { POSITIVE, NEUTRAL, NEGATIVE } Sentiment result = chatClient.prompt(\u0026#34;...\u0026#34;) .call() .entity(Sentiment.class); 当与指示模型返回结构化数据的系统提示结合使用时，此功能尤其强大。\n特定于模型的选项 # 虽然可移植的 ChatOptions 在不同的 LLM 提供程序之间提供了一致的接口，但 Spring AI 还提供了特定于模型的选项类，这些类公开了特定于提供程序的功能和配置。这些特定于模型的选项允许您利用每个 LLM 提供商的独特功能。\n// Using OpenAI-specific options OpenAiChatOptions openAiOptions = OpenAiChatOptions.builder() .model(\u0026#34;gpt-4o\u0026#34;) .temperature(0.2) .frequencyPenalty(0.5) // OpenAI-specific parameter .presencePenalty(0.3) // OpenAI-specific parameter .responseFormat(new ResponseFormat(\u0026#34;json_object\u0026#34;)) // OpenAI-specific JSON mode .seed(42) // OpenAI-specific deterministic generation .build(); String result = chatClient.prompt(\u0026#34;...\u0026#34;) .options(openAiOptions) .call() .content(); // Using Anthropic-specific options AnthropicChatOptions anthropicOptions = AnthropicChatOptions.builder() .model(\u0026#34;claude-3-7-sonnet-latest\u0026#34;) .temperature(0.2) .topK(40) // Anthropic-specific parameter .thinking(AnthropicApi.ThinkingType.ENABLED, 1000) // Anthropic-specific thinking configuration .build(); String result = chatClient.prompt(\u0026#34;...\u0026#34;) .options(anthropicOptions) .call() .content(); 每个模型提供商都有自己的聊天选项实现（例如，OpenAiChatOptions、AnthropicChatOptions、MistralAiChatOptions），这些选项公开特定于提供商的参数，同时仍实现通用接口。这种方法使您可以灵活地使用可移植选项来实现跨提供商兼容性，或者在需要访问特定提供商的独特功能时使用特定于模型的选项。 请注意，当使用特定于模型的选项时，您的代码会绑定到该特定提供程序，从而降低可移植性。这是在访问特定于提供程序的高级功能与在应用程序中保持提供程序独立性之间的权衡。\n2. 提示工程技术 # 以下每个部分都实现了指南中的特定提示工程技术。通过遵循 “Prompt Engineering” 指南和这些实现，您不仅会全面了解可用的提示工程技术，还会了解如何在生产 Java 应用程序中有效地实现它们。\n2.1 零次提示 # 零镜头提示涉及要求 AI 执行任务而不提供任何示例。这种方法测试模型从头开始理解和执行指令的能力。大型语言模型在大量文本语料库上进行训练，使它们能够理解 “翻译”、“摘要” 或 “分类” 等任务，而无需明确的演示。 Zero-shot 非常适合模型在训练期间可能看到类似示例的简单任务，以及您希望最小化提示长度时。但是，性能可能会因任务复杂性和说明的制定程度而异。\n// Implementation of Section 2.1: General prompting / zero shot (page 15) public void pt_zero_shot(ChatClient chatClient) { enum Sentiment { POSITIVE, NEUTRAL, NEGATIVE } Sentiment reviewSentiment = chatClient.prompt(\u0026#34;\u0026#34;\u0026#34; Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE. Review: \u0026#34;Her\u0026#34; is a disturbing study revealing the direction humanity is headed if AI is allowed to keep evolving, unchecked. I wish there were more movies like this masterpiece. Sentiment: \u0026#34;\u0026#34;\u0026#34;) .options(ChatOptions.builder() .model(\u0026#34;claude-3-7-sonnet-latest\u0026#34;) .temperature(0.1) .maxTokens(5) .build()) .call() .entity(Sentiment.class); System.out.println(\u0026#34;Output: \u0026#34; + reviewSentiment); } 此示例说明如何在不提供示例的情况下对电影评论情绪进行分类。请注意低温 （0.1） 以获得更确定的结果，以及直接映射到 Java 枚举的 .entity（Sentiment.class）。 参考：Brown， TB， et al. （2020 年）。“语言模型是少数机会的学习者。”arXiv：2005.14165。[ https://arxiv.org/abs/2005.14165]( https://arxiv.org/abs/2005.14165)\n2.2 一次性和几次提示 # Few-shot 提示为模型提供了一个或多个示例，以帮助指导其响应，这对于需要特定输出格式的任务特别有用。通过显示所需 input-output 对的模型示例，它可以学习模式并将其应用于新 inputs，而无需显式更新参数。 One-shot 提供单个示例，当示例成本高昂或模式相对简单时，这非常有用。Few-shot 使用多个示例（通常为 3-5 个）来帮助模型更好地理解更复杂任务中的模式或说明正确输出的不同变化。\n// Implementation of Section 2.2: One-shot \u0026amp; few-shot (page 16) public void pt_one_shot_few_shots(ChatClient chatClient) { String pizzaOrder = chatClient.prompt(\u0026#34;\u0026#34;\u0026#34; Parse a customer\u0026#39;s pizza order into valid JSON EXAMPLE 1: I want a small pizza with cheese, tomato sauce, and pepperoni. JSON Response: ``` { \u0026#34;size\u0026#34;: \u0026#34;small\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;normal\u0026#34;, \u0026#34;ingredients\u0026#34;: [\u0026#34;cheese\u0026#34;, \u0026#34;tomato sauce\u0026#34;, \u0026#34;pepperoni\u0026#34;] } ``` EXAMPLE 2: Can I get a large pizza with tomato sauce, basil and mozzarella. JSON Response: ``` { \u0026#34;size\u0026#34;: \u0026#34;large\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;normal\u0026#34;, \u0026#34;ingredients\u0026#34;: [\u0026#34;tomato sauce\u0026#34;, \u0026#34;basil\u0026#34;, \u0026#34;mozzarella\u0026#34;] } ``` Now, I would like a large pizza, with the first half cheese and mozzarella. And the other tomato sauce, ham and pineapple. \u0026#34;\u0026#34;\u0026#34;) .options(ChatOptions.builder() .model(\u0026#34;claude-3-7-sonnet-latest\u0026#34;) .temperature(0.1) .maxTokens(250) .build()) .call() .content(); } 对于需要特定格式设置、处理边缘情况的任务，或者当任务定义可能不明确而没有示例时，Few-shot 提示特别有效。示例的质量和多样性会显著影响性能。 参考：Brown， TB， et al. （2020 年）。“语言模型是少数机会的学习者。”arXiv：2005.14165。[ https://arxiv.org/abs/2005.14165]( https://arxiv.org/abs/2005.14165)\n2.3 系统、上下文和角色提示 # 系统提示 # 系统提示为语言模型设置整体上下文和目的，定义模型应该做什么的 “大局”。它为模型的响应建立了行为框架、约束和高级目标，与特定的用户查询分开。 系统提示在整个对话中充当持续的“使命宣言”，允许您设置全局参数，如输出格式、语气、道德界限或角色定义。与侧重于特定任务的用户提示不同，系统提示规定了应如何解释所有用户提示。\n// Implementation of Section 2.3.1: System prompting public void pt_system_prompting_1(ChatClient chatClient) { String movieReview = chatClient .prompt() .system(\u0026#34;Classify movie reviews as positive, neutral or negative. Only return the label in uppercase.\u0026#34;) .user(\u0026#34;\u0026#34;\u0026#34; Review: \u0026#34;Her\u0026#34; is a disturbing study revealing the direction humanity is headed if AI is allowed to keep evolving, unchecked. It\u0026#39;s so disturbing I couldn\u0026#39;t watch it. Sentiment: \u0026#34;\u0026#34;\u0026#34;) .options(ChatOptions.builder() .model(\u0026#34;claude-3-7-sonnet-latest\u0026#34;) .temperature(1.0) .topK(40) .topP(0.8) .maxTokens(5) .build()) .call() .content(); } 当与 Spring AI 的实体映射功能结合使用时，系统提示功能特别强大：\n// Implementation of Section 2.3.1: System prompting with JSON output record MovieReviews(Movie[] movie_reviews) { enum Sentiment { POSITIVE, NEUTRAL, NEGATIVE } record Movie(Sentiment sentiment, String name) { } } MovieReviews movieReviews = chatClient .prompt() .system(\u0026#34;\u0026#34;\u0026#34; Classify movie reviews as positive, neutral or negative. Return valid JSON. \u0026#34;\u0026#34;\u0026#34;) .user(\u0026#34;\u0026#34;\u0026#34; Review: \u0026#34;Her\u0026#34; is a disturbing study revealing the direction humanity is headed if AI is allowed to keep evolving, unchecked. It\u0026#39;s so disturbing I couldn\u0026#39;t watch it. JSON Response: \u0026#34;\u0026#34;\u0026#34;) .call() .entity(MovieReviews.class); 系统提示对于多轮次对话特别有价值，可确保多个查询之间的行为一致，以及建立应应用于所有响应的格式约束（如 JSON 输出）。 参考： 打开人工智能。（2022）. “系统消息。”[ https://platform.openai.com/docs/guides/chat/introduction]( https://platform.openai.com/docs/guides/chat/introduction)\n角色提示 # 角色提示指示模型采用特定角色或角色，这会影响它生成内容的方式。通过为模型分配特定的身份、专业知识或视角，您可以影响其响应的风格、语气、深度和框架。 角色提示利用模型的能力来模拟不同的专业领域和沟通方式。常见角色包括专家（例如，“您是一位经验丰富的数据科学家”）、专业人士（例如，“充当旅行指南”）或风格角色（例如，“像莎士比亚一样解释”）。\n// Implementation of Section 2.3.2: Role prompting public void pt_role_prompting_1(ChatClient chatClient) { String travelSuggestions = chatClient .prompt() .system(\u0026#34;\u0026#34;\u0026#34; I want you to act as a travel guide. I will write to you about my location and you will suggest 3 places to visit near me. In some cases, I will also give you the type of places I will visit. \u0026#34;\u0026#34;\u0026#34;) .user(\u0026#34;\u0026#34;\u0026#34; My suggestion: \u0026#34;I am in Amsterdam and I want to visit only museums.\u0026#34; Travel Suggestions: \u0026#34;\u0026#34;\u0026#34;) .call() .content(); } 可以通过样式说明来增强角色提示：\n// Implementation of Section 2.3.2: Role prompting with style instructions public void pt_role_prompting_2(ChatClient chatClient) { String humorousTravelSuggestions = chatClient .prompt() .system(\u0026#34;\u0026#34;\u0026#34; I want you to act as a travel guide. I will write to you about my location and you will suggest 3 places to visit near me in a humorous style. \u0026#34;\u0026#34;\u0026#34;) .user(\u0026#34;\u0026#34;\u0026#34; My suggestion: \u0026#34;I am in Amsterdam and I want to visit only museums.\u0026#34; Travel Suggestions: \u0026#34;\u0026#34;\u0026#34;) .call() .content(); } 此技术对于专门的领域知识特别有效，可在响应之间实现一致的语气，并与用户创建更具吸引力的个性化交互。 参考：Shanahan， M. 等人（2023 年）。“使用大型语言模型进行角色扮演。”arXiv：2305.16367。[ https://arxiv.org/abs/2305.16367]( https://arxiv.org/abs/2305.16367)\n上下文提示 # 上下文提示通过传递上下文参数为模型提供额外的背景信息。这种技术丰富了模型对特定情况的理解，从而能够实现更相关和量身定制的响应，而不会使主要指令变得混乱。 通过提供上下文信息，您可以帮助模型了解与当前查询相关的特定域、受众、约束或背景事实。这导致了更准确、相关和适当构建的响应。\n// Implementation of Section 2.3.3: Contextual prompting public void pt_contextual_prompting(ChatClient chatClient) { String articleSuggestions = chatClient .prompt() .user(u -\u0026gt; u.text(\u0026#34;\u0026#34;\u0026#34; Suggest 3 topics to write an article about with a few lines of description of what this article should contain. Context: {context} \u0026#34;\u0026#34;\u0026#34;) .param(\u0026#34;context\u0026#34;, \u0026#34;You are writing for a blog about retro 80\u0026#39;s arcade video games.\u0026#34;)) .call() .content(); } Spring AI 使用 param（） 方法注入上下文变量，使上下文提示变得干净。当模型需要特定的领域知识时，当根据特定受众或场景调整响应时，以及确保响应与特定约束或要求保持一致时，此技术特别有价值。 参考：Liu， P. 等人（2021 年）。“什么才是 GPT-3 的好上下文示例？” arXiv：2101.06804。[ https://arxiv.org/abs/2101.06804]( https://arxiv.org/abs/2101.06804)\n2.4 后退提示 # 后退提示首先通过获取背景知识，将复杂的请求分解为更简单的步骤。这种技术鼓励模型首先从直接问题中“退后一步”，以考虑更广泛的上下文、基本原则或与问题相关的一般知识，然后再解决特定查询。 通过将复杂问题分解为更易于管理的组件并首先建立基础知识，该模型可以更准确地回答困难的问题。\n// Implementation of Section 2.4: Step-back prompting public void pt_step_back_prompting(ChatClient.Builder chatClientBuilder) { // Set common options for the chat client var chatClient = chatClientBuilder .defaultOptions(ChatOptions.builder() .model(\u0026#34;claude-3-7-sonnet-latest\u0026#34;) .temperature(1.0) .topK(40) .topP(0.8) .maxTokens(1024) .build()) .build(); // First get high-level concepts String stepBack = chatClient .prompt(\u0026#34;\u0026#34;\u0026#34; Based on popular first-person shooter action games, what are 5 fictional key settings that contribute to a challenging and engaging level storyline in a first-person shooter video game? \u0026#34;\u0026#34;\u0026#34;) .call() .content(); // Then use those concepts in the main task String story = chatClient .prompt() .user(u -\u0026gt; u.text(\u0026#34;\u0026#34;\u0026#34; Write a one paragraph storyline for a new level of a first- person shooter video game that is challenging and engaging. Context: {step-back} \u0026#34;\u0026#34;\u0026#34;) .param(\u0026#34;step-back\u0026#34;, stepBack)) .call() .content(); } 后退提示对于复杂的推理任务、需要专业领域知识的问题以及您想要更全面、更周到的响应而不是立即的答案时特别有效。 参考：Zheng， Z. 等人（2023 年）。“退后一步：在大型语言模型中通过抽象唤起推理。”arXiv：2310.06117。[ https://arxiv.org/abs/2310.06117]( https://arxiv.org/abs/2310.06117)\n2.5 思维链 （CoT） # Chain of Thought 提示鼓励模型逐步推理问题，从而提高复杂推理任务的准确性。通过明确要求模型展示其工作或以逻辑步骤思考问题，您可以显著提高需要多步骤推理的任务的性能。 CoT 的工作原理是鼓励模型在产生最终答案之前生成中间推理步骤，类似于人类解决复杂问题的方式。这使得模型的思维过程清晰，并有助于它得出更准确的结论。\n// Implementation of Section 2.5: Chain of Thought (CoT) - Zero-shot approach public void pt_chain_of_thought_zero_shot(ChatClient chatClient) { String output = chatClient .prompt(\u0026#34;\u0026#34;\u0026#34; When I was 3 years old, my partner was 3 times my age. Now, I am 20 years old. How old is my partner? Let\u0026#39;s think step by step. \u0026#34;\u0026#34;\u0026#34;) .call() .content(); } // Implementation of Section 2.5: Chain of Thought (CoT) - Few-shot approach public void pt_chain_of_thought_singleshot_fewshots(ChatClient chatClient) { String output = chatClient .prompt(\u0026#34;\u0026#34;\u0026#34; Q: When my brother was 2 years old, I was double his age. Now I am 40 years old. How old is my brother? Let\u0026#39;s think step by step. A: When my brother was 2 years, I was 2 * 2 = 4 years old. That\u0026#39;s an age difference of 2 years and I am older. Now I am 40 years old, so my brother is 40 - 2 = 38 years old. The answer is 38. Q: When I was 3 years old, my partner was 3 times my age. Now, I am 20 years old. How old is my partner? Let\u0026#39;s think step by step. A: \u0026#34;\u0026#34;\u0026#34;) .call() .content(); } 关键词 “Let\u0026rsquo;s think step by step” 触发模型展示其推理过程。CoT 对于数学问题、逻辑推理任务和任何需要多步骤推理的问题特别有价值。它通过明确中间推理来帮助减少错误。 参考：Wei， J. 等人（2022 年）。“在大型语言模型中提示引发推理的思维链。”arXiv：2201.11903。[ https://arxiv.org/abs/2201.11903]( https://arxiv.org/abs/2201.11903)\n2.6 自洽 # 自洽性涉及多次运行模型并聚合结果以获得更可靠的答案。该技术通过对同一问题的不同推理路径进行采样并通过多数投票选择最一致的答案来解决 LLM 输出的可变性。 通过生成具有不同温度或采样设置的多个推理路径，然后聚合最终答案，自洽性可以提高复杂推理任务的准确性。它本质上是 LLM 输出的 ensemble 方法。\n// Implementation of Section 2.6: Self-consistency public void pt_self_consistency(ChatClient chatClient) { String email = \u0026#34;\u0026#34;\u0026#34; Hi, I have seen you use Wordpress for your website. A great open source content management system. I have used it in the past too. It comes with lots of great user plugins. And it\u0026#39;s pretty easy to set up. I did notice a bug in the contact form, which happens when you select the name field. See the attached screenshot of me entering text in the name field. Notice the JavaScript alert box that I inv0k3d. But for the rest it\u0026#39;s a great website. I enjoy reading it. Feel free to leave the bug in the website, because it gives me more interesting things to read. Cheers, Harry the Hacker. \u0026#34;\u0026#34;\u0026#34;; record EmailClassification(Classification classification, String reasoning) { enum Classification { IMPORTANT, NOT_IMPORTANT } } int importantCount = 0; int notImportantCount = 0; // Run the model 5 times with the same input for (int i = 0; i \u0026lt; 5; i++) { EmailClassification output = chatClient .prompt() .user(u -\u0026gt; u.text(\u0026#34;\u0026#34;\u0026#34; Email: {email} Classify the above email as IMPORTANT or NOT IMPORTANT. Let\u0026#39;s think step by step and explain why. \u0026#34;\u0026#34;\u0026#34;) .param(\u0026#34;email\u0026#34;, email)) .options(ChatOptions.builder() .temperature(1.0) // Higher temperature for more variation .build()) .call() .entity(EmailClassification.class); // Count results if (output.classification() == EmailClassification.Classification.IMPORTANT) { importantCount++; } else { notImportantCount++; } } // Determine the final classification by majority vote String finalClassification = importantCount \u0026gt; notImportantCount ? \u0026#34;IMPORTANT\u0026#34; : \u0026#34;NOT IMPORTANT\u0026#34;; } 自洽对于高风险决策、复杂的推理任务以及当您需要比单个回答更自信的答案时特别有价值。权衡是由于多次 API 调用而增加计算成本和延迟。 参考：Wang， X. 等人（2022 年）。“自洽性改善了语言模型中的思维链推理。”arXiv：2203.11171。[ https://arxiv.org/abs/2203.11171]( https://arxiv.org/abs/2203.11171)\n2.7 思想(ToT)之树 # Tree of Thoughts (ToT) 是一个高级推理框架，它通过同时探索多个推理路径来扩展 Chain of Thought。它将问题解决视为一个搜索过程，在该过程中，模型生成不同的中间步骤，评估它们的承诺，并探索最有前途的路径。 对于具有多种可能方法的复杂问题，或者当解决方案需要在找到最佳路径之前探索各种替代方案时，此技术特别强大。 游戏解决 ToT 示例：\n// Implementation of Section 2.7: Tree of Thoughts (ToT) - Game solving example public void pt_tree_of_thoughts_game(ChatClient chatClient) { // Step 1: Generate multiple initial moves String initialMoves = chatClient .prompt(\u0026#34;\u0026#34;\u0026#34; You are playing a game of chess. The board is in the starting position. Generate 3 different possible opening moves. For each move: 1. Describe the move in algebraic notation 2. Explain the strategic thinking behind this move 3. Rate the move\u0026#39;s strength from 1-10 \u0026#34;\u0026#34;\u0026#34;) .options(ChatOptions.builder() .temperature(0.7) .build()) .call() .content(); // Step 2: Evaluate and select the most promising move String bestMove = chatClient .prompt() .user(u -\u0026gt; u.text(\u0026#34;\u0026#34;\u0026#34; Analyze these opening moves and select the strongest one: {moves} Explain your reasoning step by step, considering: 1. Position control 2. Development potential 3. Long-term strategic advantage Then select the single best move. \u0026#34;\u0026#34;\u0026#34;).param(\u0026#34;moves\u0026#34;, initialMoves)) .call() .content(); // Step 3: Explore future game states from the best move String gameProjection = chatClient .prompt() .user(u -\u0026gt; u.text(\u0026#34;\u0026#34;\u0026#34; Based on this selected opening move: {best_move} Project the next 3 moves for both players. For each potential branch: 1. Describe the move and counter-move 2. Evaluate the resulting position 3. Identify the most promising continuation Finally, determine the most advantageous sequence of moves. \u0026#34;\u0026#34;\u0026#34;).param(\u0026#34;best_move\u0026#34;, bestMove)) .call() .content(); } 参考：Yao， S. 等人（2023 年）。“思想之树：使用大型语言模型进行刻意解决问题。”arXiv：2305.10601。[ https://arxiv.org/abs/2305.10601]( https://arxiv.org/abs/2305.10601)\n2.8 自动提示工程 # Automatic Prompt Engineering 使用 AI 生成和评估替代提示。这种元技术利用语言模型本身来创建、优化和基准测试不同的提示变体，以找到特定任务的最佳公式。 通过系统地生成和评估提示变化，APE 可以找到比手动工程更有效的提示，尤其是对于复杂的任务。这是一种使用 AI 来提高自身性能的方式。\n// Implementation of Section 2.8: Automatic Prompt Engineering public void pt_automatic_prompt_engineering(ChatClient chatClient) { // Generate variants of the same request String orderVariants = chatClient .prompt(\u0026#34;\u0026#34;\u0026#34; We have a band merchandise t-shirt webshop, and to train a chatbot we need various ways to order: \u0026#34;One Metallica t-shirt size S\u0026#34;. Generate 10 variants, with the same semantics but keep the same meaning. \u0026#34;\u0026#34;\u0026#34;) .options(ChatOptions.builder() .temperature(1.0) // High temperature for creativity .build()) .call() .content(); // Evaluate and select the best variant String output = chatClient .prompt() .user(u -\u0026gt; u.text(\u0026#34;\u0026#34;\u0026#34; Please perform BLEU (Bilingual Evaluation Understudy) evaluation on the following variants: ---- {variants} ---- Select the instruction candidate with the highest evaluation score. \u0026#34;\u0026#34;\u0026#34;).param(\u0026#34;variants\u0026#34;, orderVariants)) .call() .content(); } APE 对于优化生产系统的提示、解决手动提示工程已达到极限的具有挑战性的任务以及系统地大规模提高提示质量特别有价值。 参考： 周， Y.， et al. （2022）.“大型语言模型是人类水平的提示工程师。”arXiv：2211.01910.[ https://arxiv.org/abs/2211.01910]( https://arxiv.org/abs/2211.01910)\n2.9 代码提示 # 代码提示是指用于代码相关任务的专用技术。这些技术利用 LLM 理解和生成编程语言的能力，使它们能够编写新代码、解释现有代码、调试问题以及在语言之间进行翻译。 有效的代码提示通常涉及明确的规范、适当的上下文（库、框架、样式指南），有时还包括类似代码的示例。温度设置往往较低 （0.1-0.3） 以获得更具确定性的输出。\n// Implementation of Section 2.9.1: Prompts for writing code public void pt_code_prompting_writing_code(ChatClient chatClient) { String bashScript = chatClient .prompt(\u0026#34;\u0026#34;\u0026#34; Write a code snippet in Bash, which asks for a folder name. Then it takes the contents of the folder and renames all the files inside by prepending the name draft to the file name. \u0026#34;\u0026#34;\u0026#34;) .options(ChatOptions.builder() .temperature(0.1) // Low temperature for deterministic code .build()) .call() .content(); } // Implementation of Section 2.9.2: Prompts for explaining code public void pt_code_prompting_explaining_code(ChatClient chatClient) { String code = \u0026#34;\u0026#34;\u0026#34; #!/bin/bash echo \u0026#34;Enter the folder name: \u0026#34; read folder_name if [ ! -d \u0026#34;$folder_name\u0026#34; ]; then echo \u0026#34;Folder does not exist.\u0026#34; exit 1 fi files=( \u0026#34;$folder_name\u0026#34;/* ) for file in \u0026#34;${files[@]}\u0026#34;; do new_file_name=\u0026#34;draft_$(basename \u0026#34;$file\u0026#34;)\u0026#34; mv \u0026#34;$file\u0026#34; \u0026#34;$new_file_name\u0026#34; done echo \u0026#34;Files renamed successfully.\u0026#34; \u0026#34;\u0026#34;\u0026#34;; String explanation = chatClient .prompt() .user(u -\u0026gt; u.text(\u0026#34;\u0026#34;\u0026#34; Explain to me the below Bash code: ``` {code} ``` \u0026#34;\u0026#34;\u0026#34;).param(\u0026#34;code\u0026#34;, code)) .call() .content(); } // Implementation of Section 2.9.3: Prompts for translating code public void pt_code_prompting_translating_code(ChatClient chatClient) { String bashCode = \u0026#34;\u0026#34;\u0026#34; #!/bin/bash echo \u0026#34;Enter the folder name: \u0026#34; read folder_name if [ ! -d \u0026#34;$folder_name\u0026#34; ]; then echo \u0026#34;Folder does not exist.\u0026#34; exit 1 fi files=( \u0026#34;$folder_name\u0026#34;/* ) for file in \u0026#34;${files[@]}\u0026#34;; do new_file_name=\u0026#34;draft_$(basename \u0026#34;$file\u0026#34;)\u0026#34; mv \u0026#34;$file\u0026#34; \u0026#34;$new_file_name\u0026#34; done echo \u0026#34;Files renamed successfully.\u0026#34; \u0026#34;\u0026#34;\u0026#34;; String pythonCode = chatClient .prompt() .user(u -\u0026gt; u.text(\u0026#34;\u0026#34;\u0026#34; Translate the below Bash code to a Python snippet: {code} \u0026#34;\u0026#34;\u0026#34;).param(\u0026#34;code\u0026#34;, bashCode)) .call() .content(); } 代码提示对于自动化代码文档、原型设计、学习编程概念以及编程语言之间的翻译特别有价值。通过将其与小镜头提示或思维链等技术相结合，可以进一步提高有效性。 参考：Chen， M. 等人（2021 年）。“评估在代码上训练的大型语言模型。”arXiv：2107.03374。[ https://arxiv.org/abs/2107.03374]( https://arxiv.org/abs/2107.03374)\n结论 # Spring AI 提供了一个优雅的 Java API，用于实现所有主要的提示工程技术。通过将这些技术与 Spring 强大的实体映射和 Fluent API 相结合，开发人员可以使用干净、可维护的代码构建复杂的 AI 驱动的应用程序。 最有效的方法通常涉及组合多种技术 - 例如，使用带有少数镜头示例的系统提示，或将思路链与角色提示一起使用。Spring AI 的灵活 API 使这些组合易于实现。 借助这些技术和 Spring AI 强大的抽象，您可以创建强大的 AI 驱动的应用程序，以提供一致、高质量的结果。\n引用 # "},{"id":87,"href":"/docs/models/chat-models/zhipu-ai/","title":"智普 AI 聊天","section":"聊天模型 API","content":" 智普 AI 聊天 # Spring AI 支持 ZhiPu AI 的各种 AI 语言模型。您可以与 ZhiPu AI 语言模型交互，并基于 ZhiPuAI 模型创建多语言对话助手。\n先决条件 # 您需要使用 ZhiPuAI 创建 API 才能访问 ZhiPu AI 语言模型。 在 [ 智普 AI 注册页面]( https://open.bigmodel.cn/login)创建账号，并在 [ API Keys 页面生成 Token]( https://open.bigmodel.cn/usercenter/apikeys)。 Spring AI 项目定义了一个名为 spring.ai.zhipuai.api-key 的配置属性，您应该将其设置为从 API 密钥页面获取的 API 密钥的值。 您可以在 application.properties 文件中设置此配置属性：\nspring.ai.zhipuai.api-key=\u0026lt;your-zhipuai-api-key\u0026gt; 为了在处理 API 密钥等敏感信息时增强安全性，您可以使用 Spring 表达式语言 （SpEL） 来引用自定义环境变量：\n# In application.yml spring: ai: zhipuai: api-key: ${ZHIPUAI_API_KEY} # In your environment or .env file export ZHIPUAI_API_KEY=\u0026lt;your-zhipuai-api-key\u0026gt; 您还可以在应用程序代码中以编程方式设置此配置：\n// Retrieve API key from a secure source or environment variable String apiKey = System.getenv(\u0026#34;ZHIPUAI_API_KEY\u0026#34;); 添加存储库和 BOM # Spring AI 工件发布在 Maven Central 和 Spring Snapshot 存储库中。请参阅 [ Artifact Repositories](../../getting-started.html#artifact-repositories) 部分，将这些存储库添加到您的构建系统中。 为了帮助进行[ 依赖项管理](../../getting-started.html#dependency-management)，Spring AI 提供了一个 BOM（物料清单），以确保在整个项目中使用一致的 Spring AI 版本。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # Spring AI 为 ZhiPuAI Chat 客户端提供了 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-zhipuai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-zhipuai\u0026#39; } 聊天属性 # 重试属性 # 前缀 spring.ai.retry 用作属性前缀，允许您为 ZhiPu AI 聊天模型配置重试机制。\n连接属性 # 前缀 spring.ai.zhiPu 用作允许您连接到 ZhiPuAI 的属性前缀。\n配置属性 # 前缀 spring.ai.zhipuai.chat 是属性前缀，允许您为 ZhiPuAI 配置聊天模型实现。\n运行时选项 # [ ZhiPuAiChatOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-zhipuai/src/main/java/org/springframework/ai/zhipuai/[ZhiPuAiChatOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-zhipuai/src/main/java/org/springframework/ai/zhipuai/ZhiPuAiChatOptions.java)) 提供模型配置，例如要使用的模型、温度、频率损失等。 启动时，可以使用 ZhiPuAiChatModel(api, options) constructor 或 spring.ai.zhipuai.chat.options.* properties 配置默认选项。 在运行时，您可以通过向 Prompt 调用添加新的、特定于请求的选项来覆盖默认选项。例如，要覆盖特定请求的默认模型和温度：\nChatResponse response = chatModel.call( new Prompt( \u0026#34;Generate the names of 5 famous pirates.\u0026#34;, ZhiPuAiChatOptions.builder() .model(ZhiPuAiApi.ChatModel.GLM_3_Turbo.getValue()) .temperature(0.5) .build() )); 样品控制器 # [ 创建一个新]( https://start.spring.io/)的 Spring Boot 项目，并将 添加到您的 spring-ai-starter-model-zhipuai pom（或 gradle）依赖项中。 在 src/main/resources 目录下添加一个 application.properties 文件，用于开启和配置 ZhiPuAi 聊天模型：\nspring.ai.zhipuai.api-key=YOUR_API_KEY spring.ai.zhipuai.chat.options.model=glm-4-air spring.ai.zhipuai.chat.options.temperature=0.7 这将创建一个 ZhiPuAiChatModel 实现，您可以将其注入到您的类中。下面是一个简单的 @Controller 类示例，该类使用 chat 模型生成文本。\n@RestController public class ChatController { private final ZhiPuAiChatModel chatModel; @Autowired public ChatController(ZhiPuAiChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, this.chatModel.call(message)); } @GetMapping(\u0026#34;/ai/generateStream\u0026#34;) public Flux\u0026lt;ChatResponse\u0026gt; generateStream(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { var prompt = new Prompt(new UserMessage(message)); return this.chatModel.stream(prompt); } } 手动配置 # ZhiPuAiChatModel 实现了 ChatModel 和 StreamingChatModel，并使用[ 低级 ZhiPuAiApi Client](#low-level-api) 连接 ZhiPuAI 服务。 将 spring-ai-zhipuai 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-zhipuai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-zhipuai\u0026#39; } 接下来，创建一个 ZhiPuAiChatModel 并使用它来生成文本：\nvar zhiPuAiApi = new ZhiPuAiApi(System.getenv(\u0026#34;ZHIPU_AI_API_KEY\u0026#34;)); var chatModel = new ZhiPuAiChatModel(this.zhiPuAiApi, ZhiPuAiChatOptions.builder() .model(ZhiPuAiApi.ChatModel.GLM_3_Turbo.getValue()) .temperature(0.4) .maxTokens(200) .build()); ChatResponse response = this.chatModel.call( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); // Or with streaming responses Flux\u0026lt;ChatResponse\u0026gt; streamResponse = this.chatModel.stream( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); ZhiPuAiChatOptions 提供聊天请求的配置信息。ZhiPuAiChatOptions.Builder 是流畅的选项构建器。\n低级 ZhiPuAiApi 客户端 # [ ZhiPuAiApi]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-zhipuai/src/main/java/org/springframework/ai/zhipuai/api/[ZhiPuAiApi](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-zhipuai/src/main/java/org/springframework/ai/zhipuai/api/ZhiPuAiApi.java).java) 提供的是 [ ZhiPu AI API]( https://open.bigmodel.cn/dev/api) 的轻量级 Java 客户端。 以下是如何以编程方式使用 api 的简单代码段：\nZhiPuAiApi zhiPuAiApi = new ZhiPuAiApi(System.getenv(\u0026#34;ZHIPU_AI_API_KEY\u0026#34;)); ChatCompletionMessage chatCompletionMessage = new ChatCompletionMessage(\u0026#34;Hello world\u0026#34;, Role.USER); // Sync request ResponseEntity\u0026lt;ChatCompletion\u0026gt; response = this.zhiPuAiApi.chatCompletionEntity( new ChatCompletionRequest(List.of(this.chatCompletionMessage), ZhiPuAiApi.ChatModel.GLM_3_Turbo.getValue(), 0.7, false)); // Streaming request Flux\u0026lt;ChatCompletionChunk\u0026gt; streamResponse = this.zhiPuAiApi.chatCompletionStream( new ChatCompletionRequest(List.of(this.chatCompletionMessage), ZhiPuAiApi.ChatModel.GLM_3_Turbo.getValue(), 0.7, true)); 有关详细信息，请遵循 [ ZhiPuAiApi.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-zhipuai/src/main/java/org/springframework/ai/zhipuai/api/[ZhiPuAiApi.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-zhipuai/src/main/java/org/springframework/ai/zhipuai/api/ZhiPuAiApi.java)) 的 JavaDoc。\nZhiPuAiApi 示例 # ZhiPuAiApiIT.java 测试提供了一些如何使用轻量级库的一般示例。 "},{"id":88,"href":"/docs/building-effective-agents/","title":"Building Effective Agents","section":"Docs","content":" # 代理系统 # 该研究出版物对两种类型的代理系统进行了重要的架构区分： 关键的见解是，虽然完全自主的代理看起来很有吸引力，但工作流通常为定义明确的任务提供更好的可预测性和一致性。这与可靠性和可维护性至关重要的企业要求完全一致。 让我们看看 Spring AI 如何通过五种基本模式来实现这些概念，每种模式都服务于特定的用例：\n1. Chain 工作流程 # Chain Workflow 模式体现了将复杂任务分解为更简单、更易于管理的步骤的原则。 何时使用：\n具有明确顺序步骤的任务 当您想用延迟换取更高的准确性时 当每个步骤都基于上一步的输出时 以下是 Spring AI 实现中的一个实际示例： public class ChainWorkflow { private final ChatClient chatClient; private final String[] systemPrompts; public String chain(String userInput) { String response = userInput; for (String prompt : systemPrompts) { String input = String.format(\u0026#34;{%s}\\n {%s}\u0026#34;, prompt, response); response = chatClient.prompt(input).call().content(); } return response; } } 此实现演示了几个关键原则：\n每个步骤都有重点 一个步骤的输出成为下一个步骤的输入 该链易于扩展和维护 2. 并行化工作流程 # LLM 可以同时处理任务，并以编程方式聚合其输出。 何时使用：\n处理大量相似但独立的项目 需要多个独立视角的任务 当处理时间至关重要且任务可并行化时 List\u0026lt;String\u0026gt; parallelResponse = new ParallelizationWorkflow(chatClient) .parallel( \u0026#34;Analyze how market changes will impact this stakeholder group.\u0026#34;, List.of( \u0026#34;Customers: ...\u0026#34;, \u0026#34;Employees: ...\u0026#34;, \u0026#34;Investors: ...\u0026#34;, \u0026#34;Suppliers: ...\u0026#34; ), 4 ); 3. 路由工作流程 # 路由模式实现了智能任务分配，从而支持对不同类型的输入进行专门处理。 何时使用：\n具有不同输入类别的复杂任务 当不同的输入需要专门处理时 何时可以准确处理分类 @Autowired private ChatClient chatClient; RoutingWorkflow workflow = new RoutingWorkflow(chatClient); Map\u0026lt;String, String\u0026gt; routes = Map.of( \u0026#34;billing\u0026#34;, \u0026#34;You are a billing specialist. Help resolve billing issues...\u0026#34;, \u0026#34;technical\u0026#34;, \u0026#34;You are a technical support engineer. Help solve technical problems...\u0026#34;, \u0026#34;general\u0026#34;, \u0026#34;You are a customer service representative. Help with general inquiries...\u0026#34; ); String input = \u0026#34;My account was charged twice last week\u0026#34;; String response = workflow.route(input, routes); 4. 编排器-工作人员 # 何时使用：\n无法预先预测子任务的复杂任务 需要不同方法或观点的任务 需要适应性问题解决的情况 public class OrchestratorWorkersWorkflow { public WorkerResponse process(String taskDescription) { // 1. Orchestrator analyzes task and determines subtasks OrchestratorResponse orchestratorResponse = // ... // 2. Workers process subtasks in parallel List\u0026lt;String\u0026gt; workerResponses = // ... // 3. Results are combined into final response return new WorkerResponse(/*...*/); } } 使用示例：\nChatClient chatClient = // ... initialize chat client OrchestratorWorkersWorkflow workflow = new OrchestratorWorkersWorkflow(chatClient); WorkerResponse response = workflow.process( \u0026#34;Generate both technical and user-friendly documentation for a REST API endpoint\u0026#34; ); System.out.println(\u0026#34;Analysis: \u0026#34; + response.analysis()); System.out.println(\u0026#34;Worker Outputs: \u0026#34; + response.workerResponses()); 5. 评估器-优化器 # 何时使用：\n存在明确的评估标准 迭代优化提供可衡量的价值 任务受益于多轮批评 public class EvaluatorOptimizerWorkflow { public RefinedResponse loop(String task) { Generation generation = generate(task, context); EvaluationResponse evaluation = evaluate(generation.response(), task); return new RefinedResponse(finalSolution, chainOfThought); } } 使用示例：\nChatClient chatClient = // ... initialize chat client EvaluatorOptimizerWorkflow workflow = new EvaluatorOptimizerWorkflow(chatClient); RefinedResponse response = workflow.loop( \u0026#34;Create a Java class implementing a thread-safe counter\u0026#34; ); System.out.println(\u0026#34;Final Solution: \u0026#34; + response.solution()); System.out.println(\u0026#34;Evolution: \u0026#34; + response.chainOfThought()); Spring AI 的实现优势 # Spring AI 对这些模式的实现提供了几个与 Anthropic 的建议一致的好处：\n模型可移植性 # \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-openai-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 结构化输出 # EvaluationResponse response = chatClient.prompt(prompt) .call() .entity(EvaluationResponse.class); 一致的 API # 不同 LLM 提供商的统一接口 内置错误处理和重试 灵活的提示管理 最佳实践和建议 # 从简单开始 在增加复杂性之前，先从基本工作流程开始 使用满足您要求的最简单模式 仅在需要时添加复杂性 可靠性设计 实现清晰的错误处理 尽可能使用类型安全的响应 在每个步骤中内置验证 考虑权衡 平衡延迟与准确性 评估何时使用并行处理 在固定工作流和动态代理之间进行选择 未来的工作 # 这些指南将进行更新，以探索如何构建更高级的代理，将这些基础模式与复杂的功能相结合： 图案合成\n组合多个模式以创建更强大的工作流程 构建利用每种模式优势的混合系统 创建能够适应不断变化的需求的灵活架构 高级代理内存管理 在对话中实现持久内存 有效管理上下文窗口 制定长期知识保留策略 工具和模型上下文协议 （MCP） 集成 通过标准化接口利用外部工具 实施 MCP 以增强模型交互 构建可扩展的代理架构 结论 # Anthropic 的研究见解与 Spring AI 的实际实现相结合，为构建基于 LLM 的有效系统提供了一个强大的框架。 通过遵循这些模式和原则，开发人员可以创建强大、可维护且有效的 AI 应用程序，这些应用程序可提供真正的价值，同时避免不必要的复杂性。 关键是要记住，有时最简单的解决方案就是最有效的。从基本模式开始，全面了解您的用例，只有在它明显提高系统的性能或功能时才增加复杂性。\n"},{"id":89,"href":"/docs/vector-databases/typesense/","title":"Typesense 字体","section":"矢量数据库","content":" Typesense 字体 # 本节将引导您设置 TypesenseVectorStore 以存储文档嵌入并执行相似性搜索。 [ Typesense]( https://typesense.org) 是一个开源的错别字容忍搜索引擎，它针对低于 50 毫秒的即时搜索进行了优化，同时提供直观的开发人员体验。它提供向量搜索功能，允许您将高维向量与常规搜索数据一起存储和查询。\n先决条件 # 一个正在运行的 Typesense 实例。以下选项可用： Typesense Cloud（推荐） Docker 镜像 typesense/typesense：latest 如果需要，EmbeddingModel 的 API 密钥，用于生成 TypesenseVectorStore 存储的嵌入。 自动配置 # Spring AI 为 Typesense Vector Store 提供了 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-typesense\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-vector-store-typesense\u0026#39; } 请查看 vector store 的[ 配置参数](#_configuration_properties)列表，了解默认值和配置选项。 矢量存储实现可以为您初始化必要的架构，但您必须通过设置 ...initialize-schema=true 在 application.properties 文件中。 此外，您将需要一个配置的 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) bean。请参阅 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) 部分以了解更多信息。 现在，您可以在应用程序中将 TypesenseVectorStore 自动连接为矢量存储：\n@Autowired VectorStore vectorStore; // ... List\u0026lt;Document\u0026gt; documents = List.of( new Document(\u0026#34;Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!\u0026#34;, Map.of(\u0026#34;meta1\u0026#34;, \u0026#34;meta1\u0026#34;)), new Document(\u0026#34;The World is Big and Salvation Lurks Around the Corner\u0026#34;), new Document(\u0026#34;You walk forward facing the past and you turn back toward the future.\u0026#34;, Map.of(\u0026#34;meta2\u0026#34;, \u0026#34;meta2\u0026#34;))); // Add the documents to Typesense vectorStore.add(documents); // Retrieve documents similar to a query List\u0026lt;Document\u0026gt; results = vectorStore.similaritySearch(SearchRequest.builder().query(\u0026#34;Spring\u0026#34;).topK(5).build()); 配置属性 # 要连接到 Typesense 并使用 TypesenseVectorStore，您需要提供实例的访问详细信息。可以通过 Spring Boot 的 application.yml 提供一个简单的配置：\nspring: ai: vectorstore: typesense: initialize-schema: true collection-name: vector_store embedding-dimension: 1536 client: protocol: http host: localhost port: 8108 api-key: xyz 以 开头的属性 spring.ai.vectorstore.typesense.* 用于配置 TypesenseVectorStore：\n手动配置 # 您可以手动配置 Typesense 向量存储，而不是使用 Spring Boot 自动配置。为此，你需要将 spring-ai-typesense-store 添加到你的项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-typesense-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-typesense-store\u0026#39; } 创建一个 Typesense 客户端 Bean：\n@Bean public Client typesenseClient() { List\u0026lt;Node\u0026gt; nodes = new ArrayList\u0026lt;\u0026gt;(); nodes.add(new Node(\u0026#34;http\u0026#34;, \u0026#34;localhost\u0026#34;, \u0026#34;8108\u0026#34;)); Configuration configuration = new Configuration(nodes, Duration.ofSeconds(5), \u0026#34;xyz\u0026#34;); return new Client(configuration); } 然后使用构建器模式创建 TypesenseVectorStore Bean：\n@Bean public VectorStore vectorStore(Client client, EmbeddingModel embeddingModel) { return TypesenseVectorStore.builder(client, embeddingModel) .collectionName(\u0026#34;custom_vectors\u0026#34;) // Optional: defaults to \u0026#34;vector_store\u0026#34; .embeddingDimension(1536) // Optional: defaults to 1536 .initializeSchema(true) // Optional: defaults to false .batchingStrategy(new TokenCountBatchingStrategy()) // Optional: defaults to TokenCountBatchingStrategy .build(); } // This can be any EmbeddingModel implementation @Bean public EmbeddingModel embeddingModel() { return new OpenAiEmbeddingModel(new OpenAiApi(System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;))); } 元数据筛选 # 您也可以将通用可移植[ 元数据过滤器](../vectordbs.html#metadata-filters)与 Typesense store 结合使用。 例如，您可以使用文本表达式语言：\nvectorStore.similaritySearch( SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(\u0026#34;country in [\u0026#39;UK\u0026#39;, \u0026#39;NL\u0026#39;] \u0026amp;\u0026amp; year \u0026gt;= 2020\u0026#34;).build()); 或使用 Filter.Expression DSL 以编程方式：\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(b.and( b.in(\u0026#34;country\u0026#34;, \u0026#34;UK\u0026#34;, \u0026#34;NL\u0026#34;), b.gte(\u0026#34;year\u0026#34;, 2020)).build()).build()); 例如，此可移植筛选条件表达式：\ncountry in [\u0026#39;UK\u0026#39;, \u0026#39;NL\u0026#39;] \u0026amp;\u0026amp; year \u0026gt;= 2020 转换为专有的 Typesense 过滤器格式：\ncountry: [\u0026#39;UK\u0026#39;, \u0026#39;NL\u0026#39;] \u0026amp;\u0026amp; year: \u0026gt;=2020 访问 Native Client # Typesense Vector Store 实现通过 getNativeClient（） 方法提供对底层原生 Typesense 客户端 （Client） 的访问：\nTypesenseVectorStore vectorStore = context.getBean(TypesenseVectorStore.class); Optional\u0026lt;Client\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { Client client = nativeClient.get(); // Use the native client for Typesense-specific operations } 本机客户端允许您访问特定于 Typesense 的功能和作，这些功能和作可能不会通过 VectorStore 接口公开。\n"},{"id":90,"href":"/docs/deploying-to-the-cloud/","title":"云绑定","section":"Docs","content":" 云绑定 # Spring AI 基于 [ spring-cloud-bindings]( https://github.com/spring-cloud/[spring-cloud-bindings](https://github.com/spring-cloud/spring-cloud-bindings)) 中的基础提供对云绑定的支持。这允许应用程序为提供程序指定绑定类型，然后使用泛型格式表示属性。spring-ai 云绑定将处理这些属性并将它们绑定到 spring-ai 本机属性。 例如，使用 OpenAi 时，绑定类型为 openai。使用属性 spring.ai.cloud.bindings.openai.enabled ，可以启用或禁用绑定处理器。默认情况下，在指定绑定类型时，将启用此属性。可以指定 api-key、uri、 用户名 、 密码等的配置，spring-ai 会将它们映射到受支持系统中的相应属性。 要启用云绑定支持，请在应用程序中包括以下依赖项。 或您的 Gradle build.gradle 构建文件。\n可用的 Cloud Bindings # 以下是 spring-ai-spring-clou-bindings 模块中当前提供云绑定支持的组件：\n"},{"id":91,"href":"/docs/vector-databases/weaviate/","title":"维维亚特","section":"矢量数据库","content":" 维维亚特 # 本节将引导您设置 Weaviate VectorStore 以存储文档嵌入并执行相似性搜索。 [ Weaviate]( https://weaviate.io/) 是一个开源矢量数据库，允许您存储来自您最喜欢的 ML 模型的数据对象和矢量嵌入，并无缝扩展到数十亿个数据对象。它提供了用于存储文档嵌入、内容和元数据以及搜索这些嵌入（包括元数据筛选）的工具。\n先决条件 # 一个正在运行的 Weaviate 实例。以下选项可用： Weaviate Cloud Service（需要创建帐户和 API 密钥） Docker 容器 如果需要，EmbeddingModel 的 API 密钥，用于生成由 WeaviateVectorStore 存储的嵌入。 依赖 # 将 Weaviate Vector Store 依赖项添加到您的项目中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-weaviate-store\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-weaviate-store\u0026#39; } 配置 # 要连接到 Weaviate 并使用 WeaviateVectorStore，您需要提供实例的访问详细信息。可以通过 Spring Boot 的 application.properties 提供配置：\nspring.ai.vectorstore.weaviate.host=\u0026lt;host_of_your_weaviate_instance\u0026gt; spring.ai.vectorstore.weaviate.scheme=\u0026lt;http_or_https\u0026gt; spring.ai.vectorstore.weaviate.api-key=\u0026lt;your_api_key\u0026gt; # API key if needed, e.g. OpenAI spring.ai.openai.api-key=\u0026lt;api-key\u0026gt; 如果您更喜欢对 API 密钥等敏感信息使用环境变量，则有多种选择：\n选项 1：使用 Spring 表达式语言 （SpEL） # 您可以使用自定义环境变量名称并在应用程序配置中引用它们：\n# In application.yml spring: ai: vectorstore: weaviate: host: ${WEAVIATE_HOST} scheme: ${WEAVIATE_SCHEME} api-key: ${WEAVIATE_API_KEY} openai: api-key: ${OPENAI_API_KEY} # In your environment or .env file export WEAVIATE_HOST=\u0026lt;host_of_your_weaviate_instance\u0026gt; export WEAVIATE_SCHEME=\u0026lt;http_or_https\u0026gt; export WEAVIATE_API_KEY=\u0026lt;your_api_key\u0026gt; export OPENAI_API_KEY=\u0026lt;api-key\u0026gt; 选项 2：以编程方式访问环境变量 # 或者，您可以在 Java 代码中访问环境变量：\nString weaviateApiKey = System.getenv(\u0026#34;WEAVIATE_API_KEY\u0026#34;); String openAiApiKey = System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;); 自动配置 # Spring AI 为 Weaviate Vector Store 提供了 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-vector-store-weaviate\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-vector-store-weaviate\u0026#39; } 请查看 vector store 的[ 配置参数](#_weaviatevectorstore_properties)列表，了解默认值和配置选项。 此外，您将需要一个已配置的 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) bean。请参阅 [[EmbeddingModel](../embeddings.html#available-implementations)](../embeddings.html#available-implementations) 部分以了解更多信息。 以下是所需 bean 的示例：\n@Bean public EmbeddingModel embeddingModel() { // Retrieve API key from a secure source or environment variable String apiKey = System.getenv(\u0026#34;OPENAI_API_KEY\u0026#34;); // Can be any other EmbeddingModel implementation return new OpenAiEmbeddingModel(OpenAiApi.builder().apiKey(apiKey).build()); } 现在，您可以将 WeaviateVectorStore 自动连接为应用程序中的矢量存储。\n手动配置 # 你可以使用构建器模式手动配置 WeaviateVectorStore，而不是使用 Spring Boot 自动配置：\n@Bean public WeaviateClient weaviateClient() { return new WeaviateClient(new Config(\u0026#34;http\u0026#34;, \u0026#34;localhost:8080\u0026#34;)); } @Bean public VectorStore vectorStore(WeaviateClient weaviateClient, EmbeddingModel embeddingModel) { return WeaviateVectorStore.builder(weaviateClient, embeddingModel) .objectClass(\u0026#34;CustomClass\u0026#34;) // Optional: defaults to \u0026#34;SpringAiWeaviate\u0026#34; .consistencyLevel(ConsistentLevel.QUORUM) // Optional: defaults to ConsistentLevel.ONE .filterMetadataFields(List.of( // Optional: fields that can be used in filters MetadataField.text(\u0026#34;country\u0026#34;), MetadataField.number(\u0026#34;year\u0026#34;))) .build(); } 元数据筛选 # 您也可以在 Weaviate store 中使用通用的可移植[ 元数据过滤器](../vectordbs.html#metadata-filters) 。 例如，您可以使用文本表达式语言：\nvectorStore.similaritySearch( SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(\u0026#34;country in [\u0026#39;UK\u0026#39;, \u0026#39;NL\u0026#39;] \u0026amp;\u0026amp; year \u0026gt;= 2020\u0026#34;).build()); 或使用 Filter.Expression DSL 以编程方式：\nFilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.builder() .query(\u0026#34;The World\u0026#34;) .topK(TOP_K) .similarityThreshold(SIMILARITY_THRESHOLD) .filterExpression(b.and( b.in(\u0026#34;country\u0026#34;, \u0026#34;UK\u0026#34;, \u0026#34;NL\u0026#34;), b.gte(\u0026#34;year\u0026#34;, 2020)).build()).build()); 例如，此可移植筛选条件表达式：\ncountry in [\u0026#39;UK\u0026#39;, \u0026#39;NL\u0026#39;] \u0026amp;\u0026amp; year \u0026gt;= 2020 转换为专有的 Weaviate GraphQL 过滤器格式：\noperator: And operands: [{ operator: Or operands: [{ path: [\u0026#34;meta_country\u0026#34;] operator: Equal valueText: \u0026#34;UK\u0026#34; }, { path: [\u0026#34;meta_country\u0026#34;] operator: Equal valueText: \u0026#34;NL\u0026#34; }] }, { path: [\u0026#34;meta_year\u0026#34;] operator: GreaterThanEqual valueNumber: 2020 }] 在 Docker 中运行 Weaviate # 要快速开始使用本地 Weaviate 实例，您可以在 Docker 中运行它：\ndocker run -it --rm --name weaviate \\ -e AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED=true \\ -e PERSISTENCE_DATA_PATH=/var/lib/weaviate \\ -e QUERY_DEFAULTS_LIMIT=25 \\ -e DEFAULT_VECTORIZER_MODULE=none \\ -e CLUSTER_HOSTNAME=node1 \\ -p 8080:8080 \\ semitechnologies/weaviate:1.22.4 这将启动一个可在 [ localhost：8080](http://localhost:8080) 访问的 Weaviate 实例。\nWeaviateVectorStore 属性 # 您可以在 Spring Boot 配置中使用以下属性来自定义 Weaviate 矢量存储。\n访问 Native Client # Weaviate Vector Store 实现通过 getNativeClient（） 方法提供对底层原生 Weaviate 客户端 （WeaviateClient） 的访问：\nWeaviateVectorStore vectorStore = context.getBean(WeaviateVectorStore.class); Optional\u0026lt;WeaviateClient\u0026gt; nativeClient = vectorStore.getNativeClient(); if (nativeClient.isPresent()) { WeaviateClient client = nativeClient.get(); // Use the native client for Weaviate-specific operations } 本机客户端允许您访问特定于 Weaviate 的功能和作，这些功能和作可能无法通过 VectorStore 接口公开。\n"},{"id":92,"href":"/docs/testcontainers/","title":"测试容器","section":"Docs","content":" 测试容器 # Spring AI 提供了 Spring Boot 自动配置，用于建立与通过 Testcontainers 运行的模型服务或矢量存储的连接。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 文件中： 或您的 Gradle build.gradle 构建文件。\n服务连接 # 该 spring-ai-spring-boot-testcontainers 模块中提供了以下服务连接工厂：\n"},{"id":93,"href":"/docs/cohere/","title":"OCI GenAI Cohere 聊天","section":"Docs","content":" OCI GenAI Cohere 聊天 # [ OCI GenAI 服务]( https://www.oracle.com/artificial-intelligence/generative-ai/generative-ai-service/)提供与按需模型或专用 AI 集群的生成式 AI 聊天。 [ OCI 聊天模型页面]( https://docs.oracle.com/en-us/iaas/Content/generative-ai/chat-models.htm)和 [ OCI 生成式 AI 游乐场]( https://docs.oracle.com/en-us/iaas/Content/generative-ai/use-playground-embed.htm)提供了有关在 OCI 上使用和托管聊天模型的详细信息。\n先决条件 # 您需要一个有效的 [ Oracle Cloud Infrastructure （OCI）]( https://signup.oraclecloud.com/) 账户才能使用 OCI GenAI Cohere Chat 客户端。客户端提供四种不同的连接方式，包括使用用户和私钥进行简单验证、工作负载身份、实例主体或 OCI 配置文件验证。\n添加存储库和 BOM # Spring AI 工件发布在 Maven Central 和 Spring Snapshot 存储库中。请参阅 [ Artifact Repositories](../../../getting-started.html#artifact-repositories) 部分，将这些存储库添加到您的构建系统中。 为了帮助进行[ 依赖项管理](../../../getting-started.html#dependency-management)，Spring AI 提供了一个 BOM（物料清单），以确保在整个项目中使用一致的 Spring AI 版本。请参阅[ 依赖项管理](../../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # Spring AI 为 OCI GenAI Cohere 聊天客户端提供 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-oci-genai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-oci-genai\u0026#39; } 聊天属性 # 连接属性 # 前缀 spring.ai.oci.genai 是用于配置与 OCI GenAI 的连接的属性前缀。\n配置属性 # 前缀 spring.ai.oci.genai.chat.cohere 是为 OCI GenAI Cohere Chat 配置 ChatModel 实施的属性前缀。\n运行时选项 # [ OCICohereChatOptions.java]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-oci-genai/src/main/java/org/springframework/ai/oci/cohere/[OCICohereChatOptions.java](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-oci-genai/src/main/java/org/springframework/ai/oci/cohere/OCICohereChatOptions.java)) 提供模型配置，例如要使用的模型、温度、频率损失等。 启动时，可以使用 OCICohereChatModel(api, options) constructor 或 spring.ai.oci.genai.chat.cohere.options.* properties 配置默认选项。 在运行时，您可以通过向 Prompt 调用添加新的、特定于请求的选项来覆盖默认选项。例如，要覆盖特定请求的默认模型和温度：\nChatResponse response = chatModel.call( new Prompt( \u0026#34;Generate the names of 5 famous pirates.\u0026#34;, OCICohereChatOptions.builder() .model(\u0026#34;my-model-ocid\u0026#34;) .compartment(\u0026#34;my-compartment-ocid\u0026#34;) .temperature(0.5) .build() )); 样品控制器 # [ 创建一个新]( https://start.spring.io/)的 Spring Boot 项目，并将 添加到您的 spring-ai-starter-model-oci-genai pom（或 gradle）依赖项中。 在 src/main/resources 目录下添加 application.properties 文件，以启用和配置 OCI GenAI Cohere 聊天模型：\nspring.ai.oci.genai.authenticationType=file spring.ai.oci.genai.file=/path/to/oci/config/file spring.ai.oci.genai.cohere.chat.options.compartment=my-compartment-ocid spring.ai.oci.genai.cohere.chat.options.servingMode=on-demand spring.ai.oci.genai.cohere.chat.options.model=my-chat-model-ocid 这将创建一个 OCICohereChatModel 实现，您可以将其注入到您的类中。下面是一个简单的 @Controller 类示例，该类使用 chat 模型生成文本。\n@RestController public class ChatController { private final OCICohereChatModel chatModel; @Autowired public ChatController(OCICohereChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(\u0026#34;/ai/generate\u0026#34;) public Map generate(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { return Map.of(\u0026#34;generation\u0026#34;, chatModel.call(message)); } @GetMapping(\u0026#34;/ai/generateStream\u0026#34;) public Flux\u0026lt;ChatResponse\u0026gt; generateStream(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { var prompt = new Prompt(new UserMessage(message)); return chatModel.stream(prompt); } } 手动配置 # OCICohereChatModel 实施 ChatModel 并使用 OCI Java SDK 连接到 OCI GenAI 服务。 将 spring-ai-oci-genai 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-oci-genai\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-oci-genai\u0026#39; } 接下来，创建一个 OCICohereChatModel 并将其用于文本生成：\nvar CONFIG_FILE = Paths.get(System.getProperty(\u0026#34;user.home\u0026#34;), \u0026#34;.oci\u0026#34;, \u0026#34;config\u0026#34;).toString(); var COMPARTMENT_ID = System.getenv(\u0026#34;OCI_COMPARTMENT_ID\u0026#34;); var MODEL_ID = System.getenv(\u0026#34;OCI_CHAT_MODEL_ID\u0026#34;); ConfigFileAuthenticationDetailsProvider authProvider = new ConfigFileAuthenticationDetailsProvider( CONFIG_FILE, \u0026#34;DEFAULT\u0026#34; ); var genAi = GenerativeAiInferenceClient.builder() .region(Region.valueOf(\u0026#34;us-chicago-1\u0026#34;)) .build(authProvider); var chatModel = new OCICohereChatModel(genAi, OCICohereChatOptions.builder() .model(MODEL_ID) .compartment(COMPARTMENT_ID) .servingMode(\u0026#34;on-demand\u0026#34;) .build()); ChatResponse response = chatModel.call( new Prompt(\u0026#34;Generate the names of 5 famous pirates.\u0026#34;)); OCICohereChatOptions 提供聊天请求的配置信息。OCICohereChatOptions.Builder 是 Fluent 选项构建器。\n"},{"id":94,"href":"/docs/text-embedding/","title":"Google VertexAI 文本嵌入","section":"Docs","content":" Google VertexAI 文本嵌入 # Vertex AI 支持两种类型的嵌入：文本模型和多模态。本文档介绍如何使用 Vertex AI [ 文本嵌入 API]( https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/text-embeddings-api) 创建文本嵌入。 Vertex AI 文本嵌入 API 使用密集的矢量表示形式。与倾向于直接将单词映射到数字的稀疏向量不同，密集向量旨在更好地表示一段文本的含义。在生成式 AI 中使用密集向量嵌入的好处是，您可以更好地搜索与查询含义一致的段落，而不是直接搜索单词或语法匹配，即使这些段落没有使用相同的语言。\n先决条件 # 安装适合您作系统的 gcloud CLI。 通过运行以下命令进行身份验证。将 PROJECT_ID 替换为您的 Google Cloud 项目 ID，将 ACCOUNT 替换为您的 Google Cloud 用户名。 gcloud config set project \u0026lt;PROJECT_ID\u0026gt; \u0026amp;\u0026amp; gcloud auth application-default login \u0026lt;ACCOUNT\u0026gt; 添加存储库和 BOM # Spring AI 工件发布在 Maven Central 和 Spring Snapshot 存储库中。请参阅 [ Artifact Repositories](../../getting-started.html#artifact-repositories) 部分，将这些存储库添加到您的构建系统中。 为了帮助进行[ 依赖项管理](../../getting-started.html#dependency-management)，Spring AI 提供了一个 BOM（物料清单），以确保在整个项目中使用一致的 Spring AI 版本。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # Spring AI 为 VertexAI 嵌入模型提供 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-vertex-ai-embedding\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-vertex-ai-embedding\u0026#39; } 嵌入属性 # 前缀 spring.ai.vertex.ai.embedding 用作属性前缀，可让您连接到 VertexAI Embedding API。 prefix spring.ai.vertex.ai.embedding.text 是属性前缀，允许您为 VertexAI 文本嵌入配置嵌入模型实现。\n样品控制器 # [ 创建一个新]( https://start.spring.io/)的 Spring Boot 项目，并将 添加到您的 spring-ai-starter-model-vertex-ai-embedding pom（或 gradle）依赖项中。 在 src/main/resources 目录下添加一个 application.properties 文件，以启用和配置 VertexAi 聊天模型：\nspring.ai.vertex.ai.embedding.project-id=\u0026lt;YOUR_PROJECT_ID\u0026gt; spring.ai.vertex.ai.embedding.location=\u0026lt;YOUR_PROJECT_LOCATION\u0026gt; spring.ai.vertex.ai.embedding.text.options.model=text-embedding-004 这将创建一个 VertexAiTextEmbeddingModel 实现，您可以将其注入到您的类中。下面是一个简单的 @Controller 类示例，该类使用 embedding 模型进行 embedding 生成。\n@RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(\u0026#34;/ai/embedding\u0026#34;) public Map embed(@RequestParam(value = \u0026#34;message\u0026#34;, defaultValue = \u0026#34;Tell me a joke\u0026#34;) String message) { EmbeddingResponse embeddingResponse = this.embeddingModel.embedForResponse(List.of(message)); return Map.of(\u0026#34;embedding\u0026#34;, embeddingResponse); } } 手动配置 # VertexAiTextEmbeddingModel 实现 EmbeddingModel。 将 spring-ai-vertex-ai-embedding 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-vertex-ai-embedding\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-vertex-ai-embedding\u0026#39; } 接下来，创建一个 VertexAiTextEmbeddingModel 并将其用于文本生成：\nVertexAiEmbeddingConnectionDetails connectionDetails = VertexAiEmbeddingConnectionDetails.builder() .projectId(System.getenv(\u0026lt;VERTEX_AI_GEMINI_PROJECT_ID\u0026gt;)) .location(System.getenv(\u0026lt;VERTEX_AI_GEMINI_LOCATION\u0026gt;)) .build(); VertexAiTextEmbeddingOptions options = VertexAiTextEmbeddingOptions.builder() .model(VertexAiTextEmbeddingOptions.DEFAULT_MODEL_NAME) .build(); var embeddingModel = new VertexAiTextEmbeddingModel(this.connectionDetails, this.options); EmbeddingResponse embeddingResponse = this.embeddingModel .embedForResponse(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;)); 从 Google 服务帐户加载凭据 # 要从服务帐户 json 文件以编程方式加载 GoogleCredentials，您可以使用以下内容：\nGoogleCredentials credentials = GoogleCredentials.fromStream(\u0026lt;INPUT_STREAM_TO_CREDENTIALS_JSON\u0026gt;) .createScoped(\u0026#34;https://www.googleapis.com/auth/cloud-platform\u0026#34;); credentials.refreshIfExpired(); VertexAiEmbeddingConnectionDetails connectionDetails = VertexAiEmbeddingConnectionDetails.builder() .projectId(System.getenv(\u0026lt;VERTEX_AI_GEMINI_PROJECT_ID\u0026gt;)) .location(System.getenv(\u0026lt;VERTEX_AI_GEMINI_LOCATION\u0026gt;)) .apiEndpoint(endpoint) .predictionServiceSettings( PredictionServiceSettings.newBuilder() .setEndpoint(endpoint) .setCredentialsProvider(FixedCredentialsProvider.create(credentials)) .build()); "},{"id":95,"href":"/docs/multimodal-embedding/","title":"Google VertexAI 多模态嵌入","section":"Docs","content":" Google VertexAI 多模态嵌入 # Vertex AI 支持两种类型的嵌入：文本模型和多模态。本文档介绍如何使用 Vertex AI [ 多模态嵌入 API]( https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings) 创建多模态嵌入。 多模态嵌入模型根据您提供的输入生成 1408 维向量，其中可以包括图像、文本和视频数据的组合。然后，嵌入向量可用于后续任务，如图像分类或视频内容审核。 图像嵌入向量和文本嵌入向量位于同一语义空间内，具有相同的维度。因此，这些向量可以互换用于逐个文本搜索或逐个搜索视频等使用案例。\n先决条件 # 安装适合您作系统的 gcloud CLI。 通过运行以下命令进行身份验证。将 PROJECT_ID 替换为您的 Google Cloud 项目 ID，将 ACCOUNT 替换为您的 Google Cloud 用户名。 gcloud config set project \u0026lt;PROJECT_ID\u0026gt; \u0026amp;\u0026amp; gcloud auth application-default login \u0026lt;ACCOUNT\u0026gt; 添加存储库和 BOM # Spring AI 工件发布在 Maven Central 和 Spring Snapshot 存储库中。请参阅 [ Artifact Repositories](../../getting-started.html#artifact-repositories) 部分，将这些存储库添加到您的构建系统中。 为了帮助进行[ 依赖项管理](../../getting-started.html#dependency-management)，Spring AI 提供了一个 BOM（物料清单），以确保在整个项目中使用一致的 Spring AI 版本。请参阅[ 依赖项管理](../../getting-started.html#dependency-management)部分，将 Spring AI BOM 添加到您的构建系统中。\n自动配置 # Spring AI 为 VertexAI 嵌入模型提供 Spring Boot 自动配置。要启用它，请将以下依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-starter-model-vertex-ai-embedding\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-starter-model-vertex-ai-embedding\u0026#39; } 嵌入属性 # 前缀 spring.ai.vertex.ai.embedding 用作属性前缀，可让您连接到 VertexAI Embedding API。 prefix spring.ai.vertex.ai.embedding.multimodal 是属性前缀，允许您为 VertexAI 多模态嵌入配置嵌入模型实现。\n手动配置 # [ VertexAiMultimodalEmbeddingModel]( https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-vertex-ai-embedding/src/main/java/org/springframework/ai/vertexai/embedding/[VertexAiMultimodalEmbeddingModel](https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-vertex-ai-embedding/src/main/java/org/springframework/ai/vertexai/embedding/VertexAiMultimodalEmbeddingModel.java).java) 实现 DocumentEmbeddingModel。 将 spring-ai-vertex-ai-embedding 依赖项添加到项目的 Maven pom.xml 文件中：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.ai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-ai-vertex-ai-embedding\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 或您的 Gradle build.gradle 构建文件。\ndependencies { implementation \u0026#39;org.springframework.ai:spring-ai-vertex-ai-embedding\u0026#39; } 接下来，创建一个 VertexAiMultimodalEmbeddingModel 并将其用于 embeddings generations：\nVertexAiEmbeddingConnectionDetails connectionDetails = VertexAiEmbeddingConnectionDetails.builder() .projectId(System.getenv(\u0026lt;VERTEX_AI_GEMINI_PROJECT_ID\u0026gt;)) .location(System.getenv(\u0026lt;VERTEX_AI_GEMINI_LOCATION\u0026gt;)) .build(); VertexAiMultimodalEmbeddingOptions options = VertexAiMultimodalEmbeddingOptions.builder() .model(VertexAiMultimodalEmbeddingOptions.DEFAULT_MODEL_NAME) .build(); var embeddingModel = new VertexAiMultimodalEmbeddingModel(this.connectionDetails, this.options); Media imageMedial = new Media(MimeTypeUtils.IMAGE_PNG, new ClassPathResource(\u0026#34;/test.image.png\u0026#34;)); Media videoMedial = new Media(new MimeType(\u0026#34;video\u0026#34;, \u0026#34;mp4\u0026#34;), new ClassPathResource(\u0026#34;/test.video.mp4\u0026#34;)); var document = new Document(\u0026#34;Explain what do you see on this video?\u0026#34;, List.of(this.imageMedial, this.videoMedial), Map.of()); EmbeddingResponse embeddingResponse = this.embeddingModel .embedForResponse(List.of(\u0026#34;Hello World\u0026#34;, \u0026#34;World is big and salvation is near\u0026#34;)); DocumentEmbeddingRequest embeddingRequest = new DocumentEmbeddingRequest(List.of(this.document), EmbeddingOptions.EMPTY); EmbeddingResponse embeddingResponse = multiModelEmbeddingModel.call(this.embeddingRequest); assertThat(embeddingResponse.getResults()).hasSize(3); "}]